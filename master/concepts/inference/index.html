<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../torch-component/ rel=prev><link href=../../utilities/ rel=next><link rel=icon href=../../assets/logo/edsnlp.svg><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.2.8"><title>Inference - EDS-NLP</title><link rel=stylesheet href=../../assets/stylesheets/main.046329b4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.85d0ee34.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><link rel=stylesheet href=../../assets/stylesheets/extra.css><link rel=stylesheet href=../../assets/stylesheets/cards.css><link rel=stylesheet href=../../assets/termynal/termynal.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href="#inference" class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href="../.." title=EDS-NLP class="md-header__button md-logo" aria-label=EDS-NLP data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> EDS-NLP </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Inference </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href="https://github.com/aphp/edsnlp" title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> aphp/edsnlp </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href="../.." title=EDS-NLP class="md-nav__button md-logo" aria-label=EDS-NLP data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> EDS-NLP </label> <div class=md-nav__source> <a href="https://github.com/aphp/edsnlp" title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> aphp/edsnlp </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href="../.." class=md-nav__link> <span class=md-ellipsis> Getting started </span> </a> </li> <li class=md-nav__item> <a href="https://aphp.github.io/edsnlp/demo" target=_blank class=md-nav__link> <span class=md-ellipsis> Demo </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href="../../tutorials/" class=md-nav__link> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href="../../pipes/" class=md-nav__link> <span class=md-ellipsis> Pipes </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href="../../tokenizers/" class=md-nav__link> <span class=md-ellipsis> Tokenizers </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href="../../data/" class=md-nav__link> <span class=md-ellipsis> Data Connectors </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7 checked> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex=0> <span class=md-ellipsis> Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=true> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href="../pipeline/" class=md-nav__link> <span class=md-ellipsis> Pipeline </span> </a> </li> <li class=md-nav__item> <a href="../torch-component/" class=md-nav__link> <span class=md-ellipsis> Torch Component </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Inference </span> <span class="md-nav__icon md-icon"></span> </label> <a href="./" class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Inference </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href="#inference-on-a-single-document" class=md-nav__link> Inference on a single document </a> </li> <li class=md-nav__item> <a href="#edsnlp.core.lazy_collection.LazyCollection" class=md-nav__link> Inference on multiple documents </a> <nav class=md-nav aria-label="Inference on multiple documents"> <ul class=md-nav__list> <li class=md-nav__item> <a href="#lazy-collection" class=md-nav__link> Lazy collection </a> </li> <li class=md-nav__item> <a href="#applying-operations-to-a-lazy-collection" class=md-nav__link> Applying operations to a lazy collection </a> </li> <li class=md-nav__item> <a href="#edsnlp.core.lazy_collection.LazyCollection.set_processing" class=md-nav__link> Execution of a lazy collection </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href="#backends" class=md-nav__link> Backends </a> <nav class=md-nav aria-label=Backends> <ul class=md-nav__list> <li class=md-nav__item> <a href="#edsnlp.processing.simple.execute_simple_backend" class=md-nav__link> Simple backend </a> </li> <li class=md-nav__item> <a href="#edsnlp.processing.multiprocessing.execute_multiprocessing_backend" class=md-nav__link> Multiprocessing backend </a> </li> <li class=md-nav__item> <a href="#edsnlp.processing.spark.execute_spark_backend" class=md-nav__link> Spark backend </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href="../../utilities/" class=md-nav__link> <span class=md-ellipsis> Utilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href="../../reference/edsnlp/" class=md-nav__link> <span class=md-ellipsis> Code Reference </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href="../../contributing/" class=md-nav__link> <span class=md-ellipsis> Contributing to EDS-NLP </span> </a> </li> <li class=md-nav__item> <a href="../../changelog/" class=md-nav__link> <span class=md-ellipsis> Changelog </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href="#inference-on-a-single-document" class=md-nav__link> Inference on a single document </a> </li> <li class=md-nav__item> <a href="#edsnlp.core.lazy_collection.LazyCollection" class=md-nav__link> Inference on multiple documents </a> <nav class=md-nav aria-label="Inference on multiple documents"> <ul class=md-nav__list> <li class=md-nav__item> <a href="#lazy-collection" class=md-nav__link> Lazy collection </a> </li> <li class=md-nav__item> <a href="#applying-operations-to-a-lazy-collection" class=md-nav__link> Applying operations to a lazy collection </a> </li> <li class=md-nav__item> <a href="#edsnlp.core.lazy_collection.LazyCollection.set_processing" class=md-nav__link> Execution of a lazy collection </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href="#backends" class=md-nav__link> Backends </a> <nav class=md-nav aria-label=Backends> <ul class=md-nav__list> <li class=md-nav__item> <a href="#edsnlp.processing.simple.execute_simple_backend" class=md-nav__link> Simple backend </a> </li> <li class=md-nav__item> <a href="#edsnlp.processing.multiprocessing.execute_multiprocessing_backend" class=md-nav__link> Multiprocessing backend </a> </li> <li class=md-nav__item> <a href="#edsnlp.processing.spark.execute_spark_backend" class=md-nav__link> Spark backend </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=inference>Inference</h1> <p>Once you have obtained a pipeline, either by composing rule-based components, training a model or loading a model from the disk, you can use it to make predictions on documents. This is referred to as inference. This page answers the following questions :</p> <blockquote> <p>How do we leverage computational resources run a model on many documents?</p> <p>How do we connect to various data sources to retrieve documents?</p> </blockquote> <p>Be sure to check out the <a href="../../tutorials/multiple-texts">Processing multiple texts</a> tutorial for a practical example of how to use EDS-NLP to process large datasets.</p> <h2 id=inference-on-a-single-document>Inference on a single document</h2> <p>In EDS-NLP, computing the prediction on a single document is done by calling the pipeline on the document. The input can be either:</p> <ul> <li>a text string</li> <li>or a <a href="https://spacy.io/api/doc">Doc</a> object</li> </ul> <div class="no-check highlight"><pre><span></span><code><span class=kn>from</span> <span class=nn>pathlib</span> <span class=kn>import</span> <span class=n>Path</span>

<span class=n>nlp</span> <span class=o>=</span> <span class=o>...</span>
<span class=n>text</span> <span class=o>=</span> <span class=s2>&quot;... my text ...&quot;</span>
<span class=n>doc</span> <span class=o>=</span> <span class=n>nlp</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</code></pre></div> <p>If you're lucky enough to have a GPU, you can use it to speed up inference by moving the model to the GPU before calling the pipeline.</p> <div class="no-check highlight"><pre><span></span><code><span class=n>nlp</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&quot;cuda&quot;</span><span class=p>)</span>  <span class=c1># same semantics as pytorch</span>
<span class=n>doc</span> <span class=o>=</span> <span class=n>nlp</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</code></pre></div> <p>To leverage multiple GPUs when processing multiple documents, refer to the <a class="autorefs autorefs-internal" href="#edsnlp.processing.multiprocessing.execute_multiprocessing_backend">multiprocessing backend</a> description below.</p> <h2 id=edsnlp.core.lazy_collection.LazyCollection>Inference on multiple documents</h2> <p>When processing multiple documents, we can optimize the inference by parallelizing the computation on a single core, multiple cores and GPUs or even multiple machines.</p> <h3 id=lazy-collection>Lazy collection</h3> <p>These optimizations are enabled by performing <em>lazy inference</em> : the operations (e.g., reading a document, converting it to a Doc, running the different pipes of a model or writing the result somewhere) are not executed immediately but are instead scheduled in a <a class="autorefs autorefs-internal" href="#edsnlp.core.lazy_collection.LazyCollection">LazyCollection</a> object. It can then be executed by calling the <code>execute</code> method, iterating over it or calling a writing method (e.g., <code>to_pandas</code>). In fact, data connectors like <code>edsnlp.data.read_json</code> return a lazy collection, as well as the <code>nlp.pipe</code> method.</p> <p>A lazy collection contains :</p> <ul> <li>a <code>reader</code>: the source of the data (e.g., a file, a database, a list of strings, etc.)</li> <li>the list of operations to perform under a <code>pipeline</code> attribute containing the name if any, function / pipe, keyword arguments and context for each operation</li> <li>an optional <code>writer</code>: the destination of the data (e.g., a file, a database, a list of strings, etc.)</li> <li>the execution <code>config</code>, containing the backend to use and its configuration such as the number of workers, the batch size, etc.</li> </ul> <p>All methods (<code>.map</code>, <code>.map_pipeline</code>, <code>.set_processing</code>) of the lazy collection are chainable, meaning that they return a new object (no in-place modification).</p> <p>For instance, the following code will load a model, read a folder of JSON files, apply the model to each document and write the result in a Parquet folder, using 4 CPUs and 2 GPUs.</p> <div class="no-check highlight"><pre><span></span><code><span class=kn>import</span> <span class=nn>edsnlp</span>

<span class=c1># Load or create a model</span>
<span class=n>nlp</span> <span class=o>=</span> <span class=n>edsnlp</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&quot;path/to/model&quot;</span><span class=p>)</span>

<span class=c1># Read some data (this is lazy, no data will be read until the end of of this snippet)</span>
<span class=n>data</span> <span class=o>=</span> <span class=n>edsnlp</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>read_json</span><span class=p>(</span><span class=s2>&quot;path/to/json_folder&quot;</span><span class=p>,</span> <span class=n>converter</span><span class=o>=</span><span class=s2>&quot;...&quot;</span><span class=p>)</span>

<span class=c1># Apply each pipe of the model to our documents</span>
<span class=n>data</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>map_pipeline</span><span class=p>(</span><span class=n>nlp</span><span class=p>)</span>
<span class=c1># or equivalently : data = nlp.pipe(data)</span>

<span class=c1># Configure the execution</span>
<span class=n>data</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>set_processing</span><span class=p>(</span>
    <span class=c1># 4 CPUs to parallelize rule-based pipes, IO and preprocessing</span>
    <span class=n>num_cpu_workers</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span>
    <span class=c1># 2 GPUs to accelerate deep-learning pipes</span>
    <span class=n>num_gpu_workers</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
<span class=p>)</span>

<span class=c1># Write the result, this will execute the lazy collection</span>
<span class=n>data</span><span class=o>.</span><span class=n>write_parquet</span><span class=p>(</span><span class=s2>&quot;path/to/output_folder&quot;</span><span class=p>,</span> <span class=n>converter</span><span class=o>=</span><span class=s2>&quot;...&quot;</span><span class=p>,</span> <span class=n>write_in_worker</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <h3 id=applying-operations-to-a-lazy-collection>Applying operations to a lazy collection</h3> <p>To apply an operation to a lazy collection, you can use the <code>.map</code> method. It takes a callable as input and an optional dictionary of keyword arguments. The function will be applied to each element of the collection.</p> <p>To apply a model, you can use the <code>.map_pipeline</code> method. It takes a model as input and will add every pipe of the model to the scheduled operations.</p> <p>In both cases, the operations will not be executed immediately but will be scheduled to be executed when iterating of the collection, or calling the <code>.execute</code>, <code>.to_*</code> or <code>.write_*</code> methods.</p> <h3 id=edsnlp.core.lazy_collection.LazyCollection.set_processing>Execution of a lazy collection</h3> <p>You can configure how the operations performed in the lazy collection is executed by calling its <code>set_processing(...)</code> method. The following options are available :</p> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <table> <thead> <tr> <th><b>PARAMETER</b></th> <th><b>DESCRIPTION</b></th> </tr> </thead> <tbody> <tr> <td><code>batch_size</code></td> <td class=doc-param-details> <p>Number of documents to process at a time in a CPU/GPU worker (or in the main process if no workers are used).</p> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code>int</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> <tr> <td><code>num_cpu_workers</code></td> <td class=doc-param-details> <p>Number of CPU workers. A CPU worker handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components. If no GPU workers are used, the CPU workers also handle the forward call of the deep-learning components.</p> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code>int</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> <tr> <td><code>num_gpu_workers</code></td> <td class=doc-param-details> <p>Number of GPU workers. A GPU worker handles the forward call of the deep-learning components. Only used with "multiprocessing" backend.</p> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code>int</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> <tr> <td><code>gpu_pipe_names</code></td> <td class=doc-param-details> <p>List of pipe names to accelerate on a GPUWorker, defaults to all pipes that inherit from TorchComponent. Only used with "multiprocessing" backend.</p> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title="typing.List">List</span>[str]</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> <tr> <td><code>backend</code></td> <td class=doc-param-details> <p>The backend to use for parallel processing. If not set, the backend is automatically selected based on the input data and the number of workers.</p> <ul> <li>"simple" is the default backend and is used when <code>num_cpu_workers</code> is 1 and <code>num_gpu_workers</code> is 0.</li> <li>"multiprocessing" is used when <code>num_cpu_workers</code> is greater than 1 or <code>num_gpu_workers</code> is greater than 0.</li> <li>"spark" is used when the input data is a Spark dataframe and the output writer is a Spark writer.</li> </ul> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title="typing_extensions.Literal">Literal</span>[&#39;simple&#39;, &#39;multiprocessing&#39;, &#39;spark&#39;]</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> <tr> <td><code>show_progress</code></td> <td class=doc-param-details> <p>Whether to show progress bars (only applicable with "simple" and "multiprocessing" backends).</p> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code>bool</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> <tr> <td><code>process_start_method</code></td> <td class=doc-param-details> <p>Whether to use "fork" or "spawn" as the start method for the multiprocessing backend.</p> <ul> <li>"fork" is the default start method on Unix systems and is the fastest start method, but it is not available on Windows, can cause issues with CUDA and is not safe when using multiple threads.</li> <li>"spawn" is the default start method on Windows and is the safest start method, but it is not available on Unix systems and is slower than "fork".</li> </ul> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code>bool</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> <tr> <td><code>gpu_worker_devices</code></td> <td class=doc-param-details> <p>List of GPU devices to use for the GPU workers. Defaults to all available devices, one worker per device. Only used with "multiprocessing" backend.</p> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title="typing.List">List</span>[str]</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> <tr> <td><code>cpu_worker_devices</code></td> <td class=doc-param-details> <p>List of GPU devices to use for the CPU workers. Used for debugging purposes.</p> <p> <span class=doc-param-annotation> <b>TYPE:</b> <code><span title="typing.List">List</span>[str]</code> </span> <span class=doc-param-default> <b>DEFAULT:</b> <code><span title="edsnlp.core.lazy_collection.INFER">INFER</span></code> </span> </p> </td> </tr> </tbody> </table> </div> </div><h2 id=backends>Backends</h2> <h3 id=edsnlp.processing.simple.execute_simple_backend>Simple backend</h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>This is the default execution mode which batches the documents and processes each batch on the current process in a sequential manner.</p> </div> </div><h3 id=edsnlp.processing.multiprocessing.execute_multiprocessing_backend>Multiprocessing backend</h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>If you have multiple CPU cores, and optionally multiple GPUs, we provide the <code>multiprocessing</code> backend that allows to run the inference on multiple processes.</p> <p>This accelerator dispatches the batches between multiple workers (data-parallelism), and distribute the computation of a given batch on one or two workers (model-parallelism):</p> <ul> <li>a <code>CPUWorker</code> which handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components</li> <li>a <code>GPUWorker</code> which handles the forward call of the deep-learning components</li> </ul> <p>If no GPU is available, no <code>GPUWorker</code> is started, and the <code>CPUWorkers</code> handle the forward call of the deep-learning components as well.</p> <p>The advantage of dedicating a worker to the deep-learning components is that it allows to prepare multiple batches in parallel in multiple <code>CPUWorker</code>, and ensure that the <code>GPUWorker</code> never wait for a batch to be ready.</p> <p>The overall architecture described in the following figure, for 3 CPU workers and 2 GPU workers.</p> <div style=text-align:center> <img src="../../assets/images/multiprocessing.png" style=height:400px> </div> <p>Here is how a small pipeline with rule-based components and deep-learning components is distributed between the workers:</p> <div style=text-align:center> <img src="../../assets/images/model-parallelism.png"> </div> </div> </div><h3 id=edsnlp.processing.spark.execute_spark_backend>Spark backend</h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>This execution mode uses Spark to parallelize the processing of the documents. The documents are first stored in a Spark DataFrame (if it was not already the case) and then processed in parallel using Spark.</p> <p>Beware, if the original reader was not a SparkReader (<code>edsnlp.data.from_spark</code>), the <em>local docs</em> → <em>spark dataframe</em> conversion might take some time, and the whole process might be slower than using the <code>multiprocessing</code> backend.</p> </div> </div> <div class=footnote><hr><ol/></div> </article> </div> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href="../torch-component/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Torch Component" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Torch Component </div> </div> </a> <a href="../../utilities/" class="md-footer__link md-footer__link--next" aria-label="Next: Overview" rel=next> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Overview </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href="https://squidfunk.github.io/mkdocs-material/" target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["navigation.tracking", "navigation.instant", "navigation.indexes", "navigation.prune", "navigation.top", "navigation.footer", "content.code.annotate", "content.code.copy"], "search": "../../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src=../../assets/javascripts/bundle.dff1b7c8.min.js></script> <script src=https://cdn.jsdelivr.net/npm/vega@5></script> <script src=https://cdn.jsdelivr.net/npm/vega-lite@5></script> <script src=https://cdn.jsdelivr.net/npm/vega-embed@6></script> <script src=../../assets/termynal/termynal.js></script> </body> </html>