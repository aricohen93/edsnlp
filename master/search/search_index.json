{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>EDS-NLP is a collaborative NLP framework that aims at extracting information from French clinical notes. At its core, it is a collection of components or pipes, either rule-based functions or deep learning modules. These components are organized into a novel efficient and modular pipeline system, built for hybrid and multitask models. We use spaCy to represent documents and their annotations, and Pytorch as a deep-learning backend for trainable components.</p> <p>EDS-NLP is versatile and can be used on any textual document. The rule-based components are fully compatible with spaCy's pipelines, and vice versa. This library is a product of collaborative effort, and we encourage further contributions to enhance its capabilities.</p> <p>Check out our interactive demo !</p>"},{"location":"#quick-start","title":"Quick start","text":""},{"location":"#installation","title":"Installation","text":"<p>You can install EDS-NLP via <code>pip</code>. We recommend pinning the library version in your projects, or use a strict package manager like Poetry.</p> <pre><code>pip install edsnlp==0.12.3\n</code></pre> <p>or if you want to use the trainable components (using pytorch)</p> <pre><code>pip install \"edsnlp[ml]==0.12.3\"\n</code></pre>"},{"location":"#a-first-pipeline","title":"A first pipeline","text":"<p>Once you've installed the library, let's begin with a very simple example that extracts mentions of COVID19 in a text, and detects whether they are negated.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")  # (1)\n\nterms = dict(\n    covid=[\"covid\", \"coronavirus\"],  # (2)\n)\n\n# Sentencizer component, needed for negation detection\nnlp.add_pipe(eds.sentences())  # (3)\n# Matcher component\nnlp.add_pipe(eds.matcher(terms=terms))  # (4)\n# Negation detection\nnlp.add_pipe(eds.negation())\n\n# Process your text in one call !\ndoc = nlp(\"Le patient n'est pas atteint de covid\")\n\ndoc.ents  # (5)\n# Out: (covid,)\n\ndoc.ents[0]._.negation  # (6)\n# Out: True\n</code></pre> <ol> <li>'eds' is the name of the language, which defines the tokenizer.</li> <li>This example terminology provides a very simple, and by no means exhaustive, list of synonyms for COVID19.</li> <li>Similarly to spaCy, pipes are added via the <code>nlp.add_pipe</code> method.</li> <li>See the matching tutorial for mode details.</li> <li>spaCy stores extracted entities in the <code>Doc.ents</code> attribute.</li> <li>The <code>eds.negation</code> component has adds a <code>negation</code> custom attribute.</li> </ol> <p>This example is complete, it should run as-is.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>To learn more about EDS-NLP, we have prepared a series of tutorials that should cover the main features of the library.</p> <p> Spacy representations</p><p>Learn the basics of how documents are represented with spaCy.</p><p> Matching a terminology</p><p>Extract phrases that belong to a given terminology.</p><p> Qualifying entities</p><p>Ensure extracted concepts are not invalidated by linguistic modulation.</p><p> Detecting dates</p><p>Detect and parse dates in a text.</p><p> Processing multiple texts</p><p>Improve the inference speed of your pipeline</p><p> Detecting hospitalisation reason</p><p>Identify spans mentioning the reason for hospitalisation or tag entities as the reason.</p><p>\u21b5 Detecting false endlines</p><p>Classify each line end and add the <code>excluded</code> attribute to these tokens.</p><p> Aggregating results</p><p>Aggregate the results of your pipeline at the document level.</p><p> FastAPI</p><p>Deploy your pipeline as an API.</p><p> Make a training script</p><p>Learn how to train a NER pipeline with EDS-NLP.</p>"},{"location":"#available-pipeline-components","title":"Available pipeline components","text":"CoreQualifiersMiscellaneousNERTrainable <p>See the Core components overview for more information.</p> Component Description <code>eds.normalizer</code> Non-destructive input text normalisation <code>eds.sentences</code> Better sentence boundary detection <code>eds.matcher</code> A simple yet powerful entity extractor <code>eds.terminology</code> A simple yet powerful terminology matcher <code>eds.contextual_matcher</code> A conditional entity extractor <code>eds.endlines</code> An unsupervised model to classify each end line <p>See the Qualifiers overview for more information.</p> Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based medical history detection <p>See the Miscellaneous components overview for more information.</p> Component Description <code>eds.dates</code> Date extraction and normalisation <code>eds.consultation_dates</code> Identify consultation dates <code>eds.measurements</code> Measure extraction and normalisation <code>eds.sections</code> Section detection <code>eds.reason</code> Rule-based hospitalisation reason detection <code>eds.tables</code> Tables detection <p>See the NER overview for more information.</p> Component Description <code>eds.covid</code> A COVID mentions detector <code>eds.charlson</code> A Charlson score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.elston_ellis</code> An Elston &amp; Ellis code extractor <code>eds.emergency_priority</code> A priority score extractor <code>eds.emergency_ccmu</code> A CCMU score extractor <code>eds.emergency_gemsa</code> A GEMSA score extractor <code>eds.tnm</code> A TNM score extractor <code>eds.adicap</code> A ADICAP codes extractor <code>eds.drugs</code> A drug mentions extractor <code>eds.cim10</code> A CIM10 terminology matcher <code>eds.umls</code> An UMLS terminology matcher <code>eds.ckd</code> CKD extractor <code>eds.copd</code> COPD extractor <code>eds.cerebrovascular_accident</code> Cerebrovascular accident extractor <code>eds.congestive_heart_failure</code> Congestive heart failure extractor <code>eds.connective_tissue_disease</code> Connective tissue disease extractor <code>eds.dementia</code> Dementia extractor <code>eds.diabetes</code> Diabetes extractor <code>eds.hemiplegia</code> Hemiplegia extractor <code>eds.leukemia</code> Leukemia extractor <code>eds.liver_disease</code> Liver disease extractor <code>eds.lymphoma</code> Lymphoma extractor <code>eds.myocardial_infarction</code> Myocardial infarction extractor <code>eds.peptic_ulcer_disease</code> Peptic ulcer disease extractor <code>eds.peripheral_vascular_disease</code> Peripheral vascular disease extractor <code>eds.solid_tumor</code> Solid tumor extractor <code>eds.alcohol</code> Alcohol consumption extractor <code>eds.tobacco</code> Tobacco consumption extractor <p>See the Trainable components overview for more information.</p> Name Description <code>eds.transformer</code> Embed text with a transformer model <code>eds.text_cnn</code> Contextualize embeddings with a CNN <code>eds.span_pooler</code> A span embedding component that aggregates word embeddings <code>eds.ner_crf</code> A trainable component to extract entities <code>eds.span_classifier</code> A trainable component for multi-class multi-label span classification <code>eds.span_linker</code> A trainable entity linker (i.e. to a list of concepts)"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>The performances of an extraction pipeline may depend on the population and documents that are considered.</p>"},{"location":"#contributing-to-eds-nlp","title":"Contributing to EDS-NLP","text":"<p>We welcome contributions ! Fork the project and propose a pull request. Take a look at the dedicated page for detail.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use EDS-NLP, please cite us as below.</p> <pre><code>@misc{edsnlp,\nauthor = {Wajsburt, Perceval and Petit-Jean, Thomas and Dura, Basile and Cohen, Ariel and Jean, Charline and Bey, Romain},\ndoi    = {10.5281/zenodo.6424993},\ntitle  = {EDS-NLP: efficient information extraction from French clinical notes},\nurl    = {https://aphp.github.io/edsnlp}\n}\n</code></pre>"},{"location":"tokenizers/","title":"Tokenizers","text":"<p>In addition to the standard spaCy <code>FrenchLanguage</code> (<code>fr</code>), EDS-NLP offers a new language better fit for French clinical documents: <code>EDSLanguage</code> (<code>eds</code>). Additionally, the <code>EDSLanguage</code> document creation should be around 5-6 times faster than the <code>fr</code> language. The main differences lie in the tokenization process.</p> <p>A comparison of the two tokenization methods is demonstrated below:</p> Example FrenchLanguage EDSLanguage <code>ACR5</code> [<code>ACR5</code>] [<code>ACR</code>, <code>5</code>] <code>26.5/</code> [<code>26.5/</code>] [<code>26.5</code>, <code>/</code>] <code>\\n \\n CONCLUSION</code> [<code>\\n \\n</code>, <code>CONCLUSION</code>] [<code>\\n</code>, <code>\\n</code>, <code>CONCLUSION</code>] <code>l'art\u00e8re</code> [<code>l'</code>, <code>art\u00e8re</code>] [<code>l'</code>, <code>art\u00e8re</code>] (same) <code>Dr. Pichon</code> [<code>Dr</code>, <code>.</code>, <code>Pichon</code>] [<code>Dr.</code>, <code>Pichon</code>] <code>B.H.HP.A.7.A</code> [<code>B.H.HP.A.7.A</code>] [<code>B.</code>, <code>H.</code>, <code>HP.</code>, <code>A</code>, <code>7</code>, <code>A</code>, <code>0</code>] <p>To instantiate one of the two languages, you can call the <code>spacy.blank</code> method.</p> EDSLanguageFrenchLanguage <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\n</code></pre> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"fr\")\n</code></pre>"},{"location":"advanced-tutorials/","title":"Advanced use cases","text":"<p>In this section, we review a few advanced use cases:</p> <ul> <li>Adding pre-computed word vectors to spaCy</li> <li>Deploying your spaCy pipeline as an API</li> <li>Creating your own component</li> </ul>"},{"location":"advanced-tutorials/fastapi/","title":"Deploying as an API","text":"<p>In this section, we will see how you can deploy your pipeline as a REST API using the power of FastAPI.</p>"},{"location":"advanced-tutorials/fastapi/#the-nlp-pipeline","title":"The NLP pipeline","text":"<p>Let's create a simple NLP model, that can:</p> <ul> <li>match synonyms of COVID19</li> <li>check for negation, speculation and reported speech.</li> </ul> <p>You know the drill:</p> pipeline.py<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank('eds')\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n   eds.matcher(\n    regex=dict(\n        covid=[\n            \"covid\",\n            r\"covid[-\\s]?19\",\n            r\"sars[-\\s]?cov[-\\s]?2\",\n            r\"corona[-\\s]?virus\",\n        ],\n    ),\n    attr=\"LOWER\",\n   ),\n)\nnlp.add_pipe(eds.negation())\nnlp.add_pipe(eds.family())\nnlp.add_pipe(eds.hypothesis())\nnlp.add_pipe(eds.rspeech())\n</code></pre>"},{"location":"advanced-tutorials/fastapi/#creating-the-fastapi-app","title":"Creating the FastAPI app","text":"<p>FastAPI is a incredibly efficient framework, based on Python type hints from the ground up, with the help of Pydantic (another great library for building modern Python). We won't go into too much detail about FastAPI in this tutorial. For further information on how the framework operates, go to its excellent documentation!</p> <p>We'll need to create two things:</p> <ol> <li>A module containing the models for inputs and outputs.</li> <li>The script that defines the application itself.</li> </ol> models.py<pre><code>from typing import List\n\nfrom pydantic import BaseModel\n\n\nclass Entity(BaseModel):  # (1)\n\n    # OMOP-style attributes\n    start: int\n    end: int\n    label: str\n    lexical_variant: str\n    normalized_variant: str\n\n    # Qualifiers\n    negated: bool\n    hypothesis: bool\n    family: bool\n    reported_speech: bool\n\n\nclass Document(BaseModel):  # (2)\n    text: str\n    ents: List[Entity]\n</code></pre> <ol> <li>The <code>Entity</code> model contains attributes that define a matched entity, as well as variables that contain the output of the qualifier components.</li> <li>The <code>Document</code> model contains the input text, and a list of detected entities</li> </ol> <p>Having defined the output models and the pipeline, we can move on to creating the application itself:</p> app.py<pre><code>from typing import List\n\nfrom fastapi import FastAPI\n\nfrom pipeline import nlp\nfrom models import Entity, Document\n\n\napp = FastAPI(title=\"EDS-NLP\", version=edsnlp.__version__)\n\n\n@app.post(\"/covid\", response_model=List[Document])  # (1)\nasync def process(\n    notes: List[str],  # (2)\n):\n\n    documents = []\n\n    for doc in nlp.pipe(notes):\n        entities = []\n\n        for ent in doc.ents:\n            entity = Entity(\n                start=ent.start_char,\n                end=ent.end_char,\n                label=ent.label_,\n                lexical_variant=ent.text,\n                normalized_variant=ent._.normalized_variant,\n                negated=ent._.negation,\n                hypothesis=ent._.hypothesis,\n                family=ent._.family,\n                reported_speech=ent._.reported_speech,\n            )\n            entities.append(entity)\n\n        documents.append(\n            Document(\n                text=doc.text,\n                ents=entities,\n            )\n        )\n\n    return documents\n</code></pre> <ol> <li>By telling FastAPI what output format is expected, you get automatic data validation.</li> <li>In FastAPI, input and output schemas are defined through Python type hinting.    Here, we tell FastAPI to expect a list of strings in the <code>POST</code> request body.    As a bonus, you get data validation for free.</li> </ol>"},{"location":"advanced-tutorials/fastapi/#running-the-api","title":"Running the API","text":"<p>Our simple API is ready to launch! We'll just need to install FastAPI along with a ASGI server to run it. This can be done in one go:</p> <pre><code>$ pip install 'fastapi[uvicorn]'\n---&gt; 100%\ncolor:green Successfully installed fastapi\n</code></pre> <p>Launching the API is trivial:</p> <pre><code>$ uvicorn app:app --reload\n</code></pre> <p>Go to <code>localhost:8000/docs</code> to admire the automatically generated documentation!</p>"},{"location":"advanced-tutorials/fastapi/#using-the-api","title":"Using the API","text":"<p>You can try the API directly from the documentation. Otherwise, you may use the <code>requests</code> package:</p> <pre><code>import requests\n\nnotes = [\n    \"Le p\u00e8re du patient n'est pas atteint de la covid.\",\n    \"Probable coronavirus.\",\n]\n\nr = requests.post(\n    \"http://localhost:8000/covid\",\n    json=notes,\n)\n\nr.json()\n</code></pre> <p>You should get something like:</p> <pre><code>[\n{\n\"text\": \"Le p\u00e8re du patient n'est pas atteint de la covid.\",\n\"ents\": [\n{\n\"start\": 43,\n\"end\": 48,\n\"label\": \"covid\",\n\"lexical_variant\": \"covid\",\n\"normalized_variant\": \"covid\",\n\"negated\": true,\n\"hypothesis\": false,\n\"family\": true,\n\"reported_speech\": false\n}\n]\n},\n{\n\"text\": \"Probable coronavirus.\",\n\"ents\": [\n{\n\"start\": 9,\n\"end\": 20,\n\"label\": \"covid\",\n\"lexical_variant\": \"coronavirus\",\n\"normalized_variant\": \"coronavirus\",\n\"negated\": false,\n\"hypothesis\": true,\n\"family\": false,\n\"reported_speech\": false\n}\n]\n}\n]\n</code></pre>"},{"location":"advanced-tutorials/word-vectors/","title":"Word embeddings","text":"<p>The only ready-to-use components in EDS-NLP are rule-based components. However, that does not prohibit you from exploiting spaCy's machine learning capabilities! You can mix and match machine learning pipelines, trainable or not, with EDS-NLP rule-based components.</p> <p>In this tutorial, we will explore how you can use static word vectors trained with Gensim within spaCy.</p> <p>Training the word embedding, however, is outside the scope of this post. You'll find very well designed resources on the subject in Gensim's documenation.</p> <p>Using Transformer models</p> <p>spaCy v3 introduced support for Transformer models through their helper library <code>spacy-transformers</code> that interfaces with HuggingFace's <code>transformers</code> library.</p> <p>Using transformer models can significantly increase your model's performance.</p>"},{"location":"advanced-tutorials/word-vectors/#adding-pre-trained-word-vectors","title":"Adding pre-trained word vectors","text":"<p>spaCy provides a <code>init vectors</code> CLI utility that takes a Gensim-trained binary and transforms it to a spaCy-readable pipeline.</p> <p>Using it is straightforward :</p> <pre><code>$ spacy init vectors fr /path/to/vectors /path/to/pipeline\n---&gt; 100%\ncolor:green Conversion successful!\n</code></pre> <p>See the documentation for implementation details.</p>"},{"location":"concepts/inference/","title":"Inference","text":"<p>Once you have obtained a pipeline, either by composing rule-based components, training a model or loading a model from the disk, you can use it to make predictions on documents. This is referred to as inference. This page answers the following questions :</p> <p>How do we leverage computational resources run a model on many documents?</p> <p>How do we connect to various data sources to retrieve documents?</p> <p>Be sure to check out the Processing multiple texts tutorial for a practical example of how to use EDS-NLP to process large datasets.</p>"},{"location":"concepts/inference/#inference-on-a-single-document","title":"Inference on a single document","text":"<p>In EDS-NLP, computing the prediction on a single document is done by calling the pipeline on the document. The input can be either:</p> <ul> <li>a text string</li> <li>or a Doc object</li> </ul> <pre><code>from pathlib import Path\n\nnlp = ...\ntext = \"... my text ...\"\ndoc = nlp(text)\n</code></pre> <p>If you're lucky enough to have a GPU, you can use it to speed up inference by moving the model to the GPU before calling the pipeline.</p> <pre><code>nlp.to(\"cuda\")  # same semantics as pytorch\ndoc = nlp(text)\n</code></pre> <p>To leverage multiple GPUs when processing multiple documents, refer to the multiprocessing backend description below.</p>"},{"location":"concepts/inference/#edsnlp.core.lazy_collection.LazyCollection","title":"Inference on multiple documents","text":"<p>When processing multiple documents, we can optimize the inference by parallelizing the computation on a single core, multiple cores and GPUs or even multiple machines.</p>"},{"location":"concepts/inference/#lazy-collection","title":"Lazy collection","text":"<p>These optimizations are enabled by performing lazy inference : the operations (e.g., reading a document, converting it to a Doc, running the different pipes of a model or writing the result somewhere) are not executed immediately but are instead scheduled in a LazyCollection object. It can then be executed by calling the <code>execute</code> method, iterating over it or calling a writing method (e.g., <code>to_pandas</code>). In fact, data connectors like <code>edsnlp.data.read_json</code> return a lazy collection, as well as the <code>nlp.pipe</code> method.</p> <p>A lazy collection contains :</p> <ul> <li>a <code>reader</code>: the source of the data (e.g., a file, a database, a list of strings, etc.)</li> <li>the list of operations to perform under a <code>pipeline</code> attribute containing the name if any, function / pipe, keyword arguments and context for each operation</li> <li>an optional <code>writer</code>: the destination of the data (e.g., a file, a database, a list of strings, etc.)</li> <li>the execution <code>config</code>, containing the backend to use and its configuration such as the number of workers, the batch size, etc.</li> </ul> <p>All methods (<code>.map</code>, <code>.map_batches</code>, <code>.map_gpu</code>, <code>.map_pipeline</code>, <code>.set_processing</code>) of the lazy collection are chainable, meaning that they return a new lazy collection object (no in-place modification).</p> <p>For instance, the following code will load a model, read a folder of JSON files, apply the model to each document and write the result in a Parquet folder, using 4 CPUs and 2 GPUs.</p> <pre><code>import edsnlp\n\n# Load or create a model\nnlp = edsnlp.load(\"path/to/model\")\n\n# Read some data (this is lazy, no data will be read until the end of of this snippet)\ndata = edsnlp.data.read_json(\"path/to/json_folder\", converter=\"...\")\n\n# Apply each pipe of the model to our documents\ndata = data.map_pipeline(nlp)\n# or equivalently : data = nlp.pipe(data)\n\n# Configure the execution\ndata = data.set_processing(\n    # 4 CPUs to parallelize rule-based pipes, IO and preprocessing\n    num_cpu_workers=4,\n    # 2 GPUs to accelerate deep-learning pipes\n    num_gpu_workers=2,\n\n    # Below are further options to finetune the inference throughput:\n    # Read chunks of 1024 documents before splitting them into batches\n    chunk_size=1024,\n    # Sort the documents by length before splitting them into batches\n    sort_chunks=True,\n    # Split batches such that each contains at most 100 000 padded words\n    # (padded_words = max doc size in batch * batch size)\n    batch_size=100_000,\n    batch_by=\"padded_words\",\n)\n\n# Write the result, this will execute the lazy collection\ndata.write_parquet(\"path/to/output_folder\", converter=\"...\", write_in_worker=True)\n</code></pre>"},{"location":"concepts/inference/#applying-operations-to-a-lazy-collection","title":"Applying operations to a lazy collection","text":"<p>To apply an operation to a lazy collection, you can use the <code>.map</code> method. It takes a callable as input and an optional dictionary of keyword arguments. The function will be applied to each element of the collection.</p> <p>To apply an operation to a lazy collection in batches, you can use the <code>.map_batches</code> method. It takes a callable as input and an optional dictionary of keyword arguments. The function will be applied to each batch of the collection (as a list of elements), and should return a list of results, that will be concatenated at the end.</p> <p>To apply a model, you can use the <code>.map_pipeline</code> method. It takes a model as input and will add every pipe of the model to the scheduled operations.</p> <p>To run a specific function on a GPU (for advanced users, otherwise <code>map_pipeline</code> should accommodate most use cases), you can use the <code>.map_gpu</code> method. It takes two or three callables as input: the first on (<code>prepare_batches</code>) takes a batch of inputs and should return some tensors that will be sent to the GPU and passed to the second callable (<code>forward</code>), which will apply the deep learning ops and return the results. The third callable (<code>postprocess</code>) and gets the batch of inputs as well as the <code>forward</code> results and should return the final results (for instance, the input documents annotated with the predictions).</p> <p>In each cases, the operations will not be executed immediately but will be scheduled to be executed when iterating of the collection, or calling the <code>.execute</code>, <code>.to_*</code> or <code>.write_*</code> methods.</p>"},{"location":"concepts/inference/#edsnlp.core.lazy_collection.LazyCollection.set_processing","title":"Execution of a lazy collection","text":"<p>You can configure how the operations performed in the lazy collection is executed by calling its <code>set_processing(...)</code> method. The following options are available :</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>Number of documents to process at a time in a GPU worker (or in the main process if no workers are used).</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFER</code> </p> <code>batch_by</code> <p>How to compute the batch size. Can be \"docs\" or \"words\" :</p> <ul> <li>\"docs\" (default) is the number of documents.</li> <li>\"words\" is the total number of words in the documents.</li> <li>\"padded_words\" is the total number of words in the documents, including    padding, assuming the documents are padded to the same length.</li> </ul> <p> TYPE: <code>Literal['docs', 'words', 'padded_words']</code> DEFAULT: <code>'docs'</code> </p> <code>chunk_size</code> <p>Number of documents to build before splitting into batches. Only used with \"simple\" and \"multiprocessing\" backends. This is also the number of documents that will be passed through the first components of the pipeline until a GPU worker is used (then the chunks will be split according to the <code>batch_size</code> and <code>batch_by</code> arguments).</p> <p>By default, the chunk size is equal to the batch size, or 128 if the batch size is not set.</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFER</code> </p> <code>sort_chunks</code> <p>Whether to sort the documents by size before splitting into batches.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>split_into_batches_after</code> <p>The name of the component after which to split the documents into batches. Only used with \"simple\" and \"multiprocessing\" backends. By default, the documents are split into batches as soon as the input are converted into Doc objects.</p> <p> TYPE: <code>str</code> DEFAULT: <code>INFER</code> </p> <code>num_cpu_workers</code> <p>Number of CPU workers. A CPU worker handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components. If no GPU workers are used, the CPU workers also handle the forward call of the deep-learning components.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>INFER</code> </p> <code>num_gpu_workers</code> <p>Number of GPU workers. A GPU worker handles the forward call of the deep-learning components. Only used with \"multiprocessing\" backend.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>INFER</code> </p> <code>disable_implicit_parallelism</code> <p>Whether to disable OpenMP and Huggingface tokenizers implicit parallelism in multiprocessing mode. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>gpu_pipe_names</code> <p>List of pipe names to accelerate on a GPUWorker, defaults to all pipes that inherit from TorchComponent. Only used with \"multiprocessing\" backend. Inferred from the pipeline if not set.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>INFER</code> </p> <code>backend</code> <p>The backend to use for parallel processing. If not set, the backend is automatically selected based on the input data and the number of workers.</p> <ul> <li>\"simple\" is the default backend and is used when <code>num_cpu_workers</code> is 1     and <code>num_gpu_workers</code> is 0.</li> <li>\"multiprocessing\" is used when <code>num_cpu_workers</code> is greater than 1 or     <code>num_gpu_workers</code> is greater than 0.</li> <li>\"spark\" is used when the input data is a Spark dataframe and the output     writer is a Spark writer.</li> </ul> <p> TYPE: <code>Optional[Literal['simple', 'multiprocessing', 'mp', 'spark']]</code> DEFAULT: <code>INFER</code> </p> <code>show_progress</code> <p>Whether to show progress bars (only applicable with \"simple\" and \"multiprocessing\" backends).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>process_start_method</code> <p>Whether to use \"fork\" or \"spawn\" as the start method for the multiprocessing backend. The default is \"fork\" on Unix systems and \"spawn\" on Windows.</p> <ul> <li>\"fork\" is the default start method on Unix systems and is the fastest     start method, but it is not available on Windows, can cause issues     with CUDA and is not safe when using multiple threads.</li> <li>\"spawn\" is the default start method on Windows and is the safest start     method, but it is not available on Unix systems and is slower than     \"fork\".</li> </ul> <p> TYPE: <code>Optional[Literal['fork', 'spawn']]</code> DEFAULT: <code>INFER</code> </p> <code>gpu_worker_devices</code> <p>List of GPU devices to use for the GPU workers. Defaults to all available devices, one worker per device. Only used with \"multiprocessing\" backend.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>INFER</code> </p> <code>cpu_worker_devices</code> <p>List of GPU devices to use for the CPU workers. Used for debugging purposes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>INFER</code> </p>"},{"location":"concepts/inference/#backends","title":"Backends","text":""},{"location":"concepts/inference/#edsnlp.processing.simple.execute_simple_backend","title":"Simple backend","text":"<p>This is the default execution mode which batches the documents and processes each batch on the current process in a sequential manner.</p>"},{"location":"concepts/inference/#edsnlp.processing.multiprocessing.execute_multiprocessing_backend","title":"Multiprocessing backend","text":"<p>If you have multiple CPU cores, and optionally multiple GPUs, we provide the <code>multiprocessing</code> backend that allows to run the inference on multiple processes.</p> <p>This accelerator dispatches the batches between multiple workers (data-parallelism), and distribute the computation of a given batch on one or two workers (model-parallelism):</p> <ul> <li>a <code>CPUWorker</code> which handles the non deep-learning components and the   preprocessing, collating and postprocessing of deep-learning components</li> <li>a <code>GPUWorker</code> which handles the forward call of the deep-learning components</li> </ul> <p>If no GPU is available, no <code>GPUWorker</code> is started, and the <code>CPUWorkers</code> handle the forward call of the deep-learning components as well.</p> <p>The advantage of dedicating a worker to the deep-learning components is that it allows to prepare multiple batches in parallel in multiple <code>CPUWorker</code>, and ensure that the <code>GPUWorker</code> never wait for a batch to be ready.</p> <p>The overall architecture described in the following figure, for 3 CPU workers and 2 GPU workers.</p> <p>Here is how a small pipeline with rule-based components and deep-learning components is distributed between the workers:</p> <p>Caveat</p> <p>Since workers can produce their results in any order, the order of the results may not be the same as the order of the input tasks.</p>"},{"location":"concepts/inference/#edsnlp.processing.spark.execute_spark_backend","title":"Spark backend","text":"<p>This execution mode uses Spark to parallelize the processing of the documents. The documents are first stored in a Spark DataFrame (if it was not already the case) and then processed in parallel using Spark.</p> <p>Beware, if the original reader was not a SparkReader (<code>edsnlp.data.from_spark</code>), the local docs \u2192 spark dataframe conversion might take some time, and the whole process might be slower than using the <code>multiprocessing</code> backend.</p>"},{"location":"concepts/pipeline/","title":"Pipeline","text":"<p>The goal of EDS-NLP is to provide a framework for processing textual documents.</p> <p>Processing textual documents, and clinical documents in particular, usually involves many steps such as tokenization, cleaning, named entity recognition, span classification, normalization, linking, etc. Organising these steps together, combining static and deep learning components, while remaining modular and efficient is a challenge. This is why EDS-NLP is built on top of a novel pipelining system.</p> <p>Deep learning frameworks</p> <p>Trainable components in EDS-NLP are built around the PyTorch framework. While you can use any technology in static components, we do not provide tools to train components built with other deep learning frameworks.</p>"},{"location":"concepts/pipeline/#compatibility-with-spacy-and-pytorch","title":"Compatibility with spaCy and PyTorch","text":"<p>While EDS-NLP is built on top of its own pipeline system, it is also designed to be compatible with the awesome spaCy framework. This means that you can use (non-trainable) EDS-NLP components in a spaCy pipeline, and vice-versa. Documents, objects that are passed through the pipeline, are in fact spaCy documents, and we borrow many of spaCy's method names and conventions to make the transition between the two libraries as smooth as possible.</p> <p>Trainable components, on the other hand, are built on top of the PyTorch framework. This means that you can use PyTorch components in an EDS-NLP pipeline and benefit from the latest advances in deep learning research. For more information on PyTorch components, refer to the Torch component page.</p>"},{"location":"concepts/pipeline/#creating-a-pipeline","title":"Creating a pipeline","text":"<p>A pipeline is composed of multiple pipes, i.e., callable processing blocks, like a function, that apply a transformation on a Doc object, such as adding annotations, and return the modified object.</p> <p>To create your first EDS-NLP pipeline, run the following code. We provide several ways to create a pipeline:</p> EDS-NLP APISpaCy-like APIFrom a config file <p>This is the recommended way to create a pipeline, as it allows auto-completion, type checking and introspection (you can click on the component or its arguments to see the documentation in most IDEs).</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.matcher(regex={\"smoker\": [\"fume\", \"clope\"]}))\nnlp.add_pipe(eds.negation())\n</code></pre> <p>Curried components</p> <p>Most components (like <code>eds.matcher</code>) require an <code>nlp</code> argument initialization. The above <code>eds.matcher(regex={\"smoker\": [\"fume\", \"clope\"]})</code> actually returns a \"curried\" component, that will be instantiated when added to the pipeline. To create the actual component directly and use it outside of a pipeline (not recommended), you can use <code>eds.matcher(nlp, regex={\"smoker\": [\"fume\", \"clope\"]})</code>, or use the result of the <code>nlp.add_pipe</code> call.</p> <p>Pipes can be dynamically added to the pipeline using the <code>add_pipe</code> method, with a string matching their factory name and an optional configuration dictionary.</p> <pre><code>import edsnlp  # or import spacy\n\nnlp = edsnlp.blank(\"eds\")  # or spacy.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.matcher\", config=dict(regex={\"smoker\": [\"fume\", \"clope\"]}))\nnlp.add_pipe(\"eds.negation\")\n</code></pre> <p>You can also create a pipeline from a configuration file. This is useful when you plan on changing the pipeline configuration often.</p> config.cfg<pre><code>[nlp]\nlang = \"eds\"\npipeline = [\"sentences\", \"matcher\", \"negation\"]\n\n[components.sentences]\n@factory = \"eds.sentences\"\n\n[components.matcher]\n@factory = \"eds.matcher\"\nregex = {\"smoker\": [\"fume\", \"clope\"]}\n\n[components.negation]\n@factory = \"eds.negation\"\n</code></pre> <p>and then load the pipeline with:</p> <pre><code>import edsnlp\n\nnlp = edsnlp.load(\"config.cfg\")\n</code></pre> <p>This pipeline can then be run on one or more texts documents. As the pipeline process documents, components will be called in the order they were added to the pipeline.</p> <pre><code>from pathlib import Path\n\n# Processing one document\nnlp(\"Le patient ne fume pas\")\n\n# Processing multiple documents\nmodel.pipe([text1, text2])\n</code></pre> <p>For more information on how to use the pipeline, refer to the Inference page.</p>"},{"location":"concepts/pipeline/#hybrid-models","title":"Hybrid models","text":"<p>EDS-NLP was designed to facilitate the training and inference of hybrid models that arbitrarily chain static components or trained deep learning components. Static components are callable objects that take a Doc object as input, perform arbitrary transformations over the input, and return the modified object. Torch components, on the other hand, allow for deep learning operations to be performed on the Doc object and must be trained to be used.</p> <p></p>"},{"location":"concepts/pipeline/#saving-and-loading-a-pipeline","title":"Saving and loading a pipeline","text":"<p>Pipelines can be saved and loaded using the <code>save</code> and <code>load</code> methods. Following spaCy, the saved pipeline is not a pickled objet but a folder containing the config file, the weights and extra resources for each pipeline. Deep-learning parameters are saved with the <code>safetensors</code> library to avoid any security issue. This allows for easy inspection and modification of the pipeline, and avoids the execution of arbitrary code when loading a pipeline.</p> <pre><code>nlp.to_disk(\"path/to/your/model\")\nnlp = edsnlp.load(\"path/to/your/model\")\n</code></pre>"},{"location":"concepts/pipeline/#sharing-a-pipeline","title":"Sharing a pipeline","text":"<p>To share the pipeline and turn it into a pip installable package, you can use the <code>package</code> method, which will use or create a pyproject.toml file, fill it accordingly, and create a wheel file. At the moment, we only support the poetry package manager.</p> <pre><code>nlp.package(\n    name=\"your-package-name\",  # leave None to reuse name in pyproject.toml\n    version=\"0.0.1\",\n    root_dir=\"path/to/project/root\",  # optional, to retrieve an existing pyproject.toml file\n    # if you don't have a pyproject.toml, you can provide the metadata here instead\n    metadata=dict(\n        authors=\"Firstname Lastname &lt;your.email@domain.fr&gt;\",\n        description=\"A short description of your package\",\n    ),\n)\n</code></pre> <p>This will create a wheel file in the root_dir/dist folder, which you can share and install with pip.</p>"},{"location":"concepts/torch-component/","title":"Torch Component","text":"<p>Torch components allow for deep learning operations to be performed on the Doc object and must be trained to be used. Such pipes can be used to train a model to detect named entities, predict the label of a document or an attribute of a text span, and so on.</p> <p></p> Example of sharing and nesting components"},{"location":"concepts/torch-component/#anatomy-of-a-trainable-pipe","title":"Anatomy of a trainable pipe","text":"<p>Building and running deep learning models usually requires to <code>preprocess</code> the input sample into features, to batch or <code>collate</code> these features together to process multiple samples at once, running deep learning operations over these features (in Pytorch, this step is done in the <code>forward</code> method) and to <code>postprocess</code> the outputs of these operation to complete the original sample.</p> <p>In the trainable pipes of EDS-NLP, preprocessing and postprocessing are decoupled from the deep learning code but collocated with the forward method. This is achieved by splitting the class of a trainable component into four methods, which allows us to keep the development of new deep-learning components simple while ensuring efficient models both during training and inference.</p> Methods of a trainable component"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.preprocess","title":"<code>preprocess</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network to perform its predictions.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.preprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to preprocess</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.collate","title":"<code>collate</code>","text":"<p>Collate the batch of features into a single batch of tensors that can be used by the forward method of the component.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.collate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of features</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>BatchInput</code> <p>Dictionary (optionally nested) containing the collated tensors</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.forward","title":"<code>forward</code>","text":"<p>Perform the forward pass of the neural network.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of tensors (nested dictionary) computed by the collate method</p> <p> TYPE: <code>BatchInput</code> </p> RETURNS DESCRIPTION <code>BatchOutput</code>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.postprocess","title":"<code>postprocess</code>","text":"<p>Update the documents with the predictions of the neural network. By default, this is a no-op.</p> <p>Additionally, there is a fifth method:</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.postprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of documents to update</p> <p> TYPE: <code>Sequence[Doc]</code> </p> <code>results</code> <p>Batch of predictions, as returned by the forward method</p> <p> TYPE: <code>BatchOutput</code> </p> <code>inputs</code> <p>List of preprocessed features, as returned by the preprocess method</p> <p> TYPE: <code>List[Dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>Sequence[Doc]</code>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.post_init","title":"<code>post_init</code>","text":"<p>This method completes the attributes of the component, by looking at some documents. It is especially useful to build vocabularies or detect the labels of a classification task.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.post_init--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>gold_data</code> <p>The documents to use for initialization.</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>exclude</code> <p>The names of components to exclude from initialization. This argument will be gradually updated  with the names of initialized components</p> <p> TYPE: <code>Set[str]</code> </p>"},{"location":"concepts/torch-component/#nesting-trainable-pipes","title":"Nesting trainable pipes","text":"<p>Like pytorch modules, you can compose trainable pipes together to build complex architectures. For instance, a trainable named entity recognition component may delegate some of its logic to an embedding component, which will only be responsible for converting words into a embeddings. Nesting pipes allows switching parts of the neural networks to test various architectures and keeping the modelling logic modular.</p> <p>Nested preprocessing</p> <p>This is where the idea of collocating <code>preprocess</code> and <code>collate</code> with <code>forward</code> really shines: much like pytorch modules, they can be called recursively on the subcomponents of a trainable pipe. This allows to extend the composition pattern to the preprocessing step to enable true modularity.</p>"},{"location":"concepts/torch-component/#sharing-subcomponents","title":"Sharing subcomponents","text":"<p>Sharing parts of a neural network while training on different tasks can be an effective way to improve the network efficiency. For instance, it is common to share an embedding layer between multiple tasks that require embedding the same inputs.</p> <p>In EDS-NLP, sharing a subcomponent is simply done by sharing the object between the multiple pipes. You can either refer to an existing subcomponent when configuring a new component in Python, or use the interpolation mechanism of our configuration system.</p> API-basedConfiguration-based <pre><code>nlp.add_pipe(\n    eds.ner_crf(\n        ...,\n        embedding=eds.transformer(\n            model=\"bert-base-uncased\",\n            window=128,\n            stride=96,\n        ),\n    ),\n    name=\"first\",\n)\nnlp.add_pipe(\n    some_other_task(\n        embedding=nlp.pipes.first.embedding,\n    ),\n    name=\"second\",\n)\n</code></pre> <pre><code>[components.first]\n@factory = \"eds.ner_crf\"\n...\n\n[components.first.embedding]\n@factory = \"eds.embeddings\"\n...\n\n[components.second]\n@factory = \"some_other_task\"\nembedding = ${components.first.embedding}\n</code></pre> <p>To avoid recomputing the <code>preprocess</code> / <code>forward</code> and <code>collate</code> in the multiple components that use it, we rely on a light cache system.</p> <p>During the training loop, when computing the loss for each component, the forward calls must be wrapped by the <code>pipeline.cache()</code> context to enable this caching mechanism between components.</p>"},{"location":"concepts/torch-component/#implementation-example","title":"Implementation example","text":"<p>Here is an example of a trainable component:</p> <pre><code>from typing import Any, Dict, Iterable, Sequence\n\nimport torch\nfrom tqdm import tqdm\n\nfrom edsnlp import Pipeline, registry\nfrom edsnlp.core.torch_component import TorchComponent\nfrom spacy.tokens import Doc\n\n\n@registry.factory.register(\"my-component\")\nclass MyComponent(TorchComponent):\n    def __init__(\n        self,  # A subcomponent\n        nlp: Pipeline,\n        name: str,\n        embedding: TorchComponent,\n    ):\n        super().__init__(nlp=nlp, name=name)\n        self.embedding = embedding\n\n    def post_init(self, gold_data: Iterable[Doc], exclude: set):\n        super().post_init(gold_data, exclude)\n\n        # Initialize the component with the gold documents\n        with self.label_vocabulary.initialization():\n            for doc in tqdm(gold_data, desc=\"Initializing the component\"):\n                # Do something like learning a vocabulary over the initialization\n                # documents\n                ...\n\n        # And post_init the subcomponent\n        self.embedding.post_init(gold_data, exclude)\n\n        # Initialize any layer that might be missing from the module\n        self.classifier = torch.nn.Linear(...)\n\n    def preprocess(self, doc: Doc) -&gt; Dict[str, Any]:\n        # Preprocess the doc to extract features required to run the embedding\n        # subcomponent, and this component\n        return {\n            \"embedding\": self.embedding.preprocess(doc),\n            \"my-feature\": ...(doc),\n        }\n\n    def collate(self, batch) -&gt; Dict:\n        # Collate the features of the \"embedding\" subcomponent\n        # and the features of this component as well\n        return {\n            \"embedding\": self.embedding.collate(batch[\"embedding\"]),\n            \"my-feature\": torch.as_tensor(batch[\"my-feature\"]),\n        }\n\n    def forward(self, batch: Dict) -&gt; Dict:\n        # Call the embedding subcomponent\n        embeds = self.embedding(batch[\"embedding\"])\n\n        # Do something with the embedding tensors\n        output = ...(embeds)\n\n        return output\n\n    def postprocess(self, docs: Sequence[Doc], output: Dict) -&gt; Sequence[Doc]:\n        # Annotate the docs with the outputs of the forward method\n        ...\n        return docs\n</code></pre>"},{"location":"data/","title":"Data connectors","text":"<p>We provide various connectors to read and write data from and to different formats.</p> <p>Reading from a given path or object takes the following form:</p> <pre><code>import edsnlp\n\ndocs = edsnlp.data.read_{format}(  # or .from_{format} for objects\n    # Path to the file or directory\n    \"path/to/file\",\n    # How to convert JSON-like samples to Doc objects\n    converter=predefined schema or function,\n)\n</code></pre> <p>Writing to given path or object takes the following form:</p> <pre><code>import edsnlp\n\nedsnlp.data.write_{format}(  # or .to_{format} for objects\n    # Path to the file or directory\n    \"path/to/file\",\n    # Iterable of Doc objects\n    docs,\n    # How to convert Doc objects to JSON-like samples\n    converter=predefined schema or function,\n)\n</code></pre> <p>The overall process is illustrated in the following diagram:</p> <p></p> <p>At the moment, we support the following data sources:</p> Source Description JSON <code>.json</code> and <code>.jsonl</code> files Standoff &amp; BRAT <code>.ann</code> and <code>.txt</code> files Pandas Pandas DataFrame objects Polars Polars DataFrame objects Spark Spark DataFrame objects <p>and the following schemas:</p> Schema Snippet Custom <code>converter=custom_fn</code> OMOP <code>converter=\"omop\"</code> Standoff <code>converter=\"standoff\"</code>"},{"location":"data/converters/","title":"Converters","text":"<p>Data can be read from and writen to various sources, like JSON/BRAT/CSV files or dataframes, which expect a key-value representation and not Doc object. For that purpose, we document here a set of converters that can be used to convert between these representations and Doc objects.</p> <p>Converters can be configured in the <code>from_*</code> (or <code>read_*</code> in the case of files) and <code>to_*</code> (or <code>write_*</code> in the case of files) methods, depending on the chosen <code>converter</code> argument, which can be:</p> <ul> <li>a function, in which case it will be interpreted as a custom converter</li> <li>a string, in which case it will be interpreted as the name of a pre-defined converter</li> </ul>"},{"location":"data/converters/#custom","title":"Custom converter","text":"<p>You can always define your own converter functions to convert between your data and Doc objects.</p>"},{"location":"data/converters/#reading-from-a-custom-schema","title":"Reading from a custom schema","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nfrom spacy.tokens import Doc\nfrom edsnlp.data.converters import get_current_tokenizer\nfrom typing import Dict\n\ndef convert_row_to_dict(row: Dict) -&gt; Doc:\n    # Tokenizer will be inferred from the pipeline\n    doc = get_current_tokenizer()(row[\"custom_content\"])\n    doc._.note_id = row[\"custom_id\"]\n    doc._.note_datetime = row[\"custom_datetime\"]\n    # ...\n    return doc\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.covid())\n\n# Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.from_pandas(\n    # Path to the file or directory\n    dataframe,\n    # How to convert JSON-like samples to Doc objects\n    converter=convert_row_to_dict,\n)\ndocs = docs.map_pipeline(nlp)\n</code></pre>"},{"location":"data/converters/#writing-to-a-custom-schema","title":"Writing to a custom schema","text":"<pre><code>def convert_doc_to_row(doc: Doc) -&gt; Dict:\n    return {\n        \"custom_id\": doc._.id,\n        \"custom_content\": doc.text,\n        \"custom_datetime\": doc._.note_datetime,\n        # ...\n    }\n\n# Any kind of writer (`edsnlp.data.write/to_...`) can be used here\ndocs.write_parquet(\n    \"path/to/output_folder\",\n    # How to convert Doc objects to JSON-like samples\n    converter=convert_doc_to_row,\n)\n</code></pre> <p>One row per entity</p> <p>This function can also return a list of dicts, for instance one dict per detected entity, that will be treated as multiple rows in dataframe writers (e.g., <code>to_pandas</code>, <code>to_spark</code>, <code>write_parquet</code>).</p> <pre><code>def convert_ents_to_rows(doc: Doc) -&gt; List[Dict]:\n    return [\n        {\n            \"note_id\": doc._.id,\n            \"ent_text\": ent.text,\n            \"ent_label\": ent.label_,\n            \"custom_datetime\": doc._.note_datetime,\n            # ...\n        }\n        for ent in doc.ents\n    ]\n\n\ndocs.write_parquet(\n    \"path/to/output_folder\",\n    # How to convert entities of Doc objects to JSON-like samples\n    converter=convert_ents_to_rows,\n)\n</code></pre>"},{"location":"data/converters/#omop","title":"OMOP (<code>converter=\"omop\"</code>)","text":"<p>OMOP is a schema that is used in the medical domain. It is based on the OMOP Common Data Model. We are mainly interested in the <code>note</code> table, which contains the clinical notes, and deviate from the original schema by adding an optional <code>entities</code> column that can be computed from the <code>note_nlp</code> table.</p> <p>Therefore, a complete OMOP-style document would look like this:</p> <pre><code>{\n\"note_id\": 0,\n\"note_text\": \"Le patient ...\",\n\"entities\": [\n{\n\"note_nlp_id\": 0,\n\"start_char\": 3,\n\"end_char\": 10,\n\"lexical_variant\": \"patient\",\n\"note_nlp_source_value\": \"person\",\n\n# optional fields\n\"negated\": False,\n\"certainty\": \"probable\",\n...\n},\n...\n],\n\n# optional fields\n\"custom_doc_field\": \"...\"\n...\n}\n</code></pre>"},{"location":"data/converters/#converting-omop-data-to-doc-objects","title":"Converting OMOP data to Doc objects","text":""},{"location":"data/converters/#edsnlp.data.converters.OmopDict2DocConverter--examples","title":"Examples","text":"<pre><code># Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.from_pandas(\n    df,\n    converter=\"omop\",\n\n    # Optional parameters\n    tokenizer=tokenizer,\n    doc_attributes=[\"note_datetime\"],\n\n    # Parameters below should only matter if you plan to import entities\n    # from the dataframe. If the data doesn't contain pre-annotated\n    # entities, you can ignore these.\n    span_setter={\"ents\": True, \"*\": True},\n    span_attributes={\"negation\": \"negated\"},\n    default_attributes={\"negated\": False, \"temporality\": \"present\"},\n)\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.OmopDict2DocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   LazyCollection.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>doc_attributes</code> <p>Mapping from JSON attributes to additional Span extensions (can be a list too). By default, all attributes are imported as Doc extensions with the same name.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{'note_datetime': 'note_datetime'}</code> </p> <code>span_attributes</code> <p>Mapping from JSON attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"data/converters/#converting-doc-objects-to-omop-data","title":"Converting Doc objects to OMOP data","text":""},{"location":"data/converters/#edsnlp.data.converters.OmopDoc2DictConverter--examples","title":"Examples","text":"<pre><code># Any kind of writer (`edsnlp.data.write/to_...`) can be used here\ndf = edsnlp.data.to_pandas(\n    docs,\n    converter=\"omop\",\n\n    # Optional parameters\n    span_getter={\"ents\": True},\n    doc_attributes=[\"note_datetime\"],\n    span_attributes=[\"negation\", \"family\"],\n)\n# or docs.to_pandas(...) if it's already a\n# [LazyCollection][edsnlp.core.lazy_collection.LazyCollection]\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.OmopDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>doc_attributes</code> <p>Mapping from Doc extensions to JSON attributes (can be a list too). By default, no doc attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"data/converters/#standoff","title":"Standoff (<code>converter=\"standoff\"</code>)","text":"<p>Standoff refers mostly to the BRAT standoff format, but doesn't indicate how the annotations should be stored in a JSON-like schema. We use the following schema:</p> <pre><code>{\n\"doc_id\": 0,\n\"text\": \"Le patient ...\",\n\"entities\": [\n{\n\"entity_id\": 0,\n\"label\": \"drug\",\n\"fragments\": [{\n\"start\": 0,\n\"end\": 10\n}],\n\"attributes\": {\n\"negated\": True,\n\"certainty\": \"probable\"\n}\n},\n...\n]\n}\n</code></pre>"},{"location":"data/converters/#converting-standoff-data-to-doc-objects","title":"Converting Standoff data to Doc objects","text":"<p>Why does BRAT/Standoff need a converter ?</p> <p>You may wonder : why do I need a converter ? Since BRAT is already a NLP oriented format, it should be straightforward to convert it to a Doc object.</p> <p>Indeed, we do provide a default converter for the BRAT standoff format, but we also acknowledge that there may be more than one way to convert a standoff document to a Doc object. For instance, an annotated span may be used to represent a relation between two smaller included entities, or another entity scope, etc.</p> <p>In such cases, we recommend you use a custom converter as described here.</p>"},{"location":"data/converters/#edsnlp.data.converters.StandoffDict2DocConverter--examples","title":"Examples","text":"<pre><code># Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.read_standoff(\n    \"path/to/standoff\",\n    converter=\"standoff\",  # set by default\n\n    # Optional parameters\n    tokenizer=tokenizer,\n    span_setter={\"ents\": True, \"*\": True},\n    span_attributes={\"negation\": \"negated\"},\n    keep_raw_attribute_values=False,\n    default_attributes={\"negated\": False, \"temporality\": \"present\"},\n)\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.StandoffDict2DocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   LazyCollection.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>notes_as_span_attribute</code> <p>If set, the AnnotatorNote annotations will be concatenated and stored in a span attribute with this name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>split_fragments</code> <p>Whether to split the fragments into separate spans or not. If set to False, the fragments will be concatenated into a single span.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"data/converters/#converting-doc-objects-to-standoff-data","title":"Converting Doc objects to Standoff data","text":""},{"location":"data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter--examples","title":"Examples","text":"<pre><code># Any kind of writer (`edsnlp.data.read/from_...`) can be used here\nedsnlp.data.write_standoff(\n    docs,\n    converter=\"standoff\",  # set by default\n\n    # Optional parameters\n    span_getter={\"ents\": True},\n    span_attributes={\"negation\": \"negated\"},\n)\n# or docs.to_standoff(...) if it's already a\n# [LazyCollection][edsnlp.core.lazy_collection.LazyCollection]\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>{'ents': True}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"data/converters/#ents","title":"Entities (<code>converter=\"ents\"</code>)","text":"<p>We also provide a simple one-way (export) converter to convert Doc into a list of dictionaries, one per entity, that can be used to write to a dataframe. The schema of each produced row is the following:</p> <pre><code>{\n\"note_id\": 0,\n\"start\": 3,\n\"end\": 10,\n\"label\": \"drug\",\n\"lexical_variant\": \"patient\",\n\n# Optional fields\n\"negated\": False,\n\"certainty\": \"probable\"\n...\n}\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.EntsDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>doc_attributes</code> <p>Mapping from Doc extensions to JSON attributes (can be a list too). By default, no doc attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"data/json/","title":"JSON","text":"TLDR <pre><code>import edsnlp\n\ndocs = edsnlp.data.from_json(df, converter=\"omop\")\ndocs = docs.map_pipeline(nlp)\nres = edsnlp.data.to_json(docs, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to json files.</p> <p>As an example, imagine that we have the following document that uses the OMOP schema</p> data.jsonl<pre><code>{ \"note_id\": 0, \"note_text\": \"Le patient ...\", \"note_datetime\": \"2021-10-23\", \"entities\": [...] }\n{ \"note_id\": 1, \"note_text\": \"Autre doc ...\", \"note_datetime\": \"2022-12-24\", \"entities\": [] }\n...\n</code></pre> <p>You could also have multiple <code>.json</code> files in a directory, the reader will read them all.</p>"},{"location":"data/json/#edsnlp.data.json.read_json","title":"Reading JSON files","text":"<p>The JsonReader (or <code>edsnlp.data.read_json</code>) reads a directory of JSON files and yields documents. At the moment, only entities and attributes are loaded.</p>"},{"location":"data/json/#edsnlp.data.json.read_json--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_json(\"path/to/json/dir\", converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_json</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.read_json(\"path/to/json/dir\", converter=\"omop\")\n</code></pre>"},{"location":"data/json/#edsnlp.data.json.read_json--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the JSON files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>keep_ipynb_checkpoints</code> <p>Whether to keep the files have \".ipynb_checkpoints\" in their path.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>read_in_worker</code> <p>Whether to read the files in the worker or in the main process.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the JSON objects to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"data/json/#edsnlp.data.json.write_json","title":"Writing JSON files","text":"<p><code>edsnlp.data.write_json</code> writes a list of documents using the JSON format in a directory. If <code>lines</code> is false, each document will be stored in its own JSON file, named after the FILENAME field returned by the converter (commonly the <code>note_id</code> attribute of the documents), and subdirectories will be created if the name contains <code>/</code> characters.</p>"},{"location":"data/json/#edsnlp.data.json.write_json--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_json([doc], \"path/to/json/file\", converter=\"omop\", lines=True)\n# or to write a directory of JSON files, ensure that each doc has a doc._.note_id\n# attribute, since this will be used as a filename:\nedsnlp.data.write_json([doc], \"path/to/json/dir\", converter=\"omop\", lines=False)\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_json</code> will raise an error if the directory already exists and contains files with <code>.a*</code> or <code>.txt</code> suffixes. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"data/json/#edsnlp.data.json.write_json--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>path</code> <p>Path to either - a file if <code>lines</code> is true : this will write the documents as a JSONL file - a directory if <code>lines</code> is false: this will write one JSON file per document   using the FILENAME field returned by the converter (commonly the <code>note_id</code>   attribute of the documents) as the filename.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>lines</code> <p>Whether to write the documents as a JSONL file or as a directory of JSON files. By default, this is inferred from the path: if the path is a file, lines is assumed to be true, otherwise it is assumed to be false.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before writing them. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/pandas/","title":"Pandas","text":"TLDR <pre><code>import edsnlp\n\ndocs = edsnlp.data.from_pandas(df, converter=\"omop\")\ndocs = docs.map_pipeline(nlp)\nres = edsnlp.data.to_pandas(docs, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to Pandas DataFrames.</p> <p>As an example, imagine that we have the following OMOP dataframe (we'll name it <code>note_df</code>)</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23"},{"location":"data/pandas/#edsnlp.data.pandas.from_pandas","title":"Reading from a Pandas Dataframe","text":"<p>The PandasReader (or <code>edsnlp.data.from_pandas</code>) handles reading from a table and yields documents. At the moment, only entities and attributes are loaded. Relations and events are not supported.</p>"},{"location":"data/pandas/#edsnlp.data.pandas.from_pandas--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_pandas(df, nlp=nlp, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_pandas</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_pandas(df, converter=\"omop\"))\n</code></pre>"},{"location":"data/pandas/#edsnlp.data.pandas.from_pandas--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>Pandas object</p> <p> </p> <code>converter</code> <p>Converter to use to convert the rows of the DataFrame (represented as dicts) to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"data/pandas/#edsnlp.data.pandas.to_pandas","title":"Writing to a Pandas DataFrame","text":"<p><code>edsnlp.data.to_pandas</code> writes a list of documents as a pandas table.</p>"},{"location":"data/pandas/#edsnlp.data.pandas.to_pandas--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_pandas([doc], converter=\"omop\")\n</code></pre>"},{"location":"data/pandas/#edsnlp.data.pandas.to_pandas--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>dtypes</code> <p>Dictionary of column names to dtypes. This is passed to <code>pd.DataFrame.astype</code>.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/pandas/#importing-entities-from-a-pandas-dataframe","title":"Importing entities from a Pandas DataFrame","text":"<p>If you have a dataframe with entities (e.g., <code>note_nlp</code> in OMOP), you must join it with the dataframe containing the raw text (e.g., <code>note</code> in OMOP) to obtain a single dataframe with the entities next to the raw text. For instance, the second <code>note_nlp</code> dataframe that we will name <code>note_nlp_df</code>.</p> note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol ... ... ... ... ... ... <pre><code>df = (\n    note_df\n    .set_index(\"note_id\")\n    .join(\n        note_nlp_df\n        .set_index('note_id')\n        .groupby(level=0)\n        .apply(pd.DataFrame.to_dict, orient='records')\n        .rename(\"entities\")\n    )\n).reset_index()\n</code></pre> note_id note_text note_datetime entities 0 Le patient... 2021-10-23 <code>[{\"note_nlp_id\": 0, \"start_char\": 46, ...]</code> ... ... ... ..."},{"location":"data/parquet/","title":"Parquet","text":"TLDR <pre><code>import edsnlp\n\ndocs = edsnlp.data.from_parquet(df, converter=\"omop\")\ndocs = docs.map_pipeline(nlp)\nres = edsnlp.data.to_parquet(docs, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to parquet files.</p> <p>As an example, imagine that we have the following document that uses the OMOP schema (parquet files are not actually stored as human-readable text, but this is for the sake of the example):</p> data.pq<pre><code>{ \"note_id\": 0, \"note_text\": \"Le patient ...\", \"note_datetime\": \"2021-10-23\", \"entities\": [...] }\n{ \"note_id\": 1, \"note_text\": \"Autre doc ...\", \"note_datetime\": \"2022-12-24\", \"entities\": [] }\n...\n</code></pre> <p>You could also have multiple parquet files in a directory, the reader will read them all.</p>"},{"location":"data/parquet/#edsnlp.data.parquet.read_parquet","title":"Reading Parquet files","text":"<p>The ParquetReader (or <code>edsnlp.data.read_parquet</code>) reads a directory of parquet files (or a single file) and yields documents.</p>"},{"location":"data/parquet/#edsnlp.data.parquet.read_parquet--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_parquet(\"path/to/parquet\", converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_parquet</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.read_parquet(\"path/to/parquet\", converter=\"omop\"))\n</code></pre>"},{"location":"data/parquet/#edsnlp.data.parquet.read_parquet--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the parquet files (will recursively look for files in subdirectories). Supports any filesystem supported by pyarrow.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>read_in_worker</code> <p>Whether to read the files in the worker or in the main process.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the parquet rows of the data source to Doc objects These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"data/parquet/#edsnlp.data.parquet.write_parquet","title":"Writing Parquet files","text":"<p><code>edsnlp.data.write_parquet</code> writes a list of documents as a parquet dataset.</p>"},{"location":"data/parquet/#edsnlp.data.parquet.write_parquet--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_parquet([doc], \"path/to/parquet\")\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_parquet</code> will raise an error if the directory already exists and contains parquet files. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"data/parquet/#edsnlp.data.parquet.write_parquet--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>path</code> <p>Path to the directory containing the parquet files (will recursively look for files in subdirectories). Supports any filesystem supported by pyarrow.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>num_rows_per_file</code> <p>The maximum number of documents to write in each parquet file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1024</code> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>write_in_worker</code> <p>Whether to write the files in the workers or in the main process.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>accumulate</code> <p>Whether to accumulate the results sent to the writer by workers until the batch is full or the writer is finalized. If False, each file will not be larger than the size of the batches it receives. This option requires that the writer is finalized before the end of the processing, which may not be compatible with some backends, such as <code>spark</code>.</p> <p>If <code>write_in_worker</code> is True, documents will be accumulated in each worker but not across workers, therefore leading to a larger number of files.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before writing them as Parquet rows. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/polars/","title":"Polars","text":"TLDR <pre><code>import edsnlp\n\ndocs = edsnlp.data.from_polars(df, converter=\"omop\")\ndocs = docs.map_pipeline(nlp)\nres = edsnlp.data.to_polars(docs, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to Polars DataFrames.</p> <p>As an example, imagine that we have the following OMOP dataframe (we'll name it <code>note_df</code>)</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23"},{"location":"data/polars/#edsnlp.data.polars.from_polars","title":"Reading from a Polars Dataframe","text":"<p>The PolarsReader (or <code>edsnlp.data.from_polars</code>) handles reading from a table and yields documents. At the moment, only entities and attributes are loaded. Relations and events are not supported.</p>"},{"location":"data/polars/#edsnlp.data.polars.from_polars--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_polars(df, nlp=nlp, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_polars</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_polars(df, converter=\"omop\"))\n</code></pre>"},{"location":"data/polars/#edsnlp.data.polars.from_polars--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>Polars object</p> <p> TYPE: <code>Union[DataFrame, LazyFrame]</code> </p> <code>converter</code> <p>Converter to use to convert the rows of the DataFrame (represented as dicts) to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"data/polars/#edsnlp.data.polars.to_polars","title":"Writing to a Polars DataFrame","text":"<p><code>edsnlp.data.to_polars</code> writes a list of documents as a polars dataframe.</p>"},{"location":"data/polars/#edsnlp.data.polars.to_polars--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_polars([doc], converter=\"omop\")\n</code></pre>"},{"location":"data/polars/#edsnlp.data.polars.to_polars--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>dtypes</code> <p>Dictionary of column names to dtypes. This is passed to the schema parameter of <code>pl.from_dicts</code>.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/spark/","title":"Spark","text":"TLDR <pre><code>import edsnlp\n\n\ndocs = edsnlp.data.from_spark(df, converter=\"omop\")\ndocs = docs.map_pipeline(nlp)\nres = edsnlp.data.to_spark(docs, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to Spark DataFrames.</p> <p>As an example, imagine that we have the following OMOP dataframe (we'll name it <code>note_df</code>)</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23"},{"location":"data/spark/#edsnlp.data.spark.from_spark","title":"Reading from a Spark Dataframe","text":"<p>The SparkReader (or <code>edsnlp.data.from_spark</code>) reads a pyspark (or koalas) DataFrame and yields documents. At the moment, only entities and span attributes are loaded.</p>"},{"location":"data/spark/#edsnlp.data.spark.from_spark--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_spark(note_df, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_spark</code> returns a LazyCollection To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_spark(note_df, converter=\"omop\"))\n</code></pre>"},{"location":"data/spark/#edsnlp.data.spark.from_spark--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The DataFrame to read.</p> <p> </p> <code>converter</code> <p>Converter to use to convert the rows of the DataFrame to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"data/spark/#edsnlp.data.spark.to_spark","title":"Writing to a Spark DataFrame","text":"<p><code>edsnlp.data.to_spark</code> converts a list of documents into a Spark DataFrame, usually one row per document, unless the converter returns a list in which case each entry of the resulting list will be stored in its own row.</p>"},{"location":"data/spark/#edsnlp.data.spark.to_spark--example","title":"Example","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n\nnote_df = sql('''\n    select note_id, note_text from note\n    where note_text is not null\n    limit 500\n''')\n\ndocs = edsnlp.data.from_spark(note_df, converter=\"omop\")\n\ndocs = nlp.pipe(docs)\n\nres = edsnlp.data.to_spark(docs, converter=\"omop\")\n\nres.show()\n</code></pre> <p>Mac OS X</p> <p>If you are using Mac OS X, you may need to set the following environment variable (see this thread) to run pyspark:</p> <pre><code>import os\nos.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n</code></pre>"},{"location":"data/spark/#edsnlp.data.spark.to_spark--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>dtypes</code> <p>The schema to use for the DataFrame.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>show_dtypes</code> <p>Whether to print the inferred schema (only if <code>dtypes</code> is None).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/spark/#importing-entities-from-a-spark-dataframe","title":"Importing entities from a Spark DataFrame","text":"<p>If you have a dataframe with entities (e.g., <code>note_nlp</code> in OMOP), you must join it with the dataframe containing the raw text (e.g., <code>note</code> in OMOP) to obtain a single dataframe with the entities next to the raw text. For instance, the second <code>note_nlp</code> dataframe that we will name <code>note_nlp</code>.</p> note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol <pre><code>import pyspark.sql.functions as F\n\ndf = note_df.join(\n    note_nlp_df\n    .groupBy(\"note_id\")\n    .agg(\n        F.collect_list(\n            F.struct(\n                F.col(\"note_nlp_id\"),\n                F.col(\"start_char\"),\n                F.col(\"end_char\"),\n                F.col(\"note_nlp_source_value\")\n            )\n        ).alias(\"entities\")\n    ), \"note_id\", \"left\")\n</code></pre>"},{"location":"data/standoff/","title":"BRAT and Standoff","text":"TLDR <pre><code>import edsnlp\n\ndoc_iterator = edsnlp.data.from_standoff(path)\nres = edsnlp.data.write_standoff(docs, path)\n</code></pre> <p>You can easily integrate BRAT into your project by using EDS-NLP's BRAT reader and writer.</p> <p>BRAT annotations are in the standoff format. Consider the following document:</p> doc.txt<pre><code>Le patient est admis pour une pneumopathie au coronavirus.\nOn lui prescrit du parac\u00e9tamol.\n</code></pre> <p>Brat annotations are stored in a separate file formatted as follows:</p> doc.ann<pre><code>T1  Patient 4 11    patient\nT2  Disease 31 58   pneumopathie au coronavirus\nT3  Drug 79 90  parac\u00e9tamol\n</code></pre>"},{"location":"data/standoff/#edsnlp.data.standoff.read_standoff","title":"Reading Standoff files","text":"<p>The BratReader (or <code>edsnlp.data.read_standoff</code>) reads a directory of BRAT files and yields documents. At the moment, only entities and attributes are loaded. Relations  and events are not supported.</p>"},{"location":"data/standoff/#edsnlp.data.standoff.read_standoff--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_standoff(\"path/to/brat/directory\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_standoff</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list :</p> <pre><code>docs = list(edsnlp.data.read_standoff(\"path/to/brat/directory\"))\n</code></pre> <p>True/False attributes</p> <p>Boolean values are not supported by the BRAT editor, and are stored as empty (key: empty value) if true, and not stored otherwise. This means that False values will not be assigned to attributes by default, which can be problematic when deciding if an entity is negated or not : is the entity not negated, or has the negation attribute not been annotated ?</p> <p>To avoid this issue, you can use the <code>bool_attributes</code> argument to specify which attributes should be considered as boolean when reading a BRAT dataset. These attributes will be assigned a value of <code>True</code> if they are present, and <code>False</code> otherwise.</p> <pre><code>doc_iterator = edsnlp.data.read_standoff(\n    \"path/to/brat/directory\",\n    # Mapping from 'BRAT attribute name' to 'Doc attribute name'\n    span_attributes={\"Negation\": \"negated\"},\n    bool_attributes=[\"negated\"],  # Missing values will be set to False\n)\n</code></pre>"},{"location":"data/standoff/#edsnlp.data.standoff.read_standoff--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the BRAT files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   LazyCollection.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> </p> <code>notes_as_span_attribute</code> <p>If set, the AnnotatorNote annotations will be concatenated and stored in a span attribute with this name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>split_fragments</code> <p>Whether to split the fragments into separate spans or not. If set to False, the fragments will be concatenated into a single span.</p> <p> TYPE: <code>bool</code> </p> <code>keep_ipynb_checkpoints</code> <p>Whether to keep the files that are in the <code>.ipynb_checkpoints</code> directory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>keep_txt_only_docs</code> <p>Whether to keep the <code>.txt</code> files that do not have corresponding <code>.ann</code> files.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>'standoff'</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"data/standoff/#edsnlp.data.standoff.write_standoff","title":"Writing Standoff files","text":"<p><code>edsnlp.data.write_standoff</code> writes a list of documents using the BRAT/Standoff format in a directory. The BRAT files will be named after the <code>note_id</code> attribute of the documents, and subdirectories will be created if the name contains <code>/</code> characters.</p>"},{"location":"data/standoff/#edsnlp.data.standoff.write_standoff--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_standoff([doc], \"path/to/brat/directory\")\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_standoff</code> will raise an error if the directory already exists and contains files with <code>.a*</code> or <code>.txt</code> suffixes. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"data/standoff/#edsnlp.data.standoff.write_standoff--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>path</code> <p>Path to the directory containing the BRAT files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>span_getter</code> <p>The span getter to use when listing the spans that will be exported as BRAT entities. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extension. By default, no attribute will be exported.</p> <p> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects. Defaults to the \"standoff\" format converter.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>'standoff'</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/","title":"Pipes overview","text":"<p>EDS-NLP provides easy-to-use pipeline components (aka pipes).</p>"},{"location":"pipes/#available-components","title":"Available components","text":"CoreQualifiersMiscellaneousNERTrainable <p>See the Core components overview for more information.</p> Component Description <code>eds.normalizer</code> Non-destructive input text normalisation <code>eds.sentences</code> Better sentence boundary detection <code>eds.matcher</code> A simple yet powerful entity extractor <code>eds.terminology</code> A simple yet powerful terminology matcher <code>eds.contextual_matcher</code> A conditional entity extractor <code>eds.endlines</code> An unsupervised model to classify each end line <p>See the Qualifiers overview for more information.</p> Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based medical history detection <p>See the Miscellaneous components overview for more information.</p> Component Description <code>eds.dates</code> Date extraction and normalisation <code>eds.consultation_dates</code> Identify consultation dates <code>eds.measurements</code> Measure extraction and normalisation <code>eds.sections</code> Section detection <code>eds.reason</code> Rule-based hospitalisation reason detection <code>eds.tables</code> Tables detection <p>See the NER overview for more information.</p> Component Description <code>eds.covid</code> A COVID mentions detector <code>eds.charlson</code> A Charlson score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.elston_ellis</code> An Elston &amp; Ellis code extractor <code>eds.emergency_priority</code> A priority score extractor <code>eds.emergency_ccmu</code> A CCMU score extractor <code>eds.emergency_gemsa</code> A GEMSA score extractor <code>eds.tnm</code> A TNM score extractor <code>eds.adicap</code> A ADICAP codes extractor <code>eds.drugs</code> A drug mentions extractor <code>eds.cim10</code> A CIM10 terminology matcher <code>eds.umls</code> An UMLS terminology matcher <code>eds.ckd</code> CKD extractor <code>eds.copd</code> COPD extractor <code>eds.cerebrovascular_accident</code> Cerebrovascular accident extractor <code>eds.congestive_heart_failure</code> Congestive heart failure extractor <code>eds.connective_tissue_disease</code> Connective tissue disease extractor <code>eds.dementia</code> Dementia extractor <code>eds.diabetes</code> Diabetes extractor <code>eds.hemiplegia</code> Hemiplegia extractor <code>eds.leukemia</code> Leukemia extractor <code>eds.liver_disease</code> Liver disease extractor <code>eds.lymphoma</code> Lymphoma extractor <code>eds.myocardial_infarction</code> Myocardial infarction extractor <code>eds.peptic_ulcer_disease</code> Peptic ulcer disease extractor <code>eds.peripheral_vascular_disease</code> Peripheral vascular disease extractor <code>eds.solid_tumor</code> Solid tumor extractor <code>eds.alcohol</code> Alcohol consumption extractor <code>eds.tobacco</code> Tobacco consumption extractor <p>See the Trainable components overview for more information.</p> Name Description <code>eds.transformer</code> Embed text with a transformer model <code>eds.text_cnn</code> Contextualize embeddings with a CNN <code>eds.span_pooler</code> A span embedding component that aggregates word embeddings <code>eds.ner_crf</code> A trainable component to extract entities <code>eds.span_classifier</code> A trainable component for multi-class multi-label span classification <code>eds.span_linker</code> A trainable entity linker (i.e. to a list of concepts) <p>You can add them to your pipeline by simply calling <code>add_pipe</code>, for instance:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.tnm())\n</code></pre>"},{"location":"pipes/#basic-architecture","title":"Basic architecture","text":"<p>Most components provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library:</p> <ol> <li>Implement a normaliser (see <code>eds.normalizer</code>)</li> <li>Add an entity recognition component (eg the simple but powerful <code>eds.matcher</code> component)</li> <li>Add zero or more entity qualification components, such as <code>eds.negation</code>, <code>eds.family</code> or <code>eds.hypothesis</code>. These qualifiers typically help detect false-positives.</li> </ol>"},{"location":"pipes/#extraction-components","title":"Extraction components","text":"<p>Extraction components (matchers, the date detector or NER components, for instance) keep their results to the <code>doc.ents</code> and <code>doc.spans</code> attributes directly.</p> <p>By default, some components do not write their output to <code>doc.ents</code>, such as the <code>eds.sections</code> matcher. This is mainly due to the fact that, since <code>doc.ents</code> cannot contain overlapping entities, we filter spans and keep the largest one by default. Since sections usually cover large spans of text, storing them in ents would remove every other overlapping entities.</p>"},{"location":"pipes/#entity-tagging","title":"Entity tagging","text":"<p>Moreover, most components declare extensions, on the <code>Doc</code>, <code>Span</code> and/or <code>Token</code> objects.</p> <p>These extensions are especially useful for qualifier components, but can also be used by other components to persist relevant information. For instance, the <code>eds.dates</code> component declares a <code>span._.date</code> extension to store a normalised version of each detected date.</p>"},{"location":"pipes/architecture/","title":"Basic Architecture","text":"<p>Most pipes provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library:</p> <ol> <li>Implement a normaliser (see <code>eds.normalizer</code>)</li> <li>Add an entity recognition component (eg the simple but powerful <code>eds.matcher</code>)</li> <li>Add zero or more entity qualification components, such as <code>eds.negation</code>, <code>eds.family</code> or <code>eds.hypothesis</code>. These qualifiers typically help detect false-positives.</li> </ol>"},{"location":"pipes/architecture/#scope","title":"Scope","text":"<p>Since the basic usage of EDS-NLP components is to qualify entities, most pipes can function in two modes:</p> <ol> <li>Annotation of the extracted entities (this is the default). To increase throughput, only pre-extracted entities (found in <code>doc.ents</code>) are processed.</li> <li>Full-text, token-wise annotation. This mode is activated by setting the <code>on_ents_only</code> parameter to <code>False</code>.</li> </ol> <p>The possibility to do full-text annotation implies that one could use the pipes the other way around, eg detecting all negations once and for all in an ETL phase, and reusing the results consequently. However, this is not the intended use of the library, which aims to help researchers downstream as a standalone application.</p>"},{"location":"pipes/architecture/#result-persistence","title":"Result persistence","text":"<p>Depending on their purpose (entity extraction, qualification, etc), EDS-NLP pipes write their results to <code>Doc.ents</code>, <code>Doc.spans</code> or in a custom attribute.</p>"},{"location":"pipes/architecture/#extraction-pipes","title":"Extraction pipes","text":"<p>Extraction pipes (matchers, the date detector or NER pipes, for instance) keep their results to the <code>Doc.ents</code> attribute directly.</p> <p>Note that spaCy prohibits overlapping entities within the <code>Doc.ents</code> attribute. To circumvent this limitation, we filter spans, and keep all discarded entities within the <code>discarded</code> key of the <code>Doc.spans</code> attribute.</p> <p>Some pipes write their output to the <code>Doc.spans</code> dictionary. We enforce the following doctrine:</p> <ul> <li>Should the pipe extract entities that are directly informative (typically the output of the <code>eds.matcher</code> component), said entities are stashed in the <code>Doc.ents</code> attribute.</li> <li>On the other hand, should the entity be useful to another pipe, but less so in itself (eg the output of the <code>eds.sections</code> or <code>eds.dates</code> component), it will be stashed in a specific key within the <code>Doc.spans</code> attribute.</li> </ul>"},{"location":"pipes/architecture/#entity-tagging","title":"Entity tagging","text":"<p>Moreover, most pipes declare spaCy extensions, on the <code>Doc</code>, <code>Span</code> and/or <code>Token</code> objects.</p> <p>These extensions are especially useful for qualifier pipes, but can also be used by other pipes to persist relevant information. For instance, the <code>eds.dates</code> pipeline component:</p> <ol> <li>Populates <code>Doc.spans[\"dates\"]</code></li> <li>For each detected item, keeps the normalised date in <code>Span._.date</code></li> </ol> <ol></ol>"},{"location":"pipes/core/","title":"Core Components","text":"<p>This section deals with \"core\" functionalities offered by EDS-NLP:</p> <ul> <li>Generic matchers against regular expressions and list of terms</li> <li>Text cleaning</li> <li>Sentence boundaries detection</li> </ul>"},{"location":"pipes/core/#available-components","title":"Available components","text":"Component Description <code>eds.normalizer</code> Non-destructive input text normalisation <code>eds.sentences</code> Better sentence boundary detection <code>eds.matcher</code> A simple yet powerful entity extractor <code>eds.terminology</code> A simple yet powerful terminology matcher <code>eds.contextual_matcher</code> A conditional entity extractor <code>eds.endlines</code> An unsupervised model to classify each end line"},{"location":"pipes/core/contextual-matcher/","title":"Contextual Matcher","text":"<p>During feature extraction, it may be necessary to search for additional patterns in their neighborhood, namely:</p> <ul> <li>patterns to discard irrelevant entities</li> <li>patterns to enrich these entities and store some information</li> </ul> <p>For example, to extract mentions of non-benign cancers, we need to discard all extractions that mention \"benin\" in their immediate neighborhood. Although such a filtering is feasible using a regular expression, it essentially requires modifying each of the regular expressions.</p> <p>The ContextualMatcher allows to perform this extraction in a clear and concise way.</p>"},{"location":"pipes/core/contextual-matcher/#the-configuration-file","title":"The configuration file","text":"<p>The whole ContextualMatcher pipeline component is basically defined as a list of pattern dictionaries. Let us see step by step how to build such a list using the example stated just above.</p>"},{"location":"pipes/core/contextual-matcher/#a-finding-mentions-of-cancer","title":"a. Finding mentions of cancer","text":"<p>To do this, we can build either a set of <code>terms</code> or a set of <code>regex</code>. <code>terms</code> will be used to search for exact matches in the text. While less flexible, it is faster than using regex. In our case we could use the following lists (which are of course absolutely not exhaustives):</p> <pre><code>terms = [\n    \"cancer\",\n    \"tumeur\",\n]\n\nregex = [\n    \"adeno(carcinom|[\\s-]?k)\",\n    \"neoplas\",\n    \"melanom\",\n]\n</code></pre> <p>Maybe we want to exclude mentions of benign cancers:</p> <pre><code>benign = \"benign|benin\"\n</code></pre>"},{"location":"pipes/core/contextual-matcher/#b-find-mention-of-a-stage-and-extract-its-value","title":"b. Find mention of a stage and extract its value","text":"<p>For this we will forge a RegEx with one capturing group (basically a pattern enclosed in parentheses):</p> <pre><code>stage = \"stade (I{1,3}V?|[1234])\"\n</code></pre> <p>This will extract stage between 1 and 4</p> <p>We can add a second regex to try to capture if the cancer is in a metastasis stage or not:</p> <pre><code>metastase = \"(metasta)\"\n</code></pre>"},{"location":"pipes/core/contextual-matcher/#c-the-complete-configuration","title":"c. The complete configuration","text":"<p>We can now put everything together:</p> <pre><code>cancer = dict(\n    source=\"Cancer solide\",\n    regex=regex,\n    terms=terms,\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=benign,\n        window=3,\n    ),\n    assign=[\n        dict(\n            name=\"stage\",\n            regex=stage,\n            window=(-10, 10),\n            replace_entity=False,\n            reduce_mode=None,\n        ),\n        dict(\n            name=\"metastase\",\n            regex=metastase,\n            window=10,\n            replace_entity=False,\n            reduce_mode=\"keep_last\",\n        ),\n    ],\n)\n</code></pre> <p>Here the configuration consists of a single dictionary. We might want to also include lymphoma in the matcher:</p> <pre><code>lymphome = dict(\n    source=\"Lymphome\",\n    regex=[\"lymphom\", \"lymphangio\"],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=[\"hodgkin\"],  # (1)\n        window=3,\n    ),\n)\n</code></pre> <ol> <li>We are excluding \"Lymphome de Hodgkin\" here</li> </ol> <p>In this case, the configuration can be concatenated in a list:</p> <pre><code>patterns = [cancer, lymphome]\n</code></pre>"},{"location":"pipes/core/contextual-matcher/#available-parameters-for-more-flexibility","title":"Available parameters for more flexibility","text":"<p>3 main parameters can be used to refine how entities will be formed</p>"},{"location":"pipes/core/contextual-matcher/#the-include_assigned-parameter","title":"The <code>include_assigned</code> parameter","text":"<p>Following the previous example, you might want your extracted entities to include, if found, the cancer stage and the metastasis status. This can be achieved by setting <code>include_assigned=True</code> in the pipe configuration.</p> <p>For instance, from the sentence \"Le patient a un cancer au stade 3\", the extracted entity will be:</p> <ul> <li>\"cancer\" if <code>include_assigned=False</code></li> <li>\"cancer au stade 3\" if <code>include_assigned=True</code></li> </ul>"},{"location":"pipes/core/contextual-matcher/#the-reduce_mode-parameter","title":"The <code>reduce_mode</code> parameter","text":"<p>It may happen that an assignment matches more than once. For instance, in the (nonsensical) sentence \"Le patient a un cancer au stade 3 et au stade 4\", both \"stade 3\" and \"stade 4\" will be matched by the <code>stage</code> assign key. Depending on your use case, you may want to keep all the extractions, or just one.</p> <ul> <li>If <code>reduce_mode=None</code> (default), all extractions are kept in a list</li> <li>If <code>reduce_mode=\"keep_first\"</code>, only the extraction closest to the main matched entity will be kept (in this case, it would be \"stade 3\" since it is the closest to \"cancer\")</li> <li>If <code>reduce_mode==\"keep_last\"</code>, only the furthest extraction is kept.</li> </ul>"},{"location":"pipes/core/contextual-matcher/#the-replace_entity-parameter","title":"The <code>replace_entity</code> parameter","text":"<p>This parameter can be se to <code>True</code> only for a single assign key per dictionary. This limitation comes from the purpose of this parameter: If set to <code>True</code>, the corresponding <code>assign</code> key will be returned as the entity, instead of the match itself. For clarity, let's take the same sentence \"Le patient a un cancer au stade 3\" as an example:</p> <ul> <li>if <code>replace_entity=True</code> in the <code>stage</code> assign key, then the extracted entity will be \"stade 3\" instead of \"cancer\"</li> <li>if <code>replace_entity=False</code> for every assign key, the returned entity will be, as expected, \"cancer\"</li> </ul> <p>Please notice that with <code>replace_entity</code> set to True, if the correponding assign key matches nothing, the entity will be discarded.</p>"},{"location":"pipes/core/contextual-matcher/#examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(\n    eds.contextual_matcher(\n        patterns=patterns,\n        label=\"cancer\",\n    ),\n)\n</code></pre> <p>Let us see what we can get from this pipeline with a few examples</p> Simple matchExclusion ruleExtracting additional infos <pre><code>txt = \"Le patient a eu un cancer il y a 5 ans\"\ndoc = nlp(txt)\nent = doc.ents[0]\n\nent.label_\n# Out: cancer\n\nent._.source\n# Out: Cancer solide\n\nent.text, ent.start, ent.end\n# Out: ('cancer', 5, 6)\n</code></pre> <p>Let us check that when a benign mention is present, the extraction is excluded:</p> <pre><code>txt = \"Le patient a eu un cancer relativement b\u00e9nin il y a 5 ans\"\ndoc = nlp(txt)\n\ndoc.ents\n# Out: ()\n</code></pre> <p>All informations extracted from the provided <code>assign</code> configuration can be found in the <code>assigned</code> attribute under the form of a dictionary:</p> <pre><code>txt = \"Le patient a eu un cancer de stade 3.\"\ndoc = nlp(txt)\n\ndoc.ents[0]._.assigned\n# Out: {'stage': '3'}\n</code></pre> <p>However, most of the configuration is provided in the <code>patterns</code> key, as a pattern dictionary or a list of pattern dictionaries</p>"},{"location":"pipes/core/contextual-matcher/#the-pattern-dictionary","title":"The pattern dictionary","text":""},{"location":"pipes/core/contextual-matcher/#description","title":"Description","text":"<p>A patterr is a nested dictionary with the following keys:</p> <code>source</code><code>regex</code><code>regex_attr</code><code>terms</code><code>exclude</code><code>assign</code> <p>A label describing the pattern</p> <p>A single Regex or a list of Regexes</p> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p>A single term or a list of terms (for exact matches)</p> <p>A dictionary (or list of dictionaries) to define exclusion rules. Exclusion rules are given as Regexes, and if a match is found in the surrounding context of an extraction, the extraction is removed. Each dictionary should have the following keys:</p> <code>window</code><code>regex</code> <p>Size of the context to use (in number of words). You can provide the window as:</p> <ul> <li>A positive integer, in this case the used context will be taken after the extraction</li> <li>A negative integer, in this case the used context will be taken before the extraction</li> <li>A tuple of integers <code>(start, end)</code>, in this case the used context will be the snippet from <code>start</code> tokens before the extraction to <code>end</code> tokens after the extraction</li> </ul> <p>A single Regex or a list of Regexes.</p> <p>A dictionary to refine the extraction. Similarily to the <code>exclude</code> key, you can provide a dictionary to use on the context before and after the extraction.</p> <code>name</code><code>window</code><code>regex</code><code>replace_entity</code><code>reduce_mode</code> <p>A name (string)</p> <p>Size of the context to use (in number of words). You can provide the window as:</p> <ul> <li>A positive integer, in this case the used context will be taken after the extraction</li> <li>A negative integer, in this case the used context will be taken before the extraction</li> <li>A tuple of integers <code>(start, end)</code>, in this case the used context will be the snippet from <code>start</code> tokens before the extraction to <code>end</code> tokens after the extraction</li> </ul> <p>A dictionary where keys are labels and values are Regexes with a single capturing group</p> <p>If set to <code>True</code>, the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph</p> <p>Set how multiple assign matches are handled. See the documentation of the <code>reduce_mode</code> parameter</p>"},{"location":"pipes/core/contextual-matcher/#a-full-pattern-dictionary-example","title":"A full pattern dictionary example","text":"<pre><code>dict(\n    source=\"AVC\",\n    regex=[\n        \"accidents? vasculaires? cerebr\",\n    ],\n    terms=\"avc\",\n    regex_attr=\"NORM\",\n    exclude=[\n        dict(\n            regex=[\"service\"],\n            window=3,\n        ),\n        dict(\n            regex=[\" a \"],\n            window=-2,\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"neo\",\n            regex=r\"(neonatal)\",\n            expand_entity=True,\n            window=3,\n        ),\n        dict(\n            name=\"trans\",\n            regex=\"(transitoire)\",\n            expand_entity=True,\n            window=3,\n        ),\n        dict(\n            name=\"hemo\",\n            regex=r\"(hemorragique)\",\n            expand_entity=True,\n            window=3,\n        ),\n        dict(\n            name=\"risk\",\n            regex=r\"(risque)\",\n            expand_entity=False,\n            window=-3,\n        ),\n    ],\n)\n</code></pre>"},{"location":"pipes/core/contextual-matcher/#edsnlp.pipes.core.contextual_matcher.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>patterns</code> <p>The configuration dictionary</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>expand</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_name</code> <p>Deprecated, use <code>label</code> instead. The label to assign to the matched entities</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>The label to assign to the matched entities</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"pipes/core/contextual-matcher/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/core/endlines/","title":"Endlines","text":"<p>The <code>eds.endlines</code> component classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document.</p> <p>Behind the scenes, it uses a <code>endlinesmodel</code> instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al., 2016.</p> <ol><li><p><p>Zweigenbaum P., Grouin C. and Lavergne T., 2016. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (End-of-line classification with no supervision). https://aclanthology.org/2016.jeptalnrecital-poster.7</p></p></li></ol>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--training","title":"Training","text":"<pre><code>import edsnlp\nfrom edsnlp.pipes.core.endlines.model import EndLinesModel\n\nnlp = edsnlp.blank(\"eds\")\n\ntexts = [\n\"\"\"\nLe patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010\nFumeur, il est arret\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diabete\n\"\"\",\n\"\"\"\nJ'aime le\nfromage...\n\"\"\",\n]\n\ndocs = list(nlp.pipe(texts))\n\n# Train and predict an EndLinesModel\nendlines = EndLinesModel(nlp=nlp)\n\ndf = endlines.fit_and_predict(docs)\ndf.head()\n\nPATH = \"/tmp/path_to_save\"\nendlines.save(PATH)\n</code></pre>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = edsnlp.blank(\"eds\")\n\nPATH = \"/tmp/path_to_save\"\nnlp.add_pipe(eds.endlines(model_path=PATH))\n\ndocs = list(nlp.pipe(texts))\n\ndoc_exemple = docs[1]\n\ndoc_exemple.ents = tuple(\n    Span(doc_exemple, token.i, token.i + 1, \"excluded\")\n    for token in doc_exemple\n    if token.tag_ == \"EXCLUDED\"\n)\n\ndisplacy.render(doc_exemple, style=\"ent\", options={\"colors\": {\"space\": \"red\"}})\n</code></pre>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.endlines</code> pipeline declares one extension, on both <code>Span</code> and <code>Token</code> objects. The <code>end_line</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the new line is an end line character. Otherwise, it is set to <code>False</code> if the new line is classified as a space.</p> <p>The pipeline also sets the <code>excluded</code> custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> </p> <code>model_path</code> <p>Path to trained model. If None, it will use a default model</p> <p> TYPE: <code>Optional[Union[str, EndLinesModel]]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.endlines</code> pipeline was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al., 2016.</p>"},{"location":"pipes/core/matcher/","title":"Matcher","text":"<p>EDS-NLP simplifies the matching process by exposing a <code>eds.matcher</code> component that can match on terms or regular expressions.</p>"},{"location":"pipes/core/matcher/#edsnlp.pipes.core.matcher.factory.create_component--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    patient=\"patient\",  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n        term_matcher=\"exact\",\n        term_matcher_config={},\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become  the label of the extracted entities. Dictionary values are either a single  expression or a list of expressions that match the concept.</p>"},{"location":"pipes/core/matcher/#edsnlp.pipes.core.matcher.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'matcher'</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>span_setter</code> <p>How to set the spans in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"pipes/core/matcher/#edsnlp.pipes.core.matcher.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/core/normalizer/","title":"Normalisation","text":"<p>The normalisation scheme used by EDS-NLP adheres to the non-destructive doctrine. In other words,</p> <pre><code>nlp(text).text == text\n</code></pre> <p>is always true.</p> <p>To achieve this, the input text is never modified. Instead, our normalisation strategy focuses on two axes:</p> <ol> <li>Only the <code>NORM</code> and <code>tag_</code> attributes are modified by the <code>normalizer</code> pipeline component ;</li> <li>Pipes (e.g., <code>pollution</code>) can mark tokens as excluded by setting the extension <code>Token.tag_</code> to <code>EXCLUDED</code> or as space by setting the extension <code>Token.tag_</code> to <code>SPACE</code>.    It enables downstream matchers to skip excluded tokens.</li> </ol> <p>The normaliser can act on the input text in five dimensions :</p> <ol> <li>Move the text to lowercase.</li> <li>Remove accents. We use a deterministic approach to avoid modifying the character-length of the text, which helps for RegEx matching.</li> <li>Normalize apostrophes and quotation marks, which are often coded using special characters.</li> <li>Detect spaces and new lines and mark them as such (to be skipped later)</li> <li>Detect tokens in pollutions patterns and mark them as such (to be skipped later)</li> </ol> <p>Note</p> <p>We recommend you also add an end-of-line classifier to remove excess new line characters (introduced by the PDF layout).</p> <p>We provide a <code>endlines</code> pipeline component, which requires training an unsupervised model. Refer to the dedicated page for more information.</p>"},{"location":"pipes/core/normalizer/#usage","title":"Usage","text":"<p>The normalisation is handled by the single <code>eds.normalizer</code> pipeline component. The following code snippet is complete, and should run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\n\n# Notice the special character used for the apostrophe and the quotes\ntext = \"Le patient est admis \u00e0 l'h\u00f4pital le 23 ao\u00fbt 2021 pour une douleur \u02baaffreuse\u201d \u00e0 l`estomac.\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: le patient est admis a l'hopital le 23 aout 2021 pour une douleur \"affreuse\" a l'estomac.\n</code></pre>"},{"location":"pipes/core/normalizer/#utilities","title":"Utilities","text":"<p>To simplify the use of the normalisation output, we provide the <code>get_text</code> utility function. It computes the textual representation for a <code>Span</code> or <code>Doc</code> object.</p> <p>Moreover, every span exposes a <code>normalized_variant</code> extension getter, which computes the normalised representation of an entity on the fly.</p>"},{"location":"pipes/core/normalizer/#configuration","title":"Configuration","text":"<p>The pipeline component can be configured using the following parameters :</p> <p>Normalisation pipeline. Modifies the <code>NORM</code> attribute, acting on five dimensions :</p> <ul> <li><code>lowercase</code>: using the default <code>NORM</code></li> <li><code>accents</code>: deterministic and fixed-length normalisation of accents.</li> <li><code>quotes</code>: deterministic and fixed-length normalisation of quotation marks.</li> <li><code>spaces</code>: \"removal\" of spaces tokens (via the tag_ attribute).</li> <li><code>pollution</code>: \"removal\" of pollutions (via the tag_ attribute).</li> </ul> <p>options: only_parameters: true</p>"},{"location":"pipes/core/normalizer/#edsnlp.pipes.core.normalizer.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'normalizer'</code> </p> <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>accents</code> <p><code>Accents</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>quotes</code> <p><code>Quotes</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>spaces</code> <p><code>Spaces</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> configuration object.</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> Source code in <code>edsnlp/pipes/core/normalizer/factory.py</code> <pre><code>@registry.factory.register(\n    \"eds.normalizer\",\n    assigns=[\"token.norm\", \"token.tag\"],\n    deprecated=[\"normalizer\"],\n)\ndef create_component(\n    nlp: PipelineProtocol,\n    name: str = \"normalizer\",\n    *,\n    accents: Union[bool, Dict[str, Any]] = True,\n    lowercase: Union[bool, Dict[str, Any]] = True,\n    quotes: Union[bool, Dict[str, Any]] = True,\n    spaces: Union[bool, Dict[str, Any]] = True,\n    pollution: Union[bool, Dict[str, Any]] = True,\n) -&gt; Normalizer:\n\"\"\"\n    Normalisation pipeline. Modifies the `NORM` attribute,\n    acting on five dimensions :\n\n    - `lowercase`: using the default `NORM`\n    - `accents`: deterministic and fixed-length normalisation of accents.\n    - `quotes`: deterministic and fixed-length normalisation of quotation marks.\n    - `spaces`: \"removal\" of spaces tokens (via the tag_ attribute).\n    - `pollution`: \"removal\" of pollutions (via the tag_ attribute).\n\n    Parameters\n    ----------\n    nlp: PipelineProtocol\n        The pipeline object.\n    name : str\n        The component name.\n    lowercase : bool\n        Whether to remove case.\n    accents : Union[bool, Dict[str, Any]]\n        `Accents` configuration object\n    quotes : Union[bool, Dict[str, Any]]\n        `Quotes` configuration object\n    spaces : Union[bool, Dict[str, Any]]\n        `Spaces` configuration object\n    pollution : Union[bool, Dict[str, Any]]\n        Optional `Pollution` configuration object.\n    \"\"\"\n\n    if accents:\n        accents = AccentsConverter(\n            nlp=nlp,\n            name=\"eds.accents\",\n            **(accents if accents is not True else {}),\n        )\n\n    if quotes:\n        quotes = QuotesConverter(\n            nlp=nlp,\n            name=\"eds.quotes\",\n            **(quotes if quotes is not True else {}),\n        )\n\n    if spaces:\n        spaces = SpacesTagger(\n            nlp=nlp,\n            name=\"eds.spaces\",\n            **(spaces if spaces is not True else {}),\n        )\n\n    if pollution:\n        config = dict(default_enabled_pollution)\n        if isinstance(pollution, dict):\n            pollution = (\n                pollution if \"pollution\" not in pollution else pollution[\"pollution\"]\n            )\n            config.update(pollution)\n        pollution = PollutionTagger(\n            nlp=nlp,\n            name=\"eds.pollution\",\n            pollution=config,\n        )\n\n    normalizer = Normalizer(\n        nlp=nlp,\n        name=name,\n        lowercase=lowercase,\n        accents=accents or None,\n        quotes=quotes or None,\n        pollution=pollution or None,\n        spaces=spaces or None,\n    )\n\n    return normalizer\n</code></pre>"},{"location":"pipes/core/normalizer/#pipes","title":"Pipes","text":"<p>Let's review each subcomponent.</p>"},{"location":"pipes/core/normalizer/#lowercase","title":"Lowercase","text":"<p>The <code>eds.lowercase</code> pipeline component transforms every token to lowercase. It is not configurable.</p> <p>Consider the following example :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=True,\n        accents=False,\n        quotes=False,\n        spaces=False,\n        pollution=False,\n    ),\n)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: pneumopathie \u00e0 nbnbwbwbnbwbnbnbnbwbw 'coronavirus'\n</code></pre>"},{"location":"pipes/core/normalizer/#accents","title":"Accents","text":"<p>The <code>eds.accents</code> pipeline component removes accents. To avoid edge cases, the component uses a specified list of accentuated characters and their unaccented representation, making it more predictable than using a library such as <code>unidecode</code>.</p> <p>Consider the following example :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=False,\n        accents=True,\n        quotes=False,\n        spaces=False,\n        pollution=False,\n    ),\n)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus'\n</code></pre>"},{"location":"pipes/core/normalizer/#apostrophes-and-quotation-marks","title":"Apostrophes and quotation marks","text":"<p>Apostrophes and quotation marks can be encoded using unpredictable special characters. The <code>eds.quotes</code> component transforms every such special character to <code>'</code> and <code>\"</code>, respectively.</p> <p>Consider the following example :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=False,\n        accents=False,\n        quotes=True,\n        spaces=False,\n        pollution=False,\n    ),\n)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW 'coronavirus'\n</code></pre>"},{"location":"pipes/core/normalizer/#spaces","title":"Spaces","text":"<p>This is not truly a normalisation component, but this allows us to detect spaces tokens ahead of the other components and encode it as using the <code>tag_</code> attribute for fast matching.</p> <p>Tip</p> <p>This component and its <code>spaces</code> option should be enabled if you ever set   <code>ignore_space_tokens</code> parameter token to True in a downstream component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=False,\n        accents=False,\n        quotes=False,\n        spaces=True,\n        pollution=False,\n    ),\n)\n\ndoc = nlp(\"Phrase    avec des espaces \\n et un retour \u00e0 la ligne\")\n[t.tag_ for t in doc]\n# Out: ['', 'SPACE', '', '', '', 'SPACE', '', '', '', '', '', '']\n</code></pre>"},{"location":"pipes/core/normalizer/#pollution","title":"Pollution","text":"<p>The pollution pipeline component uses a set of regular expressions to detect pollutions (irrelevant non-medical text that hinders text processing). Corresponding tokens are marked as excluded (by setting <code>Token._.excluded</code> to <code>True</code>), enabling the use of the phrase matcher.</p> <p>Consider the following example :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=False,\n        accents=True,\n        quotes=False,\n        spaces=False,\n        pollution=True,\n    ),\n)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus'\n\nget_text(doc, attr=\"TEXT\", ignore_excluded=True)\n# Out: Pneumopathie \u00e0 `coronavirus'\n</code></pre> <p>This example above shows that the normalisation scheme works on two axes: non-destructive text modification and exclusion of tokens. The two are independent: a matcher can use the <code>NORM</code> attribute but keep excluded tokens, and conversely, match on <code>TEXT</code> while ignoring excluded tokens.</p> <p></p>"},{"location":"pipes/core/normalizer/#types-of-pollution","title":"Types of pollution","text":"<p>Pollution can come in various forms in clinical texts. We provide a small set of possible pollutions patterns that can be enabled or disabled as needed.</p> <p>For instance, if we consider biology tables as pollution, we only need to instantiate the <code>normalizer</code> pipe as follows:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        pollution=dict(biology=True),\n    ),\n)\n</code></pre> Type Description Example Included by default <code>information</code> Footnote present in a lot of notes, providing information to the patient about the use of its data \"L'AP-HP collecte vos donn\u00e9es administratives \u00e0 des fins ...\" <code>True</code> <code>bars</code> Barcodes wrongly parsed as text \"...NBNbWbWbNbWbNBNbNbWbW...\" <code>True</code> <code>biology</code> Parsed biology results table. It often contains disease names that often leads to false positives with NER pipes. \"...\u00a6UI/L \u00a620 \u00a6 \u00a6 \u00a620-70 Polyarthrite rhumato\u00efde Facteur rhumatoide \u00a6UI/mL \u00a6 \u00a6&lt;10 \u00a6 \u00a6 \u00a6 \u00a60-14...\" <code>False</code> <code>doctors</code> List of doctor names and specialities, often found in left-side note margins. Also source of potential false positives. \"... Dr ABC - Diab\u00e8te/Endocrino ...\" <code>True</code> <code>web</code> Webpages URL and email adresses. Also source of potential false positives. \"... www.vascularites.fr ...\" <code>True</code> <code>coding</code> Subsection containing ICD-10 codes along with their description. Also source of potential false positives. \"... (2) E112 + Oeil (2) E113 + Neuro (2) E114 D\u00e9mence (2) F03 MA (2) F001+G301 DCL G22+G301 Vasc (2) ...\" <code>False</code> <code>footer</code> Footer of new page \"2/2Pat : NOM Prenom le 2020/01/01 IPP 12345678 Intitul\u00e9 RCP : Urologie HMN le \" <code>True</code>"},{"location":"pipes/core/normalizer/#custom-pollution","title":"Custom pollution","text":"<p>If you want to exclude specific patterns, you can provide them as a RegEx (or a list of Regexes). For instance, to consider text between \"AAA\" and \"ZZZ\" as pollution you might use:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        pollution=dict(custom_pollution=r\"AAA.*ZZZ\"),\n    ),\n)\n</code></pre>"},{"location":"pipes/core/normalizer/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.normalizer</code> pipeline component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/core/sentences/","title":"Sentences","text":"<p>The <code>eds.sentences</code> matcher provides an alternative to spaCy's default <code>sentencizer</code>, aiming to overcome some of its limitations.</p> <p>Indeed, the <code>sentencizer</code> merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our <code>eds.sentences</code> component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances.</p> <p>Moreover, the <code>eds.sentences</code> component use the output of the <code>eds.normalizer</code> and <code>eds.endlines</code> output by default when these components are added to the pipeline.</p>"},{"location":"pipes/core/sentences/#edsnlp.pipes.core.sentences.factory.create_component--examples","title":"Examples","text":"EDS-NLPspaCy sentencizer <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())  # same as nlp.add_pipe(\"eds.sentences\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\"\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out:  &lt;\\s&gt;\n# Out: &lt;s&gt; Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"sentencizer\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\"\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <p>Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings.</p>"},{"location":"pipes/core/sentences/#edsnlp.pipes.core.sentences.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sentences'</code> </p> <code>punct_chars</code> <p>Punctuation characters.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_endlines</code> <p>Whether to use endlines prediction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires the upstream <code>eds.normalizer</code> pipe).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/core/sentences/#edsnlp.pipes.core.sentences.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sentences</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/core/terminology/","title":"Terminology","text":"<p>EDS-NLP simplifies the terminology matching process by exposing a <code>eds.terminology</code> pipeline that can match on terms or regular expressions.</p> <p>The terminology matcher is very similar to the generic matcher, although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies.</p> <p>There are some key differences:</p> <ol> <li>It labels every matched entity to the same value, provided to the pipeline</li> <li>The keys provided in the <code>regex</code> and <code>terms</code> dictionaries are used as the    <code>kb_id_</code> of the entity, which handles fine-grained labelling</li> </ol> <p>For instance, a terminology matcher could detect every drug mention under the top-level label <code>drug</code>, and link each individual mention to a given drug through its <code>kb_id_</code> attribute.</p> <ol></ol>"},{"location":"pipes/core/terminology/#edsnlp.pipes.core.terminology.factory.create_component--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    flu=[\"grippe saisonni\u00e8re\"],  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.terminology(\n        label=\"disease\",\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p>"},{"location":"pipes/core/terminology/#edsnlp.pipes.core.terminology.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become the <code>kb_id_</code> of the extracted entities. Dictionary values are either a single expression or a list of expressions that match the concept (see example).</p>"},{"location":"pipes/core/terminology/#edsnlp.pipes.core.terminology.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.terminology</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/","title":"Miscellaneous","text":"<p>This section regroups components that extract information that can be used by other components, but have little medical value in itself.</p> <p>For instance, the date detection and normalisation pipeline falls in this category.</p>"},{"location":"pipes/misc/#available-components","title":"Available components","text":"Component Description <code>eds.dates</code> Date extraction and normalisation <code>eds.consultation_dates</code> Identify consultation dates <code>eds.measurements</code> Measure extraction and normalisation <code>eds.sections</code> Section detection <code>eds.reason</code> Rule-based hospitalisation reason detection <code>eds.tables</code> Tables detection"},{"location":"pipes/misc/consultation-dates/","title":"Consultation dates","text":"<p>The <code>eds.consultation-dates</code> matcher consists of two main parts:</p> <ul> <li>A matcher which finds mentions of consultation events (more details below)</li> <li>A date parser (see the corresponding pipe) that links a date to those events</li> </ul>"},{"location":"pipes/misc/consultation-dates/#edsnlp.pipes.misc.consultation_dates.factory.create_component--examples","title":"Examples","text":"<p>Note</p> <p>The matcher has been built to run on consultation notes (<code>CR-CONS</code> at APHP), so please filter accordingly before proceeding.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=True,\n        accents=True,\n        quotes=True,\n        pollution=False,\n    ),\n)\nnlp.add_pipe(eds.consultation_dates())\n\ntext = \"\"\"\nXXX\nObjet : Compte-Rendu de Consultation du 03/10/2018.\nXXX\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"consultation_dates\"]\n# Out: [Consultation du 03/10/2018]\n\ndoc.spans[\"consultation_dates\"][0]._.consultation_date.to_datetime()\n# Out: DateTime(2018, 10, 3, 0, 0, 0)\n</code></pre>"},{"location":"pipes/misc/consultation-dates/#edsnlp.pipes.misc.consultation_dates.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.consultation_dates</code> pipeline declares one extension on the <code>Span</code> object: the <code>consultation_date</code> attribute, which is a Python <code>datetime</code> object.</p>"},{"location":"pipes/misc/consultation-dates/#edsnlp.pipes.misc.consultation_dates.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>consultation_mention</code> <p>List of RegEx for consultation mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains terms directly referring to consultations, such as \"Consultation du...\" or \"Compte rendu du...\". This list is the only one enabled by default since it is fairly precise and not error-prone.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>True</code> </p> <code>town_mention</code> <p>List of RegEx for all AP-HP hospitals' towns mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \"Paris, le 13 d\u00e9cembre 2015\". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p> <code>document_date_mention</code> <p>List of RegEx for document date.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains expressions mentioning the date of creation/edition of a document, such as \"Date du rapport: 13/12/2015\" or \"Sign\u00e9 le 13/12/2015\". Like <code>town_mention</code> patterns, it has a high recall but is prone to errors since document date and consultation date aren't necessary similar.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/misc/consultation-dates/#edsnlp.pipes.misc.consultation_dates.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.consultation_dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/dates/","title":"Dates","text":"<p>The <code>eds.dates</code> matcher detects and normalize dates within a medical document. We use simple regular expressions to extract date mentions.</p>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--scope","title":"Scope","text":"<p>The <code>eds.dates</code> pipeline finds absolute (eg <code>23/08/2021</code>) and relative (eg <code>hier</code>, <code>la semaine derni\u00e8re</code>) dates alike. It also handles mentions of duration.</p> Type Example <code>absolute</code> <code>3 mai</code>, <code>03/05/2020</code> <code>relative</code> <code>hier</code>, <code>la semaine derni\u00e8re</code> <code>duration</code> <code>pendant quatre jours</code> <p>See the tutorial for a presentation of a full pipeline featuring the <code>eds.dates</code> component.</p>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nimport datetime\nimport pytz\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n\ndates = doc.spans[\"dates\"]\ndates\n# Out: [23 ao\u00fbt 2021, il y a un an, mai 1995]\n\ndates[0]._.date.to_datetime()\n# Out: 2021-08-23T00:00:00+02:00\n\ndates[1]._.date.to_datetime()\n# Out: None\n\nnote_datetime = datetime.datetime(2021, 8, 27, tzinfo=pytz.timezone(\"Europe/Paris\"))\ndoc._.note_datetime = note_datetime\n\ndates[1]._.date.to_datetime()\n# Out: 2020-08-27T00:00:00+02:00\n\ndate_2_output = dates[2]._.date.to_datetime(\n    note_datetime=note_datetime,\n    infer_from_context=True,\n    tz=\"Europe/Paris\",\n    default_day=15,\n)\ndate_2_output\n# Out: 1995-05-15T00:00:00+02:00\n\ndoc.spans[\"durations\"]\n# Out: [pendant une semaine]\n</code></pre> <p>Example on a collection of documents stored in the OMOP schema :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# with cols \"note_id\", \"note_text\" and optionally \"note_datetime\"\nmy_omop_df = ...\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates(as_ents=True))\ndocs = edsnlp.data.from_pandas(my_omop_df)\ndocs = docs.map_pipeline(nlp)\ndocs = docs.to_pandas(\n    converter=\"ents\",\n    span_attributes={\"date.datetime\": \"datetime\"},\n)\nprint(docs)\n# note_id  start  end label lexical_variant span_type datetime\n# ...\n</code></pre>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.dates</code> pipeline declares two extensions on the <code>Span</code> object:</p> <ul> <li>the <code>span._.date</code> attribute of a date contains a parsed version of the date.</li> <li>the <code>span._.duration</code> attribute of a duration contains a parsed version of the   duration.</li> </ul> <p>As with other components, you can use the <code>span._.value</code> attribute to get either the parsed date or the duration depending on the span.</p>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the pipeline component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dates'</code> </p> <code>absolute</code> <p>List of regular expressions for absolute dates.</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>relative</code> <p>List of regular expressions for relative dates (eg <code>hier</code>, <code>la semaine prochaine</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>List of regular expressions for durations (eg <code>pendant trois mois</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>false_positive</code> <p>List of regular expressions for false positive (eg phone numbers, etc).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for dates in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matched dates with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a date overlaps a span from <code>span_getter</code> (e.g. a date extracted   by a machine learning model), return the <code>span_getter</code> span instead, and   assign all the parsed information (<code>._.date</code> / <code>._.duration</code>) to it. Otherwise   don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> and <code>merge_mode</code> instead. Whether to look on dates in the whole document or in specific sentences:</p> <ul> <li>If <code>True</code>: Only look in the sentences of each entity in doc.ents</li> <li>If False: Look in the whole document</li> <li>If given a string <code>key</code> or list of string: Only look in the sentences of   each entity in <code>doc.spans[key]</code></li> </ul> <p> TYPE: <code>Union[bool, str, Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>detect_periods</code> <p>Whether to detect periods (experimental)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>detect_time</code> <p>Whether to detect time inside dates</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>period_proximity_threshold</code> <p>Max number of words between two dates to extract a period.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>as_ents</code> <p>Deprecated, use span_setter instead. Whether to treat dates as entities</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>LOWER</code> </p> <code>date_label</code> <p>Label to use for dates</p> <p> TYPE: <code>str</code> DEFAULT: <code>date</code> </p> <code>duration_label</code> <p>Label to use for durations</p> <p> TYPE: <code>str</code> DEFAULT: <code>duration</code> </p> <code>period_label</code> <p>Label to use for periods</p> <p> TYPE: <code>str</code> DEFAULT: <code>period</code> </p> <code>span_setter</code> <p>How to set matches in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'dates': ['date'], 'durations': ['duration'], ...</code> </p>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/measurements/","title":"Measurements","text":"<p>The <code>eds.measurements</code> matcher detects and normalizes numerical measurements within a medical document.</p> <p>Warning</p> <p>The <code>measurements</code> pipeline is still in active development and has not been rigorously validated. If you come across a measurement expression that goes undetected, please file an issue !</p>"},{"location":"pipes/misc/measurements/#edsnlp.pipes.misc.measurements.factory.create_component--scope","title":"Scope","text":"<p>The <code>eds.measurements</code> matcher can extract simple (e.g. <code>3cm</code>) measurements. It can also detect elliptic enumerations (eg <code>32, 33 et 34kg</code>) of measurements of the same type and split the measurements accordingly.</p> <p>The normalized value can then be accessed via the <code>span._.{measure_name}</code> attribute, for instance <code>span._.size</code> or <code>span._.weight</code> and be converted on the fly to a desired unit. Like for other components, the <code>span._.value</code> extension can also be used to access the normalized value for any measurement span.</p> <p>The current matcher annotates the following measurements out of the box:</p> Measurement name Example <code>size</code> <code>1m50</code>, <code>1.50m</code> <code>weight</code> <code>12kg</code>, <code>1kg300</code> <code>bmi</code> <code>BMI: 24</code>, <code>24 kg.m-2</code> <code>volume</code> <code>2 cac</code>, <code>8ml</code>"},{"location":"pipes/misc/measurements/#edsnlp.pipes.misc.measurements.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.measurements(\n        measurements=[\"size\", \"weight\", \"bmi\"],\n        extract_ranges=True,\n    ),\n)\n\ntext = \"\"\"\nLe patient est admis hier, fait 1m78 pour 76kg.\nLes deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm.\nBMI: 24.\n\nLe nodule fait entre 1 et 1.5 cm\n\"\"\"\n\ndoc = nlp(text)\n\nmeasurements = doc.spans[\"measurements\"]\n\nmeasurements\n# Out: [1m78, 76kg, 1,2, 2.4mm, 24, entre 1 et 1.5 cm]\n\nmeasurements[0]\n# Out: 1m78\n\nstr(measurements[0]._.size), str(measurements[0]._.value)\n# Out: ('1.78 m', '1.78 m')\n\nmeasurements[0]._.value.cm\n# Out: 178.0\n\nmeasurements[2]\n# Out: 1,2\n\nstr(measurements[2]._.value)\n# Out: '1.2 mm'\n\nstr(measurements[2]._.value.mm)\n# Out: 1.2\n\nmeasurements[4]\n# Out: 24\n\nstr(measurements[4]._.value)\n# Out: '24 kg_per_m2'\n\nstr(measurements[4]._.value.kg_per_m2)\n# Out: 24\n\nstr(measurements[5]._.value)\n# Out: 1-1.5 cm\n</code></pre> <p>To extract all sizes in centimeters, and average range measurements, you can use the following snippet:</p> <pre><code>sizes = [\n    sum(item.cm for item in m._.value) / len(m._.value)\n    for m in doc.spans[\"measurements\"]\n    if m.label_ == \"size\"\n]\nsizes\n# Out: [178.0, 0.12, 0.24, 1.25]\n</code></pre>"},{"location":"pipes/misc/measurements/#edsnlp.pipes.misc.measurements.factory.create_component--customization","title":"Customization","text":"<p>You can declare custom measurements by altering the patterns:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.measurements(\n        measurements={\n            \"my_custom_surface_measurement\": {\n                # This measurement unit is homogenous to square meters\n                \"unit\": \"m2\",\n                # Handle cases like \"surface: 1.8\" (implied m2),\n                # vs \"surface: 50\" (implied cm2)\n                \"unitless_patterns\": [\n                    {\n                        \"terms\": [\"surface\", \"aire\"],\n                        \"ranges\": [\n                            {\"unit\": \"m2\", \"min\": 0, \"max\": 9},\n                            {\"unit\": \"cm2\", \"min\": 10, \"max\": 100},\n                        ],\n                    }\n                ],\n            },\n        }\n    ),\n)\n</code></pre>"},{"location":"pipes/misc/measurements/#edsnlp.pipes.misc.measurements.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.measurements</code> pipeline declares its extensions dynamically, depending on the <code>measurements</code> parameter: each measurement gets its own extension, and is assigned to a different span group.</p>"},{"location":"pipes/misc/measurements/#edsnlp.pipes.misc.measurements.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> </p> <code>measurements</code> <p>A mapping from measure names to MsrConfig Each measure's configuration has the following shape: <pre><code>{\n  # the unit (e.g. \"kg\"),\n  \"unit\": str,\n  \"unitless_patterns\": {\n    # preceding trigger terms\n    \"terms\": List[str],\n    # unitless ranges -&gt; unit patterns\n    \"ranges\": List[\n      {\"min\": int, \"max\": int, \"unit\": str},\n      {\"min\": int, \"unit\": str},\n      ...,\n    ],\n    ...\n  }\n}\n</code></pre></p> <p> TYPE: <code>Union[str, List[Union[str, MsrConfig]], Dict[str, MsrConfig]]</code> DEFAULT: <code>['weight', 'size', 'bmi', 'volume']</code> </p> <code>number_terms</code> <p>A mapping of numbers to their lexical variants</p> <p> DEFAULT: <code>{'0.125': ['\u215b'], '0.16666666': ['\u2159'], '0.2': ['...</code> </p> <code>stopwords</code> <p>A list of stopwords that do not matter when placed between a unitless trigger and a number</p> <p> DEFAULT: <code>['par', 'sur', 'de', 'a', ',', 'et']</code> </p> <code>unit_divisors</code> <p>A list of terms used to divide two units (like: m / s)</p> <p> DEFAULT: <code>['/', 'par']</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to exclude pollution patterns when matching in the text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>compose_units</code> <p>Whether to compose units (like \"m/s\" or \"m.s-1\")</p> <p> DEFAULT: <code>True</code> </p> <code>extract_ranges</code> <p>Whether to extract ranges (like \"entre 1 et 2 cm\")</p> <p> DEFAULT: <code>False</code> </p> <code>range_patterns</code> <p>A list of \"{FROM} xx {TO} yy\" patterns to match range measurements</p> <p> DEFAULT: <code>[('De', '\u00e0'), ('De', 'a'), ('de', '\u00e0'), ('de', ...</code> </p> <code>after_snippet_limit</code> <p>Maximum word distance after to link a part of a measurement after its number</p> <p> DEFAULT: <code>6</code> </p> <code>before_snippet_limit</code> <p>Maximum word distance after to link a part of a measurement before its number</p> <p> DEFAULT: <code>10</code> </p> <code>span_setter</code> <p>How to set the spans in the document. By default, each measurement will be assigned to its own span group (using either the \"name\" field of the config, or the key if you passed a dict), and to the \"measurements\" group.</p> <p> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for measurements in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matches with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a match overlaps a span from <code>span_getter</code> (e.g. a match   extracted by a machine learning model), return the <code>span_getter</code> span   instead, and assign all the parsed information (<code>._.date</code> / <code>._.duration</code>)   to it. Otherwise, don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p>"},{"location":"pipes/misc/measurements/#edsnlp.pipes.misc.measurements.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.measurements</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/reason/","title":"Reasons","text":"<p>The <code>eds.reason</code> matcher uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.</p>"},{"location":"pipes/misc/reason/#edsnlp.pipes.misc.reason.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = edsnlp.blank(\"eds\")\n\n# Extraction of entities\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        )\n    ),\n)\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.reason(use_sections=True))\ndoc = nlp(text)\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nreason._.is_reason\n# Out: True\n\nentities = reason._.ents_reason\nentities\n# Out: [asthme]\n\nentities[0].label_\n# Out: 'respiratoire'\n\nent = entities[0]\nent._.is_reason\n# Out: True\n</code></pre>"},{"location":"pipes/misc/reason/#edsnlp.pipes.misc.reason.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.reason</code> pipeline adds the key <code>reasons</code> to <code>doc.spans</code> and declares one extension, on the <code>Span</code> objects called <code>ents_reason</code>.</p> <p>The <code>ents_reason</code> extension is a list of named entities that overlap the <code>Span</code>, typically entities found in upstream components like <code>matcher</code>.</p> <p>It also declares the boolean extension <code>is_reason</code>. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.</p>"},{"location":"pipes/misc/reason/#edsnlp.pipes.misc.reason.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'reason'</code> </p> <code>reasons</code> <p>Reason patterns</p> <p> TYPE: <code>Dict[str, Union[List[str], str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Default token attribute to use to build the text to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>use_sections</code> <p>Whether or not use the <code>sections</code> matcher to improve results.</p> <p> TYPE: <code>(bool)</code> DEFAULT: <code>False</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/misc/reason/#edsnlp.pipes.misc.reason.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reason</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/sections/","title":"Sections","text":"<p>The <code>eds.sections</code> component extracts section titles from clinical documents. A \"section\" is then defined as the span of text between two titles.</p> <p>Here is the list of sections that are currently targeted :</p> <ul> <li><code>allergies</code></li> <li><code>ant\u00e9c\u00e9dents</code></li> <li><code>ant\u00e9c\u00e9dents familiaux</code></li> <li><code>traitements entr\u00e9e</code></li> <li><code>conclusion</code></li> <li><code>conclusion entr\u00e9e</code></li> <li><code>habitus</code></li> <li><code>correspondants</code></li> <li><code>diagnostic</code></li> <li><code>donn\u00e9es biom\u00e9triques entr\u00e9e</code></li> <li><code>examens</code></li> <li><code>examens compl\u00e9mentaires</code></li> <li><code>facteurs de risques</code></li> <li><code>histoire de la maladie</code></li> <li><code>actes</code></li> <li><code>motif</code></li> <li><code>prescriptions</code></li> <li><code>traitements sortie</code></li> <li><code>evolution</code></li> <li><code>modalites sortie</code></li> <li><code>vaccinations</code></li> <li><code>introduction</code></li> </ul> <p>Remarks :</p> <ul> <li>section <code>introduction</code> corresponds to the span of text between the header   \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document)   and the title of the following detected section</li> <li>this matcher works well for hospitalization summaries (CRH), but not necessarily   for all types of documents (in particular for emergency or scan summaries   CR-IMAGERIE)</li> </ul> <p>Experimental</p> <p>Should you rely on <code>eds.sections</code> for critical downstream tasks, make sure to validate the results to make sure that the component works in your case.</p>"},{"location":"pipes/misc/sections/#edsnlp.pipes.misc.sections.factory.create_component--examples","title":"Examples","text":"<p>The following snippet detects section titles. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\n\ntext = \"\"\"\nCRU du 10/09/2021\nMotif :\nPatient admis pour suspicion de COVID\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"section_titles\"]\n# Out: [Motif]\n</code></pre>"},{"location":"pipes/misc/sections/#edsnlp.pipes.misc.sections.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.sections</code> matcher adds two fields to the <code>doc.spans</code> attribute :</p> <ol> <li>The <code>section_titles</code> key contains the list of all section titles extracted using    the list declared in the <code>terms.py</code> module.</li> <li>The <code>sections</code> key contains a list of sections, ie spans of text between two    section titles (or the last title and the end of the document).</li> </ol> <p>If the document has entities before calling this matcher an attribute <code>section</code> is added to each entity.</p>"},{"location":"pipes/misc/sections/#edsnlp.pipes.misc.sections.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>sections</code> <p>Dictionary of terms to look for.</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'allergies': ['allergies'], 'ant\u00e9c\u00e9dents': ['a...</code> </p> <code>attr</code> <p>Default attribute to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>add_patterns</code> <p>Whether add update patterns to match start / end of lines</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/misc/sections/#edsnlp.pipes.misc.sections.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sections</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/tables/","title":"Tables","text":"<p>The <code>eds.tables</code> matcher detects tables in a documents.</p>"},{"location":"pipes/misc/tables/#edsnlp.pipes.misc.tables.factory.create_component--examples","title":"Examples","text":"<p><pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.tables())\n\ntext = \"\"\"\nSERVICE\nMEDECINE INTENSIVE \u2013\nREANIMATION\nR\u00e9animation / Surveillance Continue\nM\u00e9dicale\n\nCOMPTE RENDU D'HOSPITALISATION du 05/06/2020 au 10/06/2020\nMadame DUPONT Marie, n\u00e9e le 16/05/1900, \u00e2g\u00e9e de 20 ans, a \u00e9t\u00e9 hospitalis\u00e9e en\nr\u00e9animation du 05/06/1920 au 10/06/1920 pour intoxication m\u00e9dicamenteuse volontaire.\n\nExamens compl\u00e9mentaires\nH\u00e9matologie\nNum\u00e9ration\nLeucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\nH\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\nH\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\nH\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\nVGM \u00a6fL \u00a694.4 + \u00a679.6-94\nTCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\nCCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\nPlaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\nVMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\nSur le plan neurologique : Devant la persistance d'une confusion \u00e0 distance de\nl'intoxication au\n...\n\n2/2Pat : &lt;NOM&gt; &lt;Prenom&gt;|F |&lt;date&gt; | &lt;ipp&gt; |Intitul\u00e9 RCP\n\"\"\"\n\ndoc = nlp(text)\n\n# A table span\ntable = doc.spans[\"tables\"][0]\n\n# Leucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\n# H\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\n# H\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\n# H\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\n# VGM \u00a6fL \u00a694.4 + \u00a679.6-94\n# TCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\n# CCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\n# Plaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\n# VMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\n# Convert span to Pandas table\ndf = table._.to_pd_table(\n    as_spans=False,  # set True to set the table cells as spans instead of strings\n    header=False,  # set True to use the first row as header\n    index=False,  # set True to use the first column as index\n)\ntype(df)\n# Out: pandas.core.frame.DataFrame\n</code></pre> The pandas DataFrame:</p> 0 1 2 3 0 Leucocytes x10*9/L 4.97 4.09-11 1 H\u00e9maties x10*12/L 4.68 4.53-5.79 2 H\u00e9moglobine g/dL 14.8 13.4-16.7 3 H\u00e9matocrite % 44.2 39.2-48.6 4 VGM fL 94.4 + 79.6-94 5 TCMH pg 31.6 27.3-32.8 6 CCMH g/dL 33.5 32.4-36.3 7 Plaquettes x10*9/L 191 172-398 8 VMP fL 11.5 + 7.4-10.8"},{"location":"pipes/misc/tables/#edsnlp.pipes.misc.tables.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.tables</code> pipeline declares the <code>span._.to_pd_table()</code> Span extension. This function returns a parsed pandas version of the table.</p>"},{"location":"pipes/misc/tables/#edsnlp.pipes.misc.tables.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'tables'</code> </p> <code>tables_pattern</code> <p>The regex pattern to identify tables. The key of dictionary should be <code>tables</code></p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> <code>sep_pattern</code> <p>The regex pattern to identify the separator pattern. Used when calling <code>to_pd_table</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/misc/tables/#edsnlp.pipes.misc.tables.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tables</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/ner/","title":"Named Entity Recognition Components","text":"<p>We provide several Named Entity Recognition (NER) components. Named Entity Recognition is the task of identifying short relevant spans of text, named entities, and classifying them into pre-defined categories. In the case of clinical documents, these entities can be scores, disorders, behaviors, codes, dates, measurements, etc.</p>"},{"location":"pipes/ner/#edsnlp.pipes.base.SpanSetterArg","title":"Span setters: where are stored extracted entities ?","text":"<p>A component assigns entities to a document by adding them to the <code>doc.ents</code> or <code>doc.spans[group]</code> attributes. <code>doc.ents</code> only supports non overlapping entities, therefore, if two entities overlap, the longest one will be kept. <code>doc.spans[group]</code> on the other hand, can contain overlapping entities. To control where entities are added, you can use the <code>span_setter</code> argument in any of these component.</p> <p>Valid values for the <code>span_setter</code> argument of a component can be :</p> <ul> <li>a (doc, matches) -&gt; None callable</li> <li>a span group name</li> <li>a list of span group names</li> <li>a dict of group name to True or list of labels</li> </ul> <p>The group name <code>\"ents\"</code> is a special case, and will add the matches to <code>doc.ents</code></p>"},{"location":"pipes/ner/#edsnlp.pipes.base.SpanSetterArg--examples","title":"Examples","text":"<ul> <li><code>span_setter=[\"ents\", \"ckd\"]</code> will add the matches to both <code>doc.ents</code> and <code>doc.spans[\"ckd\"]</code>. It is equivalent to <code>{\"ents\": True, \"ckd\": True}</code>.</li> <li><code>span_setter={\"ents\": [\"foo\", \"bar\"]}</code> will add the matches with label \"foo\" and \"bar\" to <code>doc.ents</code>.</li> <li><code>span_setter=\"ents\"</code> will add all matches only to <code>doc.ents</code>.</li> <li><code>span_setter=\"ckd\"</code> will add all matches only to <code>doc.spans[\"ckd\"]</code>.</li> </ul>"},{"location":"pipes/ner/#available-components","title":"Available components","text":"Component Description <code>eds.covid</code> A COVID mentions detector <code>eds.charlson</code> A Charlson score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.elston_ellis</code> An Elston &amp; Ellis code extractor <code>eds.emergency_priority</code> A priority score extractor <code>eds.emergency_ccmu</code> A CCMU score extractor <code>eds.emergency_gemsa</code> A GEMSA score extractor <code>eds.tnm</code> A TNM score extractor <code>eds.adicap</code> A ADICAP codes extractor <code>eds.drugs</code> A drug mentions extractor <code>eds.cim10</code> A CIM10 terminology matcher <code>eds.umls</code> An UMLS terminology matcher <code>eds.ckd</code> CKD extractor <code>eds.copd</code> COPD extractor <code>eds.cerebrovascular_accident</code> Cerebrovascular accident extractor <code>eds.congestive_heart_failure</code> Congestive heart failure extractor <code>eds.connective_tissue_disease</code> Connective tissue disease extractor <code>eds.dementia</code> Dementia extractor <code>eds.diabetes</code> Diabetes extractor <code>eds.hemiplegia</code> Hemiplegia extractor <code>eds.leukemia</code> Leukemia extractor <code>eds.liver_disease</code> Liver disease extractor <code>eds.lymphoma</code> Lymphoma extractor <code>eds.myocardial_infarction</code> Myocardial infarction extractor <code>eds.peptic_ulcer_disease</code> Peptic ulcer disease extractor <code>eds.peripheral_vascular_disease</code> Peripheral vascular disease extractor <code>eds.solid_tumor</code> Solid tumor extractor <code>eds.alcohol</code> Alcohol consumption extractor <code>eds.tobacco</code> Tobacco consumption extractor"},{"location":"pipes/ner/adicap/","title":"Adicap","text":"<p>The <code>eds.adicap</code> pipeline component matches the ADICAP codes. It was developped to run on anapathology reports.</p> <p>Document type</p> <p>It was developped to work on anapathology reports. We recommend also to use the <code>eds</code> language (<code>edsnlp.blank(\"eds\")</code>)</p> <p>The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes:</p> Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type <p>The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\".</p> <p>Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor.</p> <p>For further details about the ADICAP code follow this link.</p> <ol><li><p><p>sant\u00e9 A., 2019. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. http://esante.gouv.fr/terminologie-adicap</p></p></li></ol>"},{"location":"pipes/ner/adicap/#edsnlp.pipes.ner.adicap.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.adicap())\n\ntext = \"\"\"\nCOMPTE RENDU D\u2019EXAMEN\n\nAnt\u00e9riorit\u00e9(s) :  NEANT\n\n\nRenseignements cliniques :\nContexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-\nexterne du sein droit. La l\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion\ndu quadrant sup\u00e9ro-externe, \u00e0 l'union des quadrants inf\u00e9rieurs.\n\n\nMacrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants\ninf\u00e9rieurs du sein droit, mesurant 4 mm, class\u00e9e ACR4\n\n14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il\nn'y a pas eu d'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en\ntotalit\u00e9 et coup\u00e9s sur plusieurs niveaux.\nHistologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois\nl\u00e9g\u00e8rement dystrophique avec quelques petits kystes. Il n'y a pas d'hyperplasie\n\u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration tumorale. On note quelques\nsuffusions h\u00e9morragiques focales.\n\nConclusion :\nL\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit.\nAbsence d'atypies ou de prolif\u00e9ration tumorale.\n\nCodification :   BHGS0040\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (BHGS0040,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: adicap\n\nent._.adicap.dict()\n# Out: {'code': 'BHGS0040',\n# 'sampling_mode': 'BIOPSIE CHIRURGICALE',\n# 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION',\n# 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\",\n# 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE',\n# 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE',\n# 'behaviour_type': 'CARACTERES GENERAUX'}\n</code></pre>"},{"location":"pipes/ner/adicap/#edsnlp.pipes.ner.adicap.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adicap'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>([A-Z]\\.?[A-Z]\\.?[A-Z]{2}\\.?(?:\\d{4}|\\d{4}|[A-Z...</code> </p> <code>prefix</code> <p>The regex pattern to use for matching the prefix before ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?i)(codification|adicap)</code> </p> <code>window</code> <p>Number of tokens to look for prefix. It will never go further the start of the sentence</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>adicap</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'adicap': True}</code> </p>"},{"location":"pipes/ner/adicap/#edsnlp.pipes.ner.adicap.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.adicap</code> pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\", sant\u00e9, 2019)</p>"},{"location":"pipes/ner/cim10/","title":"CIM10","text":"<p>The <code>eds.cim10</code> pipeline component extract terms from documents using the CIM10 (French-language ICD) terminology as a reference.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"pipes/ner/cim10/#edsnlp.pipes.ner.cim10.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.cim10(term_matcher=\"simstring\"))\n\ntext = \"Le patient est suivi pour fi\u00e8vres typho\u00efde et paratypho\u00efde.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (fi\u00e8vres typho\u00efde et paratypho\u00efde,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: cim10\n\nent.kb_id_\n# Out: A01\n</code></pre>"},{"location":"pipes/ner/cim10/#edsnlp.pipes.ner.cim10.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cim10'</code> </p> <code>attr</code> <p>The default attribute to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cim10'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cim10': True}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code>"},{"location":"pipes/ner/cim10/#edsnlp.pipes.ner.cim10.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cim10</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/ner/covid/","title":"COVID","text":"<p>The <code>eds.covid</code> pipeline component detects mentions of COVID19.</p>"},{"location":"pipes/ner/covid/#edsnlp.pipes.ner.covid.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n\ntext = \"Le patient est admis pour une infection au coronavirus.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (infection au coronavirus,)\n</code></pre>"},{"location":"pipes/ner/covid/#edsnlp.pipes.ner.covid.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'covid'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'LOWER'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patterns</code> <p>The regex pattern to use</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns</code> </p> <code>label</code> <p>Label to use for matches</p> <p> TYPE: <code>str</code> DEFAULT: <code>'covid'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'covid': True}</code> </p> RETURNS DESCRIPTION <code>GenericMatcher</code>"},{"location":"pipes/ner/covid/#edsnlp.pipes.ner.covid.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.covid</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/ner/drugs/","title":"Drugs","text":"<p>The <code>eds.drugs</code> pipeline component detects mentions of French drugs (brand names and active ingredients) and adds them to <code>doc.ents</code>. Each drug is mapped to an ATC code through the Romedi terminology (Cossin et al., 2019). The ATC classifies drugs into groups.</p> <ol><li><p><p>Cossin S., Lebrun L., Lobre G., Loustau R., Jouhet V., Griffier R., Mougin F., Diallo G. and Thiessard F., 2019. Romedi: An Open Data Source About French Drugs on the Semantic Web. {Studies in Health Technology and Informatics}. 264, pp.79-82. 10.3233/SHTI190187</p></p></li></ol>"},{"location":"pipes/ner/drugs/#edsnlp.pipes.ner.drugs.factory.create_component--examples","title":"Examples","text":"<p>In this example, we are looking for an oral antidiabetic medication (ATC code: A10B).</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.drugs(term_matcher=\"exact\"))\n\ntext = \"Traitement habituel: Kard\u00e9gic, cardensiel (bisoprolol), glucophage, lasilix\"\n\ndoc = nlp(text)\n\ndrugs_detected = [(x.text, x.kb_id_) for x in doc.ents]\n\ndrugs_detected[0]\n# Out: ('Kard\u00e9gic', 'B01AC06')\n\nlen(drugs_detected)\n# Out: 5\n\noral_antidiabetics_detected = list(\n    filter(lambda x: (x[1].startswith(\"A10B\")), drugs_detected)\n)\noral_antidiabetics_detected\n# Out: [('glucophage', 'A10BA02')]\n</code></pre> <p>Glucophage is the brand name of a medication that contains metformine, the first-line medication for the treatment of type 2 diabetes.</p>"},{"location":"pipes/ner/drugs/#edsnlp.pipes.ner.drugs.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'drugs'</code> </p> <code>attr</code> <p>The default attribute to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'drug'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'drug': True}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code>"},{"location":"pipes/ner/drugs/#edsnlp.pipes.ner.drugs.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.drugs</code> pipeline was developed by the IAM team and CHU de Bordeaux's Data Science team.</p>"},{"location":"pipes/ner/tnm/","title":"TNM","text":"<p>The <code>eds.tnm</code> component extracts TNM mentions from clinical documents.</p> <ol><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"pipes/ner/tnm/#edsnlp.pipes.ner.tnm.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.tnm())\n\ntext = \"TNM: pTx N1 M1\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (pTx N1 M1,)\n\nent = doc.ents[0]\nent._.tnm.dict()\n# {'modifier': 'p',\n#  'tumour': None,\n#  'tumour_specification': 'x',\n#  'node': '1',\n#  'node_specification': None,\n#  'metastasis': '1',\n#  'resection_completeness': None,\n#  'version': None,\n#  'version_year': None}\n</code></pre>"},{"location":"pipes/ner/tnm/#edsnlp.pipes.ner.tnm.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tnm'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?:\\b|^)(?&lt;=\\(?(?P&lt;version&gt;uicc|accj|tnm|UICC|A...</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tnm</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tnm': True}</code> </p>"},{"location":"pipes/ner/tnm/#edsnlp.pipes.ner.tnm.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The TNM score is based on the development of S. Priou, B. Rance and E. Kempf (Kempf et al., 2022).</p>"},{"location":"pipes/ner/umls/","title":"UMLS","text":"<p>The <code>eds.umls</code> pipeline component matches the UMLS (Unified Medical Language System from NIH) terminology.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"pipes/ner/umls/#edsnlp.pipes.ner.umls.factory.create_component--examples","title":"Examples","text":"<p><code>eds.umls</code> is an additional module that needs to be setup by:</p> <ol> <li><code>pip install -U umls_downloader</code></li> <li>Signing up for a UMLS Terminology    Services Account. After filling a short form, you will receive your token API    within a few days.</li> <li>Set <code>UMLS_API_KEY</code> locally: <code>export UMLS_API_KEY=your_api_key</code></li> </ol> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.umls())\n\ntext = \"Grosse toux: le malade a \u00e9t\u00e9 mordu par des Amphibiens \" \"sous le genou\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (toux, a, par, Amphibiens, genou)\n\nent = doc.ents[0]\n\nent.label_\n# Out: umls\n\nent._.umls\n# Out: C0010200\n</code></pre> <p>You can easily change the default languages and sources with the <code>pattern_config</code> argument:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# Enable the French and English languages, through the French MeSH and LOINC\npattern_config = dict(languages=[\"FRE\", \"ENG\"], sources=[\"MSHFRE\", \"LNC\"])\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.umls(pattern_config=pattern_config))\n</code></pre> <p>See more options of languages and sources here.</p>"},{"location":"pipes/ner/umls/#edsnlp.pipes.ner.umls.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'umls'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either \"exact\" or \"simstring\"</p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>pattern_config</code> <p>The pattern retriever configuration</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>dict(languages=['FRE'], sources=None)</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'umls'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'umls': True}</code> </p>"},{"location":"pipes/ner/umls/#edsnlp.pipes.ner.umls.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.umls</code> pipeline was developed by AP-HP's Data Science team and INRIA SODA's team.</p>"},{"location":"pipes/ner/behaviors/","title":"Behaviors","text":""},{"location":"pipes/ner/behaviors/#presentation","title":"Presentation","text":"<p>EDS-NLP offers two components to extract behavioral patterns, namely the tobacco and alcohol consumption status. Each component is based on the ContextualMatcher component. Some general considerations about those components:</p> <ul> <li>Extracted entities are stored in <code>doc.ents</code> and <code>doc.spans</code>. For instance, the <code>eds.tobacco</code> component stores matches in <code>doc.spans[\"tobacco\"]</code>.</li> <li>The matched comorbidity is also available under the <code>ent.label_</code> of each match.</li> <li>Matches have an associated <code>_.status</code> attribute taking the value <code>0</code>, <code>1</code>, or <code>2</code>. A corresponding <code>_.detailed_status</code> attribute stores the human-readable status, which can be component-dependent. See each component documentation for more details.</li> <li>Some components add additional information to matches. For instance, the <code>tobacco</code> adds, if relevant, extracted pack-year (= paquet-ann\u00e9e). Those information are available under the <code>ent._.assigned</code> attribute.</li> <li> <p>Those components work on normalized documents. Please use the <code>eds.normalizer</code> pipeline with the following parameters:   <pre><code>nlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\n</code></pre></p> </li> <li> <p>Those components should be used with a qualification pipeline to avoid extracted unwanted matches. At the very least, you can use available rule-based qualifiers (<code>eds.negation</code>, <code>eds.hypothesis</code> and <code>eds.family</code>). Better, a machine learning qualification component was developed and trained specifically for those components. For privacy reason, the model isn't publicly available yet.</p> <p>Use the ML model</p> <p>The model will soon be available in the models catalogue of AP-HP's CDW.</p> </li> </ul>"},{"location":"pipes/ner/behaviors/#usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.tobacco())\nnlp.add_pipe(eds.diabetes())\n\ntext = \"\"\"\nCompte-rendu de consultation.\n\nJe vois ce jour M. SCOTT pour le suivi de sa r\u00e9tinopathie diab\u00e9tique.\nLe patient va bien depuis la derni\u00e8re fois.\nJe le f\u00e9licite pour la poursuite de son sevrage tabagique (toujours \u00e0 10 paquet-ann\u00e9e).\n\nSur le plan de son diab\u00e8te, la glyc\u00e9mie est stable.\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans\n# Out: {\n# 'pollutions': [],\n# 'tobacco': [sevrage tabagique (toujours \u00e0 10 paquet-ann\u00e9e],\n# 'diabetes': [r\u00e9tinopathie diab\u00e9tique, diab\u00e8te]\n# }\n\ntobacco_matches = doc.spans[\"tobacco\"]\ntobacco_matches[0]._.detailed_status\n# Out: \"ABSTINENCE\" #\n\ntobacco_matches[0]._.assigned[\"PA\"]  # paquet-ann\u00e9e\n# Out: 10 # (1)\n\n\ndiabetes = doc.spans[\"diabetes\"]\n(diabetes[0]._.detailed_status, diabetes[1]._.detailed_status)\n# Out: ('WITH_COMPLICATION', 'WITHOUT_COMPLICATION') # (2)\n</code></pre> <ol> <li>Here we see an example of additional information that can be extracted</li> <li>Here we see the importance of document-level aggregation to extract the correct severity of each comorbidity.</li> </ol> <ol></ol>"},{"location":"pipes/ner/behaviors/alcohol/","title":"Alcohol consumption","text":"<p>The <code>eds.alcohol</code> pipeline component extracts mentions of alcohol consumption. It won't match occasional consumption, nor acute intoxication.</p> Details of the used patterns <pre><code># fmt: off\ndefault_patterns = dict(\n    source=\"alcohol\",\n    regex=[\n        r\"\\balco[ol]\",\n        r\"\\bethyl\",\n        r\"(?&lt;!(25.?)|(sevrage)).?\\boh\\b\",\n        r\"exogenose\",\n        r\"delirium.tremens\",\n    ],\n    exclude=[\n        dict(\n            regex=[\n                \"occasion\",\n                \"episod\",\n                \"festi\",\n                \"rare\",\n                \"libre\",  # OH-libres\n                \"aigu\",\n            ],\n            window=(-3, 5),\n        ),\n        dict(\n            regex=[\"pansement\", \"compress\"],\n            window=-3,\n        ),\n    ],\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"stopped\",\n            regex=r\"(?&lt;!non )(?&lt;!pas )(sevr|arret|stop|ancien)\",\n            window=(-3, 5),\n        ),\n        dict(\n            name=\"zero_after\",\n            regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|oui|non(?! sevr)))\",\n            window=6,\n        ),\n    ],\n)\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/behaviors/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"PRESENT\"</code></li> <li><code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>\"ABSENT\"</code> if the patient has no alcohol dependence</li> </ul> </li> </ul>"},{"location":"pipes/ner/behaviors/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.alcohol\")\n</code></pre> <p>Below are a few examples:</p> 12345678 <pre><code>text = \"Patient alcoolique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [alcoolique]\n</code></pre> <pre><code>text = \"OH chronique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [OH]\n</code></pre> <pre><code>text = \"Prise d'alcool occasionnelle\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Application d'un pansement alcoolis\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Alcoolisme sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevr\u00e9]}\n</code></pre> <pre><code>text = \"Alcoolisme non sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme]\n</code></pre> <pre><code>text = \"Alcool: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcool: 0]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'zero_after': [0]}\n</code></pre> <pre><code>text = \"Le patient est en cours de sevrage \u00e9thylotabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [sevrage \u00e9thylotabagique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevrage]}\n</code></pre>"},{"location":"pipes/ner/behaviors/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>{'source': 'alcohol', 'regex': ['\\\\balco[ol]', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>alcohol</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'alcohol': True}</code> </p>"},{"location":"pipes/ner/behaviors/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.alcohol</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/behaviors/tobacco/","title":"Tobacco consumption","text":"<p>The <code>eds.tobacco</code> pipeline component extracts mentions of tobacco consumption.</p> Details of the used patterns <pre><code># fmt: off\nPA = r\"(?:\\bp/?a\\b|paquets?.?annee)\"\nQUANTITY = r\"(?P&lt;quantity&gt;[\\d]{1,3})\"\nPUNCT = r\"\\.,-;\\(\\)\"\n\ndefault_patterns = [\n    dict(\n        source=\"tobacco\",\n        regex=[\n            r\"tabagi\",\n            r\"tabac\",\n            r\"\\bfume\\b\",\n            r\"\\bfumeu\",\n            r\"\\bpipes?\\b\",\n        ],\n        exclude=dict(\n            regex=[\n                \"occasion\",\n                \"moder\",\n                \"quelqu\",\n                \"festi\",\n                \"rare\",\n                \"sujet\",  # Example : Chez le sujet fumeur ... generic sentences\n            ],\n            window=(-3, 5),\n        ),\n        regex_attr=\"NORM\",\n        assign=[\n            dict(\n                name=\"stopped\",\n                regex=r\"(?&lt;!non )(?&lt;!pas )(\\bex\\b|sevr|arret|stop|ancien)\",\n                window=(-3, 15),\n            ),\n            dict(\n                name=\"zero_after\",\n                regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non(?! sevr)))\",\n                window=6,\n            ),\n            dict(\n                name=\"PA\",\n                regex=rf\"{QUANTITY}[^{PUNCT}]{{0,10}}{PA}|{PA}[^{PUNCT}]{{0,10}}{QUANTITY}\",\n                window=(-10, 10),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"secondhand\",\n                regex=\"(passif)\",\n                window=5,\n                reduce_mode=\"keep_first\",\n            ),\n        ],\n    )\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/behaviors/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"PRESENT\"</code></li> <li><code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>\"ABSENT\"</code> if the patient has no tobacco dependence</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>PA</code>: the mentioned year-pack (= paquet-ann\u00e9e)</li> <li><code>secondhand</code>: if secondhand smoking</li> </ul> </li> </ul>"},{"location":"pipes/ner/behaviors/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.tobacco())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Tabagisme \u00e9valu\u00e9 \u00e0 15 PA\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme \u00e9valu\u00e9 \u00e0 15 PA]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'PA': 15}\n</code></pre> <pre><code>text = \"Patient tabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagique]\n</code></pre> <pre><code>text = \"Tabagisme festif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"On a un tabagisme ancien\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagisme ancien]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [ancien]}\n</code></pre> <pre><code>text = \"Tabac: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: 0]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'zero_after': [0]}\n</code></pre> <pre><code>text = \"Tabagisme passif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme passif]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'secondhand': passif}\n</code></pre> <pre><code>text = \"Tabac: sevr\u00e9 depuis 5 ans\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevr\u00e9]}\n</code></pre>"},{"location":"pipes/ner/behaviors/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'tobacco', 'regex': ['tabagi', 'tab...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tobacco</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tobacco': True}</code> </p>"},{"location":"pipes/ner/behaviors/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tobacco</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/","title":"Disorders","text":""},{"location":"pipes/ner/disorders/#presentation","title":"Presentation","text":"<p>The following components extract 16 different conditions from the Charlson Comorbidity Index. Each component is based on the ContextualMatcher component. Some general considerations about those components:</p> <ul> <li>Extracted entities are stored in <code>doc.ents</code> and <code>doc.spans</code>. For instance, the <code>eds.tobacco</code> component stores matches in <code>doc.spans[\"tobacco\"]</code>.</li> <li>The matched comorbidity is also available under the <code>ent.label_</code> of each match.</li> <li>Matches have an associated <code>_.status</code> attribute taking the value <code>0</code>, <code>1</code>, or <code>2</code>. A corresponding <code>_.detailed_status</code> attribute stores the human-readable status, which can be component-dependent. See each component documentation for more details.</li> <li>Some components add additional information to matches. For instance, the <code>tobacco</code> adds, if relevant, extracted pack-year (= paquet-ann\u00e9e). Those information are available under the <code>ent._.assigned</code> attribute.</li> <li> <p>Those components work on normalized documents. Please use the <code>eds.normalizer</code> pipeline with the following parameters:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n...\n\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\n</code></pre> </li> <li> <p>Those components should be used with a qualification pipeline to avoid extracted unwanted matches. At the very least, you can use available rule-based qualifiers (<code>eds.negation</code>, <code>eds.hypothesis</code> and <code>eds.family</code>). Better, a machine learning qualification component was developed and trained specifically for those components. For privacy reason, the model isn't publicly available yet.</p> <p>Use the ML model</p> <p>The model will soon be available in the models catalogue of AP-HP's CDW.</p> </li> </ul> <p>On the medical definition of the comorbidities</p> <p>Those components were developped to extract chronic and symptomatic conditions only.</p>"},{"location":"pipes/ner/disorders/#aggregation","title":"Aggregation","text":"<p>For relevant phenotyping, matches should be aggregated at the document-level. For instance, a document might mention a complicated diabetes at the beginning (\"Le patient a une r\u00e9tinopathie diab\u00e9tique\"), and then refer to this diabetes without mentionning that it is complicated anymore (\"Concernant son diab\u00e8te, le patient ...\"). Thus, a good and simple aggregation rule is, for each comorbidity, to</p> <ul> <li>disregard all entities tagged as irrelevant by the qualification component(s)</li> <li>take the maximum (i.e., the most severe) status of the leftover entities</li> </ul> <p>An implementation of this rule is presented here</p>"},{"location":"pipes/ner/disorders/aids/","title":"AIDS","text":"<p>The <code>eds.aids</code> pipeline component extracts mentions of AIDS. It will notably match:</p> <ul> <li>Mentions of VIH/HIV at the SIDA/AIDS stage</li> <li>Mentions of VIH/HIV with opportunistic(s) infection(s)</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre> <p>On HIV infection</p> <p>pre-AIDS HIV infection are not extracted, only AIDS.</p>"},{"location":"pipes/ner/disorders/aids/#edsnlp.pipes.ner.disorders.aids.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>opportunist</code>: list of opportunist infections extracted around the HIV mention</li> <li><code>stage</code>: stage of the HIV infection</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/aids/#edsnlp.pipes.ner.disorders.aids.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.aids\")\n</code></pre> <p>Below are a few examples:</p> SIDAVIHCoinfectionVIH stade SIDA <pre><code>text = \"Patient atteint du VIH au stade SIDA.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH au stade SIDA]\n</code></pre> <pre><code>text = \"Patient atteint du VIH.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a un VIH avec coinfection pneumocystose\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'opportunist': [coinfection, pneumocystose]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un VIH stade C\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': [C]}\n</code></pre>"},{"location":"pipes/ner/disorders/aids/#edsnlp.pipes.ner.disorders.aids.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'aids'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'aids', 'regex': ['(vih.{1,5}stade....</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>aids</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'aids': True}</code> </p>"},{"location":"pipes/ner/disorders/aids/#edsnlp.pipes.ner.disorders.aids.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.aids</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/","title":"Cerebrovascular accident","text":"<p>The <code>eds.cerebrovascular_accident</code> pipeline component extracts mentions of cerebrovascular accident. It will notably match:</p> <ul> <li>Mentions of AVC/AIT</li> <li>Mentions of bleeding, hemorrhage, thrombus, ischemia, etc., localized in the brain</li> </ul> Details of the used patterns <pre><code># fmt: off\nimport re\n\nfrom edsnlp.utils.resources import get_AVC_care_site\n\nfrom ..terms import BRAIN, HEART, PERIPHERAL\n\nAVC_CARE_SITES_REGEX = [\n    r\"\\b\" + re.escape(cs.strip()) + r\"\\b\" for cs in get_AVC_care_site(prefix=True)\n] + [\n    r\"h[o\u00f4]p\",\n    r\"\\brcp\",\n    r\"service\",\n    r\"\\bsau\",\n    r\"ap.?hp\",\n    r\"\\burg\",\n    r\"finess\",\n    r\"\\bsiret\",\n    r\"[\u00e0a] avc\",\n    r\"consult\",\n]\n\navc = dict(\n    source=\"avc\",\n    regex=[\n        r\"\\bavc\\b\",\n    ],\n    exclude=[\n        dict(\n            regex=AVC_CARE_SITES_REGEX,\n            window=(-5, 5),\n            regex_flags=re.S | re.I,\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=r\"\\b[a-z]\\.\",\n            window=2,\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"(hemorr?agie|hematome)\",\n        r\"angiopath\",\n        r\"angioplasti\",\n        r\"infarctus\",\n        r\"occlusion\",\n        r\"saignement\",\n        r\"embol\",\n        r\"vascularite\",\n        r\"\\bhsd\\b\",\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"phleb\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=[\n        dict(\n            regex=r\"pulmo|poumon\",\n            window=4,\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain_localized\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-15, 15),\n            limit_to_sentence=False,\n            include_assigned=False,\n        ),\n    ],\n)\n\ngeneral = dict(\n    source=\"general\",\n    regex=[\n        r\"accident.{1,5}vasculaires.{1,5}cereb\",\n        r\"accident.{1,5}vasculaire.{1,5}ischemi\",\n        r\"accident.{1,5}ischemi\",\n        r\"moya.?moya\",\n        r\"occlusion.{1,5}(artere|veine).{1,20}retine\",\n        r\"vasculopathies?.cerebrales?.ischemique\",\n        r\"maladies?.des.petites.arteres\",\n        r\"maladies?.des.petits.vaisseaux\",\n        r\"thrombolyse\",\n        r\"\\bsusac\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAIT = dict(\n    source=\"AIT\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bAIT\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=PERIPHERAL + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-10, 15),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    avc,\n    with_localization,\n    general,\n    acronym,\n    AIT,\n    ischemia,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.cerebrovascular_accident())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Patient hospitalis\u00e9 \u00e0 AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Hospitalisation pour un AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [AVC]\n</code></pre> <pre><code>text = \"Saignement intracranien\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Saignement]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [intracranien]}\n</code></pre> <pre><code>text = \"Thrombose p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Thrombose sylvienne\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Thrombose]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [sylvienne]}\n</code></pre> <pre><code>text = \"Infarctus c\u00e9r\u00e9bral\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Infarctus]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [c\u00e9r\u00e9bral]}\n</code></pre> <pre><code>text = \"Soign\u00e9 via un thrombolyse\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [thrombolyse]\n</code></pre>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'cerebrovascular_accident'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'avc', 'regex': ['\\\\bavc\\\\b'], 'exc...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>cerebrovascular_accident</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cerebrovascular_accident': True}</code> </p>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cerebrovascular_accident</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/ckd/","title":"CKD","text":"<p>The <code>eds.CKD</code> pipeline component extracts mentions of CKD (Chronic Kidney Disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Kidney transplantation</li> <li>Chronic dialysis</li> <li>Renal failure from stage 3 to 5. The stage is extracted by trying 3 methods:<ul> <li>Extracting the mentioned stage directly (\"IRC stade IV\")</li> <li>Extracting the severity directly (\"IRC terminale\")</li> <li>Extracting the mentioned GFR (DFG in french) (\"IRC avec DFG estim\u00e9 \u00e0 30   mL/min/1,73m2)\")</li> </ul> </li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/ckd/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: mentioned renal failure stage</li> <li><code>status</code>: mentioned renal failure severity (e.g. mod\u00e9r\u00e9e, s\u00e9v\u00e8re, terminale,   etc.)</li> <li><code>dfg</code>: mentioned DFG</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/ckd/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.ckd())\n</code></pre> <p>Below are a few examples:</p> 1234567891011 <pre><code>text = \"Patient atteint d'une glom\u00e9rulopathie.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [glom\u00e9rulopathie]\n</code></pre> <pre><code>text = \"Patient atteint d'une tubulopathie aig\u00fce.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient transplant\u00e9 r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [transplant\u00e9 r\u00e9nal]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une insuffisance r\u00e9nale aig\u00fce sur chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [insuffisance r\u00e9nale aig\u00fce sur chronique]\n</code></pre> <pre><code>text = \"Le patient a \u00e9t\u00e9 dialys\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est dialys\u00e9 chaque lundi\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [dialys\u00e9 chaque lundi]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'chronic': [lundi]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC s\u00e9v\u00e8re\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC s\u00e9v\u00e8re]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'status': s\u00e9v\u00e8re}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC au stade IV\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC au stade IV]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': IV}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC avec DFG \u00e0 30\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC avec DFG \u00e0 30]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'dfg': 30}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une maladie r\u00e9nale avec DFG \u00e0 110\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"pipes/ner/disorders/ckd/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'ckd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['glomerulonephrit...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>ckd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'ckd': True}</code> </p>"},{"location":"pipes/ner/disorders/ckd/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.CKD</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/congestive-heart-failure/","title":"Congestive heart failure","text":"<p>The <code>eds.congestive_heart_failure</code> pipeline component extracts mentions of congestive heart failure. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Heart transplantation</li> <li>AF (Atrial Fibrillation)</li> <li>Pacemaker</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"defaillance.{1,10}cardi\",\n        r\"(\u0153|oe)deme.{1,10}pulmon\",\n        r\"(\u0153|oe)deme.{1,10}poumon\",\n        r\"decompensation.{1,10}card\",\n        r\"choc.{1,30}cardio\",\n        r\"greffe.{1,10}c(\u0153|oe)ur\",\n        r\"greffe.{1,10}cardia\",\n        r\"transplantation.{1,10}c(\u0153|oe)ur\",\n        r\"transplantation.{1,10}cardia\",\n        r\"arret.{1,10}cardi\",\n        r\"c(\u0153|oe)ur pulmo\",\n        r\"foie.card\",\n        r\"pace.?maker\",\n        r\"stimulateur.cardiaque\",\n        r\"valve.{1,30}(meca|artific)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nsymptomatic = dict(\n    source=\"symptomatic\",\n    regex=[\n        r\"cardiopathi\",\n        r\"cardiomyopathi\",\n        r\"d(i|y)sfonction.{1,15}(ventricul|\\bvg|cardiaque)\",\n        r\"valvulopathie\",\n        r\"\\bic\\b.{1,10}(droite|gauche)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [r\"(?&lt;!\\bnon.)ischem\"],  # Exclusion of ischemic events\n        window=5,\n    ),\n)\n\nwith_minimum_severity = dict(\n    source=\"min_severity\",\n    regex=[\n        r\"insuffisance.{1,10}(\\bcardi|\\bdiasto|\\bventri|\\bmitral|tri.?cusp)\",\n        r\"(retrecissement|stenose).(aortique|mitral)\",\n        r\"\\brac\\b\",\n        r\"\\brm\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [\"minime\", \"modere\", r\"non.serre\"],\n        window=5,\n    ),\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bOAP\\b\",\n        r\"\\bCMH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAF_main_pattern = dict(\n    source=\"AF_main\",\n    regex=[\n        r\"fibrill?ation.{1,3}(atriale|auriculaire|ventriculaire)\",\n        r\"flutter\",\n        r\"brady.?arythmie\",\n        r\"pace.?maker\",\n    ],\n)\n\nAF_acronym = dict(\n    source=\"AF_acronym\",\n    regex=[\n        r\"\\bFA\\b\",\n        r\"\\bAC.?FA\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    symptomatic,\n    acronym,\n    AF_main_pattern,\n    AF_acronym,\n    with_minimum_severity,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/congestive-heart-failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.congestive_heart_failure())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'un oed\u00e8me pulmonaire\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [oed\u00e8me pulmonaire]\n</code></pre> <pre><code>text = \"Le patient est \u00e9quip\u00e9 d'un pace-maker\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [pace-maker]\n</code></pre> <pre><code>text = \"Un cardiopathie non d\u00e9compens\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Insuffisance cardiaque\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [Insuffisance cardiaque]\n</code></pre> <pre><code>text = \"Insuffisance cardiaque minime\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"pipes/ner/disorders/congestive-heart-failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>(str)</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['defaillance.{1,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>congestive_heart_failure</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'congestive_heart_failure': True}</code> </p>"},{"location":"pipes/ner/disorders/congestive-heart-failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.congestive_heart_failure</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/connective-tissue-disease/","title":"Connective tissue disease","text":"<p>The <code>eds.connective_tissue_disease</code> pipeline component extracts mentions of connective tissue diseases.</p> Details of the used patterns <pre><code># fmt: off\nTO_EXCLUDE = r\"(?&lt;!a )((\\bacc\\b)|anti.?coag|anti.?corps|buschke|(\\bac\\b)|(\\bbio))\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"arthrites.{1,5}juveniles.{1,5}idiopa\",\n        r\"myosite\",\n        r\"myopathie.{1,5}inflammatoire\",\n        r\"polyarthrite.{1,5}chronique.{1,5}evol\",\n        r\"polymyosie\",\n        r\"polyarthrites.{1,5}(rhizo|rhuma)\",\n        r\"sclerodermie\",\n        r\"connectivite\",\n        r\"sarcoidose\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nlupus = dict(\n    source=\"lupus\",\n    regex=[\n        r\"\\blupus\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nlupique = dict(\n    source=\"lupique\",\n    regex=[\n        r\"\\blupique\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronyms\",\n    regex=[\n        r\"\\bAJI\\b\",\n        r\"\\bLED\\b\",\n        r\"\\bPCE\\b\",\n        r\"\\bCREST\\b\",\n        r\"\\bPPR\\b\",\n        r\"\\bMICI\\b\",\n        r\"\\bMNAI\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nnamed_disease = dict(\n    source=\"named_disease\",\n    regex=[\n        r\"libman.?lack\",\n        r\"\\bstill\",\n        r\"felty\",\n        r\"forestier.?certon\",\n        r\"gou(g|j)erot\",\n        r\"raynaud\",\n        r\"thibierge.?weiss\",\n        r\"sjogren\",\n        r\"gou(g|j)erot.?sjogren\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    lupus,\n    lupique,\n    acronym,\n    named_disease,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/connective-tissue-disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"pipes/ner/disorders/connective-tissue-disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.connective_tissue_disease())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'une scl\u00e9rodermie.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [scl\u00e9rodermie]\n</code></pre> <pre><code>text = \"Patient atteint d'un lupus.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [lupus]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'anticoagulants lupiques,\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a une MICI.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [MICI]\n</code></pre> <pre><code>text = \"Syndrome de Raynaud\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [Raynaud]\n</code></pre>"},{"location":"pipes/ner/disorders/connective-tissue-disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'connective_tissue_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['arthrites.{1,5}j...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>connective_tissue_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'connective_tissue_disease': True}</code> </p>"},{"location":"pipes/ner/disorders/connective-tissue-disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.connective_tissue_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/copd/","title":"COPD","text":"<p>The <code>eds.copd</code> pipeline component extracts mentions of COPD (Chronic obstructive pulmonary disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Pulmonary hypertension</li> <li>Long-term oxygen therapy</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/copd/#edsnlp.pipes.ner.disorders.copd.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"pipes/ner/disorders/copd/#edsnlp.pipes.ner.disorders.copd.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.copd())\n</code></pre> <p>Below are a few examples:</p> 123456 <pre><code>text = \"Une fibrose interstitielle diffuse idiopathique\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [fibrose interstitielle diffuse idiopathique]\n</code></pre> <pre><code>text = \"Patient atteint de pneumoconiose\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [pneumoconiose]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une HTAP.\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [HTAP]\n</code></pre> <pre><code>text = \"On voit une hypertension pulmonaire minime\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente a \u00e9t\u00e9 mis sous oxyg\u00e9norequ\u00e9rance\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente est sous oxyg\u00e9norequ\u00e9rance au long cours\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [oxyg\u00e9norequ\u00e9rance au long cours]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'long': [long cours]}\n</code></pre>"},{"location":"pipes/ner/disorders/copd/#edsnlp.pipes.ner.disorders.copd.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'copd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['alveolites.{1,5}...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>copd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'copd': True}</code> </p>"},{"location":"pipes/ner/disorders/copd/#edsnlp.pipes.ner.disorders.copd.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.copd</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/dementia/","title":"Dementia","text":"<p>The <code>eds.dementia</code> pipeline component extracts mentions of dementia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"demence\",\n        r\"dementiel\",\n        r\"corps de le[vw]y\",\n        r\"deficits?.chroniques?.cognitif\",\n        r\"troubles?.mnesique?\",\n        r\"troubles?.praxique\",\n        r\"troubles?.attentionel\",\n        r\"troubles?.degeneratif.{1,15}fonctions.{1,5}sup\",\n        r\"maladies?.cerebrales?.degen\",\n        r\"troubles?.neurocogn\",\n        r\"deficits?.cognitif\",\n        r\"(trouble|dysfonction).{1,20} cogniti\",\n        r\"atteinte.{1,7}spheres?cogniti\",\n        r\"syndrome.{1,10}(frontal|neuro.deg)\",\n        r\"dysfonction.{1,25}cogni\",\n        r\"(?&lt;!specialisee )alzheimer\",\n        r\"demence.{1,20}(\\balz|\\bpark)\",\n        r\"binswanger\",\n        r\"gehring\",\n        r\"\\bpick\",\n        r\"de guam\",\n        r\"[kc]reutzfeld.{1,5}ja[ck]ob\",\n        r\"huntington\",\n        r\"korsako[fv]\",\n        r\"atrophie.{1,10}(cortico|hippocamp|cereb|lobe)\",\n    ],\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bSLA\\b\",\n        r\"\\bDFT\\b\",\n        r\"\\bDFT\",\n        r\"\\bTNC\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=r\"\\banti\",  # anticorps\n        window=-15,\n        regex_attr=\"NORM\",\n    ),\n)\n\ncharcot = dict(\n    source=\"charcot\",\n    regex=[\n        r\"maladie.{1,10}charcot\",\n    ],\n    exclude=dict(\n        regex=[\n            \"pied de\",\n            \"marie.?tooth\",\n        ],\n        window=(-3, 3),\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    charcot,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/dementia/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"pipes/ner/disorders/dementia/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.dementia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"D'importants d\u00e9ficits cognitifs\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9ficits cognitifs]\n</code></pre> <pre><code>text = \"Patient atteint de d\u00e9mence\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9mence]\n</code></pre> <pre><code>text = \"On retrouve des anti-SLA\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une maladie de Charcot\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [maladie de Charcot]\n</code></pre>"},{"location":"pipes/ner/disorders/dementia/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['demence', 'demen...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>dementia</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'dementia': True}</code> </p>"},{"location":"pipes/ner/disorders/dementia/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dementia</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/diabetes/","title":"Diabetes","text":"<p>The <code>eds.diabetes</code> pipeline component extracts mentions of diabetes.</p> Details of the used patterns <pre><code># fmt: off\nCOMPLICATIONS = [\n    r\"nephropat\",\n    r\"neuropat\",\n    r\"retinopat\",\n    r\"glomerulopathi\",\n    r\"glomeruloscleros\",\n    r\"angiopathi\",\n    r\"origine\",\n]\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bds?n?id\\b\",\n        r\"\\bdiabet[^o]\",\n        r\"\\bdb\\b\",\n        r\"\\bdt.?(i|ii|1|2)\\b\",\n    ],\n    exclude=dict(\n        regex=[\n            \"insipide\",\n            \"nephrogenique\",\n            \"aigu\",\n            r\"\\bdr\\b\",  # Dr. ...\n            \"endocrino\",  # Section title\n            \"soins aux pieds\",  # Section title\n            \"nutrition\",  # Section title\n            r\"\\s?:\\n+\\W+(?!oui|non|\\W)\",  # General pattern for section title\n        ],\n        window=(-5, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"complicated_before\",\n            regex=r\"(\" + r\"|\".join(COMPLICATIONS + [\"origine\"]) + r\")\",\n            window=-3,\n        ),\n        dict(\n            name=\"complicated_after\",\n            regex=r\"(\"\n            + r\"|\".join([r\"(?&lt;!sans )compli\", r\"(?&lt;!a)symptomatique\"] + COMPLICATIONS)\n            + r\")\",\n            window=12,\n        ),\n        dict(\n            name=\"type\",\n            regex=r\"type.(i|ii|1|2)\",\n            window=6,\n        ),\n        dict(\n            name=\"insulin\",\n            regex=r\"insulino.?(dep|req)\",\n            window=6,\n        ),\n        dict(\n            name=\"corticoid\",\n            regex=r\"(bctc\\b|cortico(?:.?induit)?)\",\n            window=6,\n        ),\n    ],\n)\n\ncomplicated_pattern = dict(\n    source=\"complicated\",\n    regex=[\n        r\"(mal|maux).perforants?(.plantaire)?\",\n        r\"pieds? diabeti\",\n    ],\n    exclude=dict(\n        regex=\"soins aux\",  # Section title\n        window=-2,\n    ),\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    complicated_pattern,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/diabetes/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"WITH_COMPLICATION\"</code> if the diabetes is  complicated (e.g., via organ    damages)</li> <li><code>\"WITHOUT_COMPLICATION\"</code> otherwise</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>type</code>: type of diabetes (I or II)</li> <li><code>insulin</code>: if the diabetes is insulin-dependent</li> <li><code>corticoid</code>: if the diabetes is corticoid-induced</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/diabetes/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.diabetes())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un DT2\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DT2]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un DNID\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DNID]\n</code></pre> <pre><code>text = \"Patient diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [diab\u00e9tique]\n</code></pre> <pre><code>text = \"Un diab\u00e8te insipide\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Atteinte neurologique d'origine diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [origine diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [origine]}\n</code></pre> <pre><code>text = \"Une r\u00e9tinopathie diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [r\u00e9tinopathie diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [r\u00e9tinopathie]}\n</code></pre> <pre><code>text = \"Il y a un mal perforant plantaire\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [mal perforant plantaire]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n</code></pre>"},{"location":"pipes/ner/disorders/diabetes/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'diabetes'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['\\\\bds?n?id\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>diabetes</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'diabetes': True}</code> </p>"},{"location":"pipes/ner/disorders/diabetes/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.diabetes</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/hemiplegia/","title":"Hemiplegia","text":"<p>The <code>eds.hemiplegia</code> pipeline component extracts mentions of hemiplegia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"hemiplegi\",\n        r\"tetraplegi\",\n        r\"quadriplegi\",\n        r\"paraplegi\",\n        r\"neuropathie.{1,25}motrice.{1,30}type [5V]\",\n        r\"charcot.?marie.?tooth\",\n        r\"locked.?in\",\n        r\"syndrome.{1,5}(enfermement|verrouillage)|(desafferen)\",\n        r\"paralysie.{1,10}hemicorps\",\n        r\"paralysie.{1,10}jambe\",\n        r\"paralysie.{1,10}membre\",\n        r\"paralysie.{1,10}cote\",\n        r\"paralysie.{1,5}cerebrale.{1,5}spastique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLIS\\b\",\n        r\"\\bNMSH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"pipes/ner/disorders/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.hemiplegia())\n</code></pre> <p>Below are a few examples:</p> 123 <pre><code>text = \"Patient h\u00e9mipl\u00e9gique\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [h\u00e9mipl\u00e9gique]\n</code></pre> <pre><code>text = \"Paralysie des membres inf\u00e9rieurs\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [Paralysie des membres]\n</code></pre> <pre><code>text = \"Patient en LIS\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [LIS]\n</code></pre>"},{"location":"pipes/ner/disorders/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['hemiplegi', 'tet...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>hemiplegia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'hemiplegia': True}</code> </p>"},{"location":"pipes/ner/disorders/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hemiplegia</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/leukemia/","title":"Leukemia","text":"<p>The <code>eds.leukemia</code> pipeline component extracts mentions of leukemia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"leucemie\",\n        r\"(syndrome.)?myeloproliferatif\",\n        r\"m[yi]eloprolifer\",\n    ],\n    exclude=dict(\n        regex=[\n            \"plasmocyte\",\n            \"benin\",\n            \"benign\",\n        ],\n        window=5,\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLAM\\b\",\n        r\"\\bLAM.?[0-9]\",\n        r\"\\bLAL\\b\",\n        r\"\\bLMC\\b\",\n        r\"\\bLCE\\b\",\n        r\"\\bLMM[JC]\\b\",\n        r\"\\bLCN\\b\",\n        r\"\\bAREB\\b\",\n        r\"\\bAPMF\\b\",\n        r\"\\bLLC\\b\",\n        r\"\\bSMD\\b\",\n        r\"LA my[\u00e9\u00e8e]lomonocytaire\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=\"anti\",\n        window=-20,\n    ),\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"myelofibrose\",\n        r\"vaquez\",\n        r\"thrombocytemie.{1,3}essentielle\",\n        r\"splenomegalie.{1,3}myeloide\",\n        r\"mastocytose.{1,5}maligne\",\n        r\"polyglobulie.{1,10}essentielle\",\n        r\"letterer.?siwe\",\n        r\"anemie.refractaire.{1,20}blaste\",\n        r\"m[iy]elod[iy]splasi\",\n        r\"syndrome.myelo.?dysplasique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    other,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/leukemia/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"pipes/ner/disorders/leukemia/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.leukemia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [my\u00e9loprolif\u00e9ratif]\n</code></pre> <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif b\u00e9nin\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient atteint d'une LAM\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [LAM]\n</code></pre> <pre><code>text = \"Une maladie de Vaquez\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [Vaquez]\n</code></pre>"},{"location":"pipes/ner/disorders/leukemia/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'leukemia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['leucemie', '(syn...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>leukemia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'leukemia': True}</code> </p>"},{"location":"pipes/ner/disorders/leukemia/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.leukemia</code> component was developed by AP-HP's Data Science team with a team  of medical experts. A paper describing in details the development of those  components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/liver-disease/","title":"Liver disease","text":"<p>The <code>eds.liver_disease</code> pipeline component extracts mentions of liver disease.</p> Details of the used patterns <pre><code># fmt: off\nmild = dict(\n    source=\"mild\",\n    regex=[\n        r\"cholangites?.{1,10}(sclero|secondaire)\",\n        r\"fibrose.{1,10}(hepatique|foie)\",\n        r\"hepatite.{1,15}chronique\",\n        r\"hepatopathie\",\n        r\"\\bnash\\b\",\n        r\"(maladie|sydrome).{1,10}Hanot\",\n        r\"surinfections.{1,5}delta\",\n        r\"\\bcbp\\b\",\n        r\"\\bmaf\\b\",\n        r\"(maladie|syndrome).{1,8}hanot\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"\\bdots?\\b\",\n        window=-5,\n    ),\n)\n\nmoderate_severe = dict(\n    source=\"moderate_severe\",\n    regex=[\n        r\"cirrhose\",\n        r\"necrose.{1,10}(hepati|foie)\",\n        r\"varice.{1,10}(estomac|oesopha|gastr)\",\n        r\"\\bvo\\b.{1,5}(stade|grade).(1|2|3|i{1,3})\",\n        r\"hypertension.{1,5}portale\",\n        r\"scleroses.{1,5}hepatoportale\",\n        r\"sydrome.{1,10}hepato.?ren\",\n        r\"insuffisance.{1,5}hepa\",\n        r\"encephalopathie.{1,5}hepa\",\n        r\"\\btips\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ntransplant = dict(\n    source=\"transplant\",\n    regex=[\n        r\"(?&lt;!pre.?)(greffe|transplant).{1,12}(hepatique|foie)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"chc\",\n        window=(-5, 5),\n    ),\n)\n\ndefault_patterns = [\n    mild,\n    moderate_severe,\n    transplant,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/liver-disease/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"MILD\"</code> for mild liver diseases</li> <li><code>\"MODERATE_TO_SEVERE\"</code> else</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/liver-disease/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.liver_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Il y a une fibrose h\u00e9patique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [fibrose h\u00e9patique]\n</code></pre> <pre><code>text = \"Une h\u00e9patite B chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [h\u00e9patite B chronique]\n</code></pre> <pre><code>text = \"Le patient consulte pour une cirrhose\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [cirrhose]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre> <pre><code>text = \"Greffe h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [Greffe h\u00e9patique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre>"},{"location":"pipes/ner/disorders/liver-disease/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'mild', 'regex': ['cholangites?.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>liver_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'liver_disease': True}</code> </p>"},{"location":"pipes/ner/disorders/liver-disease/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.liver_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/lymphoma/","title":"Lymphoma","text":"<p>The <code>eds.lymphoma</code> pipeline component extracts mentions of lymphoma.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"lymphom(?:.{1,10}hodgkin)\",\n        r\"lymphom\",\n        r\"lymphangio\",\n        r\"sezary\",\n        r\"burkitt\",\n        r\"kaposi\",\n        r\"hodgkin\",\n        r\"amylose\",\n        r\"plasm[ao]cytome\",\n        r\"lympho.{1,3}sarcome\",\n        r\"lympho.?prolif\",\n        r\"hemopathie.{1,10}lymphoide\",\n        r\"macroglobulinemie\",\n        r\"immunocytome\",\n        r\"maladie.des.chaine\",\n        r\"histiocytose.{1,5}(maligne|langerhans)\",\n        r\"waldenst(ro|or)m\",\n        r\"mycos.{1,10}fongoide\",\n        r\"myelome\",\n        r\"maladie.{1,5}immunoproliferative.{1,5}maligne\",\n        r\"leucemie.{1,10}plasmocyte\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLNH\\b\",\n        r\"\\bLH\\b\",\n        r\"\\bEATL\\b\",\n        r\"\\bLAGC\\b\",\n        r\"\\bLDGCB\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=[\"/L\", \"/mL\"],\n        window=10,\n    ),\n)\n\n\ngammapathy = dict(\n    source=\"gammapathy\",\n    regex=[\n        r\"gammapathie monoclonale\",\n    ],\n    exclude=dict(\n        regex=[\n            \"benin\",\n            \"benign\",\n            \"signification.indeter\",\n            \"NMSI\",\n            \"MGUS\",\n        ],\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    # gammapathy,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul> <p>Monoclonal gammapathy</p> <p>Monoclonal gammapathies are not extracted by this pipeline</p>"},{"location":"pipes/ner/disorders/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.lymphoma())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Un lymphome de Hodgkin.\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [lymphome de Hodgkin]\n</code></pre> <pre><code>text = \"Atteint d'un Waldenst\u00f6rm\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [Waldenst\u00f6rm]\n</code></pre> <pre><code>text = \"Un LAGC\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [LAGC]\n</code></pre> <pre><code>text = \"anti LAGC: 10^4/mL\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"pipes/ner/disorders/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'lymphoma'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['lymphom(?:.{1,10...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>lymphoma</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'lymphoma': True}</code> </p>"},{"location":"pipes/ner/disorders/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.lymphoma</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/myocardial-infarction/","title":"Myocardial infarction","text":"<p>The <code>eds.myocardial_infarction</code> pipeline component extracts mentions of myocardial infarction. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Mentions of stents with a heart localization</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import HEART\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"coronaropathie\",\n        r\"angor.{1,5}instable\",\n        r\"cardiopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"cardio.?myopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"ischemi.{1,15}myocard\",\n        r\"syndrome.{1,5}corona.{1,10}aigu\",\n        r\"syndrome.{1,5}corona.{1,10}st\",\n        r\"pontage.{1,5}mammaire\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"\\bstent\",\n        r\"endoprothese\",\n        r\"pontage\",\n        r\"anevr[iy]sme\",\n        \"infarctus\",\n        r\"angioplasti\",\n    ],\n    assign=[\n        dict(\n            name=\"heart_localized\",\n            regex=\"(\" + r\"|\".join(HEART) + \")\",\n            window=(-10, 10),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bidm\\b\",\n        r\"\\bsca\\b\",\n        r\"\\batl\\b\",\n    ],\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"segment\",\n        regex=r\"st([+-])\",\n        window=2,\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    with_localization,\n    acronym,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/myocardial-infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>heart_localized</code>: localization of the stent or bypass</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/myocardial-infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.myocardial_infarction())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Une cardiopathie isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [cardiopathie isch\u00e9mique]\n</code></pre> <pre><code>text = \"Une cardiopathie non-isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent sur la marginale\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [stent sur la marginale]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [marginale]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"infarctus du myocarde\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [infarctus du myocarde]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [myocarde]}\n</code></pre>"},{"location":"pipes/ner/disorders/myocardial-infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['coronaropathie',...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>myocardial_infarction</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'myocardial_infarction': True}</code> </p>"},{"location":"pipes/ner/disorders/myocardial-infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.myocardial_infarction</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/","title":"Peptic ulcer disease","text":"<p>The <code>eds.peptic_ulcer_disease</code> pipeline component extracts mentions of peptic ulcer disease.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"ulcere.{1,10}gastr\",\n        r\"ulcere.{1,10}duoden\",\n        r\"ulcere.{1,10}antra\",\n        r\"ulcere.{1,10}pept\",\n        r\"ulcere.{1,10}estomac\",\n        r\"ulcere.{1,10}curling\",\n        r\"ulcere.{1,10}bulb\",\n        r\"(\u0153|oe)sophagites.{1,5}pepti.{1,10}ulcer\",\n        r\"gastrite.{1,20}ulcer\",\n        r\"antrite.{1,5}ulcer\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bUGD\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ngeneric = dict(\n    source=\"generic\",\n    regex=r\"ulcere\",\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"is_peptic\",\n        regex=r\"\\b(gastr|digest)\",\n        window=(-20, 20),\n        limit_to_sentence=False,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    generic,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that matches, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peptic_ulcer_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Beaucoup d'ulc\u00e8res gastriques\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res gastriques]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'UGD\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [UGD]\n</code></pre> <pre><code>text = \"La patient \u00e0 des ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Au niveau gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'is_peptic': [gastrique]}\n</code></pre>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'peptic_ulcer_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['ulcere.{1,10}gas...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peptic_ulcer_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peptic_ulcer_disease': True}</code> </p>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peptic_ulcer_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/","title":"Peripheral vascular disease","text":"<p>The <code>eds.peripheral_vascular_disease</code> pipeline component extracts mentions of peripheral vascular disease.</p> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC, BRAIN, HEART, PERIPHERAL\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAOMI\\b\",\n        r\"\\bACOM\\b\",\n        r\"\\bTAO\\b\",\n        r\"\\bSAPL\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bSCS\\b\",\n        r\"\\bTVP\\b\",\n        r\"\\bCAPS\\b\",\n        r\"\\bMTEV\\b\",\n        r\"\\bPTT\\b\",\n        r\"\\bMAT\\b\",\n        r\"\\bSHU\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"\\bbuerger\",\n        r\"takayasu\",\n        r\"\\bhorton\",\n        r\"wegener\",\n        r\"churg.{1,10}strauss\",\n        r\"\\bsneddon\",\n        r\"budd.chiari\",\n        r\"infarctus.{1,5}(renal|splenique|polaire|pulmo)\",\n        r\"ulcere.{1,5}arter\",\n        r\"syndrome.?hemolytique.{1,8}uremique\",\n        r\"granulomatose.{1,10}polyangeite\",\n        r\"occlusion.{1,10}(artere|veine).{1,20}retine\",\n        r\"syndrome.{1,20}anti.?phospho\",\n        r\"embolie.{1,5}pulm\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"angiopathie\",\n        r\"arteriopathies.{1,5}obliterante\",\n        r\"gangren\",\n        r\"claudication\",\n        r\"dissection.{1,10}(aort|arter)\",\n        r\"tromboangeit\",\n        r\"tromboarterit\",\n        r\"(pontage|angioplastie).{1,10}(\\bfem|\\bpop|\\bren|\\bjamb)\",\n        r\"arterite\",\n        r\"(ischemie|infarctus).{1,10}mesenterique\",\n        r\"endarteriectomie\",\n        r\"vascularite\",\n        r\"occlusion.{1,10}terminaisons? carotid\",\n        r\"cryoglobulinemie\",\n        r\"colite.{1,5}ischemi\",\n        r\"embole.{1,10}cholesterol\",\n        r\"purpura.?thrombopenique.?idiopa\",\n        r\"micro.?angiopathie.?thrombotique\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + ASYMPTOMATIC + [r\"inr\\srecommande\\ssous\\savk\"],\n            window=(-8, 8),\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nthrombosis = dict(\n    source=\"thrombosis\",\n    regex=[\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"thrombo.?embo\",\n        r\"phlebit\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + [\"superficiel\", \"\\biv\\b\", \"intra.?vein\"],\n            window=(-15, 15),\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=[\n                \"pre\",\n                \"anti\",\n                \"bilan\",\n            ],\n            window=-4,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"peripheral\",\n            regex=\"(\" + r\"|\".join(PERIPHERAL) + \")\",\n            window=15,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nep = dict(\n    source=\"ep\",\n    regex=r\"\\bEP(?![\\w\\./-])\",\n    regex_attr=\"TEXT\",\n    exclude=[\n        dict(\n            regex=[\n                r\"fibreux\",\n                r\"retin\",\n                r\"\\bfove\",\n                r\"\\boct\\b\",\n                r\"\\bmacula\",\n                r\"prosta\",\n                r\"\\bip\\b\",\n                r\"protocole\",\n                r\"seance\",\n                r\"echange\",\n                r\"ritux\",\n                r\"ivig\",\n                r\"ig.?iv\",\n                r\"\\bctc\",\n                r\"corticoide\",\n                r\"serum\",\n                r\"\\bcure\",\n                r\"plasma\",\n                r\"mensuel\",\n                r\"semaine\",\n                r\"serologi\",\n                r\"espaces.porte\",\n                r\"projet\",\n                r\"bolus\",\n            ],\n            window=(-25, 25),\n            limit_to_sentence=False,\n            regex_attr=\"NORM\",\n        ),\n        dict(\n            regex=[r\"rdv\", r\"les\", r\"des\", r\"angine\"],\n            window=(-3, 0),\n            regex_attr=\"NORM\",\n        ),\n    ],\n)\n\nhypertension = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bhta\\b\",\n        r\"hyper.?tension.?arte\",\n        r\"hyper.?tendu\",\n        r\"hyper.?tension.?essenti\",\n        r\"hypertensi\",\n    ],\n    exclude=dict(\n        regex=\"(pulmo|porta)\",\n        window=3,\n    ),\n)\n\ndefault_patterns = [\n    acronym,\n    other,\n    with_localization,\n    thrombosis,\n    ep,\n    ischemia,\n    hypertension,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peripheral_vascular_disease())\n</code></pre> <p>Below are a few examples:</p> 12345678910111213 <pre><code>text = \"Un AOMI\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [AOMI]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un infarctus r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [infarctus r\u00e9nal]\n</code></pre> <pre><code>text = \"Une angiopathie c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une angiopathie\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [angiopathie]\n</code></pre> <pre><code>text = \"Une thrombose c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose des veines superficielles\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [thrombose]\n</code></pre> <pre><code>text = \"Effectuer un bilan pre-trombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une isch\u00e9mie des MI est remarqu\u00e9e.\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [isch\u00e9mie des MI]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'peripheral': [MI]}\n</code></pre> <pre><code>text = \"Plusieurs cas d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [EP]\n</code></pre> <pre><code>text = \"Effectuer des cures d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est hypertendu\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [hypertendu]\n</code></pre> <pre><code>text = \"Une hypertension portale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'peripheral_vascular_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'acronym', 'regex': ['\\\\bAOMI\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peripheral_vascular_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peripheral_vascular_disease': T...</code> </p>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peripheral_vascular_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/disorders/solid-tumor/","title":"Solid tumor","text":"<p>The <code>eds.solid_tumor</code> pipeline component extracts mentions of solid tumors. It will notably match:</p> Details of the used patterns <pre><code># fmt: off\nBENINE = r\"benign|benin|(grade.?\\b[i1]\\b)\"\nSTAGE = r\"stade ([^\\s]*)\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"carcinom(?!.{0,10}in.?situ)\",\n        r\"seminome\",\n        r\"(?&lt;!lympho)(?&lt;!lympho-)sarcome\",\n        r\"blastome\",\n        r\"cancer([^o]|\\s|\\b)\",\n        r\"adamantinome\",\n        r\"chordome\",\n        r\"craniopharyngiome\",\n        r\"melanome\",\n        r\"neoplasie\",\n        r\"neoplasme\",\n        r\"linite\",\n        r\"melanome\",\n        r\"mesoteliome\",\n        r\"mesotheliome\",\n        r\"seminome\",\n        r\"myxome\",\n        r\"paragangliome\",\n        r\"craniopharyngiome\",\n        r\"k .{0,5}(prostate|sein)\",\n        r\"pancoast.?tobias\",\n        r\"syndrome.{1,10}lynch\",\n        r\"li.?fraumeni\",\n        r\"germinome\",\n        r\"adeno[\\s-]?k\",\n        r\"thymome\",\n        r\"\\bnut\\b\",\n        r\"\\bgist\\b\",\n        r\"\\bchc\\b\",\n        r\"\\badk\\b\",\n        r\"\\btves\\b\",\n        r\"\\btv.tves\\b\",\n        r\"lesion.{1,20}tumor\",\n        r\"tumeur\",\n        r\"carcinoid\",\n        r\"histiocytome\",\n        r\"ependymome\",\n        # r\"primitif\", Trop de FP\n    ],\n    exclude=dict(\n        regex=BENINE,\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"metastasis\",\n            regex=r\"(metasta|multinodul)\",\n            window=(-3, 7),\n            reduce_mode=\"keep_last\",\n        ),\n        dict(\n            name=\"stage\",\n            regex=STAGE,\n            window=7,\n            reduce_mode=\"keep_last\",\n        ),\n    ],\n)\n\nmetastasis_pattern = dict(\n    source=\"metastasis\",\n    regex=[\n        r\"cellule.{1,5}tumorale.{1,5}circulantes\",\n        r\"metasta\",\n        r\"multinodul\",\n        r\"carcinose\",\n        r\"ruptures.{1,5}corticale\",\n        r\"envahissement.{0,15}parties\\smolle\",\n        r\"(localisation|lesion)s?.{0,20}second\",\n        r\"(lymphangite|meningite).{1,5}carcinomateuse\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=r\"goitre\",\n        window=-3,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    metastasis_pattern,\n]\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/disorders/solid-tumor/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"METASTASIS\"</code> for tumors at the metastatic stage</li> <li><code>\"LOCALIZED\"</code> else</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: stage of the tumor</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/solid-tumor/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.solid_tumor())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un carcinome intra-h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [carcinome]\n</code></pre> <pre><code>text = \"Patient avec un K sein.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [K sein]\n</code></pre> <pre><code>text = \"Il y a une tumeur b\u00e9nigne\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Tumeur m\u00e9tastas\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Tumeur m\u00e9tastas\u00e9e]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'metastasis': m\u00e9tastas\u00e9e}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 4\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 4]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'stage': 4}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 2\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 2]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': 2}\n</code></pre> <pre><code>text = \"Pr\u00e9sence de nombreuses l\u00e9sions secondaires\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [l\u00e9sions secondaires]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n</code></pre>"},{"location":"pipes/ner/disorders/solid-tumor/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['carcinom(?!.{0,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>solid_tumor</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'solid_tumor': True}</code> </p> <code>use_tnm</code> <p>Whether to use TNM scores matching as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/ner/disorders/solid-tumor/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.solid_tumor</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"pipes/ner/scores/","title":"Scores Overview","text":"<p>EDS-NLP provides multiple matchers for typical scores (Charlson, SOFA...) found in clinical documents. To extract a score, the matcher:</p> <ul> <li>extracts the score's name via the provided regular expressions</li> <li>extracts the score's raw value via another set of RegEx</li> <li>normalize the score's value via a normalising function</li> </ul>"},{"location":"pipes/ner/scores/#available-scores","title":"Available scores","text":"Component Description <code>eds.charlson</code> A Charlson score extractor <code>eds.emergency_ccmu</code> A CCMU score extractor <code>eds.emergency_gemsa</code> A GEMSA score extractor <code>eds.emergency_priority</code> A priority score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.tnm</code> A TNM score extractor"},{"location":"pipes/ner/scores/#implementing-your-own-score","title":"Implementing your own score","text":"<p>Using the <code>eds.score</code> pipeline, you only have to change its configuration in order to implement a simple score extraction algorithm. As an example, let us see the configuration used for the <code>eds.charlson</code> pipe The configuration consists of 4 items:</p> <ul> <li><code>score_name</code>: The name of the score</li> <li><code>regex</code>: A list of regular expression to detect the score's mention</li> <li><code>value_extract</code>: A regular expression to extract the score's value in the context of the score's mention</li> <li><code>score_normalization</code>: A function name used to normalise the score's raw value</li> </ul> <p>Note</p> <p>Functions passed as parameters to components need to be registered as follow</p> <pre><code>import spacy\n\n\n@spacy.registry.misc(\"score_normalization.charlson\")\ndef my_normalization_score(raw_score: str):\n    # Implement some filtering here\n    # Return None if you want the score to be discarded\n    return normalized_score\n</code></pre> <p>The values used for the <code>eds.charlson</code> pipe are the following:</p> <pre><code>import spacy\n\n\n@spacy.registry.misc(\"score_normalization.charlson\")\ndef score_normalization(extracted_score):\n\"\"\"\n    Charlson score normalization.\n    If available, returns the integer value of the Charlson score.\n    \"\"\"\n    score_range = list(range(0, 30))\n    if (extracted_score is not None) and (int(extracted_score) in score_range):\n        return int(extracted_score)\n\n\ncharlson_config = dict(\n    score_name=\"charlson\",\n    regex=[r\"charlson\"],\n    value_extract=r\"charlson.*[\\n\\W]*(\\d+)\",\n    score_normalization=\"score_normalization.charlson\",\n)\n</code></pre>"},{"location":"pipes/ner/scores/charlson/","title":"Charlson","text":"<p>The <code>eds.charlson</code> component extracts the Charlson Comorbidity Index.</p>"},{"location":"pipes/ner/scores/charlson/#edsnlp.pipes.ner.scores.charlson.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.charlson())\n\ntext = \"\"\"\nCharlson \u00e0 l'admission: 7.\nCharlson:\nOMS:\n\"\"\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (Charlson \u00e0 l'admission: 7,)\n</code></pre> <p>We can see that only one occurrence was extracted. The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted.</p>"},{"location":"pipes/ner/scores/charlson/#edsnlp.pipes.ner.scores.charlson.factory.create_component--extensions","title":"Extensions","text":"<p>Each extraction exposes 2 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'charlson'\n\nent._.score_value\n# Out: 7\n</code></pre>"},{"location":"pipes/ner/scores/charlson/#edsnlp.pipes.ner.scores.charlson.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'charlson'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'charlson'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'charlson': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/elston-ellis/","title":"Elston-Ellis","text":"<p>Matcher for the Elston-Ellis score.</p>"},{"location":"pipes/ner/scores/elston-ellis/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.elston_ellis())\n</code></pre>"},{"location":"pipes/ner/scores/elston-ellis/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elston_ellis'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elston_ellis'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'elston_ellis': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/emergency-ccmu/","title":"Emergency CCMU","text":"<p>Matcher for explicit mentions of the French CCMU emergency score.</p>"},{"location":"pipes/ner/scores/emergency-ccmu/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_ccmu())\n</code></pre>"},{"location":"pipes/ner/scores/emergency-ccmu/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'emergency_ccmu'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value otherwise</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_ccmu'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_ccmu': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/emergency-gemsa/","title":"Emergency GEMSA","text":"<p>Matcher for explicit mentions of the French GEMSA emergency score.</p>"},{"location":"pipes/ner/scores/emergency-gemsa/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_gemsa())\n</code></pre>"},{"location":"pipes/ner/scores/emergency-gemsa/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_gemsa'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value otherwise</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_gemsa'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_gemsa': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/emergency-priority/","title":"Emergency Priority","text":"<p>Matcher for explicit mentions of the French priority emergency score.</p>"},{"location":"pipes/ner/scores/emergency-priority/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_priority())\n</code></pre>"},{"location":"pipes/ner/scores/emergency-priority/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_priority'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_priority'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_priority': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/sofa/","title":"SOFA","text":"<p>The <code>eds.sofa</code> component extracts Sequential Organ Failure Assessment (SOFA) scores, used to track a person's status during the stay in an intensive care unit to determine the extent of a person's organ function or rate failure.</p>"},{"location":"pipes/ner/scores/sofa/#edsnlp.pipes.ner.scores.sofa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sofa())\n\ntext = \"\"\"\nSOFA (\u00e0 24H) : 12.\nOMS:\n\"\"\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (SOFA (\u00e0 24H) : 12,)\n</code></pre>"},{"location":"pipes/ner/scores/sofa/#edsnlp.pipes.ner.scores.sofa.factory.create_component--extensions","title":"Extensions","text":"<p>Each extraction exposes 3 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'sofa'\n\nent._.score_value\n# Out: 12\n\nent._.score_method\n# Out: '24H'\n</code></pre> <p>Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\"</p>"},{"location":"pipes/ner/scores/sofa/#edsnlp.pipes.ner.scores.sofa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'sofa'</code> </p> <code>regex</code> <p>A list of regexes to identify the SOFA score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex to extract the score value</p> <p> TYPE: <code>Dict[str, str]</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex, and should return - None if no score could be extracted - The desired score value else</p> <p> TYPE: <code>Callable[[Union[str, None]], Any]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Flags to pass to the regex</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sofa'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'sofa': True}</code> </p>"},{"location":"pipes/qualifiers/","title":"Qualifier Overview","text":"<p>In EDS-NLP, we call qualifiers the suite of components designed to qualify a pre-extracted entity for a linguistic modality.</p>"},{"location":"pipes/qualifiers/#available-components","title":"Available components","text":"Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based medical history detection"},{"location":"pipes/qualifiers/#rationale","title":"Rationale","text":"<p>In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents.</p> <p>Now, consider the following example:</p> FrenchEnglish <pre><code>Le patient n'est pas diab\u00e9tique.\nLe patient est peut-\u00eatre diab\u00e9tique.\nLe p\u00e8re du patient est diab\u00e9tique.\n</code></pre> <pre><code>The patient is not diabetic.\nThe patient could be diabetic.\nThe patient's father is diabetic.\n</code></pre> <p>There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort.</p> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p> <p>To curb this issue, EDS-NLP proposes rule-based pipes that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort.</p>"},{"location":"pipes/qualifiers/#edsnlp.pipes.base.SpanGetterArg","title":"Where do we get our spans ?","text":"<p>A component get entities from a document by looking up <code>doc.ents</code> or <code>doc.spans[group]</code>. This behavior is set by the <code>span_getter</code> argument in components that support it.</p> <p>Valid values for the <code>span_getter</code> argument of a component can be :</p> <ul> <li>a (doc) -&gt; spans callable</li> <li>a span group name</li> <li>a list of span group names</li> <li>a dict of group name to True or list of labels</li> </ul> <p>The group name <code>\"ents\"</code> is a special case, and will get the matches from <code>doc.ents</code></p>"},{"location":"pipes/qualifiers/#edsnlp.pipes.base.SpanGetterArg--examples","title":"Examples","text":"<ul> <li><code>span_getter=[\"ents\", \"ckd\"]</code> will get the matches from both <code>doc.ents</code> and <code>doc.spans[\"ckd\"]</code>. It is equivalent to <code>{\"ents\": True, \"ckd\": True}</code>.</li> <li><code>span_getter={\"ents\": [\"foo\", \"bar\"]}</code> will get the matches with label \"foo\" and \"bar\" from <code>doc.ents</code>.</li> <li><code>span_getter=\"ents\"</code> will get all matches from <code>doc.ents</code>.</li> <li><code>span_getter=\"ckd\"</code> will get all matches from <code>doc.spans[\"ckd\"]</code>.</li> </ul>"},{"location":"pipes/qualifiers/#under-the-hood","title":"Under the hood","text":"<p>Our qualifier pipes all follow the same basic pattern:</p> <ol> <li> <p>The pipeline extracts cues. We define three (possibly overlapping) kinds :</p> <ul> <li><code>preceding</code>, ie cues that precede modulated entities ;</li> <li><code>following</code>, ie cues that follow modulated entities ;</li> <li>in some cases, <code>verbs</code>, ie verbs that convey a modulation (treated as preceding cues).</li> </ul> </li> <li> <p>The pipeline splits the text between sentences and propositions, using annotations from a sentencizer pipeline and <code>termination</code> patterns, which define syntagma/proposition terminations.</p> </li> <li> <p>For each pre-extracted entity, the pipeline checks whether there is a cue between the start of the syntagma and the start of the entity, or a following cue between the end of the entity and the end of the proposition.</p> </li> </ol> <p>Albeit simple, this algorithm can achieve very good performance depending on the modality. For instance, our <code>eds.negation</code> pipeline reaches 88% F1-score on our dataset.</p> <p>Dealing with pseudo-cues</p> <p>The pipeline can also detect pseudo-cues, ie phrases that contain cues but that are not cues themselves. For instance: <code>sans doute</code>/<code>without doubt</code> contains <code>sans/without</code>, but does not convey negation.</p> <p>Detecting pseudo-cues lets the pipeline filter out any cue that overlaps with a pseudo-cue.</p> <p>Sentence boundaries are required</p> <p>The rule-based algorithm detects cues, and propagate their modulation on the rest of the syntagma. For that reason, a qualifier pipeline needs a sentencizer component to be defined, and will fail otherwise.</p> <p>You may use EDS-NLP's:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n...\nnlp.add_pipe(eds.sentences())\n</code></pre>"},{"location":"pipes/qualifiers/#persisting-the-results","title":"Persisting the results","text":"<p>Our qualifier pipelines write their results to a custom spaCy extension, defined on both <code>Span</code> and <code>Token</code> objects. We follow the convention of naming said attribute after the pipeline itself, eg <code>Span._.negation</code> for the<code>eds.negation</code> pipeline.</p> <p>We also provide a string representation of the result, computed on the fly by declaring a getter that reads the boolean result of the pipeline. Following spaCy convention, we give this attribute the same name, followed by a <code>_</code>.</p>"},{"location":"pipes/qualifiers/family/","title":"Family Context","text":"<p>The <code>eds.family</code> component uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.</p>"},{"location":"pipes/qualifiers/family/#edsnlp.pipes.qualifiers.family.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(\n    eds.matcher(terms=dict(douleur=\"douleur\", osteoporose=\"ost\u00e9oporose\")),\n)\nnlp.add_pipe(eds.family())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents familiaux d'ost\u00e9oporose\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, ost\u00e9oporose)\n\ndoc.ents[0]._.family\n# Out: False\n\ndoc.ents[1]._.family\n# Out: True\n</code></pre>"},{"location":"pipes/qualifiers/family/#edsnlp.pipes.qualifiers.family.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.family</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>family</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token relates to a family member.</li> <li>The <code>family_</code> property is a human-readable string, computed from the <code>family</code>    attribute. It implements a simple getter function that outputs <code>PATIENT</code> or    <code>FAMILY</code>, depending on the value of <code>family</code>.</li> </ol>"},{"location":"pipes/qualifiers/family/#edsnlp.pipes.qualifiers.family.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>family</code> <p>List of terms indicating family reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_sections</code> <p>Whether to use annotated sections (namely <code>ant\u00e9c\u00e9dents familiaux</code>).</p> <p> TYPE: <code>bool, by default `False`</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/qualifiers/family/#edsnlp.pipes.qualifiers.family.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.family</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/qualifiers/history/","title":"Medical History","text":"<p>The <code>eds.history</code> pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit.</p> <p>The mere definition of a medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history, e.g., preceded by a synonym of \"medical history\".</p> <p>This component may also use the output of:</p> <ul> <li>the <code>eds.sections</code> component In that case, the entire <code>ant\u00e9c\u00e9dent</code> section is tagged as a medical history.</li> </ul> <p>Sections</p> <p>Be careful, the <code>eds.sections</code> component may oversize the <code>ant\u00e9c\u00e9dents</code> section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the <code>ant\u00e9c\u00e9dents</code> title, some parts of the document will erroneously be tagged as a medical history.</p> <p>To curb that possibility, using the output of the <code>eds.sections</code> component is deactivated by default.</p> <ul> <li>the <code>eds.dates</code> component. In that case, it will take the   dates into account to tag extracted entities as a medical history or not.</li> </ul> <p>Dates</p> <p>To take the most of the <code>eds.dates</code> component, you may add the <code>note_datetime</code> context (cf. Adding context). It allows the component to compute the duration of absolute dates (e.g., le 28 ao\u00fbt 2022/August 28, 2022). The <code>birth_datetime</code> context allows the component to exclude the birthdate from the extracted dates.</p>"},{"location":"pipes/qualifiers/history/#edsnlp.pipes.qualifiers.history.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", malaise=\"malaises\")))\nnlp.add_pipe(\n    eds.history(\n        use_sections=True,\n        use_dates=True,\n    ),\n)\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents de malaises.\"\n    \"ANT\u00c9C\u00c9DENTS : \"\n    \"- le patient a d\u00e9j\u00e0 eu des malaises. \"\n    \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, malaises, malaises, douleur)\n\ndoc.ents[0]._.history\n# Out: False\n\ndoc.ents[1]._.history\n# Out: True\n\ndoc.ents[2]._.history  # (1)\n# Out: True\n\ndoc.ents[3]._.history  # (2)\n# Out: False\n</code></pre> <ol> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>.</li> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>, however the extracted <code>relative_date</code> refers to an event that took place within 14 days.</li> </ol>"},{"location":"pipes/qualifiers/history/#edsnlp.pipes.qualifiers.history.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.history</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>history</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a medical history.</li> <li>The <code>history_</code> property is a human-readable string, computed from the <code>history</code>    attribute. It implements a simple getter function that outputs <code>CURRENT</code> or    <code>ATCD</code>, depending on the value of <code>history</code>.</li> </ol>"},{"location":"pipes/qualifiers/history/#edsnlp.pipes.qualifiers.history.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'history'</code> </p> <code>history</code> <p>List of terms indicating medical history reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_sections</code> <p>Whether to use section pipeline to detect medical history section.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_dates</code> <p>Whether to use dates pipeline to detect if the event occurs  a long time before the document date.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>history_limit</code> <p>The number of days after which the event is considered as history.</p> <p> TYPE: <code>Union[int, timedelta]</code> DEFAULT: <code>14</code> </p> <code>exclude_birthdate</code> <p>Whether to exclude the birthdate from history dates.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>closest_dates_only</code> <p>Whether to include the closest dates only.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tz</code> <p>The timezone to use. Defaults to \"Europe/Paris\".</p> <p> TYPE: <code>Optional[Union[str, tzinfo]]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/qualifiers/history/#edsnlp.pipes.qualifiers.history.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.history</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/qualifiers/hypothesis/","title":"Hypothesis","text":"<p>The <code>eds.hypothesis</code> pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding hypothesis, ie cues that precede a hypothetical expression</li> <li>following hypothesis, ie cues that follow a hypothetical expression</li> <li>pseudo hypothesis : contain a hypothesis cue, but are not hypothesis   (eg \"pas de doute\"/\"no doubt\")</li> <li>hypothetical verbs : verbs indicating hypothesis (eg \"douter\")</li> <li>classic verbs conjugated to the conditional, thus indicating hypothesis</li> </ul> <ol><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", fracture=\"fracture\")))\nnlp.add_pipe(eds.hypothesis())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Possible fracture du radius.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, fracture)\n\ndoc.ents[0]._.hypothesis\n# Out: False\n\ndoc.ents[1]._.hypothesis\n# Out: True\n</code></pre>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.hypothesis</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>hypothesis</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a speculation.</li> <li>The <code>hypothesis_</code> property is a human-readable string, computed from the    <code>hypothesis</code> attribute. It implements a simple getter function that outputs    <code>HYP</code> or <code>CERT</code>, depending on the value of <code>hypothesis</code>.</li> </ol>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at APHP's CDW to test the   component on actual clinical notes, using pseudonymised notes from the APHP's CDW.</li> </ul> Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding hypothesis cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_hyp</code> <p>List of hypothetical verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_eds</code> <p>List of mainstream verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hypothesis</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/qualifiers/negation/","title":"Negation","text":"<p>The <code>eds.negation</code> component uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al., 2001.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding negations, i.e., cues that precede a negated expression</li> <li>following negations, i.e., cues that follow a negated expression</li> <li>pseudo negations : contain a negation cue, but are not negations   (eg \"pas de doute\"/\"no doubt\")</li> <li>negation verbs, i.e., verbs that indicate a negation</li> <li>terminations, i.e., words that delimit propositions.   The negation spans from the preceding cue to the termination.</li> </ul> <ol><li><p><p>Chapman W.W., Bridewell W., Hanbury P., Cooper G.F. and Buchanan B.G., 2001. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics. 34, pp.301--310. 10.1006/jbin.2001.1029</p></p></li><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", fracture=\"fracture\")))\nnlp.add_pipe(eds.negation())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Le scanner ne d\u00e9tecte aucune fracture.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, fracture)\n\ndoc.ents[0]._.negation  # (1)\n# Out: False\n\ndoc.ents[1]._.negation\n# Out: True\n</code></pre> <ol> <li>The result of the component is kept in the <code>negation</code> custom extension.</li> </ol>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.negation</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>negation</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is negated.</li> <li>The <code>negation_</code> property is a human-readable string, computed from the <code>negation</code>    attribute. It implements a simple getter function that outputs <code>AFF</code> or <code>NEG</code>,    depending on the value of <code>negation</code>.</li> </ol>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at AP-HP to test the component   on actual clinical notes, using pseudonymised notes from the AP-HP.</li> </ul> Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding negation cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding_regex</code> <p>List of preceding negation cues, but as regexes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of negation verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.negation</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/qualifiers/reported-speech/","title":"Reported Speech","text":"<p>The <code>eds.reported_speech</code> component uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.</p>"},{"location":"pipes/qualifiers/reported-speech/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", alcool=\"alcoolis\u00e9\")))\nnlp.add_pipe(eds.reported_speech())\n\ntext = (\n    \"Le patient est admis aux urgences ce soir pour une douleur au bras. \"\n    \"Il nie \u00eatre alcoolis\u00e9.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, alcoolis\u00e9)\n\ndoc.ents[0]._.reported_speech\n# Out: False\n\ndoc.ents[1]._.reported_speech\n# Out: True\n</code></pre>"},{"location":"pipes/qualifiers/reported-speech/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.reported_speech</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>reported_speech</code> attribute is a boolean, set to <code>True</code> if the component    predicts that the span/token is reported.</li> <li>The <code>reported_speech_</code> property is a human-readable string, computed from the    <code>reported_speech</code> attribute. It implements a simple getter function that outputs    <code>DIRECT</code> or <code>REPORTED</code>, depending on the value of <code>reported_speech</code>.</li> </ol>"},{"location":"pipes/qualifiers/reported-speech/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>quotation</code> <p>String gathering all quotation cues.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of reported speech verbs.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of terms following a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of terms preceding a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/qualifiers/reported-speech/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reported_speech</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/trainable/","title":"Trainable components overview","text":"<p>In addition to its rule-based pipeline components, EDS-NLP offers new trainable components to fit and run machine learning models for classic biomedical information extraction tasks.</p> <p>All trainable components implement the <code>TorchComponent</code> class, which provides a common API for training and inference.</p>"},{"location":"pipes/trainable/#available-components","title":"Available components :","text":"Name Description <code>eds.transformer</code> Embed text with a transformer model <code>eds.text_cnn</code> Contextualize embeddings with a CNN <code>eds.span_pooler</code> A span embedding component that aggregates word embeddings <code>eds.ner_crf</code> A trainable component to extract entities <code>eds.span_classifier</code> A trainable component for multi-class multi-label span classification <code>eds.span_linker</code> A trainable entity linker (i.e. to a list of concepts)"},{"location":"pipes/trainable/ner/","title":"Trainable NER","text":"<p>The <code>eds.ner_crf</code> component is a general purpose trainable named entity recognizer. It can extract:</p> <ul> <li>flat entities</li> <li>overlapping entities of different labels</li> </ul> <p>However, at the moment, the model cannot currently extract entities that are nested inside larger entities of the same label.</p> <p>It is based on a CRF (Conditional Random Field) layer and should therefore work well on dataset composed of entities will ill-defined boundaries. We offer a compromise between speed and performance by allowing the user to specify a window size for the CRF layer. The smaller the window, the faster the model will be, but at the cost of degraded performance.</p> <p>The pipeline assigns both <code>doc.ents</code> (in which overlapping entities are filtered out) and <code>doc.spans</code>. These destinations can be inferred from the <code>target_span_getter</code> parameter, combined with the <code>post_init</code> step.</p> <ol><li><p><p>Wajsb\u00fcrt P., 2021. Extraction and normalization of simple and structured entities in medical documents. https://hal.archives-ouvertes.fr/tel-03624928</p></p></li></ol>"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--architecture","title":"Architecture","text":"<p>The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts <code>n_labels</code> sequences of O, I, B, L, U tags. The architecture is displayed in the figure below.</p> <p>To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction.</p> <p> </p> Nested NER architecture"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--examples","title":"Examples","text":"<p>Let us define a pipeline composed of a transformer, and a NER component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.ner_crf(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        window=10,\n    ),\n    name=\"ner\"\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p>"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ner_crf'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code></p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ner_crf</code> pipeline was developed by AP-HP's Data Science team.</p> <p>The deep learning model was adapted from Wajsb\u00fcrt, 2021.</p>"},{"location":"pipes/trainable/span-classifier/","title":"Trainable Span Classifier","text":"<p>The <code>eds.span_classifier</code> component is a trainable attribute predictor. In this context, the span classification task consists in assigning values (boolean, strings or any object) to attributes/extensions of spans such as:</p> <ul> <li><code>span._.negation</code>,</li> <li><code>span._.date.mode</code></li> <li><code>span._.cui</code></li> </ul> <p>In the rest of this page, we will refer to a pair of (attribute, value) as a \"binding\". For instance, the binding <code>(\"_.negation\", True)</code> means that the attribute <code>negation</code> of the span is (or should be, when predicted) set to <code>True</code>.</p>"},{"location":"pipes/trainable/span-classifier/#edsnlp.pipes.trainable.span_classifier.factory.create_component--architecture","title":"Architecture","text":"<p>The model performs span classification by:</p> <ol> <li>Calling a word pooling embedding such as <code>eds.span_pooler</code> to compute a single embedding for each span</li> <li>Computing logits for each possible binding using a linear layer</li> <li> <p>Splitting these bindings into groups of exclusive values such as</p> <ul> <li><code>event=start</code> and <code>event=stop</code></li> <li><code>negated=False</code> and <code>negated=True</code></li> </ul> <p>Note that the above groups are not exclusive, but the values within each group are.</p> </li> <li> <p>Applying the best scoring binding in each group to each span</p> </li> </ol>"},{"location":"pipes/trainable/span-classifier/#edsnlp.pipes.trainable.span_classifier.factory.create_component--examples","title":"Examples","text":"<p>To create a span classifier component, you can use the following code:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_classifier(\n        # To embed the spans, we will use a span pooler\n        embedding=eds.span_pooler(\n            pooling_mode=\"mean\",  # mean pooling\n            # that will use a transformer to embed the doc words\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n        span_getter=[\"ents\", \"sc\"],\n        # For every span embedded by the span pooler\n        # (doc.ents and doc.spans[\"sc\"]), we will predict both\n        # span._.negation and span._.event_type\n        attributes=[\"_.negation\", \"_.event_type\"],\n    ),\n    name=\"span_classifier\",\n)\n</code></pre> <p>To infer the values of the attributes, you can use the pipeline <code>post_init</code> method:</p> <pre><code>nlp.post_init(gold_data)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>You can inspect the bindings that will be used for training and prediction <pre><code>print(nlp.pipes.attr.bindings)\n# list of (attr name, span labels or True if all, values)\n# Out: [\n#   ('_.negation', True, [True, False]),\n#   ('_.event_type', True, ['start', 'stop'])\n# ]\n</code></pre></p> <p>You can also change these values and update the bindings by calling the <code>update_bindings</code> method. Don't forget to retrain the model if new values are added !</p>"},{"location":"pipes/trainable/span-classifier/#edsnlp.pipes.trainable.span_classifier.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_classifier'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>span_getter</code> <p>How to extract the candidate spans and the attributes to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). This can be:</p> <ul> <li>a <code>SpanGetterArg</code> to retrieve contexts from a whole document. For example   <code>{\"section\": \"conclusion\"}</code> to only use the conclusion as context (you   must ensure that all spans produced by the <code>span_getter</code> argument do fall   in the conclusion in this case)</li> <li>a callable, that gets a span and should return a context for this span.   For instance, <code>lambda span: span.sent</code> to use the sentence as context.</li> </ul> <p> TYPE: <code>Optional[Union[Callable, SpanGetterArg]]</code> DEFAULT: <code>None</code> </p> <code>attributes</code> <p>The attributes to predict or train on. If a dict is given, keys are the attributes and values are the labels for which the attr is allowed, or True if the attr is allowed for all labels.</p> <p> TYPE: <code>AttributesArg</code> DEFAULT: <code>None</code> </p> <code>keep_none</code> <p>If False, skip spans for which a attr returns None. If True (default), the None values will be learned and predicted, just as any other value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/trainable/span-linker/","title":"Trainable Span Linker","text":"<p>The <code>eds.span_linker</code> component is a trainable span concept predictor, typically used to match spans in the text with concepts in a knowledge base. This task is known as \"Entity Linking\", \"Named Entity Disambiguation\" or \"Normalization\" (the latter is mostly used in the biomedical machine learning community).</p> <p>Entity Linking vs Named Entity Recognition</p> <p>Entity Linking is the task of linking existing entities to their concept in a knowledge base, while Named Entity Recognition is the task of detecting spans in the text that correspond to entities. The <code>eds.span_linker</code> component should therefore be used after the Named Entity Recognition step (e.g. using the <code>eds.ner_crf</code> component).</p> <ol><li><p><p>Wajsb\u00fcrt P., Sarfati A. and Tannier X., 2021. Medical concept normalization in French using multilingual terminologies and contextual embeddings. Journal of Biomedical Informatics. 114, pp.103684. https://doi.org/10.1016/j.jbi.2021.103684</p></p></li></ol>"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--how-it-works","title":"How it works","text":"<p>To perform this task, this components compare the embedding of a given query span (e.g. \"aspirin\") with the embeddings in the knowledge base, where each embedding represents a concept (e.g. \"B01AC06\"), and selects the most similar embedding and returns its concept id. This comparison is done using either:</p> <ul> <li>the cosine similarity between the input and output embeddings (recommended)</li> <li>a simple dot product</li> </ul> <p>We filter out the concepts that are not relevant for a given query by using groups. For each span to link, we use its label to select a group of concepts to compare with. For example, if the span is labeled as \"drug\", we only compare it with concepts that are drugs. These concepts groups are inferred from the training data when running the <code>post_init</code> method, or can be provided manually using the <code>pipe.update_concepts(concepts, mapping, [embeddings])</code> method. If a label is not found in the mapping, the span is compared with all concepts.</p> <p>We support comparing entity queries against two kind of references : either the embeddings of the concepts themselves (<code>reference_mode = \"concept\"</code>), or the embeddings of the synonyms of the concepts (<code>reference_mode = \"synonym\"</code>).</p>"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--synonym-similarity","title":"Synonym similarity","text":"<p>When performing span linking in <code>synonym</code> mode, the span linker embedding matrix contains one embedding vector per concept per synonym, and each embedding maps to the concept of its synonym. This mode is slower and more memory intensive, since you have to store multiple embeddings per concept, but it can yield good results in zero-shot scenarios (see example below).</p> <p> </p> Entity linking based on synonym similarity"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--concept-similarity","title":"Concept similarity","text":"<p>In <code>concept</code> mode, the span linker embedding matrix contains one embedding vector per concept : imagine a single vector that approximately averages all the synonyms of a concept (e.g. B01AC06 = average of \"aspirin\", \"acetyl-salicylic acid\", etc.). This mode is faster and more memory efficient, but usually requires that the concept weights are fine-tuned.</p> <p> </p> Entity linking based on concept similarity"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--examples","title":"Examples","text":"<p>Here is how you can use the <code>eds.span_linker</code> component to link spans without training, in <code>synonym</code> mode. You will still need to pre-compute the embeddings of the target synonyms.</p> <p>First, initialize the component:</p> <pre><code>import pandas as pd\nimport edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_linker(\n        rescale=20.0,\n        threshold=0.8,\n        metric=\"cosine\",\n        reference_mode=\"synonym\",\n        probability_mode=\"sigmoid\",\n        span_getter=[\"ents\"],\n        embedding=eds.span_pooler(\n            hidden_size=128,\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"linker\",\n)\n</code></pre> <p>We will assume you have a list of synonyms with their concept and label with the columns:</p> <ul> <li><code>STR</code>: synonym text</li> <li><code>CUI</code>: concept id</li> <li><code>GRP</code>: label.</li> </ul> <p>All we need to do is to initialize the component with the synonyms and that's it ! Since we have set <code>init_weights</code> to True, and we are in <code>synonym</code> mode, the embeddings of the synonyms will be stored in the component and used to compute the similarity scores</p> <pre><code>synonyms_df = pd.read_csv(\"synonyms.csv\")\n\ndef make_doc(row):\n    doc = nlp.make_doc(row[\"STR\"])\n    span = doc[:]\n    span.label_ = row[\"GRP\"]\n    doc.ents = [span]\n    span._.cui = row[\"CUI\"]\n    return doc\n\nnlp.post_init(\n    edsnlp.data.from_pandas(\n        synonyms_df,\n        converter=make_doc,\n    )\n)\n</code></pre> <p>Now, you can now use it in a text: <pre><code>doc = nlp.make_doc(\"Aspirin is a drug\")\nspan = doc[0:1]  # \"Aspirin\"\nspan.label_ = \"Drug\"\ndoc.ents = [span]\n\ndoc = nlp(doc)\nprint(doc.ents[0]._.cui)\n# \"B01AC06\"\n</code></pre></p> <p>To use the <code>eds.span_linker</code> component in <code>class</code> mode, we refer to the following repository: deep_multilingual_normalization based on the work of Wajsb\u00fcrt et al., 2021.</p>"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Spacy vocabulary</p> <p> </p> <code>name</code> <p>Name of the component</p> <p> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>metric</code> <p>Whether to compute the cosine similarity between the input and output embeddings or the dot product.</p> <p> TYPE: <code>Literal[\"cosine\", \"dot\"] = \"cosine\"</code> DEFAULT: <code>cosine</code> </p> <code>rescale</code> <p>Rescale the output cosine similarities by a constant factor.</p> <p> TYPE: <code>float</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>Threshold probability to consider a concept as valid</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>attribute</code> <p>The attribute to store the concept id</p> <p> TYPE: <code>str</code> DEFAULT: <code>cui</code> </p> <code>reference_mode</code> <p>Whether to compare the embeddings with the concepts embeddings (one per concept) or the synonyms embeddings (one per concept per synonym). See above for more details.</p> <p> TYPE: <code>Literal['concept', 'synonym']</code> DEFAULT: <code>concept</code> </p> <code>span_getter</code> <p>How to extract the candidate spans to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the entity only, so no context)</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>probability_mode</code> <p>Whether to compute the probabilities using a softmax or a sigmoid function. This will also determine the loss function to use, either cross-entropy or binary cross-entropy.</p> <p>Subsetting the concepts</p> <p>The probabilities returned in <code>softmax</code> mode depend on the number of concepts (as an extreme cas, if you have only one concept, its softmax probability will always be 1). This is why we recommend using the <code>sigmoid</code> mode in which the probabilities are computed independently for each concept.</p> <p> TYPE: <code>Literal['softmax', 'sigmoid']</code> DEFAULT: <code>sigmoid</code> </p> <code>init_weights</code> <p>Whether to initialize the weights of the component with the embeddings of the entities of the docs provided to the <code>post_init</code> method. How this is done depends on the <code>reference_mode</code> parameter:</p> <ul> <li><code>concept</code>: the embeddings are averaged</li> <li><code>synonym</code>: the embeddings are stored as is</li> </ul> <p>By default, this is set to <code>True</code> if <code>reference_mode</code> is <code>synonym</code>, and <code>False</code> otherwise.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.span_linker</code> component was developed by AP-HP's Data Science team.</p> <p>The deep learning concept-based architecture was adapted from Wajsb\u00fcrt et al., 2021.</p>"},{"location":"pipes/trainable/embeddings/span_pooler/","title":"Span Pooler","text":"<p>The <code>eds.span_pooler</code> component is a trainable span embedding component. It generates span embeddings from a word embedding component and a span getter. It can be used to train a span classifier, as in <code>eds.span_classifier</code>.</p>"},{"location":"pipes/trainable/embeddings/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_pooler'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>pooling_mode</code> <p>How word embeddings are aggregated into a single embedding per span.</p> <p> TYPE: <code>Literal['max', 'sum', 'mean']</code> DEFAULT: <code>mean</code> </p> <code>hidden_size</code> <p>The size of the hidden layer. If None, no projection is done and the output of the span pooler is used directly.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/trainable/embeddings/text_cnn/","title":"Text CNN","text":"<p>The <code>eds.text_cnn</code> component is a simple 1D convolutional network to contextualize word embeddings (as computed by the <code>embedding</code> component passed as argument).</p>"},{"location":"pipes/trainable/embeddings/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>embedding</code> <p>Embedding module to apply to the input</p> <p> TYPE: <code>TorchComponent[WordEmbeddingBatchOutput, BatchInput]</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>relu</code> </p> <code>residual</code> <p>Whether to use residual connections</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize</code> <p>Whether to normalize before or after the residual connection</p> <p> TYPE: <code>Literal['pre', 'post', 'none']</code> DEFAULT: <code>pre</code> </p>"},{"location":"pipes/trainable/embeddings/transformer/","title":"Transformer","text":"<p>The <code>eds.transformer</code> component is a wrapper around HuggingFace's transformers library. If you are not familiar with transformers, a good way to start is the Illustrated Transformer tutorial.</p> <p>Compared to using the raw Huggingface model, we offer a simple mechanism to split long documents into strided windows before feeding them to the model.</p>"},{"location":"pipes/trainable/embeddings/transformer/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--windowing","title":"Windowing","text":"<p>EDS-NLP's Transformer component splits long documents into smaller windows before feeding them to the model. This is done to avoid hitting the maximum number of tokens that can be processed by the model on a single device. The window size and stride can be configured using the <code>window</code> and <code>stride</code> parameters. The default values are 512 and 256 respectively, which means that the model will process windows of 512 tokens, each separated by 256 tokens. Whenever a token appears in multiple windows, the embedding of the \"most contextualized\" occurrence is used, i.e. the occurrence that is the closest to the center of its window.</p> <p>Here is an overview how this works to produce embeddings (shown in red) for each word of the document :</p> <p></p>"},{"location":"pipes/trainable/embeddings/transformer/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--examples","title":"Examples","text":"<p>Here is an example of how to define a pipeline with a Transformer component:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.transformer(\n        model=\"prajjwal1/bert-tiny\",\n        window=128,\n        stride=96,\n    ),\n)\n</code></pre> <p>You can then compose this embedding with a task specific component such as <code>eds.ner_crf</code>.</p>"},{"location":"pipes/trainable/embeddings/transformer/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline instance</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name</p> <p> TYPE: <code>str</code> DEFAULT: <code>'transformer'</code> </p> <code>model</code> <p>The Huggingface model name or path</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>window</code> <p>The window size to use when splitting long documents into smaller windows before feeding them to the Transformer model (default: 512 = 512 - 2)</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>stride</code> <p>The stride (distance between windows) to use when splitting long documents into smaller windows: (default: 96)</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>training_stride</code> <p>If False, the stride will be set to the window size during training, meaning that there will be no overlap between windows. If True, the stride will be set to the <code>stride</code> parameter during training, just like during inference.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_tokens_per_device</code> <p>The maximum number of tokens that can be processed by the model on a single device. This does not affect the results but can be used to reduce the memory usage of the model, at the cost of a longer processing time.</p> <p>If \"auto\", the component will try to estimate the maximum number of tokens that can be processed by the model on the current device at a given time.</p> <p> TYPE: <code>Union[int, Literal['auto']]</code> DEFAULT: <code>auto</code> </p> <code>span_getter</code> <p>Which spans of the document should be embedded. Defaults to the full document if None.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>We provide step-by-step guides to get you started. We cover the following use-cases:</p> <p> Spacy representations</p><p>Learn the basics of how documents are represented with spaCy.</p><p> Matching a terminology</p><p>Extract phrases that belong to a given terminology.</p><p> Qualifying entities</p><p>Ensure extracted concepts are not invalidated by linguistic modulation.</p><p> Detecting dates</p><p>Detect and parse dates in a text.</p><p> Processing multiple texts</p><p>Improve the inference speed of your pipeline</p><p> Detecting hospitalisation reason</p><p>Identify spans mentioning the reason for hospitalisation or tag entities as the reason.</p><p>\u21b5 Detecting false endlines</p><p>Classify each line end and add the <code>excluded</code> attribute to these tokens.</p><p> Aggregating results</p><p>Aggregate the results of your pipeline at the document level.</p><p> FastAPI</p><p>Deploy your pipeline as an API.</p><p> Make a training script</p><p>Learn how to train a NER pipeline with EDS-NLP.</p>"},{"location":"tutorials/aggregating-results/","title":"Aggregating results","text":""},{"location":"tutorials/aggregating-results/#rationale","title":"Rationale","text":"<p>In some cases, you are not interested in individual extractions, but rather in document-level aggregated variables. For instance, you may be interested to know if a patient is diabetic without caring abou the actual mentions of diabetes. Here, we propose a simple and generic rule which work by:</p> <ul> <li>Extracting entities via methods of your choice</li> <li>Qualifiy those entities and discard appropriate entities</li> <li>Set a threshold on the minimal number of entities that should be present in the document to aggregate them.</li> </ul>"},{"location":"tutorials/aggregating-results/#an-example-for-the-disorders-pipes","title":"An example for the disorders pipes","text":"<p>Below is a simple implementation of this aggregation rule (this can be adapted for other comorbidity components and other qualification methods):</p> <pre><code>MIN_NUMBER_ENTITIES = 2  # (1)!\n\nif not Doc.has_extension(\"aggregated\"):\n    Doc.set_extension(\"aggregated\", default={})  # (2)!\n\nspans = doc.spans[\"diabetes\"]  # (3)!\nkept_spans = [\n    (span, span._.status, span._.detailed_status)\n    for span in spans\n    if not any([span._.negation, span._.hypothesis, span._.family])\n]  # (4)!\n\nif len(kept_spans) &lt; MIN_NUMBER_ENTITIES:  # (5)!\n    status = \"ABSENT\"\n\nelse:\n    status = max(kept_spans, key=itemgetter(1))[2]  # (6)!\n\ndoc._.aggregated[\"diabetes\"] = status\n</code></pre> <ol> <li>We want at least 2 correct entities</li> <li>Storing the status in the <code>doc._.aggregated</code> dictionary</li> <li>Getting status for the <code>diabetes</code> component</li> <li>Disregarding entities which are either negated, hypothetical, or not about the patient himself</li> <li>Setting the status to 0 if less than 2 relevant entities are left:</li> <li>Getting the maximum severity status</li> </ol> <ol></ol>"},{"location":"tutorials/detecting-dates/","title":"Detecting dates","text":"<p>We now know how to match a terminology and qualify detected entities, which covers most use cases for a typical medical NLP project. In this tutorial, we'll see how to use EDS-NLP to detect and normalise date mentions using <code>eds.dates</code>.</p> <p>This can have many applications, for dating medical events in particular. The <code>eds.consultation_dates</code> component, for instance, combines the date detection capabilities with a few simple patterns to detect the date of the consultation, when mentioned in clinical reports.</p>"},{"location":"tutorials/detecting-dates/#dates-in-clinical-notes","title":"Dates in clinical notes","text":"<p>Consider the following example:</p> FrenchEnglish <pre><code>Le patient est admis le 21 janvier pour une douleur dans le cou.\nIl se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\n</code></pre> <pre><code>The patient is admitted on January 21st for a neck pain.\nHe complains about chronique pain that started three years ago.\n</code></pre> <p>Clinical notes contain many different types of dates. To name a few examples:</p> Type Description Examples Absolute Explicit date <code>2022-03-03</code> Partial Date missing the day, month or year <code>le 3 janvier/on January 3rd</code>, <code>en 2021/in 2021</code> Relative Relative dates <code>hier/yesterday</code>, <code>le mois dernier/last month</code> Duration Durations <code>pendant trois mois/for three months</code> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p>"},{"location":"tutorials/detecting-dates/#extracting-dates","title":"Extracting dates","text":"<p>The followings snippet adds the <code>eds.dates</code> component to the pipeline:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates())  # (1)\n\ntext = (\n    \"Le patient est admis le 21 janvier pour une douleur dans le cou.\\n\"\n    \"Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\"\n)\n\n# Detecting dates becomes trivial\ndoc = nlp(text)\n\n# Likewise, accessing detected dates is hassle-free\ndates = doc.spans[\"dates\"]  # (2)\n</code></pre> <ol> <li>The date detection component is declared with <code>eds.dates</code></li> <li>Dates are saved in the <code>doc.spans[\"dates\"]</code> key</li> </ol> <p>After this, accessing dates and there normalisation becomes trivial:</p> <pre><code># \u2191 Omitted code above \u2191\n\ndates  # (1)\n# Out: [21 janvier, il y a trois ans]\n</code></pre> <ol> <li><code>dates</code> is a list of spaCy <code>Span</code> objects.</li> </ol>"},{"location":"tutorials/detecting-dates/#normalisation","title":"Normalisation","text":"<p>We can review each date and get its normalisation:</p> <code>date.text</code> <code>date._.date</code> <code>21 janvier</code> <code>{\"day\": 21, \"month\": 1}</code> <code>il y a trois ans</code> <code>{\"direction\": \"past\", \"year\": 3}</code> <p>Dates detected by the pipeline component are parsed into a dictionary-like object. It includes every information that is actually contained in the text.</p> <p>To get a more usable representation, you may call the <code>to_datetime()</code> method. If there's enough information, the date will be represented in a <code>datetime.datetime</code> or <code>datetime.timedelta</code> object. If some information is missing, It will return <code>None</code>. Alternatively for this case, you can optionally set to <code>True</code> the parameter <code>infer_from_context</code> and you may also give a value for <code>note_datetime</code>.</p> <p>Date normalisation</p> <p>Since dates can be missing some information (eg <code>en ao\u00fbt</code>), we refrain from outputting a <code>datetime</code> object in that case. Doing so would amount to guessing, and we made the choice of letting you decide how you want to handle missing dates.</p>"},{"location":"tutorials/detecting-dates/#what-next","title":"What next?","text":"<p>The <code>eds.dates</code> pipe component's role is merely to detect and normalise dates. It is the user's responsibility to use this information in a downstream application.</p> <p>For instance, you could use this pipeline to date medical entities. Let's do that.</p>"},{"location":"tutorials/detecting-dates/#a-medical-event-tagger","title":"A medical event tagger","text":"<p>Our pipeline will detect entities and events separately, and we will post-process the output <code>Doc</code> object to determine whether a given entity can be linked to a date.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom datetime import datetime\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(\n    eds.matcher(\n        regex=dict(admission=[\"admissions?\", \"admise?\", \"prise? en charge\"]),\n        attr=\"LOWER\",\n    )\n)\n\ntext = (\n    \"Le patient est admis le 12 avril pour une douleur \"\n    \"survenue il y a trois jours. \"\n    \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n</code></pre> <p>At this point, the document is ready to be post-processed: its <code>ents</code> and <code>spans[\"dates\"]</code> are populated:</p> <pre><code># \u2191 Omitted code above \u2191\n\ndoc.ents\n# Out: (admis, pris en charge)\n\ndoc.spans[\"dates\"]\n# Out: [12 avril, il y a trois jours, l'ann\u00e9e derni\u00e8re, mai 1995]\n\nnote_datetime = datetime(year=1999, month=8, day=27)\n\nfor i, date in enumerate(doc.spans[\"dates\"]):\n    print(\n        i,\n        \" - \",\n        date,\n        \" - \",\n        date._.date.to_datetime(\n            note_datetime=note_datetime, infer_from_context=False, tz=None\n        ),\n    )\n    # Out: 0  -  12 avril  -  None\n    # Out: 1  -  il y a trois jours  -  1999-08-24 00:00:00\n    # Out: 2  -  l'ann\u00e9e derni\u00e8re  -  1998-08-27 00:00:00\n    # Out: 3  -  mai 1995  -  None\n\n\nfor i, date in enumerate(doc.spans[\"dates\"]):\n    print(\n        i,\n        \" - \",\n        date,\n        \" - \",\n        date._.date.to_datetime(\n            note_datetime=note_datetime,\n            infer_from_context=True,\n            tz=None,\n            default_day=15,\n        ),\n    )\n    # Out: 0  -  12 avril  -  1999-04-12T00:00:00\n    # Out: 1  -  il y a trois jours  -  1999-08-24 00:00:00\n    # Out: 2  -  l'ann\u00e9e derni\u00e8re  -  1998-08-27 00:00:00\n    # Out: 3  -  mai 1995  -  1995-05-15T00:00:00\n</code></pre> <p>As a first heuristic, let's consider that an entity can be linked to a date if the two are in the same sentence. In the case where multiple dates are present, we'll select the closest one.</p> utils.py<pre><code>from edsnlp.tokens import Span\nfrom typing import List, Optional\n\n\ndef candidate_dates(ent: Span) -&gt; List[Span]:\n\"\"\"Return every dates in the same sentence as the entity\"\"\"\n    return [date for date in ent.doc.spans[\"dates\"] if date.sent == ent.sent]\n\n\ndef get_event_date(ent: Span) -&gt; Optional[Span]:\n\"\"\"Link an entity to the closest date in the sentence, if any\"\"\"\n\n    dates = candidate_dates(ent)  # (1)\n\n    if not dates:\n        return\n\n    dates = sorted(\n        dates,\n        key=lambda d: min(abs(d.start - ent.end), abs(ent.start - d.end)),\n    )\n\n    return dates[0]  # (2)\n</code></pre> <ol> <li>Get all dates present in the same sentence.</li> <li>Sort the dates, and keep the first item.</li> </ol> <p>We can apply this simple function:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom utils import get_event_date\nfrom datetime import datetime\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(\n    eds.matcher(\n        regex=dict(admission=[\"admissions?\", \"admise?\", \"prise? en charge\"]),\n        attr=\"LOWER\",\n    )\n)\n\ntext = (\n    \"Le patient est admis le 12 avril pour une douleur \"\n    \"survenue il y a trois jours. \"\n    \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\"\n)\n\ndoc = nlp(text)\nnow = datetime.now()\n\nfor ent in doc.ents:\n    if ent.label_ != \"admission\":\n        continue\n    date = get_event_date(ent)\n    print(f\"{ent.text:&lt;20}{date.text:&lt;20}{date._.date.to_datetime(now).strftime('%d/%m/%Y'):&lt;15}{date._.date.to_duration(now)}\")\n# Out: admis               12 avril            12/04/2023     21 weeks 4 days 6 hours 3 minutes 26 seconds\n# Out: pris en charge      l'ann\u00e9e derni\u00e8re    10/09/2022     -1 year\n</code></pre> <p>Which will output:</p> <code>ent</code> <code>get_event_date(ent)</code> <code>get_event_date(ent)._.date.to_datetime()</code> admis 12 avril <code>2020-04-12T00:00:00+02:00</code> pris en charge l'ann\u00e9e derni\u00e8re <code>-1 year</code> <ol></ol>"},{"location":"tutorials/endlines/","title":"Detecting end-of-lines","text":"<p>A common problem in medical corpus is that the character <code>\\n</code> does not necessarily correspond to a real new line as in other domains.</p> <p>For example, it is common to find texts like:</p> <pre><code>Il doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\n</code></pre> <p>Inserted new line characters</p> <p>This issue is especially impactful for clinical notes that have been extracted from PDF documents. In that case, the new line character could be deliberately inserted by the doctor, or more likely added to respect the layout during the edition of the PDF.</p> <p>The aim of this tutorial is to train a unsupervised model to detect this false endlines and to use it for inference. The implemented model is based on the work of Zweigenbaum et alZweigenbaum et al., 2016.</p>"},{"location":"tutorials/endlines/#training-the-model","title":"Training the model","text":"<p>Let's train the model using an example corpus of three documents:</p> <pre><code>import edsnlp\nfrom edsnlp.pipes.core.endlines.model import EndLinesModel\n\nnlp = edsnlp.blank(\"eds\")\n\ntext1 = \"\"\"Le patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010;\nFumeur, il est arr\u00eat\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diab\u00e8te\n\"\"\"\n\ntext2 = \"\"\"J'aime le \\nfromage...\\n\"\"\"\ntext3 = (\n    \"/n\"\n    \"Intervention(s) - acte(s) r\u00e9alis\u00e9(s) :/n\"\n    \"Parathyro\u00efdectomie \u00e9lective le [DATE]\"\n)\n\ntexts = [\n    text1,\n    text2,\n    text3,\n]\n\ncorpus = nlp.pipe(texts)\n\n# Fit the model\nendlines = EndLinesModel(nlp=nlp)  # (1)\ndf = endlines.fit_and_predict(corpus)  # (2)\n\n# Save model\nPATH = \"/tmp/path_to_model\"\nendlines.save(PATH)\n</code></pre> <ol> <li>Initialize the <code>EndLinesModel</code>    object and then fit (and predict) in the training corpus.</li> <li>The corpus should be an iterable of edsnlp documents.</li> </ol>"},{"location":"tutorials/endlines/#use-a-trained-model-for-inference","title":"Use a trained model for inference","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nPATH = \"/path_to_model\"\nnlp.add_pipe(eds.endlines(model_path=PATH))  # (1)\nnlp.add_pipe(eds.sentences())  # (1)\n\ndocs = list(nlp.pipe([text1, text2, text3]))\n\ndoc = docs[1]\ndoc\n# Out: J'aime le\n# Out: fromage...\n\nlist(doc.sents)[0]\n# Out: J'aime le\n# Out: fromage...\n</code></pre> <ol> <li>You should specify the path to the trained model here.</li> <li>All fake new line are excluded by setting their <code>tag</code> to 'EXCLUDED' and all true new lines' <code>tag</code> are set to 'ENDLINE'.</li> </ol>"},{"location":"tutorials/endlines/#declared-extensions","title":"Declared extensions","text":"<p>It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p> <ol><li><p><p>Zweigenbaum P., Grouin C. and Lavergne T., 2016. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (End-of-line classification with no supervision). https://aclanthology.org/2016.jeptalnrecital-poster.7</p></p></li></ol>"},{"location":"tutorials/make-a-training-script/","title":"Making a training script","text":"<p>In this tutorial, we'll see how we can train a deep learning model with EDS-NLP. We will implement a script to train a named-entity recognition (NER) model.</p>"},{"location":"tutorials/make-a-training-script/#step-by-step-walkthrough","title":"Step-by-step walkthrough","text":"<p>Training a supervised deep-learning model consists in feeding batches of annotated samples taken from a training corpus to a model and optimizing its parameters of the model to decrease its prediction error. The process of training a pipeline with EDS-NLP is structured as follows:</p>"},{"location":"tutorials/make-a-training-script/#1-defining-the-model","title":"1. Defining the model","text":"<p>We first start by seeding the random states and instantiating a new trainable pipeline. The model described here computes text embeddings with a pre-trained transformer followed by a CNN, and performs the NER prediction task using a Conditional Random Field (CRF) token classifier. To compose deep-learning modules, we nest them in a dictionary : each new dictionary will instantiate a new module, and the <code>@factory</code> key will be used to select the class of the module.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom confit.utils.random import set_seed\n\nset_seed(42)\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.ner_crf(  # (1)\n        mode=\"joint\",  # (2)\n        target_span_getter=\"ml-ner\",  # (3)\n        window=20,\n        embedding=eds.text_cnn(  # (4)\n            kernel_sizes=[3],\n            embedding=eds.transformer(  # (5)\n                model=\"prajjwal1/bert-tiny\",  # (6)\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"ner\",\n)\n</code></pre> <ol> <li>We use the <code>eds.ner_crf</code> NER task module, which classifies word embeddings into NER labels (BIOUL scheme) using a CRF.</li> <li>Each component of the pipeline can be configured with a dictionary, using the parameter described in the component's page.</li> <li>The <code>target_span_getter</code> parameter defines the name of the span group used to train the NER model. We will need to make sure the entities from the training dataset are assigned to this span group (next section).</li> <li>The word embeddings used by the CRF are computed by a CNN, which builds on top of another embedding layer.</li> <li>The base embedding layer is a pretrained transformer, which computes contextualized word embeddings.</li> <li>We chose the <code>prajjwal1/bert-tiny</code> model in this tutorial for testing purposes, but we recommend using a larger model like <code>bert-base-cased</code> or <code>camembert-base</code> (French) for real-world applications.</li> </ol>"},{"location":"tutorials/make-a-training-script/#2-adapting-a-dataset","title":"2. Adapting a dataset","text":"<p>To train a pipeline, we must convert our annotated data into documents that will be either used as training samples or a evaluation samples. This is done by designing a function to convert the dataset into a list of spaCy Doc objects. We will assume the dataset has been annotated using Brat, but any format can be used.</p> <p>At this step, we might also want to perform data augmentation, filtering, splitting or any other data transformation. Note that this function will be used to load both the training data and the test data.</p> <pre><code>from pydantic import DirectoryPath\nimport edsnlp\n\n\n@edsnlp.registry.adapters.register(\"ner_adapter\")\ndef ner_adapter(\n    path: DirectoryPath,\n    skip_empty: bool = False,  # (1)\n):\n    def generator(nlp):\n        # Read the data from the brat directory and convert it into Docs,\n        docs = edsnlp.data.read_standoff(\n            path,\n            # Store spans in default \"ents\", and \"ml-ner\" for the training (prev. section)\n            span_setter=[\"ents\", \"ml-ner\"],\n            # Tokenize the training docs with the same tokenizer as the trained model\n            tokenizer=nlp.tokenizer,\n        )\n        for doc in docs:\n            if skip_empty and len(doc.ents) == 0:\n                continue\n            yield doc\n\n    return generator\n</code></pre> <ol> <li>We can skip documents that do not contain any annotations. However, this parameter should be false when loading documents used to evaluate the pipeline.</li> </ol>"},{"location":"tutorials/make-a-training-script/#3-loading-the-data","title":"3. Loading the data","text":"<p>We then load and adapt (i.e., convert into spaCy Doc objects) the training and validation dataset. Since the adaption of raw documents depends on tokenization used in the trained model, we need to pass the model to the adapter function.</p> <pre><code>train_adapter = ner_adapter(train_data_path)\nval_adapter = ner_adapter(val_data_path)\n\ntrain_docs = list(train_adapter(nlp))\nval_docs = list(val_adapter(nlp))\n</code></pre>"},{"location":"tutorials/make-a-training-script/#4-complete-the-initialization-with-the-training-data","title":"4. Complete the initialization with the training data","text":"<p>We initialize the missing or incomplete components attributes (such as label vocabularies) with the training dataset</p> <pre><code>nlp.post_init(train_docs)\n</code></pre>"},{"location":"tutorials/make-a-training-script/#5-preprocessing-the-data","title":"5. Preprocessing the data","text":"<p>The training dataset is then preprocessed into features. The resulting preprocessed dataset is then wrapped into a pytorch DataLoader to be fed to the model during the training loop with the model's own collate method.</p> <pre><code>import torch\n\nbatch_size = 8\n\npreprocessed = list(\n    nlp.preprocess_many(  # (1)\n       train_docs,\n       supervision=True,\n    )\n)\ndataloader = torch.utils.data.DataLoader(\n    preprocessed,\n    batch_size=batch_size,\n    collate_fn=nlp.collate,\n    shuffle=True,\n)\n</code></pre> <ol> <li>This will call the <code>preprocess_supervised</code> method of the TorchComponent class on every document and return a list of dictionaries containing the    features and labels of each document.</li> </ol>"},{"location":"tutorials/make-a-training-script/#6-looping-through-the-training-data","title":"6. Looping through the training data","text":"<p>We instantiate an optimizer and start the training loop</p> <pre><code>from itertools import chain, repeat\nfrom tqdm import tqdm\n\nlr = 3e-4\nmax_steps = 400\n\noptimizer = torch.optim.AdamW(\n    params=nlp.parameters(),\n    lr=lr,\n)\n\n# We will loop over the dataloader\niterator = chain.from_iterable(repeat(dataloader))\n\nfor step in tqdm(range(max_steps), \"Training model\", leave=True):\n    batch = next(iterator)\n    optimizer.zero_grad()\n</code></pre>"},{"location":"tutorials/make-a-training-script/#7-optimizing-the-weights","title":"7. Optimizing the weights","text":"<p>Inside the training loop, the trainable components are fed the collated batches from the dataloader by calling the <code>TorchComponent.forward</code> method (via a simple call) to compute the losses. In the case we train a multi-task model (not in this tutorial), the outputs of shared embedding are reused between components, we enable caching by wrapping this step in a cache context. The training loop is otherwise carried in a similar fashion to a standard pytorch training loop</p> <pre><code>    with nlp.cache():\n        loss = torch.zeros((), device=\"cpu\")\n        for name, component in nlp.torch_components():\n            output = component(batch[name])  # (1)\n            if \"loss\" in output:\n                loss += output[\"loss\"]\n\n    loss.backward()\n\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/make-a-training-script/#8-evaluating-the-model","title":"8. Evaluating the model","text":"<p>Finally, the model is evaluated on the validation dataset and saved at regular intervals.</p> <pre><code>from edsnlp.scorers.ner import create_ner_exact_scorer\nfrom copy import deepcopy\n\nscorer = create_ner_exact_scorer(nlp.pipes.ner.target_span_getter)\n\n    ...\n\n    if (step % 100) == 0:\n        with nlp.select_pipes(enable=[\"ner\"]):  # (1)\n            print(scorer(val_docs, nlp.pipe(deepcopy(val_docs))))  # (2)\n\n    nlp.to_disk(\"model\")  # (3)\n</code></pre> <ol> <li>In the case we have multiple pipes in our model, we may want to selectively evaluate each pipe, thus we use the <code>select_pipes</code> method to disable every pipe except \"ner\".</li> <li>We use the <code>pipe</code> method to run the \"ner\" component on the validation dataset. This method is similar to the <code>__call__</code> method of EDS-NLP components, but it is used to run a component on a list of    spaCy Docs.</li> <li>We could also have saved the model with <code>torch.save(model, \"model.pt\")</code>, but <code>nlp.to_disk</code> avoids pickling and allows to inspect the model's files by saving them into a structured directory.</li> </ol>"},{"location":"tutorials/make-a-training-script/#full-example","title":"Full example","text":"<p>Let's wrap the training code in a function, and make it callable from the command line using confit !</p> train.py <pre><code>from copy import deepcopy\nfrom itertools import chain, repeat\nfrom typing import Callable, Iterable\n\nimport torch\nfrom confit import Cli\nfrom pydantic import DirectoryPath\nfrom spacy.tokens import Doc\nfrom tqdm import tqdm\n\nimport edsnlp, edsnlp.pipes as eds\nfrom edsnlp import registry, Pipeline\nfrom edsnlp.scorers.ner import create_ner_exact_scorer\n\n\n@registry.adapters.register(\"ner_adapter\")\ndef ner_adapter(\n    path: DirectoryPath,\n    skip_empty: bool = False,\n):\n    def generator(nlp):\n        # Read the data from the brat directory and convert it into Docs,\n        docs = edsnlp.data.read_standoff(\n           path,\n           # Store spans in default \"ents\", and \"ml-ner\" for the training\n           span_setter=[\"ents\", \"ml-ner\"],\n           # Tokenize the training docs with the same tokenizer as the trained model\n           tokenizer=nlp.tokenizer,\n        )\n        for doc in docs:\n            if skip_empty and len(doc.ents) == 0:\n                continue\n            doc.spans[\"ml-ner\"] = doc.ents\n            yield doc\n\n    return generator\n\n\napp = Cli(pretty_exceptions_show_locals=False)\n\n\n@app.command(name=\"train\", registry=registry)  # (1)\ndef train(\n    nlp: Pipeline,\n    train_adapter: Callable[[Pipeline], Iterable[Doc]],\n    val_adapter: Callable[[Pipeline], Iterable[Doc]],\n    max_steps: int = 1000,\n    seed: int = 42,\n    lr: float = 3e-4,\n    batch_size: int = 4,\n):\n    # Adapting a dataset\n    train_docs = list(train_adapter(nlp))\n    val_docs = list(val_adapter(nlp))\n\n    # Complete the initialization with the training data\n    nlp.post_init(train_docs)\n\n    # Preprocessing the data\n    preprocessed = list(\n        nlp.preprocess_many(\n            train_docs,\n            supervision=True,\n        )\n    )\n    dataloader = torch.utils.data.DataLoader(\n        preprocessed,\n        batch_size=batch_size,\n        collate_fn=nlp.collate,\n        shuffle=True,\n    )\n\n    scorer = create_ner_exact_scorer(nlp.pipes.ner.target_span_getter)\n\n    optimizer = torch.optim.AdamW(\n        params=nlp.parameters(),\n        lr=lr,\n    )\n\n    iterator = chain.from_iterable(repeat(dataloader))\n\n    # Looping through the training data\n    for step in tqdm(range(max_steps), \"Training model\", leave=True):\n        batch = next(iterator)\n        optimizer.zero_grad()\n\n        loss = torch.zeros((), device=\"cpu\")\n        with nlp.cache():\n            for name, component in nlp.torch_components():\n                output = component(batch[name])\n                if \"loss\" in output:\n                    loss += output[\"loss\"]\n\n        loss.backward()\n\n        optimizer.step()\n\n        # Evaluating the model\n        if (step % 100) == 0:\n            with nlp.select_pipes(enable=[\"ner\"]):  #\n                print(scorer(val_docs, nlp.pipe(deepcopy(val_docs))))  #\n\n        nlp.to_disk(\"model\")\n\n\nif __name__ == \"__main__\":\n    nlp = edsnlp.blank(\"eds\")\n    nlp.add_pipe(\n        eds.ner_crf(\n            mode=\"joint\",\n            target_span_getter=\"ml-ner\",\n            window=20,\n            embedding=eds.text_cnn(\n                kernel_sizes=[3],\n                embedding=eds.transformer(\n                    model=\"prajjwal1/bert-tiny\",\n                    window=128,\n                    stride=96,\n                ),\n            ),\n        ),\n        name=\"ner\",\n    )\n    train(\n        nlp=nlp,\n        train_adapter=ner_adapter(\"data/train\"),\n        val_adapter=ner_adapter(\"data/val\"),\n        max_steps=400,\n        seed=42,\n        lr=3e-4,\n        batch_size=8,\n    )\n</code></pre> <ol> <li>This will become useful in the next section, when we will use the configuration file to define the pipeline. If you don't want to use a configuration file, you can remove this decorator.</li> </ol> <p>We can now copy the above code in a notebook and run it, or call this script from the command line:</p> <pre><code>python train.py --seed 42\n</code></pre> <p>At the end of the training, the pipeline is ready to use (with the <code>.pipe</code> method) since every trained component of the pipeline is self-sufficient, ie contains the preprocessing, inference and postprocessing code required to run it.</p>"},{"location":"tutorials/make-a-training-script/#configuration","title":"Configuration","text":"<p>To decouple the configuration and the code of our training script, let's define a configuration file where we will describe both our training parameters and the pipeline. You can either write the config of the pipeline by hand, or generate a pipeline config draft from an instantiated pipeline by running:</p> <pre><code>print(nlp.config.to_str())\n</code></pre> config.cfg<pre><code># This is this equivalent of the API-based declaration\n# at the beginning of the tutorial\n[nlp]\nlang = \"eds\"\npipeline = [\"ner\"]\ncomponents = ${ components }\n\n[components]\n\n[components.ner]\n@factory = \"eds.ner_crf\"\nmode = \"joint\"\ntarget_span_getter = \"ml-ner\"\nwindow = 20\nembedding = ${ cnn }\n\n[cnn]\n@factory = \"eds.text_cnn\"\nkernel_sizes = [3]\nembedding = ${ transformer }\n\n[transformer]\n@factory = \"eds.transformer\"\nmodel = \"prajjwal1/bert-tiny\"\nwindow = 128\nstride = ${ transformer.window//2 }\n\n# This is were we define the training script parameters\n# the \"train\" section refers to the name of the command\n# in the training script\n[train]\nnlp = ${ nlp }\ntrain_adapter = { \"@adapters\": \"ner_adapter\", \"path\": \"data/train\" }\nval_adapter = { \"@adapters\": \"ner_adapter\", \"path\": \"data/val\" }\nmax_steps = 400\nseed = 42\nlr = 3e-4\nbatch_size = 8\n</code></pre> <p>And replace the end of the script by</p> <pre><code>if __name__ == \"__main__\":\n    app.run()\n</code></pre> <p>That's it ! We can now call the training script with the configuration file as a parameter, and override some of its values:</p> <pre><code>python train.py --config config.cfg --transformer.window=64 --seed 43\n</code></pre>"},{"location":"tutorials/make-a-training-script/#going-further","title":"Going further","text":"<p>This tutorial gave you a glimpse of the training API of EDS-NLP. We provide a more complete example of a training script in tests at tests/training/test_training.py. To build a custom trainable component, you can refer to the TorchComponent class or look up the implementation of some of the trainable components on GitHub.</p>"},{"location":"tutorials/matching-a-terminology/","title":"Matching a terminology","text":"<p>Matching a terminology is perhaps the most basic application of a medical NLP pipeline.</p> <p>In this tutorial, we will cover :</p> <ul> <li>Matching a terminology using spaCy's matchers, as well as RegExps</li> <li>Matching on a specific attribute</li> </ul> <p>You should consider reading the matcher's specific documentation for a description.</p> <p>Comparison to spaCy's matcher</p> <p>spaCy's <code>Matcher</code> and <code>PhraseMatcher</code> use a very efficient algorithm that compare a hashed representation token by token. They are not components by themselves, but can underpin rule-based pipes.</p> <p>EDS-NLP's <code>RegexMatcher</code> lets the user match entire expressions using regular expressions. To achieve this, the matcher has to get to the text representation, match on it, and get back to spaCy's abstraction.</p> <p>The <code>EDSPhraseMatcher</code> lets EDS-NLP reuse spaCy's efficient algorithm, while adding the ability to skip pollution tokens (see the normalisation documentation for detail)</p>"},{"location":"tutorials/matching-a-terminology/#a-simple-use-case-finding-covid19","title":"A simple use case : finding COVID19","text":"<p>Let's try to find mentions of COVID19 and references to patients within a clinical note.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],\n    respiratoire=[\"asthmatique\", \"respiratoire\"],\n)\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.matcher(terms=terms))\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (asthmatique,)\n</code></pre> <p>Let's unpack what happened:</p> <ol> <li>We defined a dictionary of terms to look for, in the form <code>{'label': list of terms}</code>.</li> <li>We declared a spaCy pipeline, and add the <code>eds.matcher</code> component.</li> <li>We applied the pipeline to the texts...</li> <li>... and explored the extracted entities.</li> </ol> <p>This example showcases a limitation of our term dictionary : the phrases <code>COVID19</code> and <code>difficult\u00e9s respiratoires</code> were not detected by the pipeline.</p> <p>To increase recall, we could just add every possible variation :</p> <pre><code>terms = dict(\n-    covid=[\"coronavirus\", \"covid19\"],\n+    covid=[\"coronavirus\", \"covid19\", \"COVID19\"],\n-    respiratoire=[\"asthmatique\", \"respiratoire\"],\n+    respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n</code></pre> <p>But what if we come across <code>Coronavirus</code>? Surely we can do better!</p>"},{"location":"tutorials/matching-a-terminology/#matching-on-normalised-text","title":"Matching on normalised text","text":"<p>We can modify the matcher's configuration to match on other attributes instead of the verbatim input. You can refer to spaCy's list of available token attributes.</p> <p>Let's focus on two:</p> <ol> <li>The <code>LOWER</code> attribute, which lets you match on a lowercased version of the text.</li> <li>The <code>NORM</code> attribute, which adds some basic normalisation (eg <code>\u0153</code> to <code>oe</code>). EDS-NLP provides a <code>eds.normalizer</code> component that extends the level of cleaning on the <code>NORM</code> attribute.</li> </ol>"},{"location":"tutorials/matching-a-terminology/#the-lower-attribute","title":"The <code>LOWER</code> attribute","text":"<p>Matching on the lowercased version is extremely easy:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],\n    respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        attr=\"LOWER\",  # (1)\n    ),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>The matcher's <code>attr</code> parameter defines the attribute that the matcher will use. It is set to <code>\"TEXT\"</code> by default (ie verbatim text).</li> </ol> <p>This code is complete, and should run as is.</p>"},{"location":"tutorials/matching-a-terminology/#using-the-normalisation-component","title":"Using the normalisation component","text":"<p>EDS-NLP provides its own normalisation component, which modifies the <code>NORM</code> attribute in place. It handles:</p> <ul> <li>removal of accentuated characters;</li> <li>normalisation of quotes and apostrophes;</li> <li>lowercasing, which enabled by default in spaCy \u2013 EDS-NLP lets you disable it;</li> <li>removal of pollution.</li> </ul> <p>Pollution in clinical texts</p> <p>EDS-NLP is meant to be deployed on clinical reports extracted from hospitals information systems. As such, it is often riddled with extraction issues or administrative artifacts that \"pollute\" the report.</p> <p>As a core principle, EDS-NLP never modifies the input text, and <code>nlp(text).text == text</code> is always true. However, we can tag some tokens as pollution elements, and avoid using them for matching the terminology.</p> <p>You can activate it like any other component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n\"Motif de prise en charge : probable pneumopathie a ===== COVID19, \"  # (1)\n\"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\ncovid=[\"coronavirus\", \"covid19\", \"pneumopathie \u00e0 covid19\"],  # (2)\nrespiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n\nnlp = edsnlp.blank(\"eds\")\n\n# Add the normalisation component\nnlp.add_pipe(eds.normalizer())  # (3)\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        attr=\"NORM\",  # (4)\nignore_excluded=True,  # (5)\n),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (pneumopathie a ===== COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>We've modified the example to include a simple pollution.</li> <li>We've added <code>pneumopathie \u00e0 covid19</code> to the list of synonyms detected by the pipeline.    Note that in the synonym we provide, we kept the accentuated <code>\u00e0</code>, whereas the example    displays an unaccentuated <code>a</code>.</li> <li>The component can be configured. See the specific documentation for detail.</li> <li>The normalisation lives in the <code>NORM</code> attribute</li> <li>We can tell the matcher to ignore excluded tokens (tokens tagged as pollution by the normalisation component).    This is not an obligation.</li> </ol> <p>Using the normalisation component, you can match on a normalised version of the text, as well as skip pollution tokens during the matching process.</p> <p>Using term matching with the normalisation</p> <p>If you use the term matcher with the normalisation, bear in mind that the examples go through the pipeline. That's how the matcher was able to recover <code>pneumopathie a ===== COVID19</code> despite the fact that we used an accentuated <code>\u00e0</code> in the terminology.</p> <p>The term matcher matches the input text to the provided terminology, using the selected attribute in both cases. The <code>NORM</code> attribute that corresponds to <code>\u00e0</code> and <code>a</code> is the same: <code>a</code>.</p>"},{"location":"tutorials/matching-a-terminology/#preliminary-conclusion","title":"Preliminary conclusion","text":"<p>We have matched all mentions! However, we had to spell out the singular and plural form of <code>respiratoire</code>... And what if we wanted to detect <code>covid 19</code>, or <code>covid-19</code> ? Of course, we could write out every imaginable possibility, but this will quickly become tedious.</p>"},{"location":"tutorials/matching-a-terminology/#using-regular-expressions","title":"Using regular expressions","text":"<p>Let us redefine the pipeline once again, this time using regular expressions:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.matcher(\n        regex=regex,  # (1)\n        terms=terms,  # (2)\n        attr=\"LOWER\",  # (3)\n    ),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>We can now match using regular expressions.</li> <li>We can mix and match patterns! Here we keep looking for patients using spaCy's term matching.</li> <li>RegExp matching is not limited to the verbatim text! You can choose to use one of spaCy's native attribute, ignore excluded tokens, etc.</li> </ol> <p>This code is complete, and should run as is.</p> <p>Using regular expressions can help define richer patterns using more compact queries.</p>"},{"location":"tutorials/matching-a-terminology/#visualising-matched-entities","title":"Visualising matched entities","text":"<p>EDS-NLP is part of the spaCy ecosystem, which means we can benefit from spaCy helper functions. For instance, spaCy's visualiser displacy can let us visualise the matched entities:</p> <pre><code># \u2191 Omitted code above \u2191\n\nfrom spacy import displacy\n\ncolors = {\n    \"covid\": \"orange\",\n    \"respiratoire\": \"steelblue\",\n}\noptions = {\n    \"colors\": colors,\n}\n\ndisplacy.render(doc, style=\"ent\", options=options)\n</code></pre> <p>If you run this within a notebook, you should get:</p> Motif de prise en charge : probable pneumopathie a              COVID19         covid      , sans difficult\u00e9s              respiratoires         respiratoire Le p\u00e8re du patient est              asthmatique         respiratoire      ."},{"location":"tutorials/multiple-texts/","title":"Processing multiple texts","text":"<p>In the previous tutorials, we've seen how to apply a spaCy NLP pipeline to a single text. Once the pipeline is tested and ready to be applied on an entire corpus, we'll want to deploy it efficiently.</p> <p>In this tutorial, we'll cover a few best practices and some caveats to avoid. Then, we'll explore methods that EDS-NLP provides to perform inference on multiple texts.</p> <p>Consider this simple pipeline:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\n\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(patient=[\"patient\", \"malade\"]),\n        attr=\"NORM\",\n    ),\n)\n\n# Add qualifiers\nnlp.add_pipe(eds.negation())\nnlp.add_pipe(eds.hypothesis())\nnlp.add_pipe(eds.family())\n\n# Add date detection\nnlp.add_pipe(eds.dates())\n</code></pre> <p>Let's deploy it on a large number of documents.</p>"},{"location":"tutorials/multiple-texts/#what-about-a-for-loop","title":"What about a <code>for</code> loop?","text":"<p>Suppose we have a corpus of text:</p> <pre><code>text = (\n    \"Patient admis le 25 septembre 2021 pour suspicion de Covid.\\n\"\n    \"Pas de cas de coronavirus dans ce service.\\n\"\n    \"Le p\u00e8re du patient est atteint du covid.\"\n)\n\ncorpus = [text] * 10000  # (1)\n</code></pre> <ol> <li>This is admittedly ugly. But you get the idea, we have a corpus of 10 000 documents we want to process...</li> </ol> <p>You could just apply the pipeline document by document.</p> <pre><code># \u2191 Omitted code above \u2191\n\ndocs = [nlp(text) for text in corpus]\n</code></pre> <p>Next, you might want to convert these documents to a DataFrame for further analysis or storage. You could do this with a loop like this:</p> <pre><code>import pandas as pd\n\nrows = []\nfor doc in docs:\n    for ent in doc.ents:\n        d = dict(\n            begin=ent.start_char,\n            end=ent.end_char,\n            label=ent.label_,\n            entity_text=ent.text,\n            negation=ent._.negation,\n            hypothesis=ent._.hypothesis,\n            family=ent._.family,\n        )\n        rows.append(d)\n\n    for date in doc.spans.get(\"dates\", []):\n        d = dict(\n            begin=date.start_char,\n            end=date.end_char,\n            label=\"date\",\n            entity_text=date.text,\n            datetime=date._.date.datetime,\n        )\n        rows.append(d)\ndf = pd.DataFrame(rows)\n</code></pre> <p>There are a few issues with this approach:</p> <ul> <li>If our model contains deep learning components (which it does not in this tutorial), we don't benefit from optimized batched matrix operations : ideally, we'd like to process multiple documents at   once.</li> <li>We may have multiple cores available but we don't use them to apply the pipes of our model to multiple documents at the same time.</li> <li>We would also like to perform the Doc -&gt; Dict conversion step in parallel and avoid transferring full Doc instances back and forth between processes.</li> <li>And ideally, being able to switch between input/output formats, or sequential/parallel processing, without changing the code too much.</li> </ul>"},{"location":"tutorials/multiple-texts/#lazy-inference-and-parallelization","title":"Lazy inference and parallelization","text":"<p>To efficiently perform the same operations on multiple documents at once, EDS-NLP uses lazy collections, which record the operations to perform on the documents without actually executing them directly, similar to the way Spark does, or polars with its LazyFrame.</p> <p>This allows EDS-NLP to distribute these operations on multiple cores or machines when it is time to execute them. We can configure how the collection operations are run (how many jobs/workers, how many gpus, whether to use the spark engine) via the lazy collection <code>.set_processing(...)</code> method.</p> <p>For instance,</p> <pre><code>docs = edsnlp.data.from_iterable(corpus)\nprint(docs)\n# &lt;edsnlp.core.lazy_collection.LazyCollection object at 0x7f3e3c3e3d90&gt;\n</code></pre> <p>as well as any <code>edsnlp.data.read_*</code> or <code>edsnlp.data.from_*</code> return a lazy collection, that we can iterate over or complete with more operations. To apply the model on our collection of documents, we can simply do:</p> <pre><code>docs = docs.map_pipeline(nlp)\n# or \u00e0 la spaCy :\n# docs = nlp.pipe(docs)\n</code></pre> SpaCy vs EDS-NLP <p>SpaCy's <code>nlp.pipe</code> method is not the same as EDS-NLP's <code>nlp.pipe</code> method, and will iterate over anything you pass to it, therefore executing the operations scheduled in our lazy collection.</p> <p>We recommend you instantiate your models using <code>nlp = edsnlp.blank(...)</code> or <code>nlp = edsnlp.load(...)</code>.</p> <p>Otherwise, use the following to apply a spaCy model on a lazy collection <code>docs</code> without triggering its execution:</p> <pre><code>docs = docs.map_pipeline(nlp)\n</code></pre> <p>Finally, we can convert the documents to a DataFrame (or other formats / files) using the <code>edsnlp.data.to_*</code> or <code>edsnlp.data.write_*</code> methods. This triggers the execution of the operations scheduled in the lazy collection and produces the rows of the DataFrame.</p> <p>We can pass our previous Doc to Dict code as a function to the <code>converter</code> parameter of the <code>to_pandas</code> method. Note that this specific conversion is already implemented in EDS-NLP, so you can just pass the string <code>\"ents\"</code> to the <code>converter</code> parameter, and customize the conversion by passing more parameters to the <code>to_pandas</code> method, as described here.</p> <pre><code>def convert_doc_to_rows(doc):\n    entities = []\n\n    for ent in doc.ents:\n        d = dict(\n            start=ent.start_char,\n            end=ent.end_char,\n            label=ent.label_,\n            lexical_variant=ent.text,\n            negation=ent._.negation,\n            hypothesis=ent._.hypothesis,\n            family=ent._.family,\n        )\n        entities.append(d)\n\n    for date in doc.spans.get(\"dates\", []):\n        d = dict(\n            begin=date.start_char,\n            end=date.end_char,\n            label=\"date\",\n            lexical_variant=date.text,\n            datetime=date._.date.datetime,\n        )\n        entities.append(d)\n\n    return entities\n\n\ndf = docs.to_pandas(converter=convert_doc_to_rows)\n# or equivalently:\ndf = docs.to_pandas(\n    converter=\"ents\",\n    span_getter=[\"ents\", \"dates\"],\n    span_attributes={\n        # span._.*** name: column name\n        \"negation\": \"negation\",\n        \"hypothesis\": \"hypothesis\",\n        \"family\": \"family\",\n        \"date.datetime\": \"datetime\",\n    },\n)\n</code></pre> <p>We can also iterate over the documents, which also triggers the execution of the operations scheduled in the lazy collection.</p> <pre><code>for doc in docs:\n    # do something with the doc\n    pass\n</code></pre>"},{"location":"tutorials/multiple-texts/#processing-a-dataframe","title":"Processing a DataFrame","text":"<p>Processing text within a pandas DataFrame is a very common use case. In many applications, you'll select a corpus of documents over a distributed cluster, load it in memory and process all texts.</p> <p>The OMOP CDM</p> <p>In every tutorial that mentions distributing EDS-NLP over a corpus of documents, we will expect the data to be organised using a flavour of the OMOP Common Data Model.</p> <p>The OMOP CDM defines two tables of interest to us:</p> <ul> <li>the <code>note</code> table contains the clinical notes</li> <li>the <code>note_nlp</code> table holds the results of   a NLP pipeline applied to the <code>note</code> table.</li> </ul> <p>We can use the <code>converter=\"omop\"</code> argument to the <code>edsnlp.data</code> methods to read data in this format. More information about this converter can be found here.</p> <p>To make sure we can follow along, we propose three recipes for getting the DataFrame: using a dummy dataset like before, loading a CSV or by loading a Spark DataFrame into memory.</p> Dummy exampleLoading data from a CSVLoading data from a Spark DataFrame <pre><code>import pandas as pd\n\ntext = (\n    \"Patient admis le 25 septembre 2021 pour suspicion de Covid.\\n\"\n    \"Pas de cas de coronavirus dans ce service.\\n\"\n    \"Le p\u00e8re du patient est atteint du covid.\"\n)\n\ncorpus = [text] * 1000\n\ndata = pd.DataFrame(dict(note_text=corpus))\ndata[\"note_id\"] = range(len(data))\n</code></pre> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"note.csv\")\n</code></pre> <pre><code>from pyspark.sql.session import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.sql(\"SELECT * FROM note\")\ndf = df.select(\"note_id\", \"note_text\")\n\ndata = df.limit(1000).toPandas()  # (1)\n</code></pre> <ol> <li>We limit the size of the DataFrame to make sure we do not overwhelm our machine.</li> </ol> <p>We'll see in what follows how we can efficiently deploy our pipeline on the <code>data</code> object.</p>"},{"location":"tutorials/multiple-texts/#locally-without-parallelization","title":"Locally without parallelization","text":"<pre><code># Read from a dataframe &amp; use the omop converter\ndocs = edsnlp.data.from_pandas(data, converter=\"omop\")\n\n# Add the pipeline to operations that will be run\ndocs = docs.map_pipeline(nlp)\n\n# Convert each doc to a list of dicts (one by entity)\n# and store the result in a pandas DataFrame\nnote_nlp = docs.to_pandas(\n    converter=\"ents\",\n    # Below are the arguments to the converter\n    span_getter=[\"ents\", \"dates\"],\n    span_attributes={  # (1)\n        # span._.*** name: column name\n        \"negation\": \"negation\",\n        \"hypothesis\": \"hypothesis\",\n        \"family\": \"family\",\n        \"date.datetime\": \"datetime\",\n        # having individual columns for each date part\n        # can be useful for incomplete dates (eg, \"in May\")\n        \"date.day\": \"date_day\",\n        \"date.month\": \"date_month\",\n        \"date.year\": \"date_year\",\n    },\n)\n</code></pre> <ol> <li>You can just pass a list if you don't want to rename the attributes.</li> </ol> <p>The result on the first note:</p> note_id start end label lexical_variant negation hypothesis family key 0 0 7 patient Patient 0 0 0 ents 0 114 121 patient patient 0 0 1 ents 0 17 34 2021-09-25 25 septembre 2021 nan nan nan dates"},{"location":"tutorials/multiple-texts/#locally-using-multiple-parallel-workers","title":"Locally, using multiple parallel workers","text":"<p>Caveat</p> <p>Since workers can produce their results in any order, the order of the rows in the resulting DataFrame may not be the same as the order of the input data.</p> <pre><code># Read from a dataframe &amp; use the omop converter\ndocs = edsnlp.data.from_pandas(data, converter=\"omop\")\n\n# Add the pipeline to operations that will be run\ndocs = docs.map_pipeline(nlp)\n\n# The operations of our lazy collection will be distributed on multiple workers\ndocs = docs.set_processing(backend=\"multiprocessing\")\n# Convert each doc to a list of dicts (one by entity)\n# and store the result in a pandas DataFrame\nnote_nlp = docs.to_pandas(\n    converter=\"ents\",\n    span_getter=[\"ents\", \"dates\"],\n    span_attributes={\n        \"negation\": \"negation\",\n        \"hypothesis\": \"hypothesis\",\n        \"family\": \"family\",\n        \"date.datetime\": \"datetime\",\n\n        # having individual columns for each date part\n        # can be useful for incomplete dates (eg, \"in May\")\n        \"date.day\": \"date_day\",\n        \"date.month\": \"date_month\",\n        \"date.year\": \"date_year\",\n    },\n)\n</code></pre>"},{"location":"tutorials/multiple-texts/#in-a-distributed-fashion-with-spark","title":"In a distributed fashion with spark","text":"<p>To use the Spark engine to distribute the computation, we create our lazy collection from the Spark dataframe directly and write the result to a new Spark dataframe. EDS-NLP will automatically distribute the operations on the cluster (setting <code>backend=\"spark\"</code> behind the scenes), but you can change the backend (for instance to <code>multiprocessing</code> to run locally).</p> <pre><code># Read from the pyspark dataframe &amp; use the omop converter\ndocs = edsnlp.data.from_spark(df, converter=\"omop\")\n# Add the pipeline to operations that will be run\ndocs = docs.map_pipeline(nlp\n\n# Convert each doc to a list of dicts (one by entity)\n# and store the result in a pyspark DataFrame\nnote_nlp = docs.to_spark(\nconverter=\"ents\",\n    span_getter=[\"ents\", \"dates\"],\n    span_attributes={\n        \"negation\": \"negation\",\n        \"hypothesis\": \"hypothesis\",\n        \"family\": \"family\",\n        \"date.datetime\": \"datetime\",\n\n        # having individual columns for each date part\n        # can be useful for incomplete dates (eg, \"in May\")\n        \"date.day\": \"date_day\",\n        \"date.month\": \"date_month\",\n        \"date.year\": \"date_year\",\n    },\n    dtypes=None,  # (1)\n)\n</code></pre> <ol> <li>If you don't pass a <code>dtypes</code> argument, EDS-NLP will print the inferred schema it such that you can copy-paste it in your code.</li> </ol> <ol></ol>"},{"location":"tutorials/qualifying-entities/","title":"Qualifying entities","text":"<p>In the previous tutorial, we saw how to match a terminology on a text. Using the <code>doc.ents</code> attribute, we can check whether a document mentions a concept of interest to build a cohort or describe patients.</p>"},{"location":"tutorials/qualifying-entities/#the-issue","title":"The issue","text":"<p>However, consider the classical example where we look for the <code>diabetes</code> concept:</p> FrenchEnglish <pre><code>Le patient n'est pas diab\u00e9tique.\nLe patient est peut-\u00eatre diab\u00e9tique.\nLe p\u00e8re du patient est diab\u00e9tique.\n</code></pre> <pre><code>The patient is not diabetic.\nThe patient could be diabetic.\nThe patient's father is diabetic.\n</code></pre> <p>None of these expressions should be used to build a cohort: the detected entity is either negated, speculative, or does not concern the patient themself. That's why we need to qualify the matched entities.</p> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p>"},{"location":"tutorials/qualifying-entities/#the-solution","title":"The solution","text":"<p>We can use EDS-NLP's qualifier pipes to achieve that. Let's add specific components to our pipeline to detect these three modalities.</p>"},{"location":"tutorials/qualifying-entities/#adding-qualifiers","title":"Adding qualifiers","text":"<p>Adding qualifier pipes is straightforward:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.matcher(\n        regex=regex,\n        terms=terms,\n        attr=\"LOWER\",\n    ),\n)\n\nnlp.add_pipe(eds.sentences())  # (1)\nnlp.add_pipe(eds.negation())  # Negation component\nnlp.add_pipe(eds.hypothesis())  # Speculation pipe\nnlp.add_pipe(eds.family())  # Family context detection\n</code></pre> <ol> <li>Qualifiers pipes need sentence boundaries to be set (see the specific documentation for detail).</li> </ol> <p>This code is complete, and should run as is.</p>"},{"location":"tutorials/qualifying-entities/#reading-the-results","title":"Reading the results","text":"<p>Let's output the results as a pandas DataFrame for better readability:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nimport pandas as pd\ntext = (\n    \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.matcher(\n        regex=regex,\n        terms=terms,\n        attr=\"LOWER\",\n    ),\n)\n\nnlp.add_pipe(eds.sentences())\n\nnlp.add_pipe(eds.negation())  # Negation component\nnlp.add_pipe(eds.hypothesis())  # Speculation pipe\nnlp.add_pipe(eds.family())  # Family context detection\n\ndoc = nlp(text)\n\n# Extraction as a pandas DataFrame\nentities = []\nfor ent in doc.ents:\nd = dict(\nlexical_variant=ent.text,\nlabel=ent.label_,\nnegation=ent._.negation,\nhypothesis=ent._.hypothesis,\nfamily=ent._.family,\n)\nentities.append(d)\ndf = pd.DataFrame.from_records(entities)\n</code></pre> <p>This code is complete, and should run as is.</p> <p>We get the following result:</p> lexical_variant label negation hypothesis family COVID19 covid False True False respiratoires respiratoire True False False asthmatique respiratoire False False True"},{"location":"tutorials/qualifying-entities/#conclusion","title":"Conclusion","text":"<p>The qualifier pipes limits the number of false positives by detecting linguistic modulations such as negations or speculations. Go to the full documentation for a complete presentation of the different pipes, their configuration options and validation performance.</p>"},{"location":"tutorials/quick-examples/","title":"Display single text outputs","text":"<p>If you are</p> <ul> <li>Developping a new component</li> <li>Testing various inputs on an existing component</li> <li>...</li> </ul> <p>you might want to quickly apply a pipeline and display the output <code>doc</code> in a comprehensible way.</p> <pre><code>from edsnlp.viz import QuickExample\n\nE = QuickExample(nlp)  # (1)\n</code></pre> <ol> <li>This is the <code>Language</code> instance that should be defined beforehands</li> </ol> <p>Next, simply call <code>E</code> with any string:</p> <pre><code>txt = \"Le patient pr\u00e9sente une anomalie.\"\nE(txt)\n</code></pre> <pre>                              Le patient pr\u00e9sente une anomalie                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Entity   \u2503 Source   \u2503 eds.hypoth\u2026 \u2503 eds.negation \u2503 eds.family \u2503 eds.history \u2503 eds.report\u2026 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 patient  \u2502 patient  \u2502 False       \u2502 False        \u2502 False      \u2502 False       \u2502 False       \u2502\n\u2502 anomalie \u2502 anomalie \u2502 False       \u2502 False        \u2502 False      \u2502 False       \u2502 False       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>By default, each <code>Qualifiers</code> in <code>nlp</code> adds a corresponding column to the output. Additionnal informations can be displayed by using the <code>extensions</code> parameter. For instance, if entities have a custom <code>ent._.custom_ext</code> extensions, it can be displayed by providing the extension when instantiating <code>QuickExample</code>:</p> <pre><code>E = QuickExample(nlp, extensions=[\"_.custom_ext\"])\n</code></pre> <p>Finally, if you prefer to output a DataFrame instead of displaying a table, set the <code>as_dataframe</code> parameter to True:</p> <pre><code>E = QuickExample(nlp)\nE(txt, as_dataframe=True)\n</code></pre> <ol></ol>"},{"location":"tutorials/reason/","title":"Detecting Reason of Hospitalisation","text":"<p>In this tutorial we will use the pipe <code>eds.reason</code> to :</p> <ul> <li>Identify spans that corresponds to the reason of hospitalisation</li> <li>Check if there are named entities overlapping with my span of 'reason of hospitalisation'</li> <li>Check for all named entities if they are tagged <code>is_reason</code></li> </ul> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = edsnlp.blank(\"eds\")\n\n# Extraction d'entit\u00e9s nomm\u00e9es\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        ),\n    ),\n)\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\nnlp.add_pipe(eds.reason(use_sections=True))\n\ndoc = nlp(text)\n</code></pre> <p>The pipe <code>reason</code> will add a key of spans called <code>reasons</code>. We check the first item in this list.</p> <pre><code># \u2191 Omitted code above \u2191\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n</code></pre> <p>Naturally, all spans included the <code>reasons</code> key have the attribute <code>reason._.is_reason == True</code>.</p> <pre><code># \u2191 Omitted code above \u2191\n\nreason._.is_reason\n# Out: True\n</code></pre> <pre><code># \u2191 Omitted code above \u2191\n\nentities = reason._.ents_reason  # (1)\nfor e in entities:\n    print(\n        \"Entity:\",\n        e.text,\n        \"-- Label:\",\n        e.label_,\n        \"-- is_reason:\",\n        e._.is_reason,\n    )\n# Out: Entity: asthme -- Label: respiratoire -- is_reason: True\n</code></pre> <ol> <li>We check if the span include named entities, their labels and the attribute is_reason</li> </ol> <p>We can verify that named entities that do not overlap with the spans of reason, have their attribute <code>reason._.is_reason == False</code>:</p> <pre><code>for e in doc.ents:\n    print(e.start, e, e._.is_reason)\n# Out: 42 asthme True\n# Out: 54 asthme False\n</code></pre> <ol></ol>"},{"location":"tutorials/spacy101/","title":"SpaCy representations","text":"<p>EDS-NLP uses spaCy to represent documents and their annotations. You will need to familiarise yourself with some key spaCy concepts.</p> <p>Skip if you're familiar with spaCy objects</p> <p>This page is intended as a crash course for the very basic spaCy concepts that are needed to use EDS-NLP. If you've already used spaCy, you should probably skip to the next page.</p>"},{"location":"tutorials/spacy101/#the-doc-object","title":"The <code>Doc</code> object","text":"<p>The <code>doc</code> object carries the result of the entire processing. It's the most important abstraction in spaCy, hence its use in EDS-NLP, and holds a token-based representation of the text along with the results of every pipeline components. It also keeps track of the input text in a non-destructive manner, meaning that <code>doc.text == text</code> is always true.</p> <p>To obtain a doc, run the following code: <pre><code>import edsnlp  # (1)\n\n# Initialize a pipeline\nnlp = edsnlp.blank(\"eds\")  # (2)\n\ntext = \"Michel est un penseur lat\u00e9ral.\"  # (3)\n\n# Apply the pipeline\ndoc = nlp(text)  # (4)\n\ndoc.text\n# Out: 'Michel est un penseur lat\u00e9ral.'\n\n# If you do not want to run the pipeline but only tokenize the text\ndoc = nlp.make_doc(text)\n\n# Text processing in spaCy is non-destructive\ndoc.text == text\n\n# You can access a specific token\ntoken = doc[2]  # (5)\n\n# And create a Span using slices\nspan = doc[:3]  # (6)\n\n# Entities are tracked in the ents attribute\ndoc.ents  # (7)\n# Out: ()\n</code></pre></p> <ol> <li>Import edsnlp...</li> <li>Load a pipeline. The parameter corresponds to the language code and affects the tokenization.</li> <li>Define a text you want to process.</li> <li>Apply the pipeline and get a spaCy <code>Doc</code> object.</li> <li><code>token</code> is a <code>Token</code> object referencing the third token</li> <li><code>span</code> is a <code>Span</code> object referencing the first three tokens.</li> <li>We have not declared any entity recognizer in our pipeline, hence this attribute is empty.</li> </ol> <p>We just created a pipeline and applied it to a sample text. It's that simple.</p>"},{"location":"tutorials/spacy101/#the-span-objects","title":"The <code>Span</code> objects","text":"<p>Span of text are represented by the <code>Span</code> object and represent slices of the <code>Doc</code> object. You can either create a span by slicing a <code>Doc</code> object, or by running a pipeline component that creates spans. There are different types of spans:</p> <ul> <li><code>doc.ents</code> are non-overlapping spans that represent entities</li> <li><code>doc.sents</code> are the sentences of the document</li> <li><code>doc.spans</code> is dict of groups of spans (that can overlap)</li> </ul> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nnlp.add_pipe(eds.sentences())  # (1)\nnlp.add_pipe(eds.dates())  # (2)\n\ntext = \"Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.\"\n\ndoc = nlp(text)\n</code></pre> <ol> <li>Like the name suggests, this pipeline component is declared by EDS-NLP.    <code>eds.sentences</code> is a rule-based sentence boundary prediction.    See its documentation for detail.</li> <li>Like the name suggests, this pipeline component is declared by EDS-NLP.    <code>eds.dates</code> is a date extraction and normalisation component.    See its documentation for detail.</li> </ol> <p>The <code>doc</code> object just became more interesting!</p> <pre><code># \u2191 Omitted code above \u2191\n\n# We can split the document into sentences spans\nlist(doc.sents)  # (1)\n# Out: [Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.]\n\n# And list dates spans\ndoc.spans[\"dates\"]  # (2)\n# Out: [5 mai 2005]\n\nspan = doc.spans[\"dates\"][0]  # (3)\n</code></pre> <ol> <li>In this example, there is only one sentence...</li> <li>The <code>eds.dates</code> adds a key to the <code>doc.spans</code> attribute</li> <li><code>span</code> is a spaCy <code>Span</code> object.</li> </ol>"},{"location":"tutorials/spacy101/#spacy-extensions","title":"SpaCy extensions","text":"<p>We can add custom attributes (or \"extensions\") to spaCy objects via the <code>_</code> attribute. For example, the <code>eds.dates</code> pipeline adds a <code>Span._.date</code> extension to the <code>Span</code> object. The attributes can be any Python object.</p> <pre><code># \u2191 Omitted code above \u2191\n\nspan._.date.to_datetime()  # (1)\n# Out: DateTime(2005, 5, 5, 0, 0, 0, tzinfo=Timezone('Europe/Paris'))\n</code></pre> <ol> <li>We use the <code>to_datetime()</code> method of the extension to get an object that is usable by Python.</li> </ol> <ol></ol>"},{"location":"utilities/","title":"Utilities","text":"<p>EDS-NLP provides a few utilities to deploy pipelines, process RegExps, etc.</p>"},{"location":"utilities/evaluation/","title":"Pipeline evaluation","text":""},{"location":"utilities/matchers/","title":"Matchers","text":"<p>We implemented three pattern matchers that are fit to clinical documents:</p> <ul> <li>the <code>EDSPhraseMatcher</code></li> <li>the <code>RegexMatcher</code></li> <li>the <code>SimstringMatcher</code></li> </ul> <p>However, note that for most use-cases, you should instead use the <code>eds.matcher</code> pipe that wraps these classes to annotate documents.</p>"},{"location":"utilities/matchers/#edsphrasematcher","title":"EDSPhraseMatcher","text":"<p>The EDSPhraseMatcher lets you efficiently match large terminology lists, by comparing tokenx against a given attribute. This matcher differs from the <code>spacy.PhraseMatcher</code> in that it allows to skip pollution tokens. To make it efficient, we have reimplemented the matching algorithm in Cython, like the original <code>spacy.PhraseMatcher</code>.</p> <p>You can use it as described in the code below.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.phrase import EDSPhraseMatcher\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\ndoc = nlp(\"On ne rel\u00e8ve pas de signe du Corona =============== virus.\")\n\nmatcher = EDSPhraseMatcher(nlp.vocab, attr=\"NORM\")\nmatcher.build_patterns(\n    nlp,\n    {\n        \"covid\": [\"corona virus\", \"coronavirus\", \"covid\"],\n        \"diabete\": [\"diabete\", \"diabetique\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: Corona =============== virus\n</code></pre>"},{"location":"utilities/matchers/#regexmatcher","title":"RegexMatcher","text":"<p>The <code>RegexMatcher</code> performs full-text regex matching. It is especially useful to handle spelling variations like <code>mammo-?graphies?</code>. Like the <code>EDSPhraseMatcher</code>, this class allows to skip pollution tokens. Note that this class is significantly slower than the <code>EDSPhraseMatcher</code>: if you can, try enumerating lexical variations of the target phrases and feed them to the <code>PhraseMatcher</code> instead.</p> <p>You can use it as described in the code below.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.regex import RegexMatcher\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\ndoc = nlp(\"On ne rel\u00e8ve pas de signe du Corona =============== virus.\")\n\nmatcher = RegexMatcher(attr=\"NORM\", ignore_excluded=True)\nmatcher.build_patterns(\n    {\n        \"covid\": [\"corona[ ]*virus\", \"covid\"],\n        \"diabete\": [\"diabete\", \"diabetique\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: Corona =============== virus\n</code></pre>"},{"location":"utilities/matchers/#simstringmatcher","title":"SimstringMatcher","text":"<p>The <code>SimstringMatcher</code> performs fuzzy term matching by comparing spans of text with a similarity metric. It is especially useful to handle spelling variations like <code>paracetomol</code> (instead of <code>paracetamol</code>).</p> <p>The <code>simstring</code> algorithm compares two strings by enumerating their char trigrams and measuring the overlap between the two sets. In the previous example: - <code>paracetomol</code> becomes <code>##p #pa par ara rac ace cet eto tom omo mol ol# l##</code> - <code>paracetamol</code> becomes <code>##p #pa par ara rac ace cet eta tam amo mol ol# l##</code> and the Dice (or F1) similarity between the two sets is 0.75.</p> <p>Like the <code>EDSPhraseMatcher</code>, this class allows to skip pollution tokens. Just like the <code>RegexMatcher</code>, this class is significantly slower than the <code>EDSPhraseMatcher</code>: if you can, try enumerating lexical variations of the target phrases and feed them to the <code>PhraseMatcher</code> instead.</p> <p>You can use it as described in the code below.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.simstring import SimstringMatcher\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\ndoc = nlp(\n    \"On ne rel\u00e8ve pas de signe du corona-virus. Historique d'un hepatocellulaire carcinome.\"\n)\n\nmatcher = SimstringMatcher(\n    nlp.vocab,\n    attr=\"NORM\",\n    ignore_excluded=True,\n    measure=\"dice\",\n    threshold=0.75,\n    windows=5,\n)\nmatcher.build_patterns(\n    nlp,\n    {\n        \"covid\": [\"coronavirus\", \"covid\"],\n        \"carcinome\": [\"carcinome hepatocellulaire\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: corona-virus\n\nlist(matcher(doc, as_spans=True))[1].text\n# Out: hepatocellulaire carcinome\n</code></pre>"},{"location":"utilities/regex/","title":"Work with RegExp","text":""},{"location":"utilities/connectors/brat/","title":"BRAT Connector","text":"<p>BRAT is currently the only supported in-text annotation editor at EDS. BRAT annotations are in the standoff format. Consider the following document:</p> <pre><code>Le patient est admis pour une pneumopathie au coronavirus.\nOn lui prescrit du parac\u00e9tamol.\n</code></pre> <p>It could be annotated as follows :</p> <pre><code>T1  Patient 4 11    patient\nT2  Disease 31 58   pneumopathie au coronavirus\nT3  Drug 79 90  parac\u00e9tamol\n</code></pre> <p>The point of the BRAT connector is to go from the standoff annotation format to an annotated spaCy document :</p> <pre><code>import edsnlp\nfrom edsnlp.connectors.brat import BratConnector\n\n# Instantiate the connector\nbrat = BratConnector(\"path/to/brat\")\n\n# Instantiate the spacy pipeline\nnlp = edsnlp.blank(\"eds\")\n\n# Convert all BRAT files to a list of documents\ndocs = brat.brat2docs(nlp)\ndoc = docs[0]\n\ndoc.ents\n# Out: [patient, pneumopathie au coronavirus, parac\u00e9tamol]\n\ndoc.ents[0].label_\n# Out: Patient\n</code></pre> <p>The connector can also go the other way around, enabling pre-annotations and an ersatz of active learning.</p>"},{"location":"utilities/connectors/labeltool/","title":"LabelTool Connector","text":"<p>LabelTool is an in-house module enabling rapid annotation of pre-extracted entities.</p> <p>We provide a ready-to-use function that converts a list of annotated spaCy documents into a <code>pandas</code> DataFrame that is readable to LabelTool.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nfrom edsnlp.connectors.labeltool import docs2labeltool\n\ncorpus = [\n    \"Ceci est un document m\u00e9dical.\",\n    \"Le patient n'est pas malade.\",\n]\n\n# Instantiate the spacy pipeline\nnlp = edsnlp.blank(\"fr\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.matcher(terms=dict(medical=\"m\u00e9dical\", malade=\"malade\")))\nnlp.add_pipe(eds.negation())\n\n# Convert all BRAT files to a list of documents\ndocs = nlp.pipe(corpus)\n\ndf = docs2labeltool(docs, extensions=[\"negation\"])\n</code></pre> <p>The results:</p> note_id note_text start end label lexical_variant negation 0 Ceci est un document m\u00e9dical. 21 28 medical m\u00e9dical False 1 Le patient n'est pas malade. 21 27 malade malade True"},{"location":"utilities/connectors/omop/","title":"OMOP Connector","text":"<p>We provide a connector between OMOP-formatted dataframes and spaCy documents.</p>"},{"location":"utilities/connectors/omop/#omop-style-dataframes","title":"OMOP-style dataframes","text":"<p>Consider a corpus of just one document:</p> <pre><code>Le patient est admis pour une pneumopathie au coronavirus.\nOn lui prescrit du parac\u00e9tamol.\n</code></pre> <p>And its OMOP-style representation, separated in two tables <code>note</code> and <code>note_nlp</code> (here with selected columns) :</p> <p><code>note</code>:</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23 <p><code>note_nlp</code>:</p> note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol"},{"location":"utilities/connectors/omop/#using-the-connector","title":"Using the connector","text":"<p>The following snippet expects the tables <code>note</code> and <code>note_nlp</code> to be already defined (eg through PySpark's <code>toPandas()</code> method).</p> <pre><code>import spacy\nfrom edsnlp.connectors.omop import OmopConnector\n\n# Instantiate a spacy pipeline\nnlp = spacy.blank(\"eds\")\n\n# Instantiate the connector\nconnector = OmopConnector(nlp)\n\n# Convert OMOP tables (note and note_nlp) to a list of documents\ndocs = connector.omop2docs(note, note_nlp)\ndoc = docs[0]\n\ndoc.ents\n# Out: [coronavirus, parac\u00e9tamol]\n\ndoc.ents[0].label_\n# Out: 'disease'\n\ndoc.text == note.loc[0].note_text\n# Out: True\n</code></pre> <p>The object <code>docs</code> now contains a list of documents that reflects the information contained in the OMOP-formatted dataframes.</p>"},{"location":"utilities/connectors/overview/","title":"Overview of connectors","text":"<p>EDS-NLP provides a series of connectors apt to convert back and forth from different formats into spaCy representation.</p> <p>We provide the following connectors:</p> <ul> <li>BRAT</li> <li>OMOP</li> </ul>"},{"location":"utilities/tests/","title":"Tests Utilities","text":"<p>We provide a few testing utilities that simplify the process of:</p> <ul> <li>creating testing examples for NLP pipelines;</li> <li>testing documentation code blocs.</li> </ul>"},{"location":"utilities/tests/blocs/","title":"Testing Code Blocs","text":"<p>We created a utility that scans through markdown files, extracts code blocs and executes them to check that everything is indeed functional.</p> <p>There is more! Whenever the utility comes across an example (denoted by <code># Out:</code>, see example below), an <code>assert</code> statement is dynamically added to the snippet to check that the output matches.</p> <p>For instance:</p> <pre><code>a = 1\n\na\n# Out: 1\n</code></pre> <p>Is transformed into:</p> <pre><code>a = 1\n\nv = a\nassert repr(v) == \"1\"\n</code></pre> <p>We can disable code checking for a specific code bloc by adding <code>&lt;!-- no-check --&gt;</code> above it:</p> <pre><code>```{ .python .no-check }\ntest = undeclared_function(42)\n```\n</code></pre> <p>See the dedicated reference for more information</p>"},{"location":"utilities/tests/examples/","title":"Creating Examples","text":"<p>Testing a NER/qualifier pipeline can be a hassle. We created a utility to simplify that process.</p> <p>Using the <code>parse_example</code> method, you can define a full example in a human-readable way:</p> <pre><code>from edsnlp.utils.examples import parse_example\n\nexample = \"Absence d'&lt;ent negated=true&gt;image osseuse d'allure \u00e9volutive&lt;/ent&gt;.\"\n\ntext, entities = parse_example(example)\n\ntext\n# Out: \"Absence d'image osseuse d'allure \u00e9volutive.\"\n\nentities\n# Out: [Entity(start_char=10, end_char=42, modifiers=[Modifier(key='negated', value=True)])]\n</code></pre> <p>Entities are defined using the <code>&lt;ent&gt;</code> tag. You can encode complexe information by adding keys into the tag (see example above). The <code>parse_example</code> method strips the text of the tags, and outputs a list of <code>Entity</code> objects that contain:</p> <ul> <li>the character indices of the entity ;</li> <li>custom user-defined \"modifiers\".</li> </ul> <p>See the dedicated reference page for more information.</p>"},{"location":"reference/edsnlp/","title":"<code>edsnlp</code>","text":"<p>EDS-NLP</p>"},{"location":"reference/edsnlp/conjugator/","title":"<code>edsnlp.conjugator</code>","text":""},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.conjugate_verb","title":"<code>conjugate_verb</code>","text":"<p>Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas <code>DataFrame</code>.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.conjugate_verb--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verb</code> <p>Verb to conjugate.</p> <p> TYPE: <code>str</code> </p> <code>conjugator</code> <p>mlconjug3 instance for conjugating.</p> <p> TYPE: <code>Conjugator</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Normalized dataframe containing all conjugated forms for the verb.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.conjugate","title":"<code>conjugate</code>","text":"<p>Conjugate a list of verbs.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.conjugate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to conjugate</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>language</code> <p>Language to conjugate. Defaults to French (<code>fr</code>).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'fr'</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Dataframe containing the conjugations for the provided verbs. Columns: <code>verb</code>, <code>mode</code>, <code>tense</code>, <code>person</code>, <code>term</code></p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.get_conjugated_verbs","title":"<code>get_conjugated_verbs</code>","text":"<p>Get a list of conjugated verbs.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.get_conjugated_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to conjugate.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>matches</code> <p>List of dictionary describing the mode/tense/persons to keep.</p> <p> TYPE: <code>Union[List[Dict[str, str]], Dict[str, str]]</code> </p> <code>language</code> <p>[description], by default \"fr\" (French)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'fr'</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of terms to look for.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.get_conjugated_verbs--examples","title":"Examples","text":"<p>get_conjugated_verbs(         \"aimer\",         dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"),     ) ['aimons']</p>"},{"location":"reference/edsnlp/connectors/","title":"<code>edsnlp.connectors</code>","text":""},{"location":"reference/edsnlp/connectors/brat/","title":"<code>edsnlp.connectors.brat</code>","text":""},{"location":"reference/edsnlp/connectors/brat/#edsnlp.connectors.brat.BratConnector","title":"<code>BratConnector</code>","text":"<p>           Bases: <code>object</code></p> <p>Deprecated. Use <code>edsnlp.data.read_standoff</code> and <code>edsnlp.data.write_standoff</code> instead. Two-way connector with BRAT. Supports entities only.</p>"},{"location":"reference/edsnlp/connectors/brat/#edsnlp.connectors.brat.BratConnector--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>directory</code> <p>Directory containing the BRAT files.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>n_jobs</code> <p>Number of jobs for multiprocessing, by default 1</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>attributes</code> <p>Mapping from BRAT attributes to spaCy Span extensions. Extensions / attributes that are not in the mapping are not imported or exported If left to None, the mapping is filled with all BRAT attributes.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>span_groups</code> <p>Additional span groups to look for entities in spaCy documents when exporting. Missing label (resp. span group) names are not imported (resp. exported) If left to None, the sequence is filled with all BRAT entity labels.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>['ents', '*']</code> </p>"},{"location":"reference/edsnlp/connectors/brat/#edsnlp.connectors.brat.BratConnector.docs2brat","title":"<code>docs2brat</code>","text":"<p>Writes a list of spaCy documents to file.</p>"},{"location":"reference/edsnlp/connectors/labeltool/","title":"<code>edsnlp.connectors.labeltool</code>","text":""},{"location":"reference/edsnlp/connectors/labeltool/#edsnlp.connectors.labeltool.docs2labeltool","title":"<code>docs2labeltool</code>","text":"<p>Returns a labeltool-ready dataframe from a list of annotated document.</p>"},{"location":"reference/edsnlp/connectors/labeltool/#edsnlp.connectors.labeltool.docs2labeltool--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of annotated spacy docs.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>List of extensions to use by labeltool.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>df</code> <p>DataFrame tailored for labeltool.</p> <p> TYPE: <code>DataFrame</code> </p>"},{"location":"reference/edsnlp/connectors/omop/","title":"<code>edsnlp.connectors.omop</code>","text":""},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector","title":"<code>OmopConnector</code>","text":"<p>           Bases: <code>object</code></p> <p>[summary]</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline instance</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>start_char</code> <p>Name of the column containing the start character index of the entity, by default \"start_char\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'start_char'</code> </p> <code>end_char</code> <p>Name of the column containing the end character index of the entity, by default \"end_char\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'end_char'</code> </p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.preprocess","title":"<code>preprocess</code>","text":"<p>Preprocess the input OMOP tables: modification of the column names.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.preprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.postprocess","title":"<code>postprocess</code>","text":"<p>Postprocess the input OMOP tables: modification of the column names.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.postprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.omop2docs","title":"<code>omop2docs</code>","text":"<p>Transforms OMOP tables to a list of spaCy documents.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.omop2docs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Doc]</code> <p>List of spaCy documents.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.docs2omop","title":"<code>docs2omop</code>","text":"<p>Transforms a list of spaCy documents to a pair of OMOP tables.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.docs2omop--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of spaCy documents.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.omop2docs","title":"<code>omop2docs</code>","text":"<p>Transforms an OMOP-formatted pair of dataframes into a list of documents.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.omop2docs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>note</code> <p>The OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>The OMOP <code>note_nlp</code> table</p> <p> TYPE: <code>DataFrame</code> </p> <code>nlp</code> <p>The pipeline instance</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Doc] :</code> <p>List of spaCy documents</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.docs2omop","title":"<code>docs2omop</code>","text":"<p>Transforms a list of spaCy docs to a pair of OMOP tables.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.docs2omop--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of documents to transform.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <p>Pair of OMOP tables (<code>note</code> and <code>note_nlp</code>)</p>"},{"location":"reference/edsnlp/core/","title":"<code>edsnlp.core</code>","text":""},{"location":"reference/edsnlp/core/lazy_collection/","title":"<code>edsnlp.core.lazy_collection</code>","text":""},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection","title":"<code>LazyCollection</code>","text":""},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.set_processing","title":"<code>set_processing</code>","text":""},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.set_processing--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch_size</code> <p>Number of documents to process at a time in a GPU worker (or in the main process if no workers are used).</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFER</code> </p> <code>batch_by</code> <p>How to compute the batch size. Can be \"docs\" or \"words\" :</p> <ul> <li>\"docs\" (default) is the number of documents.</li> <li>\"words\" is the total number of words in the documents.</li> <li>\"padded_words\" is the total number of words in the documents, including    padding, assuming the documents are padded to the same length.</li> </ul> <p> TYPE: <code>Literal['docs', 'words', 'padded_words']</code> DEFAULT: <code>'docs'</code> </p> <code>chunk_size</code> <p>Number of documents to build before splitting into batches. Only used with \"simple\" and \"multiprocessing\" backends. This is also the number of documents that will be passed through the first components of the pipeline until a GPU worker is used (then the chunks will be split according to the <code>batch_size</code> and <code>batch_by</code> arguments).</p> <p>By default, the chunk size is equal to the batch size, or 128 if the batch size is not set.</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFER</code> </p> <code>sort_chunks</code> <p>Whether to sort the documents by size before splitting into batches.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>split_into_batches_after</code> <p>The name of the component after which to split the documents into batches. Only used with \"simple\" and \"multiprocessing\" backends. By default, the documents are split into batches as soon as the input are converted into Doc objects.</p> <p> TYPE: <code>str</code> DEFAULT: <code>INFER</code> </p> <code>num_cpu_workers</code> <p>Number of CPU workers. A CPU worker handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components. If no GPU workers are used, the CPU workers also handle the forward call of the deep-learning components.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>INFER</code> </p> <code>num_gpu_workers</code> <p>Number of GPU workers. A GPU worker handles the forward call of the deep-learning components. Only used with \"multiprocessing\" backend.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>INFER</code> </p> <code>disable_implicit_parallelism</code> <p>Whether to disable OpenMP and Huggingface tokenizers implicit parallelism in multiprocessing mode. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>gpu_pipe_names</code> <p>List of pipe names to accelerate on a GPUWorker, defaults to all pipes that inherit from TorchComponent. Only used with \"multiprocessing\" backend. Inferred from the pipeline if not set.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>INFER</code> </p> <code>backend</code> <p>The backend to use for parallel processing. If not set, the backend is automatically selected based on the input data and the number of workers.</p> <ul> <li>\"simple\" is the default backend and is used when <code>num_cpu_workers</code> is 1     and <code>num_gpu_workers</code> is 0.</li> <li>\"multiprocessing\" is used when <code>num_cpu_workers</code> is greater than 1 or     <code>num_gpu_workers</code> is greater than 0.</li> <li>\"spark\" is used when the input data is a Spark dataframe and the output     writer is a Spark writer.</li> </ul> <p> TYPE: <code>Optional[Literal['simple', 'multiprocessing', 'mp', 'spark']]</code> DEFAULT: <code>INFER</code> </p> <code>show_progress</code> <p>Whether to show progress bars (only applicable with \"simple\" and \"multiprocessing\" backends).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>process_start_method</code> <p>Whether to use \"fork\" or \"spawn\" as the start method for the multiprocessing backend. The default is \"fork\" on Unix systems and \"spawn\" on Windows.</p> <ul> <li>\"fork\" is the default start method on Unix systems and is the fastest     start method, but it is not available on Windows, can cause issues     with CUDA and is not safe when using multiple threads.</li> <li>\"spawn\" is the default start method on Windows and is the safest start     method, but it is not available on Unix systems and is slower than     \"fork\".</li> </ul> <p> TYPE: <code>Optional[Literal['fork', 'spawn']]</code> DEFAULT: <code>INFER</code> </p> <code>gpu_worker_devices</code> <p>List of GPU devices to use for the GPU workers. Defaults to all available devices, one worker per device. Only used with \"multiprocessing\" backend.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>INFER</code> </p> <code>cpu_worker_devices</code> <p>List of GPU devices to use for the CPU workers. Used for debugging purposes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>INFER</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.map","title":"<code>map</code>","text":"<p>Maps a callable to the documents.</p>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.map--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>pipe</code> <p>The callable to map to the documents.</p> <p> </p> <code>name</code> <p>The name of the pipeline step.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>The keyword arguments to pass to the callable.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.map_batches","title":"<code>map_batches</code>","text":"<p>Maps a callable to a batch of documents. The callable should take a list of inputs and return a list of outputs (not a single output).</p>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.map_batches--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>pipe</code> <p>The callable to map to the documents.</p> <p> </p> <code>name</code> <p>The name of the pipeline step.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>The keyword arguments to pass to the callable.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.map_gpu","title":"<code>map_gpu</code>","text":"<p>Maps a deep learning operation to a batch of documents, on a GPU worker.</p>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.map_gpu--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>prepare_batch</code> <p>A callable that takes a list of documents and a device and returns a batch of tensors (or anything that can be passed to the <code>forward</code> callable). This will be called on a CPU-bound worker, and may be parallelized.</p> <p> TYPE: <code>Callable[[List, Union[str, device]], Any]</code> </p> <code>forward</code> <p>A callable that takes the output of <code>prepare_batch</code> and returns the output of the deep learning operation. This will be called on a GPU-bound worker.</p> <p> TYPE: <code>Callable[[Any], Any]</code> </p> <code>postprocess</code> <p>An optional callable that takes the list of documents and the output of the deep learning operation, and returns the final output. This will be called on the same CPU-bound worker that called the <code>prepare_batch</code> function.</p> <p> TYPE: <code>Optional[Callable[[List, Any], Any]]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the pipeline step.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.torch_components","title":"<code>torch_components</code>","text":"<p>Yields components that are PyTorch modules.</p>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.torch_components--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>disable</code> <p>The names of disabled components, which will be skipped.</p> <p> TYPE: <code>Container[str]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Iterable[Tuple[str, 'edsnlp.core.torch_component.TorchComponent']]</code>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.to","title":"<code>to</code>","text":"<p>Moves the pipeline to a given device</p>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.train","title":"<code>train</code>","text":"<p>Enables training mode on pytorch modules</p>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.train--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>mode</code> <p>Whether to enable training or not</p> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/core/lazy_collection/#edsnlp.core.lazy_collection.LazyCollection.eval","title":"<code>eval</code>","text":"<p>Enables evaluation mode on pytorch modules</p>"},{"location":"reference/edsnlp/core/pipeline/","title":"<code>edsnlp.core.pipeline</code>","text":""},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>New pipeline to use as a drop-in replacement for spaCy's pipeline. It uses PyTorch as the deep-learning backend and allows components to share subcomponents.</p> <p>See the documentation for more details.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>lang</code> <p>Language code</p> <p> TYPE: <code>str</code> </p> <code>create_tokenizer</code> <p>Function that creates a tokenizer for the pipeline</p> <p> TYPE: <code>Callable[[Pipeline], Optional[Tokenizer]]</code> DEFAULT: <code>None</code> </p> <code>vocab</code> <p>Whether to create a new vocab or use an existing one</p> <p> TYPE: <code>Union[bool, Vocab]</code> DEFAULT: <code>True</code> </p> <code>batch_size</code> <p>Batch size to use in the <code>.pipe()</code> method</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>128</code> </p> <code>vocab_config</code> <p>Configuration for the vocab</p> <p> TYPE: <code>Type[BaseDefaults]</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Meta information about the pipeline</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.disabled","title":"<code>disabled</code>  <code>property</code>","text":"<p>The names of the disabled components</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.cfg","title":"<code>cfg: Config</code>  <code>property</code>","text":"<p>Returns the config of the pipeline, including the config of all components. Updated from spacy to allow references between components.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.get_pipe","title":"<code>get_pipe</code>","text":"<p>Get a component by its name.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.get_pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>The name of the component to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Pipe</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.has_pipe","title":"<code>has_pipe</code>","text":"<p>Check if a component exists in the pipeline.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.has_pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>The name of the component to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.create_pipe","title":"<code>create_pipe</code>","text":"<p>Create a component from a factory name.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.create_pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>factory</code> <p>The name of the factory to use</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>The config to pass to the factory</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Pipe</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.add_pipe","title":"<code>add_pipe</code>","text":"<p>Add a component to the pipeline.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.add_pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>factory</code> <p>The name of the component to add or the component itself</p> <p> TYPE: <code>Union[str, Pipe]</code> </p> <code>name</code> <p>The name of the component. If not provided, the name of the component will be used if it has one (.name), otherwise the factory name will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>first</code> <p>Whether to add the component to the beginning of the pipeline. This argument is mutually exclusive with <code>before</code> and <code>after</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>before</code> <p>The name of the component to add the new component before. This argument is mutually exclusive with <code>after</code> and <code>first</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>after</code> <p>The name of the component to add the new component after. This argument is mutually exclusive with <code>before</code> and <code>first</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The arguments to pass to the component factory.</p> <p>Note that instead of replacing arguments with the same keys, the config will be merged with the default config of the component. This means that you can override specific nested arguments without having to specify the entire config.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Pipe</code> <p>The component that was added to the pipeline.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.get_pipe_meta","title":"<code>get_pipe_meta</code>","text":"<p>Get the meta information for a component.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.get_pipe_meta--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>The name of the component to get the meta for.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.make_doc","title":"<code>make_doc</code>","text":"<p>Create a Doc from text.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.make_doc--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>text</code> <p>The text to create the Doc from.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.__call__","title":"<code>__call__</code>","text":"<p>Apply each component successively on a document.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>text</code> <p>The text to create the Doc from, or a Doc.</p> <p> TYPE: <code>Union[str, Doc]</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.pipe","title":"<code>pipe</code>","text":"<p>Process a stream of documents by applying each component successively on batches of documents.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>inputs</code> <p>The inputs to create the Docs from, or Docs directly.</p> <p> TYPE: <code>Union[Iterable, LazyCollection]</code> </p> <code>batch_size</code> <p>The batch size to use. If not provided, the batch size of the pipeline object will be used.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>n_process</code> <p>Deprecated. Use the \".set(num_cpu_workers=n_process)\" method on the returned data lazy collection instead. The number of parallel workers to use. If 0, the operations will be executed sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.cache","title":"<code>cache</code>","text":"<p>Enable caching for all (trainable) components in the pipeline</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.torch_components","title":"<code>torch_components</code>","text":"<p>Yields components that are PyTorch modules.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.torch_components--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>disable</code> <p>The names of disabled components, which will be skipped.</p> <p> TYPE: <code>Container[str]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Iterable[Tuple[str, TorchComponent]]</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.post_init","title":"<code>post_init</code>","text":"<p>Completes the initialization of the pipeline by calling the post_init method of all components that have one. This is useful for components that need to see some data to build their vocabulary, for instance.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.post_init--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The documents to use for initialization. Each component will not necessarily see all the data.</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>exclude</code> <p>Components to exclude from post initialization on data</p> <p> TYPE: <code>Optional[Set]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.from_config","title":"<code>from_config</code>  <code>classmethod</code>","text":"<p>Create a pipeline from a config object</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.from_config--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>config</code> <p>The config to use</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>vocab</code> <p>The spaCy vocab to use. If True, a new vocab will be created</p> <p> TYPE: <code>Union[Vocab, bool]</code> DEFAULT: <code>True</code> </p> <code>disable</code> <p>Components to disable</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> DEFAULT: <code>EMPTY_LIST</code> </p> <code>enable</code> <p>Components to enable</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> DEFAULT: <code>EMPTY_LIST</code> </p> <code>exclude</code> <p>Components to exclude</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> DEFAULT: <code>EMPTY_LIST</code> </p> <code>meta</code> <p>Metadata to add to the pipeline</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>FrozenDict()</code> </p> RETURNS DESCRIPTION <code>Pipeline</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.__get_validators__","title":"<code>__get_validators__</code>  <code>classmethod</code>","text":"<p>Pydantic validators generator</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.validate","title":"<code>validate</code>  <code>classmethod</code>","text":"<p>Pydantic validator, used in the <code>validate_arguments</code> decorated functions</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.preprocess_many","title":"<code>preprocess_many</code>","text":"<p>Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.preprocess_many--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>compress</code> <p>Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information.</p> <p> DEFAULT: <code>True</code> </p> <code>supervision</code> <p>Whether to include supervision information in the preprocessing</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.collate","title":"<code>collate</code>","text":"<p>Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.collate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>The batch of preprocessed samples</p> <p> TYPE: <code>Union[Iterable[Dict[str, Any]], Dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>The collated batch</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.parameters","title":"<code>parameters</code>","text":"<p>Returns an iterator over the Pytorch parameters of the components in the pipeline</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.named_parameters","title":"<code>named_parameters</code>","text":"<p>Returns an iterator over the Pytorch parameters of the components in the pipeline</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.to","title":"<code>to</code>","text":"<p>Moves the pipeline to a given device</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.train","title":"<code>train</code>","text":"<p>Enables training mode on pytorch modules</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.train--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>mode</code> <p>Whether to enable training or not</p> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.to_disk","title":"<code>to_disk</code>","text":"<p>Save the pipeline to a directory.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.to_disk--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>The path to the directory to save the pipeline to. Every component will be saved to separated subdirectories of this directory, except for tensors that will be saved to a shared files depending on the references between the components.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>exclude</code> <p>The names of the components, or attributes to exclude from the saving process. By default, the vocabulary is excluded since it may contain personal identifiers and can be rebuilt during inference.</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.from_disk","title":"<code>from_disk</code>","text":"<p>Load the pipeline from a directory. Components will be updated in-place.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.from_disk--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>The path to the directory to load the pipeline from</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>exclude</code> <p>The names of the components, or attributes to exclude from the loading process.</p> <p> TYPE: <code>Optional[Union[str, Sequence[str]]]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to use when loading the tensors</p> <p> TYPE: <code>Optional[Union[str, device]]</code> DEFAULT: <code>'cpu'</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.select_pipes","title":"<code>select_pipes</code>","text":"<p>Temporarily disable and enable components in the pipeline.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.select_pipes--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>disable</code> <p>The name of the component to disable, or a list of names.</p> <p> TYPE: <code>Optional[Union[str, Iterable[str]]]</code> DEFAULT: <code>None</code> </p> <code>enable</code> <p>The name of the component to enable, or a list of names.</p> <p> TYPE: <code>Optional[Union[str, Iterable[str]]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.blank","title":"<code>blank</code>","text":"<p>Loads an empty EDS-NLP Pipeline, similarly to <code>spacy.blank</code>. In addition to standard components, this pipeline supports EDS-NLP trainable torch components.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.blank--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n</code></pre>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.blank--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>lang</code> <p>Language ID, e.g. \"en\", \"fr\", \"eds\", etc.</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>The config to use for the pipeline</p> <p> TYPE: <code>Union[Dict[str, Any], Config]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The new empty pipeline instance.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load","title":"<code>load</code>","text":"<p>Load a pipeline from a config file or a directory.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load--examples","title":"Examples","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.load(\n    \"path/to/config.cfg\",\n    overrides={\"components\": {\"my_component\": {\"arg\": \"value\"}}},\n)\n</code></pre>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>model</code> <p>The config to use for the pipeline, or the path to a config file or a directory.</p> <p> TYPE: <code>Union[Path, str, Config]</code> </p> <code>overrides</code> <p>Overrides to apply to the config when loading the pipeline. These are the same parameters as the ones used when initializing the pipeline.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>exclude</code> <p>The names of the components, or attributes to exclude from the loading process.  The <code>exclude</code> argument will be mutated in place.</p> <p> TYPE: <code>Optional[Union[str, Iterable[str]]]</code> DEFAULT: <code>None</code> </p> <code>auto_update</code> <p>When installing a pipeline from the Hugging Face Hub, whether to automatically try to update the model, even if a local version is found. Only applies when loading from the Hugging Face Hub.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>install_dependencies</code> <p>When installing a pipeline from the Hugging Face Hub, whether to install the dependencies of the model if they are not already installed. Only applies when loading from the Hugging Face Hub.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Pipeline</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load_from_huggingface","title":"<code>load_from_huggingface</code>","text":"<p>Load a model from the Hugging Face Hub.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load_from_huggingface--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>repo_id</code> <p>The repository ID of the model to load (e.g. \"username/repo_name\").</p> <p> TYPE: <code>str</code> </p> <code>auto_update</code> <p>Whether to automatically try to update the model, even if a local version is found.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>install_dependencies</code> <p>Whether to install the dependencies of the model if they are not already installed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>token</code> <p>The Hugging Face Hub API token to use.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>revision</code> <p>The revision of the model to load.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the model's <code>load</code> method.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/core/registries/","title":"<code>edsnlp.core.registries</code>","text":""},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.CurriedFactory","title":"<code>CurriedFactory</code>","text":""},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.CurriedFactory.instantiate","title":"<code>instantiate</code>","text":"<p>To ensure compatibility with spaCy's API, we need to support passing in the nlp object and name to factories. Since they can be nested, we need to add them to every factory in the config.</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.FactoryRegistry","title":"<code>FactoryRegistry</code>","text":"<p>           Bases: <code>Registry</code></p> <p>A registry that validates the input arguments of the registered functions.</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.FactoryRegistry.get","title":"<code>get</code>","text":"<p>Get the registered function for a given name. Since we want to be able to get functions registered under spacy namespace, and functions defined in memory but not accessible via entry points (spacy internal namespace), we need to check multiple registries.</p> <p>The strategy is the following:</p> <ol> <li>If the function exists in the edsnlp_factories namespace, return it as a    curried function.</li> <li>Otherwise, check spacy namespaces and re-register it under edsnlp_factories    namespace, then return it as a curried function.</li> <li>Otherwise, search in edsnlp's entry points and redo steps 1 &amp; 2</li> <li>Otherwise, search in spacy's points and redo steps 1 &amp; 2</li> <li>Fail</li> </ol> <p>name (str): The name. RETURNS (Any): The registered function.</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.FactoryRegistry.register","title":"<code>register</code>","text":"<p>This is a convenience wrapper around <code>confit.Registry.register</code>, that curries the function to be registered, allowing to instantiate the class later once <code>nlp</code> and <code>name</code> are known.</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.FactoryRegistry.register--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p> TYPE: <code>str</code> </p> <code>func</code> <p> TYPE: <code>Optional[InFunc]</code> DEFAULT: <code>None</code> </p> <code>default_config</code> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>FrozenDict()</code> </p> <code>assigns</code> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>FrozenList()</code> </p> <code>requires</code> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>FrozenList()</code> </p> <code>retokenizes</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_score_weights</code> <p> TYPE: <code>Dict[str, Optional[float]]</code> DEFAULT: <code>FrozenDict()</code> </p> <code>invoker</code> <p> TYPE: <code>Callable</code> DEFAULT: <code>None</code> </p> <code>deprecated</code> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>()</code> </p> <code>spacy_compatible</code> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Callable[[InFunc], InFunc]</code>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.accepted_arguments","title":"<code>accepted_arguments</code>","text":"<p>Checks that a function accepts a list of keyword arguments</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.accepted_arguments--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>func</code> <p>Function to check</p> <p> TYPE: <code>Callable</code> </p> <code>args</code> <p>Argument or list of arguments to check</p> <p> TYPE: <code>Sequence[str]</code> </p> RETURNS DESCRIPTION <code>List[str]</code>"},{"location":"reference/edsnlp/core/torch_component/","title":"<code>edsnlp.core.torch_component</code>","text":""},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent","title":"<code>TorchComponent</code>","text":"<p>           Bases: <code>BaseComponent</code>, <code>Module</code>, <code>Generic[BatchOutput, BatchInput]</code></p> <p>A TorchComponent is a Component that can be trained and inherits <code>torch.nn.Module</code>. You can use it either as a torch module inside a more complex neural network, or as a standalone component in a Pipeline.</p> <p>In addition to the methods of a torch module, a TorchComponent adds a few methods to handle preprocessing and collating features, as well as caching intermediate results for components that share a common subcomponent.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.post_init","title":"<code>post_init</code>","text":"<p>This method completes the attributes of the component, by looking at some documents. It is especially useful to build vocabularies or detect the labels of a classification task.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.post_init--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>gold_data</code> <p>The documents to use for initialization.</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>exclude</code> <p>The names of components to exclude from initialization. This argument will be gradually updated  with the names of initialized components</p> <p> TYPE: <code>Set[str]</code> </p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.preprocess","title":"<code>preprocess</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network to perform its predictions.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.preprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to preprocess</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.collate","title":"<code>collate</code>","text":"<p>Collate the batch of features into a single batch of tensors that can be used by the forward method of the component.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.collate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of features</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>BatchInput</code> <p>Dictionary (optionally nested) containing the collated tensors</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.batch_to_device","title":"<code>batch_to_device</code>","text":"<p>Move the batch of tensors to the specified device.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.batch_to_device--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of tensors</p> <p> TYPE: <code>BatchInput</code> </p> <code>device</code> <p>Device to move the tensors to</p> <p> TYPE: <code>Optional[Union[str, device]]</code> </p> RETURNS DESCRIPTION <code>BatchInput</code>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.forward","title":"<code>forward</code>","text":"<p>Perform the forward pass of the neural network.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of tensors (nested dictionary) computed by the collate method</p> <p> TYPE: <code>BatchInput</code> </p> RETURNS DESCRIPTION <code>BatchOutput</code>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.module_forward","title":"<code>module_forward</code>","text":"<p>This is a wrapper around <code>torch.nn.Module.__call__</code> to avoid conflict with the components <code>__call__</code> method.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.prepare_batch","title":"<code>prepare_batch</code>","text":"<p>Convenience method to preprocess a batch of documents and collate them Features corresponding to the same path are grouped together in a list, under the same key.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.prepare_batch--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>Batch of documents</p> <p> TYPE: <code>Sequence[Doc]</code> </p> <code>supervision</code> <p>Whether to extract supervision features or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>device</code> <p>Device to move the tensors to</p> <p> TYPE: <code>Optional[Union[str, device]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, Sequence[Any]]</code>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.batch_process","title":"<code>batch_process</code>","text":"<p>Process a batch of documents using the neural network. This differs from the <code>pipe</code> method in that it does not return an iterator, but executes the component on the whole batch at once.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.batch_process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>Batch of documents</p> <p> TYPE: <code>Sequence[Doc]</code> </p> RETURNS DESCRIPTION <code>Sequence[Doc]</code> <p>Batch of updated documents</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.postprocess","title":"<code>postprocess</code>","text":"<p>Update the documents with the predictions of the neural network. By default, this is a no-op.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.postprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of documents to update</p> <p> TYPE: <code>Sequence[Doc]</code> </p> <code>results</code> <p>Batch of predictions, as returned by the forward method</p> <p> TYPE: <code>BatchOutput</code> </p> <code>inputs</code> <p>List of preprocessed features, as returned by the preprocess method</p> <p> TYPE: <code>List[Dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>Sequence[Doc]</code>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.preprocess_supervised","title":"<code>preprocess_supervised</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network to perform its training. By default, this returns the same features as the <code>preprocess</code> method.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.preprocess_supervised--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to preprocess</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.pipe","title":"<code>pipe</code>","text":"<p>Applies the component on a collection of documents. It is recommended to use the <code>Pipeline.pipe</code> method instead of this one to apply a pipeline on a collection of documents, to benefit from the caching of intermediate results.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>Input docs</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>batch_size</code> <p>Batch size to use when making batched to be process at once</p> <p> DEFAULT: <code>1</code> </p>"},{"location":"reference/edsnlp/data/","title":"<code>edsnlp.data</code>","text":""},{"location":"reference/edsnlp/data/base/","title":"<code>edsnlp.data.base</code>","text":""},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.BaseReader","title":"<code>BaseReader</code>","text":"<p>The BaseReader servers as a base class for all readers. It expects two methods:</p> <ul> <li><code>read_main</code> method which is called in the main process and should return a     generator of fragments (like filenames) with their estimated size (number of     documents)</li> <li><code>read_worker</code> method which is called in the worker processes and receives     batches of fragments and should return a list of dictionaries (one per     document), ready to be converted to a Doc object by the converter.</li> </ul> <p>Additionally, the subclass should define a <code>DATA_FIELDS</code> class attribute which contains the names of all attributes that should not be copied when the reader is copied to the worker processes. This is useful for example when the reader holds a reference to a large object like a DataFrame that should not be copied to the worker processes.</p>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.from_iterable","title":"<code>from_iterable</code>","text":"<p>The IterableReader (or <code>edsnlp.data.from_iterable</code>) reads a list of Python objects ( texts, dictionaries, ...) and yields documents by passing them through the <code>converter</code> if given, or returns them as is.</p>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.from_iterable--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_iterable([{...}], nlp=nlp, converter=...)\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_iterable</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_iterable([{...}], converter=...)\n</code></pre>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.from_iterable--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to read</p> <p> TYPE: <code>Iterable</code> </p> <code>converter</code> <p>Converter to use to convert the JSON rows of the data source to Doc objects</p> <p> TYPE: <code>Union[str, Callable]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.to_iterable","title":"<code>to_iterable</code>","text":"<p><code>edsnlp.data.to_items</code> returns an iterator of documents, as converted by the <code>converter</code>. In comparison to just iterating over a LazyCollection, this will also apply the <code>converter</code> to the documents, which can lower the data transfer overhead when using multiprocessing.</p>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.to_iterable--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_items([doc], converter=\"omop\")\n</code></pre>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.to_iterable--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments passed to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/brat/","title":"<code>edsnlp.data.brat</code>","text":""},{"location":"reference/edsnlp/data/converters/","title":"<code>edsnlp.data.converters</code>","text":"<p>Converters are used to convert documents between python dictionaries and Doc objects. There are two types of converters: readers and writers. Readers convert dictionaries to Doc objects, and writers convert Doc objects to dictionaries.</p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.AttributesMappingArg","title":"<code>AttributesMappingArg</code>","text":"<p>A mapping from JSON attributes to Span extensions (can be a list too).</p> <p>For instance:</p> <ul> <li><code>doc_attributes={\"datetime\": \"note_datetime\"}</code> will map the <code>datetime</code> JSON   attribute to the <code>note_datetime</code> extension.</li> <li><code>doc_attributes=\"note_datetime\"</code> will map the <code>note_datetime</code> JSON attribute to   the <code>note_datetime</code> extension.</li> <li><code>span_attributes=[\"negation\", \"family\"]</code> will map the <code>negation</code> and <code>family</code> JSON   attributes to the <code>negation</code> and <code>family</code> extensions.</li> </ul>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDict2DocConverter","title":"<code>StandoffDict2DocConverter</code>","text":"<p>Why does BRAT/Standoff need a converter ?</p> <p>You may wonder : why do I need a converter ? Since BRAT is already a NLP oriented format, it should be straightforward to convert it to a Doc object.</p> <p>Indeed, we do provide a default converter for the BRAT standoff format, but we also acknowledge that there may be more than one way to convert a standoff document to a Doc object. For instance, an annotated span may be used to represent a relation between two smaller included entities, or another entity scope, etc.</p> <p>In such cases, we recommend you use a custom converter as described here.</p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDict2DocConverter--examples","title":"Examples","text":"<pre><code># Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.read_standoff(\n    \"path/to/standoff\",\n    converter=\"standoff\",  # set by default\n\n    # Optional parameters\n    tokenizer=tokenizer,\n    span_setter={\"ents\": True, \"*\": True},\n    span_attributes={\"negation\": \"negated\"},\n    keep_raw_attribute_values=False,\n    default_attributes={\"negated\": False, \"temporality\": \"present\"},\n)\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDict2DocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   LazyCollection.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>notes_as_span_attribute</code> <p>If set, the AnnotatorNote annotations will be concatenated and stored in a span attribute with this name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>split_fragments</code> <p>Whether to split the fragments into separate spans or not. If set to False, the fragments will be concatenated into a single span.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter","title":"<code>StandoffDoc2DictConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter--examples","title":"Examples","text":"<pre><code># Any kind of writer (`edsnlp.data.read/from_...`) can be used here\nedsnlp.data.write_standoff(\n    docs,\n    converter=\"standoff\",  # set by default\n\n    # Optional parameters\n    span_getter={\"ents\": True},\n    span_attributes={\"negation\": \"negated\"},\n)\n# or docs.to_standoff(...) if it's already a\n# [LazyCollection][edsnlp.core.lazy_collection.LazyCollection]\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>{'ents': True}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDict2DocConverter","title":"<code>OmopDict2DocConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDict2DocConverter--examples","title":"Examples","text":"<pre><code># Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.from_pandas(\n    df,\n    converter=\"omop\",\n\n    # Optional parameters\n    tokenizer=tokenizer,\n    doc_attributes=[\"note_datetime\"],\n\n    # Parameters below should only matter if you plan to import entities\n    # from the dataframe. If the data doesn't contain pre-annotated\n    # entities, you can ignore these.\n    span_setter={\"ents\": True, \"*\": True},\n    span_attributes={\"negation\": \"negated\"},\n    default_attributes={\"negated\": False, \"temporality\": \"present\"},\n)\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDict2DocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   LazyCollection.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>doc_attributes</code> <p>Mapping from JSON attributes to additional Span extensions (can be a list too). By default, all attributes are imported as Doc extensions with the same name.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{'note_datetime': 'note_datetime'}</code> </p> <code>span_attributes</code> <p>Mapping from JSON attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDoc2DictConverter","title":"<code>OmopDoc2DictConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDoc2DictConverter--examples","title":"Examples","text":"<pre><code># Any kind of writer (`edsnlp.data.write/to_...`) can be used here\ndf = edsnlp.data.to_pandas(\n    docs,\n    converter=\"omop\",\n\n    # Optional parameters\n    span_getter={\"ents\": True},\n    doc_attributes=[\"note_datetime\"],\n    span_attributes=[\"negation\", \"family\"],\n)\n# or docs.to_pandas(...) if it's already a\n# [LazyCollection][edsnlp.core.lazy_collection.LazyCollection]\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>doc_attributes</code> <p>Mapping from Doc extensions to JSON attributes (can be a list too). By default, no doc attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.EntsDoc2DictConverter","title":"<code>EntsDoc2DictConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.EntsDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>doc_attributes</code> <p>Mapping from Doc extensions to JSON attributes (can be a list too). By default, no doc attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/json/","title":"<code>edsnlp.data.json</code>","text":""},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.read_json","title":"<code>read_json</code>","text":"<p>The JsonReader (or <code>edsnlp.data.read_json</code>) reads a directory of JSON files and yields documents. At the moment, only entities and attributes are loaded.</p>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.read_json--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_json(\"path/to/json/dir\", converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_json</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.read_json(\"path/to/json/dir\", converter=\"omop\")\n</code></pre>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.read_json--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the JSON files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>keep_ipynb_checkpoints</code> <p>Whether to keep the files have \".ipynb_checkpoints\" in their path.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>read_in_worker</code> <p>Whether to read the files in the worker or in the main process.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the JSON objects to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.write_json","title":"<code>write_json</code>","text":"<p><code>edsnlp.data.write_json</code> writes a list of documents using the JSON format in a directory. If <code>lines</code> is false, each document will be stored in its own JSON file, named after the FILENAME field returned by the converter (commonly the <code>note_id</code> attribute of the documents), and subdirectories will be created if the name contains <code>/</code> characters.</p>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.write_json--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_json([doc], \"path/to/json/file\", converter=\"omop\", lines=True)\n# or to write a directory of JSON files, ensure that each doc has a doc._.note_id\n# attribute, since this will be used as a filename:\nedsnlp.data.write_json([doc], \"path/to/json/dir\", converter=\"omop\", lines=False)\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_json</code> will raise an error if the directory already exists and contains files with <code>.a*</code> or <code>.txt</code> suffixes. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.write_json--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>path</code> <p>Path to either - a file if <code>lines</code> is true : this will write the documents as a JSONL file - a directory if <code>lines</code> is false: this will write one JSON file per document   using the FILENAME field returned by the converter (commonly the <code>note_id</code>   attribute of the documents) as the filename.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>lines</code> <p>Whether to write the documents as a JSONL file or as a directory of JSON files. By default, this is inferred from the path: if the path is a file, lines is assumed to be true, otherwise it is assumed to be false.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before writing them. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/pandas/","title":"<code>edsnlp.data.pandas</code>","text":""},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.from_pandas","title":"<code>from_pandas</code>","text":"<p>The PandasReader (or <code>edsnlp.data.from_pandas</code>) handles reading from a table and yields documents. At the moment, only entities and attributes are loaded. Relations and events are not supported.</p>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.from_pandas--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_pandas(df, nlp=nlp, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_pandas</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_pandas(df, converter=\"omop\"))\n</code></pre>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.from_pandas--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>Pandas object</p> <p> </p> <code>converter</code> <p>Converter to use to convert the rows of the DataFrame (represented as dicts) to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.to_pandas","title":"<code>to_pandas</code>","text":"<p><code>edsnlp.data.to_pandas</code> writes a list of documents as a pandas table.</p>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.to_pandas--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_pandas([doc], converter=\"omop\")\n</code></pre>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.to_pandas--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>dtypes</code> <p>Dictionary of column names to dtypes. This is passed to <code>pd.DataFrame.astype</code>.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/parquet/","title":"<code>edsnlp.data.parquet</code>","text":""},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.read_parquet","title":"<code>read_parquet</code>","text":"<p>The ParquetReader (or <code>edsnlp.data.read_parquet</code>) reads a directory of parquet files (or a single file) and yields documents.</p>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.read_parquet--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_parquet(\"path/to/parquet\", converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_parquet</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.read_parquet(\"path/to/parquet\", converter=\"omop\"))\n</code></pre>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.read_parquet--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the parquet files (will recursively look for files in subdirectories). Supports any filesystem supported by pyarrow.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>read_in_worker</code> <p>Whether to read the files in the worker or in the main process.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the parquet rows of the data source to Doc objects These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.write_parquet","title":"<code>write_parquet</code>","text":"<p><code>edsnlp.data.write_parquet</code> writes a list of documents as a parquet dataset.</p>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.write_parquet--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_parquet([doc], \"path/to/parquet\")\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_parquet</code> will raise an error if the directory already exists and contains parquet files. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.write_parquet--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>path</code> <p>Path to the directory containing the parquet files (will recursively look for files in subdirectories). Supports any filesystem supported by pyarrow.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>num_rows_per_file</code> <p>The maximum number of documents to write in each parquet file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1024</code> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>write_in_worker</code> <p>Whether to write the files in the workers or in the main process.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>accumulate</code> <p>Whether to accumulate the results sent to the writer by workers until the batch is full or the writer is finalized. If False, each file will not be larger than the size of the batches it receives. This option requires that the writer is finalized before the end of the processing, which may not be compatible with some backends, such as <code>spark</code>.</p> <p>If <code>write_in_worker</code> is True, documents will be accumulated in each worker but not across workers, therefore leading to a larger number of files.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before writing them as Parquet rows. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/polars/","title":"<code>edsnlp.data.polars</code>","text":""},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.from_polars","title":"<code>from_polars</code>","text":"<p>The PolarsReader (or <code>edsnlp.data.from_polars</code>) handles reading from a table and yields documents. At the moment, only entities and attributes are loaded. Relations and events are not supported.</p>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.from_polars--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_polars(df, nlp=nlp, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_polars</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_polars(df, converter=\"omop\"))\n</code></pre>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.from_polars--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>Polars object</p> <p> TYPE: <code>Union[DataFrame, LazyFrame]</code> </p> <code>converter</code> <p>Converter to use to convert the rows of the DataFrame (represented as dicts) to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.to_polars","title":"<code>to_polars</code>","text":"<p><code>edsnlp.data.to_polars</code> writes a list of documents as a polars dataframe.</p>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.to_polars--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_polars([doc], converter=\"omop\")\n</code></pre>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.to_polars--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>dtypes</code> <p>Dictionary of column names to dtypes. This is passed to the schema parameter of <code>pl.from_dicts</code>.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/spark/","title":"<code>edsnlp.data.spark</code>","text":""},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.from_spark","title":"<code>from_spark</code>","text":"<p>The SparkReader (or <code>edsnlp.data.from_spark</code>) reads a pyspark (or koalas) DataFrame and yields documents. At the moment, only entities and span attributes are loaded.</p>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.from_spark--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_spark(note_df, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_spark</code> returns a LazyCollection To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_spark(note_df, converter=\"omop\"))\n</code></pre>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.from_spark--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The DataFrame to read.</p> <p> </p> <code>converter</code> <p>Converter to use to convert the rows of the DataFrame to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.to_spark","title":"<code>to_spark</code>","text":"<p><code>edsnlp.data.to_spark</code> converts a list of documents into a Spark DataFrame, usually one row per document, unless the converter returns a list in which case each entry of the resulting list will be stored in its own row.</p>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.to_spark--example","title":"Example","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n\nnote_df = sql('''\n    select note_id, note_text from note\n    where note_text is not null\n    limit 500\n''')\n\ndocs = edsnlp.data.from_spark(note_df, converter=\"omop\")\n\ndocs = nlp.pipe(docs)\n\nres = edsnlp.data.to_spark(docs, converter=\"omop\")\n\nres.show()\n</code></pre> <p>Mac OS X</p> <p>If you are using Mac OS X, you may need to set the following environment variable (see this thread) to run pyspark:</p> <pre><code>import os\nos.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n</code></pre>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.to_spark--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>dtypes</code> <p>The schema to use for the DataFrame.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>show_dtypes</code> <p>Whether to print the inferred schema (only if <code>dtypes</code> is None).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/standoff/","title":"<code>edsnlp.data.standoff</code>","text":""},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.parse_standoff_file","title":"<code>parse_standoff_file</code>","text":"<p>Load a brat file</p> <p>Adapted from https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py</p>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.parse_standoff_file--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path or glob path of the brat text file (.txt, not .ann)</p> <p> </p> <code>merge_spaced_fragments</code> <p>Merge fragments of an entity that was split by brat because it overlapped an end of line</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>fs</code> <p>Filesystem to use</p> <p> TYPE: <code>FileSystem</code> DEFAULT: <code>LOCAL_FS</code> </p> RETURNS DESCRIPTION <code>Iterator[Dict]</code>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.read_standoff","title":"<code>read_standoff</code>","text":"<p>The BratReader (or <code>edsnlp.data.read_standoff</code>) reads a directory of BRAT files and yields documents. At the moment, only entities and attributes are loaded. Relations  and events are not supported.</p>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.read_standoff--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_standoff(\"path/to/brat/directory\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_standoff</code> returns a LazyCollection. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list :</p> <pre><code>docs = list(edsnlp.data.read_standoff(\"path/to/brat/directory\"))\n</code></pre> <p>True/False attributes</p> <p>Boolean values are not supported by the BRAT editor, and are stored as empty (key: empty value) if true, and not stored otherwise. This means that False values will not be assigned to attributes by default, which can be problematic when deciding if an entity is negated or not : is the entity not negated, or has the negation attribute not been annotated ?</p> <p>To avoid this issue, you can use the <code>bool_attributes</code> argument to specify which attributes should be considered as boolean when reading a BRAT dataset. These attributes will be assigned a value of <code>True</code> if they are present, and <code>False</code> otherwise.</p> <pre><code>doc_iterator = edsnlp.data.read_standoff(\n    \"path/to/brat/directory\",\n    # Mapping from 'BRAT attribute name' to 'Doc attribute name'\n    span_attributes={\"Negation\": \"negated\"},\n    bool_attributes=[\"negated\"],  # Missing values will be set to False\n)\n</code></pre>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.read_standoff--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the BRAT files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   LazyCollection.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> </p> <code>notes_as_span_attribute</code> <p>If set, the AnnotatorNote annotations will be concatenated and stored in a span attribute with this name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>split_fragments</code> <p>Whether to split the fragments into separate spans or not. If set to False, the fragments will be concatenated into a single span.</p> <p> TYPE: <code>bool</code> </p> <code>keep_ipynb_checkpoints</code> <p>Whether to keep the files that are in the <code>.ipynb_checkpoints</code> directory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>keep_txt_only_docs</code> <p>Whether to keep the <code>.txt</code> files that do not have corresponding <code>.ann</code> files.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>'standoff'</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LazyCollection</code>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.write_standoff","title":"<code>write_standoff</code>","text":"<p><code>edsnlp.data.write_standoff</code> writes a list of documents using the BRAT/Standoff format in a directory. The BRAT files will be named after the <code>note_id</code> attribute of the documents, and subdirectories will be created if the name contains <code>/</code> characters.</p>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.write_standoff--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_standoff([doc], \"path/to/brat/directory\")\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_standoff</code> will raise an error if the directory already exists and contains files with <code>.a*</code> or <code>.txt</code> suffixes. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.write_standoff--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a LazyCollection).</p> <p> TYPE: <code>Union[Any, LazyCollection]</code> </p> <code>path</code> <p>Path to the directory containing the BRAT files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>span_getter</code> <p>The span getter to use when listing the spans that will be exported as BRAT entities. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extension. By default, no attribute will be exported.</p> <p> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects. Defaults to the \"standoff\" format converter.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>'standoff'</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/extensions/","title":"<code>edsnlp.extensions</code>","text":""},{"location":"reference/edsnlp/language/","title":"<code>edsnlp.language</code>","text":""},{"location":"reference/edsnlp/language/#edsnlp.language.EDSDefaults","title":"<code>EDSDefaults</code>","text":"<p>           Bases: <code>FrenchDefaults</code></p> <p>Defaults for the EDSLanguage class Mostly identical to the FrenchDefaults, but without tokenization info</p>"},{"location":"reference/edsnlp/language/#edsnlp.language.EDSLanguage","title":"<code>EDSLanguage</code>","text":"<p>           Bases: <code>French</code></p> <p>French clinical language. It is shipped with the <code>EDSTokenizer</code> tokenizer that better handles tokenization for French clinical documents</p>"},{"location":"reference/edsnlp/language/#edsnlp.language.EDSTokenizer","title":"<code>EDSTokenizer</code>","text":"<p>           Bases: <code>Tokenizer</code></p> <pre><code>    Tokenizer class for French clinical documents.\n    It better handles tokenization around:\n    - numbers: \"ACR5\" -&gt; [\"ACR\", \"5\"] instead of [\"ACR5\"]\n    - newlines: \"\n</code></pre> <p>\" -&gt; [\" \", \" \", \" \"] instead of [\"</p> <p>\"]         and should be around 5-6 times faster than its standard French counterpart.         Parameters         ----------         vocab: Vocab             The spacy vocabulary</p>"},{"location":"reference/edsnlp/language/#edsnlp.language.EDSTokenizer.__call__","title":"<code>__call__</code>","text":"<p>Tokenizes the text using the EDSTokenizer</p>"},{"location":"reference/edsnlp/language/#edsnlp.language.EDSTokenizer.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>text</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/language/#edsnlp.language.create_eds_tokenizer","title":"<code>create_eds_tokenizer</code>","text":"<p>Creates a factory that returns new EDSTokenizer instances</p> RETURNS DESCRIPTION <code>EDSTokenizer</code>"},{"location":"reference/edsnlp/matchers/","title":"<code>edsnlp.matchers</code>","text":""},{"location":"reference/edsnlp/matchers/regex/","title":"<code>edsnlp.matchers.regex</code>","text":""},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher","title":"<code>RegexMatcher</code>","text":"<p>           Bases: <code>object</code></p> <p>Simple RegExp matcher.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>alignment_mode</code> <p>How spans should be aligned with tokens. Possible values are <code>strict</code> (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to <code>expand</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'expand'</code> </p> <code>attr</code> <p>Default attribute to match on, by default \"TEXT\". Can be overiden in the <code>add</code> method.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>flags</code> <p>Additional flags provided to the <code>re</code> module. Can be overiden in the <code>add</code> method.</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>ignore_excluded</code> <p>Whether to skip exclusions</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>span_from_group</code> <p>If set to <code>False</code>, will create spans basede on the regex's full match. If set to <code>True</code>, will use the first matching capturing group as a span (and fall back to using the full match if no capturing group is matching)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.build_patterns","title":"<code>build_patterns</code>","text":"<p>Build patterns and adds them for matching. Helper function for pipelines using this matcher.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.build_patterns--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>regex</code> <p>Dictionary of label/terms, or label/dictionary of terms/attribute.</p> <p> TYPE: <code>Patterns</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.add","title":"<code>add</code>","text":"<p>Add a pattern to the registry.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.add--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>key</code> <p>Key of the new/updated pattern.</p> <p> TYPE: <code>str</code> </p> <code>patterns</code> <p>List of patterns to add.</p> <p> TYPE: <code>List[str]</code> </p> <code>attr</code> <p>Attribute to use for matching. By default, uses the <code>default_attr</code> attribute</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <p>alignment_mode : Optional[str]     Overwrite alignment mode.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.remove","title":"<code>remove</code>","text":"<p>Remove a pattern for the registry.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.remove--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>key</code> <p>key of the pattern to remove.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the key is not present in the registered patterns.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match","title":"<code>match</code>","text":"<p>Iterates on the matches.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object to match on.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE:: <code>Tuple[Span, Match]</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match_with_groupdict_as_spans","title":"<code>match_with_groupdict_as_spans</code>","text":"<p>Iterates on the matches.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match_with_groupdict_as_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object to match on.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE:: <code>Tuple[Span, Dict[str, Span]]</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__call__","title":"<code>__call__</code>","text":"<p>Performs matching. Yields matches.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>as_spans</code> <p>Returns matches as spans.</p> <p> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE:: <code>Union[Span, Tuple[Span, Dict[str, Any]]]</code> </p> <code>groupdict</code> <p>Additional information coming from the named patterns in the regular expression.</p> <p> TYPE:: <code>Union[Span, Tuple[Span, Dict[str, Any]]]</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.spans_generator","title":"<code>spans_generator</code>","text":"<p>Iterates over every group, and then yields the full match</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.spans_generator--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>match</code> <p>A match object</p> <p> TYPE: <code>Match</code> </p> YIELDS DESCRIPTION <code>Tuple[int, int]</code> <p>A tuple containing the start and end of the group or match</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.span_from_match","title":"<code>span_from_match</code>","text":"<p>Return the span (as a (start, end) tuple) of the first matching group. If <code>span_from_group=True</code>, returns the full match instead.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.span_from_match--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>match</code> <p>The Match object</p> <p> TYPE: <code>Match</code> </p> <code>span_from_group</code> <p>Whether to work on groups or on the full match</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>Tuple[int, int]</code> <p>A tuple containing the start and end of the group or match</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.create_span","title":"<code>create_span</code>","text":"<p>spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.create_span--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p><code>Doc</code> or <code>Span</code>.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>start_char</code> <p>Character index within the Doc-like object.</p> <p> TYPE: <code>int</code> </p> <code>end_char</code> <p>Character index of the end, within the Doc-like object.</p> <p> TYPE: <code>int</code> </p> <code>key</code> <p>The key used to match.</p> <p> TYPE: <code>str</code> </p> <code>alignment_mode</code> <p>The alignment mode.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens.</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>span</code> <p>A span matched on the Doc-like object.</p> <p> TYPE: <code>Optional[Span]</code> </p>"},{"location":"reference/edsnlp/matchers/simstring/","title":"<code>edsnlp.matchers.simstring</code>","text":""},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter","title":"<code>SimstringWriter</code>","text":"<p>A context class to write a simstring database</p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to database</p> <p> TYPE: <code>Union[str, Path]</code> </p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher","title":"<code>SimstringMatcher</code>","text":"<p>PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS</p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>vocab</code> <p>spaCy vocabulary to match on.</p> <p> TYPE: <code>Vocab</code> </p> <code>path</code> <p>Path where we will store the precomputed patterns</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> <code>measure</code> <p>Name of the similarity measure. One of [jaccard, dice, overlap, cosine]</p> <p> TYPE: <code>SimilarityMeasure</code> DEFAULT: <code>dice</code> </p> <code>windows</code> <p>Maximum number of words in a candidate span</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>threshold</code> <p>Minimum similarity value to match a concept's synonym</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>ignore_excluded</code> <p>Whether to exclude tokens that have an EXCLUDED tag, by default False</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to exclude tokens that have a \"SPACE\" tag, by default False</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Default attribute to match on, by default \"TEXT\". Can be overridden in the <code>add</code> method. To match on a custom attribute, prepend the attribute name with <code>_</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.build_patterns","title":"<code>build_patterns</code>","text":"<p>Build patterns and adds them for matching.</p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.build_patterns--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The instance of the spaCy language class.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>terms</code> <p>Dictionary of label/terms, or label/dictionary of terms/attribute.</p> <p> TYPE: <code>Patterns</code> </p> <code>progress</code> <p>Whether to track progress when preprocessing terms</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.get_text_and_offsets","title":"<code>get_text_and_offsets</code>  <code>cached</code>","text":"<p>Align different representations of a <code>Doc</code> or <code>Span</code> object.</p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.get_text_and_offsets--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy <code>Doc</code> or <code>Span</code> object</p> <p> TYPE: <code>Doc</code> </p> <code>attr</code> <p>Attribute to use, by default <code>\"TEXT\"</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to remove excluded tokens, by default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_space_tokens</code> <p>Whether to remove space tokens, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Tuple[str, List[Tuple[int, int, int, int]]]</code> <p>The new clean text and offset tuples for each word giving the begin char indice of the word in the new text, the end char indice of its preceding word and the begin / end indices of the word in the original document</p>"},{"location":"reference/edsnlp/matchers/utils/","title":"<code>edsnlp.matchers.utils</code>","text":""},{"location":"reference/edsnlp/matchers/utils/offset/","title":"<code>edsnlp.matchers.utils.offset</code>","text":""},{"location":"reference/edsnlp/matchers/utils/text/","title":"<code>edsnlp.matchers.utils.text</code>","text":""},{"location":"reference/edsnlp/optimization/","title":"<code>edsnlp.optimization</code>","text":""},{"location":"reference/edsnlp/package/","title":"<code>edsnlp.package</code>","text":""},{"location":"reference/edsnlp/patch_spacy/","title":"<code>edsnlp.patch_spacy</code>","text":""},{"location":"reference/edsnlp/patch_spacy/#edsnlp.patch_spacy.factory","title":"<code>factory</code>  <code>classmethod</code>","text":"<p>Patched from spaCy to allow back dots in factory names (https://github.com/aphp/edsnlp/pull/152)</p> <p>Register a new pipeline component factory. Can be used as a decorator on a function or classmethod, or called as a function with the factory provided as the func keyword argument. To create a component and add it to the pipeline, you can use nlp.add_pipe(name).</p> <p>name (str): The name of the component factory. default_config (Dict[str, Any]): Default configuration, describing the     default values of the factory arguments. assigns (Iterable[str]): Doc/Token attributes assigned by this component,     e.g. \"token.ent_id\". Used for pipeline analysis. requires (Iterable[str]): Doc/Token attributes required by this component,     e.g. \"token.ent_id\". Used for pipeline analysis. retokenizes (bool): Whether the component changes the tokenization.     Used for pipeline analysis. default_score_weights (Dict[str, Optional[float]]): The scores to report during     training, and their default weight towards the final score used to     select the best model. Weights should sum to 1.0 per component and     will be combined and normalized for the whole pipeline. If None,     the score won't be shown in the logs or be weighted. func (Optional[Callable]): Factory function if not used as a decorator.</p> <p>DOCS: https://spacy.io/api/language#factory</p>"},{"location":"reference/edsnlp/patch_spacy/#edsnlp.patch_spacy.__init__","title":"<code>__init__</code>","text":"<p>EDS-NLP: Patched from spaCy do enable lazy-loading components</p> <p>Initialise a Language object.</p> <p>vocab (Vocab): A <code>Vocab</code> object. If <code>True</code>, a vocab is created. meta (dict): Custom meta data for the Language class. Is written to by     models to add model meta data. max_length (int): Maximum number of characters in a single text. The     current models may run out memory on extremely long texts, due to     large internal allocations. You should segment these texts into     meaningful units, e.g. paragraphs, subsections etc, before passing     them to spaCy. Default maximum length is 1,000,000 charas (1mb). As     a rule of thumb, if all pipeline components are enabled, spaCy's     default models currently requires roughly 1GB of temporary memory per     100,000 characters in one text. create_tokenizer (Callable): Function that takes the nlp object and     returns a tokenizer. batch_size (int): Default batch size for pipe and evaluate.</p> <p>DOCS: https://spacy.io/api/language#init</p>"},{"location":"reference/edsnlp/pipes/","title":"<code>edsnlp.pipes</code>","text":""},{"location":"reference/edsnlp/pipes/base/","title":"<code>edsnlp.pipes.base</code>","text":""},{"location":"reference/edsnlp/pipes/base/#edsnlp.pipes.base.BaseComponent","title":"<code>BaseComponent</code>","text":"<p>           Bases: <code>ABC</code></p> <p>The <code>BaseComponent</code> adds a <code>set_extensions</code> method, called at the creation of the object.</p> <p>It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset.</p>"},{"location":"reference/edsnlp/pipes/base/#edsnlp.pipes.base.BaseComponent.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set <code>Doc</code>, <code>Span</code> and <code>Token</code> extensions.</p>"},{"location":"reference/edsnlp/pipes/base/#edsnlp.pipes.base.BaseComponent.get_spans","title":"<code>get_spans</code>","text":"<p>Returns sorted spans of interest according to the possible value of <code>on_ents_only</code>. Includes <code>doc.ents</code> by default, and adds eventual SpanGroups.</p>"},{"location":"reference/edsnlp/pipes/core/","title":"<code>edsnlp.pipes.core</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/","title":"<code>edsnlp.pipes.core.contextual_matcher</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/","title":"<code>edsnlp.pipes.core.contextual_matcher.contextual_matcher</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher","title":"<code>ContextualMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>Allows additional matching in the surrounding context of the main match group, for qualification/filtering.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'contextual_matcher'</code> </p> <code>patterns</code> <p>The configuration dictionary</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>expand</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_name</code> <p>Deprecated, use <code>label</code> instead. The label to assign to the matched entities</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>The label to assign to the matched entities</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.filter_one","title":"<code>filter_one</code>","text":"<p>Filter extracted entity based on the \"exclusion filter\" mentioned in the configuration</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.filter_one--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to filter</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>Optional[Span]</code> <p>None if the span was filtered, the span else</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.assign_one","title":"<code>assign_one</code>","text":"<p>Get additional information in the context of each entity. This function will populate two custom attributes:</p> <ul> <li><code>ent._.source</code></li> <li><code>ent._.assigned</code>, a dictionary with all retrieved information</li> </ul>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.assign_one--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to enrich</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>Span</code> <p>Span with additional information</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.process","title":"<code>process</code>","text":"<p>Process the document, looking for named entities.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected spans.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds spans to document.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/factory/","title":"<code>edsnlp.pipes.core.contextual_matcher.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/factory/#edsnlp.pipes.core.contextual_matcher.factory.create_component","title":"<code>create_component = registry.factory.register('eds.contextual-matcher', deprecated=['contextual-matcher'])(ContextualMatcher)</code>  <code>module-attribute</code>","text":"<p>Allows additional matching in the surrounding context of the main match group, for qualification/filtering.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/factory/#edsnlp.pipes.core.contextual_matcher.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'contextual_matcher'</code> </p> <code>patterns</code> <p>The configuration dictionary</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>expand</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_name</code> <p>Deprecated, use <code>label</code> instead. The label to assign to the matched entities</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>The label to assign to the matched entities</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/","title":"<code>edsnlp.pipes.core.contextual_matcher.models</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.AssignDict","title":"<code>AssignDict</code>","text":"<p>           Bases: <code>dict</code></p> <p>Custom dictionary that overrides the setitem method depending on the reduce_mode</p>"},{"location":"reference/edsnlp/pipes/core/endlines/","title":"<code>edsnlp.pipes.core.endlines</code>","text":""},{"location":"reference/edsnlp/pipes/core/endlines/endlines/","title":"<code>edsnlp.pipes.core.endlines.endlines</code>","text":"<ol><li><p><p>Zweigenbaum P., Grouin C. and Lavergne T., 2016. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (End-of-line classification with no supervision). https://aclanthology.org/2016.jeptalnrecital-poster.7</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher","title":"<code>EndLinesMatcher</code>","text":"<p>           Bases: <code>GenericMatcher</code></p> <p>The <code>eds.endlines</code> component classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document.</p> <p>Behind the scenes, it uses a <code>endlinesmodel</code> instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al., 2016.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--training","title":"Training","text":"<pre><code>import edsnlp\nfrom edsnlp.pipes.core.endlines.model import EndLinesModel\n\nnlp = edsnlp.blank(\"eds\")\n\ntexts = [\n\"\"\"\nLe patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010\nFumeur, il est arret\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diabete\n\"\"\",\n\"\"\"\nJ'aime le\nfromage...\n\"\"\",\n]\n\ndocs = list(nlp.pipe(texts))\n\n# Train and predict an EndLinesModel\nendlines = EndLinesModel(nlp=nlp)\n\ndf = endlines.fit_and_predict(docs)\ndf.head()\n\nPATH = \"/tmp/path_to_save\"\nendlines.save(PATH)\n</code></pre>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = edsnlp.blank(\"eds\")\n\nPATH = \"/tmp/path_to_save\"\nnlp.add_pipe(eds.endlines(model_path=PATH))\n\ndocs = list(nlp.pipe(texts))\n\ndoc_exemple = docs[1]\n\ndoc_exemple.ents = tuple(\n    Span(doc_exemple, token.i, token.i + 1, \"excluded\")\n    for token in doc_exemple\n    if token.tag_ == \"EXCLUDED\"\n)\n\ndisplacy.render(doc_exemple, style=\"ent\", options={\"colors\": {\"space\": \"red\"}})\n</code></pre>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.endlines</code> pipeline declares one extension, on both <code>Span</code> and <code>Token</code> objects. The <code>end_line</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the new line is an end line character. Otherwise, it is set to <code>False</code> if the new line is classified as a space.</p> <p>The pipeline also sets the <code>excluded</code> custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> </p> <code>model_path</code> <p>Path to trained model. If None, it will use a default model</p> <p> TYPE: <code>Optional[Union[str, EndLinesModel]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.endlines</code> pipeline was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al., 2016.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher.__call__","title":"<code>__call__</code>","text":"<p>Predict for each new line if it's an end of line or a space.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>spaCy Doc object, with each new line annotated</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/","title":"<code>edsnlp.pipes.core.endlines.factory</code>","text":"<ol><li><p><p>Zweigenbaum P., Grouin C. and Lavergne T., 2016. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (End-of-line classification with no supervision). https://aclanthology.org/2016.jeptalnrecital-poster.7</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component","title":"<code>create_component = registry.factory.register('eds.endlines', assigns=['doc.ents', 'doc.spans'], deprecated=['spaces'])(EndLinesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.endlines</code> component classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document.</p> <p>Behind the scenes, it uses a <code>endlinesmodel</code> instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al., 2016.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--training","title":"Training","text":"<pre><code>import edsnlp\nfrom edsnlp.pipes.core.endlines.model import EndLinesModel\n\nnlp = edsnlp.blank(\"eds\")\n\ntexts = [\n\"\"\"\nLe patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010\nFumeur, il est arret\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diabete\n\"\"\",\n\"\"\"\nJ'aime le\nfromage...\n\"\"\",\n]\n\ndocs = list(nlp.pipe(texts))\n\n# Train and predict an EndLinesModel\nendlines = EndLinesModel(nlp=nlp)\n\ndf = endlines.fit_and_predict(docs)\ndf.head()\n\nPATH = \"/tmp/path_to_save\"\nendlines.save(PATH)\n</code></pre>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = edsnlp.blank(\"eds\")\n\nPATH = \"/tmp/path_to_save\"\nnlp.add_pipe(eds.endlines(model_path=PATH))\n\ndocs = list(nlp.pipe(texts))\n\ndoc_exemple = docs[1]\n\ndoc_exemple.ents = tuple(\n    Span(doc_exemple, token.i, token.i + 1, \"excluded\")\n    for token in doc_exemple\n    if token.tag_ == \"EXCLUDED\"\n)\n\ndisplacy.render(doc_exemple, style=\"ent\", options={\"colors\": {\"space\": \"red\"}})\n</code></pre>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.endlines</code> pipeline declares one extension, on both <code>Span</code> and <code>Token</code> objects. The <code>end_line</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the new line is an end line character. Otherwise, it is set to <code>False</code> if the new line is classified as a space.</p> <p>The pipeline also sets the <code>excluded</code> custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> </p> <code>model_path</code> <p>Path to trained model. If None, it will use a default model</p> <p> TYPE: <code>Optional[Union[str, EndLinesModel]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.endlines</code> pipeline was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al., 2016.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/functional/","title":"<code>edsnlp.pipes.core.endlines.functional</code>","text":""},{"location":"reference/edsnlp/pipes/core/endlines/functional/#edsnlp.pipes.core.endlines.functional.build_path","title":"<code>build_path</code>","text":"<p>Function to build an absolut path.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/functional/#edsnlp.pipes.core.endlines.functional.build_path--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>file</code> <code>relative_path</code> <p>relative path from the main file to the desired output</p> <p> </p> RETURNS DESCRIPTION <code>path</code> <p> TYPE: <code>absolute path</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/","title":"<code>edsnlp.pipes.core.endlines.model</code>","text":""},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel","title":"<code>EndLinesModel</code>","text":"<p>Model to classify if an end line is a real one or it should be a space.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>PipelineProtocol</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.fit_and_predict","title":"<code>fit_and_predict</code>","text":"<p>Fit the model and predict for the training data</p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.fit_and_predict--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>corpus</code> <p>An iterable of Documents</p> <p> TYPE: <code>Iterable[Doc]</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>one line by end_line prediction</p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.predict","title":"<code>predict</code>","text":"<p>Use the model for inference</p> <p>The df should have the following columns: <code>[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]</code></p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.predict--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>df</code> <p>The df should have the following columns: <code>[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]</code></p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The result is added to the column <code>PREDICTED_END_LINE</code></p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.save","title":"<code>save</code>","text":"<p>Save a pickle of the model. It could be read by the pipeline later.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.save--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>path to file .pkl, by default <code>base_model.pkl</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'base_model.pkl'</code> </p>"},{"location":"reference/edsnlp/pipes/core/matcher/","title":"<code>edsnlp.pipes.core.matcher</code>","text":""},{"location":"reference/edsnlp/pipes/core/matcher/factory/","title":"<code>edsnlp.pipes.core.matcher.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/matcher/factory/#edsnlp.pipes.core.matcher.factory.create_component","title":"<code>create_component = registry.factory.register('eds.matcher', assigns=['doc.ents', 'doc.spans'], deprecated=['matcher'])(GenericMatcher)</code>  <code>module-attribute</code>","text":"<p>EDS-NLP simplifies the matching process by exposing a <code>eds.matcher</code> component that can match on terms or regular expressions.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/factory/#edsnlp.pipes.core.matcher.factory.create_component--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    patient=\"patient\",  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n        term_matcher=\"exact\",\n        term_matcher_config={},\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become  the label of the extracted entities. Dictionary values are either a single  expression or a list of expressions that match the concept.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/factory/#edsnlp.pipes.core.matcher.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'matcher'</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>span_setter</code> <p>How to set the spans in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"reference/edsnlp/pipes/core/matcher/factory/#edsnlp.pipes.core.matcher.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/","title":"<code>edsnlp.pipes.core.matcher.matcher</code>","text":""},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher","title":"<code>GenericMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>EDS-NLP simplifies the matching process by exposing a <code>eds.matcher</code> component that can match on terms or regular expressions.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    patient=\"patient\",  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n        term_matcher=\"exact\",\n        term_matcher_config={},\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become  the label of the extracted entities. Dictionary values are either a single  expression or a list of expressions that match the concept.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'matcher'</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>span_setter</code> <p>How to set the spans in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher.process","title":"<code>process</code>","text":"<p>Find matching spans in doc.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>List of Spans returned by the matchers.</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds spans to document.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/","title":"<code>edsnlp.pipes.core.normalizer</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/accents/","title":"<code>edsnlp.pipes.core.normalizer.accents</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/","title":"<code>edsnlp.pipes.core.normalizer.accents.accents</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/#edsnlp.pipes.core.normalizer.accents.accents.AccentsConverter","title":"<code>AccentsConverter</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>Normalises accents, using a same-length strategy.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/#edsnlp.pipes.core.normalizer.accents.accents.AccentsConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>accents</code> <p>List of accentuated characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> DEFAULT: <code>[('\u00e7', 'c'), ('\u00e0\u00e1\u00e2\u00e4', 'a'), ('\u00e8\u00e9\u00ea\u00eb', 'e'), ('\u00ec\u00ed...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/#edsnlp.pipes.core.normalizer.accents.accents.AccentsConverter.__call__","title":"<code>__call__</code>","text":"<p>Remove accents from spacy <code>NORM</code> attribute.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/#edsnlp.pipes.core.normalizer.accents.accents.AccentsConverter.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The spaCy <code>Doc</code> object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>The document, with accents removed in <code>Token.norm_</code>.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/factory/","title":"<code>edsnlp.pipes.core.normalizer.accents.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/accents/factory/#edsnlp.pipes.core.normalizer.accents.factory.create_component","title":"<code>create_component = registry.factory.register('eds.accents', assigns=['token.norm'], deprecated=['accents'])(AccentsConverter)</code>  <code>module-attribute</code>","text":"<p>Normalises accents, using a same-length strategy.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/factory/#edsnlp.pipes.core.normalizer.accents.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>accents</code> <p>List of accentuated characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> DEFAULT: <code>[('\u00e7', 'c'), ('\u00e0\u00e1\u00e2\u00e4', 'a'), ('\u00e8\u00e9\u00ea\u00eb', 'e'), ('\u00ec\u00ed...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/patterns/","title":"<code>edsnlp.pipes.core.normalizer.accents.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/factory/","title":"<code>edsnlp.pipes.core.normalizer.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/factory/#edsnlp.pipes.core.normalizer.factory.create_component","title":"<code>create_component</code>","text":"<p>Normalisation pipeline. Modifies the <code>NORM</code> attribute, acting on five dimensions :</p> <ul> <li><code>lowercase</code>: using the default <code>NORM</code></li> <li><code>accents</code>: deterministic and fixed-length normalisation of accents.</li> <li><code>quotes</code>: deterministic and fixed-length normalisation of quotation marks.</li> <li><code>spaces</code>: \"removal\" of spaces tokens (via the tag_ attribute).</li> <li><code>pollution</code>: \"removal\" of pollutions (via the tag_ attribute).</li> </ul>"},{"location":"reference/edsnlp/pipes/core/normalizer/factory/#edsnlp.pipes.core.normalizer.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'normalizer'</code> </p> <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>accents</code> <p><code>Accents</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>quotes</code> <p><code>Quotes</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>spaces</code> <p><code>Spaces</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> configuration object.</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/","title":"<code>edsnlp.pipes.core.normalizer.normalizer</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/#edsnlp.pipes.core.normalizer.normalizer.Normalizer","title":"<code>Normalizer</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>Normalisation pipeline. Modifies the <code>NORM</code> attribute, acting on five dimensions :</p> <ul> <li><code>lowercase</code>: using the default <code>NORM</code></li> <li><code>accents</code>: deterministic and fixed-length normalisation of accents.</li> <li><code>quotes</code>: deterministic and fixed-length normalisation of quotation marks.</li> <li><code>spaces</code>: \"removal\" of spaces tokens (via the tag_ attribute).</li> <li><code>pollution</code>: \"removal\" of pollutions (via the tag_ attribute).</li> </ul>"},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/#edsnlp.pipes.core.normalizer.normalizer.Normalizer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'normalizer'</code> </p> <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>accents</code> <p>Optional <code>Accents</code> object.</p> <p> TYPE: <code>Optional[Accents]</code> DEFAULT: <code>None</code> </p> <code>quotes</code> <p>Optional <code>Quotes</code> object.</p> <p> TYPE: <code>Optional[Quotes]</code> DEFAULT: <code>None</code> </p> <code>spaces</code> <p>Optional <code>Spaces</code> object.</p> <p> TYPE: <code>Optional[Spaces]</code> DEFAULT: <code>None</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> object.</p> <p> TYPE: <code>Optional[Pollution]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/#edsnlp.pipes.core.normalizer.normalizer.Normalizer.__call__","title":"<code>__call__</code>","text":"<p>Apply the normalisation pipeline, one component at a time.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/#edsnlp.pipes.core.normalizer.normalizer.Normalizer.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy <code>Doc</code> object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>Doc object with <code>NORM</code> attribute modified</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/","title":"<code>edsnlp.pipes.core.normalizer.pollution</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/factory/","title":"<code>edsnlp.pipes.core.normalizer.pollution.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/factory/#edsnlp.pipes.core.normalizer.pollution.factory.create_component","title":"<code>create_component = registry.factory.register('eds.pollution', assigns=['doc.spans'], deprecated=['pollution'])(PollutionTagger)</code>  <code>module-attribute</code>","text":"<p>Tags pollution tokens.</p> <p>Populates a number of spaCy extensions :</p> <ul> <li><code>Token._.pollution</code> : indicates whether the token is a pollution</li> <li><code>Doc._.clean</code> : lists non-pollution tokens</li> <li><code>Doc._.clean_</code> : original text with pollutions removed.</li> <li><code>Doc._.char_clean_span</code> : method to create a Span using character   indices extracted using the cleaned text.</li> </ul>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/factory/#edsnlp.pipes.core.normalizer.pollution.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>pollution</code> <p>Dictionary containing regular expressions of pollution.</p> <p> TYPE: <code>Dict[str, Union[str, List[str]]]</code> DEFAULT: <code>{'information': True, 'bars': True, 'biology': ...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/patterns/","title":"<code>edsnlp.pipes.core.normalizer.pollution.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/","title":"<code>edsnlp.pipes.core.normalizer.pollution.pollution</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger","title":"<code>PollutionTagger</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>Tags pollution tokens.</p> <p>Populates a number of spaCy extensions :</p> <ul> <li><code>Token._.pollution</code> : indicates whether the token is a pollution</li> <li><code>Doc._.clean</code> : lists non-pollution tokens</li> <li><code>Doc._.clean_</code> : original text with pollutions removed.</li> <li><code>Doc._.char_clean_span</code> : method to create a Span using character   indices extracted using the cleaned text.</li> </ul>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>pollution</code> <p>Dictionary containing regular expressions of pollution.</p> <p> TYPE: <code>Dict[str, Union[str, List[str]]]</code> DEFAULT: <code>{'information': True, 'bars': True, 'biology': ...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.build_patterns","title":"<code>build_patterns</code>","text":"<p>Builds the patterns for phrase matching.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.process","title":"<code>process</code>","text":"<p>Find pollutions in doc and clean candidate negations to remove pseudo negations</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>pollution</code> <p>list of pollution spans</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.__call__","title":"<code>__call__</code>","text":"<p>Tags pollutions.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for pollutions.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/","title":"<code>edsnlp.pipes.core.normalizer.quotes</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/factory/","title":"<code>edsnlp.pipes.core.normalizer.quotes.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/factory/#edsnlp.pipes.core.normalizer.quotes.factory.create_component","title":"<code>create_component = registry.factory.register('eds.quotes', assigns=['token.norm'], deprecated=['quotes'])(QuotesConverter)</code>  <code>module-attribute</code>","text":"<p>We normalise quotes, following this <code>source &lt;https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html&gt;</code>_.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/factory/#edsnlp.pipes.core.normalizer.quotes.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'spaces'</code> </p> <code>quotes</code> <p>List of quotation characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> DEFAULT: <code>[('\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\u201f', '\"'), ('\uff40\u0384\uff07\u02c8\u02ca\u144a\u02cb\ua78c\u16cc\ud81b\udf52\ud81b\udf51\u2018\u2019\u05d9\u055a\u201b\u055d`\u1fef\u2032...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/patterns/","title":"<code>edsnlp.pipes.core.normalizer.quotes.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/","title":"<code>edsnlp.pipes.core.normalizer.quotes.quotes</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/#edsnlp.pipes.core.normalizer.quotes.quotes.QuotesConverter","title":"<code>QuotesConverter</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>We normalise quotes, following this <code>source &lt;https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html&gt;</code>_.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/#edsnlp.pipes.core.normalizer.quotes.quotes.QuotesConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'spaces'</code> </p> <code>quotes</code> <p>List of quotation characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> DEFAULT: <code>[('\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\u201f', '\"'), ('\uff40\u0384\uff07\u02c8\u02ca\u144a\u02cb\ua78c\u16cc\ud81b\udf52\ud81b\udf51\u2018\u2019\u05d9\u055a\u201b\u055d`\u1fef\u2032...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/#edsnlp.pipes.core.normalizer.quotes.quotes.QuotesConverter.__call__","title":"<code>__call__</code>","text":"<p>Normalises quotes.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/#edsnlp.pipes.core.normalizer.quotes.quotes.QuotesConverter.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to process.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>Same document, with quotes normalised.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/","title":"<code>edsnlp.pipes.core.normalizer.remove_lowercase</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/","title":"<code>edsnlp.pipes.core.normalizer.remove_lowercase.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/#edsnlp.pipes.core.normalizer.remove_lowercase.factory.remove_lowercase","title":"<code>remove_lowercase</code>","text":"<p>Add case on the <code>NORM</code> custom attribute. Should always be applied first.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/#edsnlp.pipes.core.normalizer.remove_lowercase.factory.remove_lowercase--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The spaCy <code>Doc</code> object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>The document, with case put back in <code>NORM</code>.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/#edsnlp.pipes.core.normalizer.remove_lowercase.factory.create_component","title":"<code>create_component</code>","text":"<p>Add case on the <code>NORM</code> custom attribute. Should always be applied first.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/#edsnlp.pipes.core.normalizer.remove_lowercase.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/","title":"<code>edsnlp.pipes.core.normalizer.spaces</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/factory/","title":"<code>edsnlp.pipes.core.normalizer.spaces.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/factory/#edsnlp.pipes.core.normalizer.spaces.factory.create_component","title":"<code>create_component = registry.factory.register('eds.spaces', assigns=['token.tag'], deprecated=['spaces'])(SpacesTagger)</code>  <code>module-attribute</code>","text":"<p>We assign \"SPACE\" to <code>token.tag</code> to be used by optimized components such as the EDSPhraseMatcher</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/factory/#edsnlp.pipes.core.normalizer.spaces.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>newline</code> <p>Whether to update the newline tokens too</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/","title":"<code>edsnlp.pipes.core.normalizer.spaces.spaces</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/#edsnlp.pipes.core.normalizer.spaces.spaces.SpacesTagger","title":"<code>SpacesTagger</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>We assign \"SPACE\" to <code>token.tag</code> to be used by optimized components such as the EDSPhraseMatcher</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/#edsnlp.pipes.core.normalizer.spaces.spaces.SpacesTagger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>newline</code> <p>Whether to update the newline tokens too</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/#edsnlp.pipes.core.normalizer.spaces.spaces.SpacesTagger.__call__","title":"<code>__call__</code>","text":"<p>Apply the component to the doc.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/#edsnlp.pipes.core.normalizer.spaces.spaces.SpacesTagger.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/core/sentences/","title":"<code>edsnlp.pipes.core.sentences</code>","text":""},{"location":"reference/edsnlp/pipes/core/sentences/factory/","title":"<code>edsnlp.pipes.core.sentences.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/sentences/factory/#edsnlp.pipes.core.sentences.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.sentences</code> matcher provides an alternative to spaCy's default <code>sentencizer</code>, aiming to overcome some of its limitations.</p> <p>Indeed, the <code>sentencizer</code> merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our <code>eds.sentences</code> component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances.</p> <p>Moreover, the <code>eds.sentences</code> component use the output of the <code>eds.normalizer</code> and <code>eds.endlines</code> output by default when these components are added to the pipeline.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/factory/#edsnlp.pipes.core.sentences.factory.create_component--examples","title":"Examples","text":"EDS-NLPspaCy sentencizer <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())  # same as nlp.add_pipe(\"eds.sentences\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\"\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out:  &lt;\\s&gt;\n# Out: &lt;s&gt; Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"sentencizer\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\"\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <p>Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/factory/#edsnlp.pipes.core.sentences.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sentences'</code> </p> <code>punct_chars</code> <p>Punctuation characters.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_endlines</code> <p>Whether to use endlines prediction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires the upstream <code>eds.normalizer</code> pipe).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/core/sentences/factory/#edsnlp.pipes.core.sentences.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sentences</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/sentences/","title":"<code>edsnlp.pipes.core.sentences.sentences</code>","text":""},{"location":"reference/edsnlp/pipes/core/sentences/sentences/#edsnlp.pipes.core.sentences.sentences.SentenceSegmenter","title":"<code>SentenceSegmenter</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>Segments the Doc into sentences using a rule-based strategy, specific to AP-HP documents.</p> <p>Applies the same rule-based pipeline as spaCy's sentencizer, and adds a simple rule on the new lines : if a new line is followed by a capitalised word, then it is also an end of sentence.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/sentences/#edsnlp.pipes.core.sentences.sentences.SentenceSegmenter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The EDS-NLP pipeline</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'sentences'</code> </p> <code>punct_chars</code> <p>Punctuation characters.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_endlines</code> <p>Whether to use endlines prediction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/core/sentences/terms/","title":"<code>edsnlp.pipes.core.sentences.terms</code>","text":""},{"location":"reference/edsnlp/pipes/core/terminology/","title":"<code>edsnlp.pipes.core.terminology</code>","text":""},{"location":"reference/edsnlp/pipes/core/terminology/factory/","title":"<code>edsnlp.pipes.core.terminology.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/terminology/factory/#edsnlp.pipes.core.terminology.factory.create_component","title":"<code>create_component = registry.factory.register('eds.terminology', assigns=['doc.ents', 'doc.spans'], deprecated=['terminology'])(TerminologyMatcher)</code>  <code>module-attribute</code>","text":"<p>EDS-NLP simplifies the terminology matching process by exposing a <code>eds.terminology</code> pipeline that can match on terms or regular expressions.</p> <p>The terminology matcher is very similar to the generic matcher, although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies.</p> <p>There are some key differences:</p> <ol> <li>It labels every matched entity to the same value, provided to the pipeline</li> <li>The keys provided in the <code>regex</code> and <code>terms</code> dictionaries are used as the    <code>kb_id_</code> of the entity, which handles fine-grained labelling</li> </ol> <p>For instance, a terminology matcher could detect every drug mention under the top-level label <code>drug</code>, and link each individual mention to a given drug through its <code>kb_id_</code> attribute.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/factory/#edsnlp.pipes.core.terminology.factory.create_component--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    flu=[\"grippe saisonni\u00e8re\"],  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.terminology(\n        label=\"disease\",\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/factory/#edsnlp.pipes.core.terminology.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become the <code>kb_id_</code> of the extracted entities. Dictionary values are either a single expression or a list of expressions that match the concept (see example).</p>"},{"location":"reference/edsnlp/pipes/core/terminology/factory/#edsnlp.pipes.core.terminology.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.terminology</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/","title":"<code>edsnlp.pipes.core.terminology.terminology</code>","text":""},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher","title":"<code>TerminologyMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>EDS-NLP simplifies the terminology matching process by exposing a <code>eds.terminology</code> pipeline that can match on terms or regular expressions.</p> <p>The terminology matcher is very similar to the generic matcher, although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies.</p> <p>There are some key differences:</p> <ol> <li>It labels every matched entity to the same value, provided to the pipeline</li> <li>The keys provided in the <code>regex</code> and <code>terms</code> dictionaries are used as the    <code>kb_id_</code> of the entity, which handles fine-grained labelling</li> </ol> <p>For instance, a terminology matcher could detect every drug mention under the top-level label <code>drug</code>, and link each individual mention to a given drug through its <code>kb_id_</code> attribute.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    flu=[\"grippe saisonni\u00e8re\"],  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.terminology(\n        label=\"disease\",\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become the <code>kb_id_</code> of the extracted entities. Dictionary values are either a single expression or a list of expressions that match the concept (see example).</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.terminology</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher.process","title":"<code>process</code>","text":"<p>Find matching spans in doc.</p> <p>Post-process matches to account for terminology.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>List of Spans returned by the matchers.</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds spans to document.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/misc/","title":"<code>edsnlp.pipes.misc</code>","text":""},{"location":"reference/edsnlp/pipes/misc/consultation_dates/","title":"<code>edsnlp.pipes.misc.consultation_dates</code>","text":""},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/","title":"<code>edsnlp.pipes.misc.consultation_dates.consultation_dates</code>","text":""},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher","title":"<code>ConsultationDatesMatcher</code>","text":"<p>           Bases: <code>GenericMatcher</code></p> <p>The <code>eds.consultation-dates</code> matcher consists of two main parts:</p> <ul> <li>A matcher which finds mentions of consultation events (more details below)</li> <li>A date parser (see the corresponding pipe) that links a date to those events</li> </ul>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher--examples","title":"Examples","text":"<p>Note</p> <p>The matcher has been built to run on consultation notes (<code>CR-CONS</code> at APHP), so please filter accordingly before proceeding.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=True,\n        accents=True,\n        quotes=True,\n        pollution=False,\n    ),\n)\nnlp.add_pipe(eds.consultation_dates())\n\ntext = \"\"\"\nXXX\nObjet : Compte-Rendu de Consultation du 03/10/2018.\nXXX\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"consultation_dates\"]\n# Out: [Consultation du 03/10/2018]\n\ndoc.spans[\"consultation_dates\"][0]._.consultation_date.to_datetime()\n# Out: DateTime(2018, 10, 3, 0, 0, 0)\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.consultation_dates</code> pipeline declares one extension on the <code>Span</code> object: the <code>consultation_date</code> attribute, which is a Python <code>datetime</code> object.</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>consultation_mention</code> <p>List of RegEx for consultation mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains terms directly referring to consultations, such as \"Consultation du...\" or \"Compte rendu du...\". This list is the only one enabled by default since it is fairly precise and not error-prone.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>True</code> </p> <code>town_mention</code> <p>List of RegEx for all AP-HP hospitals' towns mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \"Paris, le 13 d\u00e9cembre 2015\". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p> <code>document_date_mention</code> <p>List of RegEx for document date.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains expressions mentioning the date of creation/edition of a document, such as \"Date du rapport: 13/12/2015\" or \"Sign\u00e9 le 13/12/2015\". Like <code>town_mention</code> patterns, it has a high recall but is prone to errors since document date and consultation date aren't necessary similar.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.consultation_dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher.process","title":"<code>process</code>","text":"<p>Finds entities</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object with additional <code>doc.spans['consultation_dates]</code> <code>SpanGroup</code></p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/","title":"<code>edsnlp.pipes.misc.consultation_dates.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component","title":"<code>create_component = registry.factory.register('eds.consultation_dates', assigns=['doc.spans', 'doc.ents'], deprecated=['consultation_dates'])(ConsultationDatesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.consultation-dates</code> matcher consists of two main parts:</p> <ul> <li>A matcher which finds mentions of consultation events (more details below)</li> <li>A date parser (see the corresponding pipe) that links a date to those events</li> </ul>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component--examples","title":"Examples","text":"<p>Note</p> <p>The matcher has been built to run on consultation notes (<code>CR-CONS</code> at APHP), so please filter accordingly before proceeding.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=True,\n        accents=True,\n        quotes=True,\n        pollution=False,\n    ),\n)\nnlp.add_pipe(eds.consultation_dates())\n\ntext = \"\"\"\nXXX\nObjet : Compte-Rendu de Consultation du 03/10/2018.\nXXX\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"consultation_dates\"]\n# Out: [Consultation du 03/10/2018]\n\ndoc.spans[\"consultation_dates\"][0]._.consultation_date.to_datetime()\n# Out: DateTime(2018, 10, 3, 0, 0, 0)\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.consultation_dates</code> pipeline declares one extension on the <code>Span</code> object: the <code>consultation_date</code> attribute, which is a Python <code>datetime</code> object.</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>consultation_mention</code> <p>List of RegEx for consultation mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains terms directly referring to consultations, such as \"Consultation du...\" or \"Compte rendu du...\". This list is the only one enabled by default since it is fairly precise and not error-prone.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>True</code> </p> <code>town_mention</code> <p>List of RegEx for all AP-HP hospitals' towns mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \"Paris, le 13 d\u00e9cembre 2015\". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p> <code>document_date_mention</code> <p>List of RegEx for document date.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains expressions mentioning the date of creation/edition of a document, such as \"Date du rapport: 13/12/2015\" or \"Sign\u00e9 le 13/12/2015\". Like <code>town_mention</code> patterns, it has a high recall but is prone to errors since document date and consultation date aren't necessary similar.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.consultation_dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/patterns/","title":"<code>edsnlp.pipes.misc.consultation_dates.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/","title":"<code>edsnlp.pipes.misc.dates</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/dates/","title":"<code>edsnlp.pipes.misc.dates.dates</code>","text":"<p><code>eds.dates</code> pipeline.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher","title":"<code>DatesMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>The <code>eds.dates</code> matcher detects and normalize dates within a medical document. We use simple regular expressions to extract date mentions.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--scope","title":"Scope","text":"<p>The <code>eds.dates</code> pipeline finds absolute (eg <code>23/08/2021</code>) and relative (eg <code>hier</code>, <code>la semaine derni\u00e8re</code>) dates alike. It also handles mentions of duration.</p> Type Example <code>absolute</code> <code>3 mai</code>, <code>03/05/2020</code> <code>relative</code> <code>hier</code>, <code>la semaine derni\u00e8re</code> <code>duration</code> <code>pendant quatre jours</code> <p>See the tutorial for a presentation of a full pipeline featuring the <code>eds.dates</code> component.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nimport datetime\nimport pytz\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n\ndates = doc.spans[\"dates\"]\ndates\n# Out: [23 ao\u00fbt 2021, il y a un an, mai 1995]\n\ndates[0]._.date.to_datetime()\n# Out: 2021-08-23T00:00:00+02:00\n\ndates[1]._.date.to_datetime()\n# Out: None\n\nnote_datetime = datetime.datetime(2021, 8, 27, tzinfo=pytz.timezone(\"Europe/Paris\"))\ndoc._.note_datetime = note_datetime\n\ndates[1]._.date.to_datetime()\n# Out: 2020-08-27T00:00:00+02:00\n\ndate_2_output = dates[2]._.date.to_datetime(\n    note_datetime=note_datetime,\n    infer_from_context=True,\n    tz=\"Europe/Paris\",\n    default_day=15,\n)\ndate_2_output\n# Out: 1995-05-15T00:00:00+02:00\n\ndoc.spans[\"durations\"]\n# Out: [pendant une semaine]\n</code></pre> <p>Example on a collection of documents stored in the OMOP schema :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# with cols \"note_id\", \"note_text\" and optionally \"note_datetime\"\nmy_omop_df = ...\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates(as_ents=True))\ndocs = edsnlp.data.from_pandas(my_omop_df)\ndocs = docs.map_pipeline(nlp)\ndocs = docs.to_pandas(\n    converter=\"ents\",\n    span_attributes={\"date.datetime\": \"datetime\"},\n)\nprint(docs)\n# note_id  start  end label lexical_variant span_type datetime\n# ...\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.dates</code> pipeline declares two extensions on the <code>Span</code> object:</p> <ul> <li>the <code>span._.date</code> attribute of a date contains a parsed version of the date.</li> <li>the <code>span._.duration</code> attribute of a duration contains a parsed version of the   duration.</li> </ul> <p>As with other components, you can use the <code>span._.value</code> attribute to get either the parsed date or the duration depending on the span.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the pipeline component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dates'</code> </p> <code>absolute</code> <p>List of regular expressions for absolute dates.</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>relative</code> <p>List of regular expressions for relative dates (eg <code>hier</code>, <code>la semaine prochaine</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>List of regular expressions for durations (eg <code>pendant trois mois</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>false_positive</code> <p>List of regular expressions for false positive (eg phone numbers, etc).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for dates in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matched dates with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a date overlaps a span from <code>span_getter</code> (e.g. a date extracted   by a machine learning model), return the <code>span_getter</code> span instead, and   assign all the parsed information (<code>._.date</code> / <code>._.duration</code>) to it. Otherwise   don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> and <code>merge_mode</code> instead. Whether to look on dates in the whole document or in specific sentences:</p> <ul> <li>If <code>True</code>: Only look in the sentences of each entity in doc.ents</li> <li>If False: Look in the whole document</li> <li>If given a string <code>key</code> or list of string: Only look in the sentences of   each entity in <code>doc.spans[key]</code></li> </ul> <p> TYPE: <code>Union[bool, str, Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>detect_periods</code> <p>Whether to detect periods (experimental)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>detect_time</code> <p>Whether to detect time inside dates</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>period_proximity_threshold</code> <p>Max number of words between two dates to extract a period.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>as_ents</code> <p>Deprecated, use span_setter instead. Whether to treat dates as entities</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>LOWER</code> </p> <code>date_label</code> <p>Label to use for dates</p> <p> TYPE: <code>str</code> DEFAULT: <code>date</code> </p> <code>duration_label</code> <p>Label to use for durations</p> <p> TYPE: <code>str</code> DEFAULT: <code>duration</code> </p> <code>period_label</code> <p>Label to use for periods</p> <p> TYPE: <code>str</code> DEFAULT: <code>period</code> </p> <code>span_setter</code> <p>How to set matches in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'dates': ['date'], 'durations': ['duration'], ...</code> </p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set extensions for the dates pipeline.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.process","title":"<code>process</code>","text":"<p>Find dates in doc.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>dates</code> <p>list of date spans</p> <p> TYPE: <code>List[Tuple[Span, Dict[str, str]]]</code> </p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.parse","title":"<code>parse</code>","text":"<p>Parse dates/durations using the groupdict returned by the matcher.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.parse--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>matches</code> <p>List of tuples containing the spans and groupdict returned by the matcher.</p> <p> TYPE: <code>List[Tuple[Span, Dict[str, str]]]</code> </p> RETURNS DESCRIPTION <code>Tuple[List[Span], List[Span]]</code> <p>List of processed spans, with the date parsed.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.process_periods","title":"<code>process_periods</code>","text":"<p>Experimental period detection.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.process_periods--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>dates</code> <p>List of detected dates.</p> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected periods.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.__call__","title":"<code>__call__</code>","text":"<p>Tags dates.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for dates</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/","title":"<code>edsnlp.pipes.misc.dates.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component","title":"<code>create_component = registry.factory.register('eds.dates', assigns=['doc.spans', 'doc.ents'], deprecated=['dates'])(DatesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.dates</code> matcher detects and normalize dates within a medical document. We use simple regular expressions to extract date mentions.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--scope","title":"Scope","text":"<p>The <code>eds.dates</code> pipeline finds absolute (eg <code>23/08/2021</code>) and relative (eg <code>hier</code>, <code>la semaine derni\u00e8re</code>) dates alike. It also handles mentions of duration.</p> Type Example <code>absolute</code> <code>3 mai</code>, <code>03/05/2020</code> <code>relative</code> <code>hier</code>, <code>la semaine derni\u00e8re</code> <code>duration</code> <code>pendant quatre jours</code> <p>See the tutorial for a presentation of a full pipeline featuring the <code>eds.dates</code> component.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nimport datetime\nimport pytz\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n\ndates = doc.spans[\"dates\"]\ndates\n# Out: [23 ao\u00fbt 2021, il y a un an, mai 1995]\n\ndates[0]._.date.to_datetime()\n# Out: 2021-08-23T00:00:00+02:00\n\ndates[1]._.date.to_datetime()\n# Out: None\n\nnote_datetime = datetime.datetime(2021, 8, 27, tzinfo=pytz.timezone(\"Europe/Paris\"))\ndoc._.note_datetime = note_datetime\n\ndates[1]._.date.to_datetime()\n# Out: 2020-08-27T00:00:00+02:00\n\ndate_2_output = dates[2]._.date.to_datetime(\n    note_datetime=note_datetime,\n    infer_from_context=True,\n    tz=\"Europe/Paris\",\n    default_day=15,\n)\ndate_2_output\n# Out: 1995-05-15T00:00:00+02:00\n\ndoc.spans[\"durations\"]\n# Out: [pendant une semaine]\n</code></pre> <p>Example on a collection of documents stored in the OMOP schema :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# with cols \"note_id\", \"note_text\" and optionally \"note_datetime\"\nmy_omop_df = ...\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates(as_ents=True))\ndocs = edsnlp.data.from_pandas(my_omop_df)\ndocs = docs.map_pipeline(nlp)\ndocs = docs.to_pandas(\n    converter=\"ents\",\n    span_attributes={\"date.datetime\": \"datetime\"},\n)\nprint(docs)\n# note_id  start  end label lexical_variant span_type datetime\n# ...\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.dates</code> pipeline declares two extensions on the <code>Span</code> object:</p> <ul> <li>the <code>span._.date</code> attribute of a date contains a parsed version of the date.</li> <li>the <code>span._.duration</code> attribute of a duration contains a parsed version of the   duration.</li> </ul> <p>As with other components, you can use the <code>span._.value</code> attribute to get either the parsed date or the duration depending on the span.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the pipeline component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dates'</code> </p> <code>absolute</code> <p>List of regular expressions for absolute dates.</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>relative</code> <p>List of regular expressions for relative dates (eg <code>hier</code>, <code>la semaine prochaine</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>List of regular expressions for durations (eg <code>pendant trois mois</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>false_positive</code> <p>List of regular expressions for false positive (eg phone numbers, etc).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for dates in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matched dates with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a date overlaps a span from <code>span_getter</code> (e.g. a date extracted   by a machine learning model), return the <code>span_getter</code> span instead, and   assign all the parsed information (<code>._.date</code> / <code>._.duration</code>) to it. Otherwise   don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> and <code>merge_mode</code> instead. Whether to look on dates in the whole document or in specific sentences:</p> <ul> <li>If <code>True</code>: Only look in the sentences of each entity in doc.ents</li> <li>If False: Look in the whole document</li> <li>If given a string <code>key</code> or list of string: Only look in the sentences of   each entity in <code>doc.spans[key]</code></li> </ul> <p> TYPE: <code>Union[bool, str, Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>detect_periods</code> <p>Whether to detect periods (experimental)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>detect_time</code> <p>Whether to detect time inside dates</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>period_proximity_threshold</code> <p>Max number of words between two dates to extract a period.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>as_ents</code> <p>Deprecated, use span_setter instead. Whether to treat dates as entities</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>LOWER</code> </p> <code>date_label</code> <p>Label to use for dates</p> <p> TYPE: <code>str</code> DEFAULT: <code>date</code> </p> <code>duration_label</code> <p>Label to use for durations</p> <p> TYPE: <code>str</code> DEFAULT: <code>duration</code> </p> <code>period_label</code> <p>Label to use for periods</p> <p> TYPE: <code>str</code> DEFAULT: <code>period</code> </p> <code>span_setter</code> <p>How to set matches in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'dates': ['date'], 'durations': ['duration'], ...</code> </p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/","title":"<code>edsnlp.pipes.misc.dates.models</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.BaseDate","title":"<code>BaseDate</code>","text":"<p>           Bases: <code>BaseModel</code></p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.BaseDate.remove_space","title":"<code>remove_space</code>","text":"<p>Remove spaces. Useful for coping with ill-formatted PDF extractions.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.AbsoluteDate","title":"<code>AbsoluteDate</code>","text":"<p>           Bases: <code>BaseDate</code></p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.AbsoluteDate.to_datetime","title":"<code>to_datetime</code>","text":"<p>Convert the date to a datetime.datetime object.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.AbsoluteDate.to_datetime--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>tz</code> <p>The timezone to use. Defaults to None.</p> <p> TYPE: <code>Optional[Union[str, timezone]]</code> DEFAULT: <code>None</code> </p> <code>note_datetime</code> <p>The datetime of the note. Used to infer missing parts of the date.</p> <p> TYPE: <code>Optional[Union[datetime, datetime]]</code> DEFAULT: <code>None</code> </p> <code>infer_from_context</code> <p>Whether to infer missing parts of the date from the note datetime. In a (year, month, day) triplet:</p> <pre><code>- if only year is missing, it will be inferred from the note datetime\n- if only month is missing, it will be inferred from the note datetime\n- if only day is missing, it will be set to `default_day`\n- if only the year is given, the day and month will be set to\n  `default_day` and `default_month`\n- if only the month is given, the day will be set to `default_day`\n  and the year will be inferred from the note datetime\n- if only the day is given, the month and year will be inferred from\n  the note datetime\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>default_day</code> <p>Default day to use when inferring missing parts of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>default_month</code> <p>Default month to use when inferring missing parts of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Union[datetime, None]</code>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.Relative","title":"<code>Relative</code>","text":"<p>           Bases: <code>BaseDate</code></p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.Relative.parse_unit","title":"<code>parse_unit</code>","text":"<p>Units need to be handled separately.</p> <p>This validator modifies the key corresponding to the unit with the detected value</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.Relative.parse_unit--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>d</code> <p>Original data</p> <p> TYPE: <code>Dict[str, str]</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>Transformed data</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.RelativeDate","title":"<code>RelativeDate</code>","text":"<p>           Bases: <code>Relative</code></p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.RelativeDate.handle_specifics","title":"<code>handle_specifics</code>","text":"<p>Specific patterns such as <code>aujourd'hui</code>, <code>hier</code>, etc, need to be handled separately.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.RelativeDate.handle_specifics--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>d</code> <p>Original data.</p> <p> TYPE: <code>Dict[str, str]</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>Modified data.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/patterns/","title":"<code>edsnlp.pipes.misc.dates.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/absolute/","title":"<code>edsnlp.pipes.misc.dates.patterns.absolute</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/days/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.days</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/delimiters/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.delimiters</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/directions/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.directions</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/modes/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.modes</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/months/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.months</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/numbers/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.numbers</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/time/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.time</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/units/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.units</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/years/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.years</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/current/","title":"<code>edsnlp.pipes.misc.dates.patterns.current</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/duration/","title":"<code>edsnlp.pipes.misc.dates.patterns.duration</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/false_positive/","title":"<code>edsnlp.pipes.misc.dates.patterns.false_positive</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/relative/","title":"<code>edsnlp.pipes.misc.dates.patterns.relative</code>","text":""},{"location":"reference/edsnlp/pipes/misc/measurements/","title":"<code>edsnlp.pipes.misc.measurements</code>","text":""},{"location":"reference/edsnlp/pipes/misc/measurements/factory/","title":"<code>edsnlp.pipes.misc.measurements.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/measurements/factory/#edsnlp.pipes.misc.measurements.factory.create_component","title":"<code>create_component = registry.factory.register('eds.measurements', assigns=['doc.spans', 'doc.ents'], deprecated=['eds.measures'])(MeasurementsMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.measurements</code> matcher detects and normalizes numerical measurements within a medical document.</p> <p>Warning</p> <p>The <code>measurements</code> pipeline is still in active development and has not been rigorously validated. If you come across a measurement expression that goes undetected, please file an issue !</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/factory/#edsnlp.pipes.misc.measurements.factory.create_component--scope","title":"Scope","text":"<p>The <code>eds.measurements</code> matcher can extract simple (e.g. <code>3cm</code>) measurements. It can also detect elliptic enumerations (eg <code>32, 33 et 34kg</code>) of measurements of the same type and split the measurements accordingly.</p> <p>The normalized value can then be accessed via the <code>span._.{measure_name}</code> attribute, for instance <code>span._.size</code> or <code>span._.weight</code> and be converted on the fly to a desired unit. Like for other components, the <code>span._.value</code> extension can also be used to access the normalized value for any measurement span.</p> <p>The current matcher annotates the following measurements out of the box:</p> Measurement name Example <code>size</code> <code>1m50</code>, <code>1.50m</code> <code>weight</code> <code>12kg</code>, <code>1kg300</code> <code>bmi</code> <code>BMI: 24</code>, <code>24 kg.m-2</code> <code>volume</code> <code>2 cac</code>, <code>8ml</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/factory/#edsnlp.pipes.misc.measurements.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.measurements(\n        measurements=[\"size\", \"weight\", \"bmi\"],\n        extract_ranges=True,\n    ),\n)\n\ntext = \"\"\"\nLe patient est admis hier, fait 1m78 pour 76kg.\nLes deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm.\nBMI: 24.\n\nLe nodule fait entre 1 et 1.5 cm\n\"\"\"\n\ndoc = nlp(text)\n\nmeasurements = doc.spans[\"measurements\"]\n\nmeasurements\n# Out: [1m78, 76kg, 1,2, 2.4mm, 24, entre 1 et 1.5 cm]\n\nmeasurements[0]\n# Out: 1m78\n\nstr(measurements[0]._.size), str(measurements[0]._.value)\n# Out: ('1.78 m', '1.78 m')\n\nmeasurements[0]._.value.cm\n# Out: 178.0\n\nmeasurements[2]\n# Out: 1,2\n\nstr(measurements[2]._.value)\n# Out: '1.2 mm'\n\nstr(measurements[2]._.value.mm)\n# Out: 1.2\n\nmeasurements[4]\n# Out: 24\n\nstr(measurements[4]._.value)\n# Out: '24 kg_per_m2'\n\nstr(measurements[4]._.value.kg_per_m2)\n# Out: 24\n\nstr(measurements[5]._.value)\n# Out: 1-1.5 cm\n</code></pre> <p>To extract all sizes in centimeters, and average range measurements, you can use the following snippet:</p> <pre><code>sizes = [\n    sum(item.cm for item in m._.value) / len(m._.value)\n    for m in doc.spans[\"measurements\"]\n    if m.label_ == \"size\"\n]\nsizes\n# Out: [178.0, 0.12, 0.24, 1.25]\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/measurements/factory/#edsnlp.pipes.misc.measurements.factory.create_component--customization","title":"Customization","text":"<p>You can declare custom measurements by altering the patterns:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.measurements(\n        measurements={\n            \"my_custom_surface_measurement\": {\n                # This measurement unit is homogenous to square meters\n                \"unit\": \"m2\",\n                # Handle cases like \"surface: 1.8\" (implied m2),\n                # vs \"surface: 50\" (implied cm2)\n                \"unitless_patterns\": [\n                    {\n                        \"terms\": [\"surface\", \"aire\"],\n                        \"ranges\": [\n                            {\"unit\": \"m2\", \"min\": 0, \"max\": 9},\n                            {\"unit\": \"cm2\", \"min\": 10, \"max\": 100},\n                        ],\n                    }\n                ],\n            },\n        }\n    ),\n)\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/measurements/factory/#edsnlp.pipes.misc.measurements.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.measurements</code> pipeline declares its extensions dynamically, depending on the <code>measurements</code> parameter: each measurement gets its own extension, and is assigned to a different span group.</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/factory/#edsnlp.pipes.misc.measurements.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> </p> <code>measurements</code> <p>A mapping from measure names to MsrConfig Each measure's configuration has the following shape: <pre><code>{\n  # the unit (e.g. \"kg\"),\n  \"unit\": str,\n  \"unitless_patterns\": {\n    # preceding trigger terms\n    \"terms\": List[str],\n    # unitless ranges -&gt; unit patterns\n    \"ranges\": List[\n      {\"min\": int, \"max\": int, \"unit\": str},\n      {\"min\": int, \"unit\": str},\n      ...,\n    ],\n    ...\n  }\n}\n</code></pre></p> <p> TYPE: <code>Union[str, List[Union[str, MsrConfig]], Dict[str, MsrConfig]]</code> DEFAULT: <code>['weight', 'size', 'bmi', 'volume']</code> </p> <code>number_terms</code> <p>A mapping of numbers to their lexical variants</p> <p> DEFAULT: <code>{'0.125': ['\u215b'], '0.16666666': ['\u2159'], '0.2': ['...</code> </p> <code>stopwords</code> <p>A list of stopwords that do not matter when placed between a unitless trigger and a number</p> <p> DEFAULT: <code>['par', 'sur', 'de', 'a', ',', 'et']</code> </p> <code>unit_divisors</code> <p>A list of terms used to divide two units (like: m / s)</p> <p> DEFAULT: <code>['/', 'par']</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to exclude pollution patterns when matching in the text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>compose_units</code> <p>Whether to compose units (like \"m/s\" or \"m.s-1\")</p> <p> DEFAULT: <code>True</code> </p> <code>extract_ranges</code> <p>Whether to extract ranges (like \"entre 1 et 2 cm\")</p> <p> DEFAULT: <code>False</code> </p> <code>range_patterns</code> <p>A list of \"{FROM} xx {TO} yy\" patterns to match range measurements</p> <p> DEFAULT: <code>[('De', '\u00e0'), ('De', 'a'), ('de', '\u00e0'), ('de', ...</code> </p> <code>after_snippet_limit</code> <p>Maximum word distance after to link a part of a measurement after its number</p> <p> DEFAULT: <code>6</code> </p> <code>before_snippet_limit</code> <p>Maximum word distance after to link a part of a measurement before its number</p> <p> DEFAULT: <code>10</code> </p> <code>span_setter</code> <p>How to set the spans in the document. By default, each measurement will be assigned to its own span group (using either the \"name\" field of the config, or the key if you passed a dict), and to the \"measurements\" group.</p> <p> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for measurements in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matches with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a match overlaps a span from <code>span_getter</code> (e.g. a match   extracted by a machine learning model), return the <code>span_getter</code> span   instead, and assign all the parsed information (<code>._.date</code> / <code>._.duration</code>)   to it. Otherwise, don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p>"},{"location":"reference/edsnlp/pipes/misc/measurements/factory/#edsnlp.pipes.misc.measurements.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.measurements</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/","title":"<code>edsnlp.pipes.misc.measurements.measurements</code>","text":""},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.Measurement","title":"<code>Measurement</code>","text":"<p>           Bases: <code>ABC</code></p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.Measurement.__len__","title":"<code>__len__</code>  <code>abstractmethod</code>","text":"<p>Number of items in the measure (only one for SimpleMeasurement)</p> RETURNS DESCRIPTION <code>Iterable[SimpleMeasurement]</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.Measurement.__iter__","title":"<code>__iter__</code>  <code>abstractmethod</code>","text":"<p>Iter over items of the measure (only one for SimpleMeasurement)</p> RETURNS DESCRIPTION <code>Iterable[SimpleMeasurement]</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.Measurement.__getitem__","title":"<code>__getitem__</code>  <code>abstractmethod</code>","text":"<p>Access items of the measure (only one for SimpleMeasurement)</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.Measurement.__getitem__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>item</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>SimpleMeasurement</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.SimpleMeasurement","title":"<code>SimpleMeasurement</code>","text":"<p>           Bases: <code>Measurement</code></p> <p>The SimpleMeasurement class contains the value and unit for a single non-composite measure</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.SimpleMeasurement--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>value</code> <p> TYPE: <code>float</code> </p> <code>unit</code> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher","title":"<code>MeasurementsMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>The <code>eds.measurements</code> matcher detects and normalizes numerical measurements within a medical document.</p> <p>Warning</p> <p>The <code>measurements</code> pipeline is still in active development and has not been rigorously validated. If you come across a measurement expression that goes undetected, please file an issue !</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher--scope","title":"Scope","text":"<p>The <code>eds.measurements</code> matcher can extract simple (e.g. <code>3cm</code>) measurements. It can also detect elliptic enumerations (eg <code>32, 33 et 34kg</code>) of measurements of the same type and split the measurements accordingly.</p> <p>The normalized value can then be accessed via the <code>span._.{measure_name}</code> attribute, for instance <code>span._.size</code> or <code>span._.weight</code> and be converted on the fly to a desired unit. Like for other components, the <code>span._.value</code> extension can also be used to access the normalized value for any measurement span.</p> <p>The current matcher annotates the following measurements out of the box:</p> Measurement name Example <code>size</code> <code>1m50</code>, <code>1.50m</code> <code>weight</code> <code>12kg</code>, <code>1kg300</code> <code>bmi</code> <code>BMI: 24</code>, <code>24 kg.m-2</code> <code>volume</code> <code>2 cac</code>, <code>8ml</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.measurements(\n        measurements=[\"size\", \"weight\", \"bmi\"],\n        extract_ranges=True,\n    ),\n)\n\ntext = \"\"\"\nLe patient est admis hier, fait 1m78 pour 76kg.\nLes deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm.\nBMI: 24.\n\nLe nodule fait entre 1 et 1.5 cm\n\"\"\"\n\ndoc = nlp(text)\n\nmeasurements = doc.spans[\"measurements\"]\n\nmeasurements\n# Out: [1m78, 76kg, 1,2, 2.4mm, 24, entre 1 et 1.5 cm]\n\nmeasurements[0]\n# Out: 1m78\n\nstr(measurements[0]._.size), str(measurements[0]._.value)\n# Out: ('1.78 m', '1.78 m')\n\nmeasurements[0]._.value.cm\n# Out: 178.0\n\nmeasurements[2]\n# Out: 1,2\n\nstr(measurements[2]._.value)\n# Out: '1.2 mm'\n\nstr(measurements[2]._.value.mm)\n# Out: 1.2\n\nmeasurements[4]\n# Out: 24\n\nstr(measurements[4]._.value)\n# Out: '24 kg_per_m2'\n\nstr(measurements[4]._.value.kg_per_m2)\n# Out: 24\n\nstr(measurements[5]._.value)\n# Out: 1-1.5 cm\n</code></pre> <p>To extract all sizes in centimeters, and average range measurements, you can use the following snippet:</p> <pre><code>sizes = [\n    sum(item.cm for item in m._.value) / len(m._.value)\n    for m in doc.spans[\"measurements\"]\n    if m.label_ == \"size\"\n]\nsizes\n# Out: [178.0, 0.12, 0.24, 1.25]\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher--customization","title":"Customization","text":"<p>You can declare custom measurements by altering the patterns:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.measurements(\n        measurements={\n            \"my_custom_surface_measurement\": {\n                # This measurement unit is homogenous to square meters\n                \"unit\": \"m2\",\n                # Handle cases like \"surface: 1.8\" (implied m2),\n                # vs \"surface: 50\" (implied cm2)\n                \"unitless_patterns\": [\n                    {\n                        \"terms\": [\"surface\", \"aire\"],\n                        \"ranges\": [\n                            {\"unit\": \"m2\", \"min\": 0, \"max\": 9},\n                            {\"unit\": \"cm2\", \"min\": 10, \"max\": 100},\n                        ],\n                    }\n                ],\n            },\n        }\n    ),\n)\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.measurements</code> pipeline declares its extensions dynamically, depending on the <code>measurements</code> parameter: each measurement gets its own extension, and is assigned to a different span group.</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> </p> <code>measurements</code> <p>A mapping from measure names to MsrConfig Each measure's configuration has the following shape: <pre><code>{\n  # the unit (e.g. \"kg\"),\n  \"unit\": str,\n  \"unitless_patterns\": {\n    # preceding trigger terms\n    \"terms\": List[str],\n    # unitless ranges -&gt; unit patterns\n    \"ranges\": List[\n      {\"min\": int, \"max\": int, \"unit\": str},\n      {\"min\": int, \"unit\": str},\n      ...,\n    ],\n    ...\n  }\n}\n</code></pre></p> <p> TYPE: <code>Union[str, List[Union[str, MsrConfig]], Dict[str, MsrConfig]]</code> DEFAULT: <code>['weight', 'size', 'bmi', 'volume']</code> </p> <code>number_terms</code> <p>A mapping of numbers to their lexical variants</p> <p> DEFAULT: <code>{'0.125': ['\u215b'], '0.16666666': ['\u2159'], '0.2': ['...</code> </p> <code>stopwords</code> <p>A list of stopwords that do not matter when placed between a unitless trigger and a number</p> <p> DEFAULT: <code>['par', 'sur', 'de', 'a', ',', 'et']</code> </p> <code>unit_divisors</code> <p>A list of terms used to divide two units (like: m / s)</p> <p> DEFAULT: <code>['/', 'par']</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to exclude pollution patterns when matching in the text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>compose_units</code> <p>Whether to compose units (like \"m/s\" or \"m.s-1\")</p> <p> DEFAULT: <code>True</code> </p> <code>extract_ranges</code> <p>Whether to extract ranges (like \"entre 1 et 2 cm\")</p> <p> DEFAULT: <code>False</code> </p> <code>range_patterns</code> <p>A list of \"{FROM} xx {TO} yy\" patterns to match range measurements</p> <p> DEFAULT: <code>[('De', '\u00e0'), ('De', 'a'), ('de', '\u00e0'), ('de', ...</code> </p> <code>after_snippet_limit</code> <p>Maximum word distance after to link a part of a measurement after its number</p> <p> DEFAULT: <code>6</code> </p> <code>before_snippet_limit</code> <p>Maximum word distance after to link a part of a measurement before its number</p> <p> DEFAULT: <code>10</code> </p> <code>span_setter</code> <p>How to set the spans in the document. By default, each measurement will be assigned to its own span group (using either the \"name\" field of the config, or the key if you passed a dict), and to the \"measurements\" group.</p> <p> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for measurements in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matches with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a match overlaps a span from <code>span_getter</code> (e.g. a match   extracted by a machine learning model), return the <code>span_getter</code> span   instead, and assign all the parsed information (<code>._.date</code> / <code>._.duration</code>)   to it. Otherwise, don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.measurements</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set extensions for the measurements pipeline.</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.extract_units","title":"<code>extract_units</code>","text":"<p>Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" =&gt; we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) =&gt; we aggregate these adjacent matches together to compose a new unit g_per_day</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.extract_units--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>term_matches</code> <p> TYPE: <code>Iterable[Span]</code> </p> RETURNS DESCRIPTION <code>Iterable[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.make_pseudo_sentence","title":"<code>make_pseudo_sentence</code>  <code>classmethod</code>","text":"<p>Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font {1}{,} {2} {et} {3} {cm} de long{.}\" is transformed into \"wn,n,nuw.\"</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.make_pseudo_sentence--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>The document or span to transform</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>matches</code> <p>List of tuple of span and whether the span represents a sentence end</p> <p> TYPE: <code>List[Tuple[Span, bool]]</code> </p> <code>pseudo_mapping</code> <p>A mapping from label to char in the pseudo sentence</p> <p> TYPE: <code>Dict[int, str]</code> </p> RETURNS DESCRIPTION <code>(str, List[int])</code> <ul> <li>the pseudo sentence</li> <li>a list of offsets to convert match indices into pseudo sent char indices</li> </ul>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.get_matches","title":"<code>get_matches</code>","text":"<p>Extract and filter regex and phrase matches in the document to prepare the measurement extraction. Returns the matches and a list of hashes to quickly find unit matches</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.get_matches--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> RETURNS DESCRIPTION <code>Tuple[List[Span, bool], Set[int]]</code> <ul> <li>List of tuples of spans and whether the spans represents a sentence end</li> <li>List of hash label to distinguish unit from other matches</li> </ul>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.extract_measurements","title":"<code>extract_measurements</code>","text":"<p>Extracts measure entities from the document</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.extract_measurements--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.merge_adjacent_measurements","title":"<code>merge_adjacent_measurements</code>  <code>classmethod</code>","text":"<p>Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\"</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.merge_adjacent_measurements--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>measurements</code> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.merge_measurements_in_ranges","title":"<code>merge_measurements_in_ranges</code>","text":"<p>Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\"</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.merge_measurements_in_ranges--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>measurements</code> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.merge_with_existing","title":"<code>merge_with_existing</code>","text":"<p>Merges the extracted measurements with the existing measurements in the document.</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.merge_with_existing--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>extracted</code> <p>The extracted measurements</p> <p> TYPE: <code>List[Span]</code> </p> <code>existing</code> <p>The existing measurements in the document</p> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds measurements to document's \"measurements\" SpanGroup.</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/measurements/#edsnlp.pipes.misc.measurements.measurements.MeasurementsMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted measurements.</p>"},{"location":"reference/edsnlp/pipes/misc/measurements/patterns/","title":"<code>edsnlp.pipes.misc.measurements.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/","title":"<code>edsnlp.pipes.misc.reason</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/factory/","title":"<code>edsnlp.pipes.misc.reason.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component","title":"<code>create_component = registry.factory.register('eds.reason', assigns=['doc.spans', 'doc.ents'], deprecated=['reason'])(ReasonMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.reason</code> matcher uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = edsnlp.blank(\"eds\")\n\n# Extraction of entities\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        )\n    ),\n)\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.reason(use_sections=True))\ndoc = nlp(text)\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nreason._.is_reason\n# Out: True\n\nentities = reason._.ents_reason\nentities\n# Out: [asthme]\n\nentities[0].label_\n# Out: 'respiratoire'\n\nent = entities[0]\nent._.is_reason\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.reason</code> pipeline adds the key <code>reasons</code> to <code>doc.spans</code> and declares one extension, on the <code>Span</code> objects called <code>ents_reason</code>.</p> <p>The <code>ents_reason</code> extension is a list of named entities that overlap the <code>Span</code>, typically entities found in upstream components like <code>matcher</code>.</p> <p>It also declares the boolean extension <code>is_reason</code>. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'reason'</code> </p> <code>reasons</code> <p>Reason patterns</p> <p> TYPE: <code>Dict[str, Union[List[str], str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Default token attribute to use to build the text to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>use_sections</code> <p>Whether or not use the <code>sections</code> matcher to improve results.</p> <p> TYPE: <code>(bool)</code> DEFAULT: <code>False</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reason</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/patterns/","title":"<code>edsnlp.pipes.misc.reason.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/reason/","title":"<code>edsnlp.pipes.misc.reason.reason</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher","title":"<code>ReasonMatcher</code>","text":"<p>           Bases: <code>GenericMatcher</code></p> <p>The <code>eds.reason</code> matcher uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = edsnlp.blank(\"eds\")\n\n# Extraction of entities\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        )\n    ),\n)\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.reason(use_sections=True))\ndoc = nlp(text)\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nreason._.is_reason\n# Out: True\n\nentities = reason._.ents_reason\nentities\n# Out: [asthme]\n\nentities[0].label_\n# Out: 'respiratoire'\n\nent = entities[0]\nent._.is_reason\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.reason</code> pipeline adds the key <code>reasons</code> to <code>doc.spans</code> and declares one extension, on the <code>Span</code> objects called <code>ents_reason</code>.</p> <p>The <code>ents_reason</code> extension is a list of named entities that overlap the <code>Span</code>, typically entities found in upstream components like <code>matcher</code>.</p> <p>It also declares the boolean extension <code>is_reason</code>. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'reason'</code> </p> <code>reasons</code> <p>Reason patterns</p> <p> TYPE: <code>Dict[str, Union[List[str], str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Default token attribute to use to build the text to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>use_sections</code> <p>Whether or not use the <code>sections</code> matcher to improve results.</p> <p> TYPE: <code>(bool)</code> DEFAULT: <code>False</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reason</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher.__call__","title":"<code>__call__</code>","text":"<p>Find spans related to the reasons of the hospitalisation</p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/pipes/misc/sections/","title":"<code>edsnlp.pipes.misc.sections</code>","text":""},{"location":"reference/edsnlp/pipes/misc/sections/factory/","title":"<code>edsnlp.pipes.misc.sections.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component","title":"<code>create_component = registry.factory.register('eds.sections', assigns=['doc.spans', 'doc.ents'], deprecated=['sections'])(SectionsMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.sections</code> component extracts section titles from clinical documents. A \"section\" is then defined as the span of text between two titles.</p> <p>Here is the list of sections that are currently targeted :</p> <ul> <li><code>allergies</code></li> <li><code>ant\u00e9c\u00e9dents</code></li> <li><code>ant\u00e9c\u00e9dents familiaux</code></li> <li><code>traitements entr\u00e9e</code></li> <li><code>conclusion</code></li> <li><code>conclusion entr\u00e9e</code></li> <li><code>habitus</code></li> <li><code>correspondants</code></li> <li><code>diagnostic</code></li> <li><code>donn\u00e9es biom\u00e9triques entr\u00e9e</code></li> <li><code>examens</code></li> <li><code>examens compl\u00e9mentaires</code></li> <li><code>facteurs de risques</code></li> <li><code>histoire de la maladie</code></li> <li><code>actes</code></li> <li><code>motif</code></li> <li><code>prescriptions</code></li> <li><code>traitements sortie</code></li> <li><code>evolution</code></li> <li><code>modalites sortie</code></li> <li><code>vaccinations</code></li> <li><code>introduction</code></li> </ul> <p>Remarks :</p> <ul> <li>section <code>introduction</code> corresponds to the span of text between the header   \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document)   and the title of the following detected section</li> <li>this matcher works well for hospitalization summaries (CRH), but not necessarily   for all types of documents (in particular for emergency or scan summaries   CR-IMAGERIE)</li> </ul> <p>Experimental</p> <p>Should you rely on <code>eds.sections</code> for critical downstream tasks, make sure to validate the results to make sure that the component works in your case.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component--examples","title":"Examples","text":"<p>The following snippet detects section titles. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\n\ntext = \"\"\"\nCRU du 10/09/2021\nMotif :\nPatient admis pour suspicion de COVID\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"section_titles\"]\n# Out: [Motif]\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.sections</code> matcher adds two fields to the <code>doc.spans</code> attribute :</p> <ol> <li>The <code>section_titles</code> key contains the list of all section titles extracted using    the list declared in the <code>terms.py</code> module.</li> <li>The <code>sections</code> key contains a list of sections, ie spans of text between two    section titles (or the last title and the end of the document).</li> </ol> <p>If the document has entities before calling this matcher an attribute <code>section</code> is added to each entity.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>sections</code> <p>Dictionary of terms to look for.</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'allergies': ['allergies'], 'ant\u00e9c\u00e9dents': ['a...</code> </p> <code>attr</code> <p>Default attribute to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>add_patterns</code> <p>Whether add update patterns to match start / end of lines</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sections</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/patterns/","title":"<code>edsnlp.pipes.misc.sections.patterns</code>","text":"<p>These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles.</p> <p>The section titles were reviewed by Gilles Chatellier, who gave meaningful insights.</p> <p>See sections/section-dataset notebook for detail.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/","title":"<code>edsnlp.pipes.misc.sections.sections</code>","text":""},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher","title":"<code>SectionsMatcher</code>","text":"<p>           Bases: <code>GenericMatcher</code></p> <p>The <code>eds.sections</code> component extracts section titles from clinical documents. A \"section\" is then defined as the span of text between two titles.</p> <p>Here is the list of sections that are currently targeted :</p> <ul> <li><code>allergies</code></li> <li><code>ant\u00e9c\u00e9dents</code></li> <li><code>ant\u00e9c\u00e9dents familiaux</code></li> <li><code>traitements entr\u00e9e</code></li> <li><code>conclusion</code></li> <li><code>conclusion entr\u00e9e</code></li> <li><code>habitus</code></li> <li><code>correspondants</code></li> <li><code>diagnostic</code></li> <li><code>donn\u00e9es biom\u00e9triques entr\u00e9e</code></li> <li><code>examens</code></li> <li><code>examens compl\u00e9mentaires</code></li> <li><code>facteurs de risques</code></li> <li><code>histoire de la maladie</code></li> <li><code>actes</code></li> <li><code>motif</code></li> <li><code>prescriptions</code></li> <li><code>traitements sortie</code></li> <li><code>evolution</code></li> <li><code>modalites sortie</code></li> <li><code>vaccinations</code></li> <li><code>introduction</code></li> </ul> <p>Remarks :</p> <ul> <li>section <code>introduction</code> corresponds to the span of text between the header   \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document)   and the title of the following detected section</li> <li>this matcher works well for hospitalization summaries (CRH), but not necessarily   for all types of documents (in particular for emergency or scan summaries   CR-IMAGERIE)</li> </ul> <p>Experimental</p> <p>Should you rely on <code>eds.sections</code> for critical downstream tasks, make sure to validate the results to make sure that the component works in your case.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher--examples","title":"Examples","text":"<p>The following snippet detects section titles. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\n\ntext = \"\"\"\nCRU du 10/09/2021\nMotif :\nPatient admis pour suspicion de COVID\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"section_titles\"]\n# Out: [Motif]\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.sections</code> matcher adds two fields to the <code>doc.spans</code> attribute :</p> <ol> <li>The <code>section_titles</code> key contains the list of all section titles extracted using    the list declared in the <code>terms.py</code> module.</li> <li>The <code>sections</code> key contains a list of sections, ie spans of text between two    section titles (or the last title and the end of the document).</li> </ol> <p>If the document has entities before calling this matcher an attribute <code>section</code> is added to each entity.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>sections</code> <p>Dictionary of terms to look for.</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'allergies': ['allergies'], 'ant\u00e9c\u00e9dents': ['a...</code> </p> <code>attr</code> <p>Default attribute to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>add_patterns</code> <p>Whether add update patterns to match start / end of lines</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sections</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher.__call__","title":"<code>__call__</code>","text":"<p>Divides the doc into sections</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for sections</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/misc/tables/","title":"<code>edsnlp.pipes.misc.tables</code>","text":""},{"location":"reference/edsnlp/pipes/misc/tables/factory/","title":"<code>edsnlp.pipes.misc.tables.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component","title":"<code>create_component = registry.factory.register('eds.tables', assigns=['doc.spans', 'doc.ents'], deprecated=['tables'])(TablesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.tables</code> matcher detects tables in a documents.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component--examples","title":"Examples","text":"<p><pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.tables())\n\ntext = \"\"\"\nSERVICE\nMEDECINE INTENSIVE \u2013\nREANIMATION\nR\u00e9animation / Surveillance Continue\nM\u00e9dicale\n\nCOMPTE RENDU D'HOSPITALISATION du 05/06/2020 au 10/06/2020\nMadame DUPONT Marie, n\u00e9e le 16/05/1900, \u00e2g\u00e9e de 20 ans, a \u00e9t\u00e9 hospitalis\u00e9e en\nr\u00e9animation du 05/06/1920 au 10/06/1920 pour intoxication m\u00e9dicamenteuse volontaire.\n\nExamens compl\u00e9mentaires\nH\u00e9matologie\nNum\u00e9ration\nLeucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\nH\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\nH\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\nH\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\nVGM \u00a6fL \u00a694.4 + \u00a679.6-94\nTCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\nCCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\nPlaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\nVMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\nSur le plan neurologique : Devant la persistance d'une confusion \u00e0 distance de\nl'intoxication au\n...\n\n2/2Pat : &lt;NOM&gt; &lt;Prenom&gt;|F |&lt;date&gt; | &lt;ipp&gt; |Intitul\u00e9 RCP\n\"\"\"\n\ndoc = nlp(text)\n\n# A table span\ntable = doc.spans[\"tables\"][0]\n\n# Leucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\n# H\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\n# H\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\n# H\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\n# VGM \u00a6fL \u00a694.4 + \u00a679.6-94\n# TCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\n# CCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\n# Plaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\n# VMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\n# Convert span to Pandas table\ndf = table._.to_pd_table(\n    as_spans=False,  # set True to set the table cells as spans instead of strings\n    header=False,  # set True to use the first row as header\n    index=False,  # set True to use the first column as index\n)\ntype(df)\n# Out: pandas.core.frame.DataFrame\n</code></pre> The pandas DataFrame:</p> 0 1 2 3 0 Leucocytes x10*9/L 4.97 4.09-11 1 H\u00e9maties x10*12/L 4.68 4.53-5.79 2 H\u00e9moglobine g/dL 14.8 13.4-16.7 3 H\u00e9matocrite % 44.2 39.2-48.6 4 VGM fL 94.4 + 79.6-94 5 TCMH pg 31.6 27.3-32.8 6 CCMH g/dL 33.5 32.4-36.3 7 Plaquettes x10*9/L 191 172-398 8 VMP fL 11.5 + 7.4-10.8"},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.tables</code> pipeline declares the <code>span._.to_pd_table()</code> Span extension. This function returns a parsed pandas version of the table.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'tables'</code> </p> <code>tables_pattern</code> <p>The regex pattern to identify tables. The key of dictionary should be <code>tables</code></p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> <code>sep_pattern</code> <p>The regex pattern to identify the separator pattern. Used when calling <code>to_pd_table</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tables</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/patterns/","title":"<code>edsnlp.pipes.misc.tables.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/tables/tables/","title":"<code>edsnlp.pipes.misc.tables.tables</code>","text":""},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher","title":"<code>TablesMatcher</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>The <code>eds.tables</code> matcher detects tables in a documents.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher--examples","title":"Examples","text":"<p><pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.tables())\n\ntext = \"\"\"\nSERVICE\nMEDECINE INTENSIVE \u2013\nREANIMATION\nR\u00e9animation / Surveillance Continue\nM\u00e9dicale\n\nCOMPTE RENDU D'HOSPITALISATION du 05/06/2020 au 10/06/2020\nMadame DUPONT Marie, n\u00e9e le 16/05/1900, \u00e2g\u00e9e de 20 ans, a \u00e9t\u00e9 hospitalis\u00e9e en\nr\u00e9animation du 05/06/1920 au 10/06/1920 pour intoxication m\u00e9dicamenteuse volontaire.\n\nExamens compl\u00e9mentaires\nH\u00e9matologie\nNum\u00e9ration\nLeucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\nH\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\nH\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\nH\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\nVGM \u00a6fL \u00a694.4 + \u00a679.6-94\nTCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\nCCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\nPlaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\nVMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\nSur le plan neurologique : Devant la persistance d'une confusion \u00e0 distance de\nl'intoxication au\n...\n\n2/2Pat : &lt;NOM&gt; &lt;Prenom&gt;|F |&lt;date&gt; | &lt;ipp&gt; |Intitul\u00e9 RCP\n\"\"\"\n\ndoc = nlp(text)\n\n# A table span\ntable = doc.spans[\"tables\"][0]\n\n# Leucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\n# H\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\n# H\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\n# H\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\n# VGM \u00a6fL \u00a694.4 + \u00a679.6-94\n# TCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\n# CCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\n# Plaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\n# VMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\n# Convert span to Pandas table\ndf = table._.to_pd_table(\n    as_spans=False,  # set True to set the table cells as spans instead of strings\n    header=False,  # set True to use the first row as header\n    index=False,  # set True to use the first column as index\n)\ntype(df)\n# Out: pandas.core.frame.DataFrame\n</code></pre> The pandas DataFrame:</p> 0 1 2 3 0 Leucocytes x10*9/L 4.97 4.09-11 1 H\u00e9maties x10*12/L 4.68 4.53-5.79 2 H\u00e9moglobine g/dL 14.8 13.4-16.7 3 H\u00e9matocrite % 44.2 39.2-48.6 4 VGM fL 94.4 + 79.6-94 5 TCMH pg 31.6 27.3-32.8 6 CCMH g/dL 33.5 32.4-36.3 7 Plaquettes x10*9/L 191 172-398 8 VMP fL 11.5 + 7.4-10.8"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.tables</code> pipeline declares the <code>span._.to_pd_table()</code> Span extension. This function returns a parsed pandas version of the table.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'tables'</code> </p> <code>tables_pattern</code> <p>The regex pattern to identify tables. The key of dictionary should be <code>tables</code></p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> <code>sep_pattern</code> <p>The regex pattern to identify the separator pattern. Used when calling <code>to_pd_table</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tables</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.set_extensions","title":"<code>set_extensions</code>  <code>classmethod</code>","text":"<p>Set extensions for the tables pipeline.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.get_table","title":"<code>get_table</code>","text":"<p>Convert spans of tables to dictionaries</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.get_table--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>table</code> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.__call__","title":"<code>__call__</code>","text":"<p>Find spans that contain tables</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.to_pd_table","title":"<code>to_pd_table</code>","text":"<p>Return pandas DataFrame</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.to_pd_table--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>The span containing the table</p> <p> TYPE: <code>Span</code> </p> <code>as_spans</code> <p>Whether to return the table cells as spans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>header</code> <p>Whether the table has a header</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>index</code> <p>Whether the table has an index</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/ner/","title":"<code>edsnlp.pipes.ner</code>","text":""},{"location":"reference/edsnlp/pipes/ner/adicap/","title":"<code>edsnlp.pipes.ner.adicap</code>","text":""},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/","title":"<code>edsnlp.pipes.ner.adicap.adicap</code>","text":"<p><code>eds.adicap</code> pipeline</p> <ol><li><p><p>sant\u00e9 A., 2019. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. http://esante.gouv.fr/terminologie-adicap</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher","title":"<code>AdicapMatcher</code>","text":"<p>           Bases: <code>ContextualMatcher</code></p> <p>The <code>eds.adicap</code> pipeline component matches the ADICAP codes. It was developped to run on anapathology reports.</p> <p>Document type</p> <p>It was developped to work on anapathology reports. We recommend also to use the <code>eds</code> language (<code>edsnlp.blank(\"eds\")</code>)</p> <p>The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes:</p> Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type <p>The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\".</p> <p>Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor.</p> <p>For further details about the ADICAP code follow this link.</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.adicap())\n\ntext = \"\"\"\nCOMPTE RENDU D\u2019EXAMEN\n\nAnt\u00e9riorit\u00e9(s) :  NEANT\n\n\nRenseignements cliniques :\nContexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-\nexterne du sein droit. La l\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion\ndu quadrant sup\u00e9ro-externe, \u00e0 l'union des quadrants inf\u00e9rieurs.\n\n\nMacrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants\ninf\u00e9rieurs du sein droit, mesurant 4 mm, class\u00e9e ACR4\n\n14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il\nn'y a pas eu d'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en\ntotalit\u00e9 et coup\u00e9s sur plusieurs niveaux.\nHistologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois\nl\u00e9g\u00e8rement dystrophique avec quelques petits kystes. Il n'y a pas d'hyperplasie\n\u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration tumorale. On note quelques\nsuffusions h\u00e9morragiques focales.\n\nConclusion :\nL\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit.\nAbsence d'atypies ou de prolif\u00e9ration tumorale.\n\nCodification :   BHGS0040\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (BHGS0040,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: adicap\n\nent._.adicap.dict()\n# Out: {'code': 'BHGS0040',\n# 'sampling_mode': 'BIOPSIE CHIRURGICALE',\n# 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION',\n# 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\",\n# 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE',\n# 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE',\n# 'behaviour_type': 'CARACTERES GENERAUX'}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adicap'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>([A-Z]\\.?[A-Z]\\.?[A-Z]{2}\\.?(?:\\d{4}|\\d{4}|[A-Z...</code> </p> <code>prefix</code> <p>The regex pattern to use for matching the prefix before ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?i)(codification|adicap)</code> </p> <code>window</code> <p>Number of tokens to look for prefix. It will never go further the start of the sentence</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>adicap</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'adicap': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.adicap</code> pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\", sant\u00e9, 2019)</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher.process","title":"<code>process</code>","text":"<p>Tags ADICAP mentions.</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for ADICAP</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/","title":"<code>edsnlp.pipes.ner.adicap.factory</code>","text":"<ol><li><p><p>sant\u00e9 A., 2019. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. http://esante.gouv.fr/terminologie-adicap</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/#edsnlp.pipes.ner.adicap.factory.create_component","title":"<code>create_component = registry.factory.register('eds.adicap', assigns=['doc.ents', 'doc.spans'])(AdicapMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.adicap</code> pipeline component matches the ADICAP codes. It was developped to run on anapathology reports.</p> <p>Document type</p> <p>It was developped to work on anapathology reports. We recommend also to use the <code>eds</code> language (<code>edsnlp.blank(\"eds\")</code>)</p> <p>The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes:</p> Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type <p>The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\".</p> <p>Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor.</p> <p>For further details about the ADICAP code follow this link.</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/#edsnlp.pipes.ner.adicap.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.adicap())\n\ntext = \"\"\"\nCOMPTE RENDU D\u2019EXAMEN\n\nAnt\u00e9riorit\u00e9(s) :  NEANT\n\n\nRenseignements cliniques :\nContexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-\nexterne du sein droit. La l\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion\ndu quadrant sup\u00e9ro-externe, \u00e0 l'union des quadrants inf\u00e9rieurs.\n\n\nMacrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants\ninf\u00e9rieurs du sein droit, mesurant 4 mm, class\u00e9e ACR4\n\n14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il\nn'y a pas eu d'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en\ntotalit\u00e9 et coup\u00e9s sur plusieurs niveaux.\nHistologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois\nl\u00e9g\u00e8rement dystrophique avec quelques petits kystes. Il n'y a pas d'hyperplasie\n\u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration tumorale. On note quelques\nsuffusions h\u00e9morragiques focales.\n\nConclusion :\nL\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit.\nAbsence d'atypies ou de prolif\u00e9ration tumorale.\n\nCodification :   BHGS0040\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (BHGS0040,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: adicap\n\nent._.adicap.dict()\n# Out: {'code': 'BHGS0040',\n# 'sampling_mode': 'BIOPSIE CHIRURGICALE',\n# 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION',\n# 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\",\n# 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE',\n# 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE',\n# 'behaviour_type': 'CARACTERES GENERAUX'}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/#edsnlp.pipes.ner.adicap.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adicap'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>([A-Z]\\.?[A-Z]\\.?[A-Z]{2}\\.?(?:\\d{4}|\\d{4}|[A-Z...</code> </p> <code>prefix</code> <p>The regex pattern to use for matching the prefix before ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?i)(codification|adicap)</code> </p> <code>window</code> <p>Number of tokens to look for prefix. It will never go further the start of the sentence</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>adicap</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'adicap': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/#edsnlp.pipes.ner.adicap.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.adicap</code> pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\", sant\u00e9, 2019)</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/models/","title":"<code>edsnlp.pipes.ner.adicap.models</code>","text":""},{"location":"reference/edsnlp/pipes/ner/adicap/patterns/","title":"<code>edsnlp.pipes.ner.adicap.patterns</code>","text":"<p>Source : https://esante.gouv.fr/sites/default/files/media_entity/documents/cgts_sem_adicap_fiche-detaillee.pdf</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/","title":"<code>edsnlp.pipes.ner.behaviors</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/","title":"<code>edsnlp.pipes.ner.behaviors.alcohol</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/","title":"<code>edsnlp.pipes.ner.behaviors.alcohol.alcohol</code>","text":"<p><code>eds.alcohol</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher","title":"<code>AlcoholMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.alcohol</code> pipeline component extracts mentions of alcohol consumption. It won't match occasional consumption, nor acute intoxication.</p> Details of the used patterns <pre><code># fmt: off\ndefault_patterns = dict(\n    source=\"alcohol\",\n    regex=[\n        r\"\\balco[ol]\",\n        r\"\\bethyl\",\n        r\"(?&lt;!(25.?)|(sevrage)).?\\boh\\b\",\n        r\"exogenose\",\n        r\"delirium.tremens\",\n    ],\n    exclude=[\n        dict(\n            regex=[\n                \"occasion\",\n                \"episod\",\n                \"festi\",\n                \"rare\",\n                \"libre\",  # OH-libres\n                \"aigu\",\n            ],\n            window=(-3, 5),\n        ),\n        dict(\n            regex=[\"pansement\", \"compress\"],\n            window=-3,\n        ),\n    ],\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"stopped\",\n            regex=r\"(?&lt;!non )(?&lt;!pas )(sevr|arret|stop|ancien)\",\n            window=(-3, 5),\n        ),\n        dict(\n            name=\"zero_after\",\n            regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|oui|non(?! sevr)))\",\n            window=6,\n        ),\n    ],\n)\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"PRESENT\"</code></li> <li><code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>\"ABSENT\"</code> if the patient has no alcohol dependence</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.alcohol\")\n</code></pre> <p>Below are a few examples:</p> 12345678 <pre><code>text = \"Patient alcoolique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [alcoolique]\n</code></pre> <pre><code>text = \"OH chronique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [OH]\n</code></pre> <pre><code>text = \"Prise d'alcool occasionnelle\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Application d'un pansement alcoolis\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Alcoolisme sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevr\u00e9]}\n</code></pre> <pre><code>text = \"Alcoolisme non sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme]\n</code></pre> <pre><code>text = \"Alcool: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcool: 0]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'zero_after': [0]}\n</code></pre> <pre><code>text = \"Le patient est en cours de sevrage \u00e9thylotabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [sevrage \u00e9thylotabagique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevrage]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>{'source': 'alcohol', 'regex': ['\\\\balco[ol]', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>alcohol</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'alcohol': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.alcohol</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/","title":"<code>edsnlp.pipes.ner.behaviors.alcohol.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component","title":"<code>create_component = registry.factory.register('eds.alcohol', assigns=['doc.ents', 'doc.spans'])(AlcoholMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.alcohol</code> pipeline component extracts mentions of alcohol consumption. It won't match occasional consumption, nor acute intoxication.</p> Details of the used patterns <pre><code># fmt: off\ndefault_patterns = dict(\n    source=\"alcohol\",\n    regex=[\n        r\"\\balco[ol]\",\n        r\"\\bethyl\",\n        r\"(?&lt;!(25.?)|(sevrage)).?\\boh\\b\",\n        r\"exogenose\",\n        r\"delirium.tremens\",\n    ],\n    exclude=[\n        dict(\n            regex=[\n                \"occasion\",\n                \"episod\",\n                \"festi\",\n                \"rare\",\n                \"libre\",  # OH-libres\n                \"aigu\",\n            ],\n            window=(-3, 5),\n        ),\n        dict(\n            regex=[\"pansement\", \"compress\"],\n            window=-3,\n        ),\n    ],\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"stopped\",\n            regex=r\"(?&lt;!non )(?&lt;!pas )(sevr|arret|stop|ancien)\",\n            window=(-3, 5),\n        ),\n        dict(\n            name=\"zero_after\",\n            regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|oui|non(?! sevr)))\",\n            window=6,\n        ),\n    ],\n)\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"PRESENT\"</code></li> <li><code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>\"ABSENT\"</code> if the patient has no alcohol dependence</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.alcohol\")\n</code></pre> <p>Below are a few examples:</p> 12345678 <pre><code>text = \"Patient alcoolique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [alcoolique]\n</code></pre> <pre><code>text = \"OH chronique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [OH]\n</code></pre> <pre><code>text = \"Prise d'alcool occasionnelle\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Application d'un pansement alcoolis\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Alcoolisme sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevr\u00e9]}\n</code></pre> <pre><code>text = \"Alcoolisme non sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme]\n</code></pre> <pre><code>text = \"Alcool: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcool: 0]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'zero_after': [0]}\n</code></pre> <pre><code>text = \"Le patient est en cours de sevrage \u00e9thylotabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [sevrage \u00e9thylotabagique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevrage]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>{'source': 'alcohol', 'regex': ['\\\\balco[ol]', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>alcohol</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'alcohol': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.alcohol</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/patterns/","title":"<code>edsnlp.pipes.ner.behaviors.alcohol.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/","title":"<code>edsnlp.pipes.ner.behaviors.tobacco</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/","title":"<code>edsnlp.pipes.ner.behaviors.tobacco.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component","title":"<code>create_component = registry.factory.register('eds.tobacco', assigns=['doc.ents', 'doc.spans'])(TobaccoMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.tobacco</code> pipeline component extracts mentions of tobacco consumption.</p> Details of the used patterns <pre><code># fmt: off\nPA = r\"(?:\\bp/?a\\b|paquets?.?annee)\"\nQUANTITY = r\"(?P&lt;quantity&gt;[\\d]{1,3})\"\nPUNCT = r\"\\.,-;\\(\\)\"\n\ndefault_patterns = [\n    dict(\n        source=\"tobacco\",\n        regex=[\n            r\"tabagi\",\n            r\"tabac\",\n            r\"\\bfume\\b\",\n            r\"\\bfumeu\",\n            r\"\\bpipes?\\b\",\n        ],\n        exclude=dict(\n            regex=[\n                \"occasion\",\n                \"moder\",\n                \"quelqu\",\n                \"festi\",\n                \"rare\",\n                \"sujet\",  # Example : Chez le sujet fumeur ... generic sentences\n            ],\n            window=(-3, 5),\n        ),\n        regex_attr=\"NORM\",\n        assign=[\n            dict(\n                name=\"stopped\",\n                regex=r\"(?&lt;!non )(?&lt;!pas )(\\bex\\b|sevr|arret|stop|ancien)\",\n                window=(-3, 15),\n            ),\n            dict(\n                name=\"zero_after\",\n                regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non(?! sevr)))\",\n                window=6,\n            ),\n            dict(\n                name=\"PA\",\n                regex=rf\"{QUANTITY}[^{PUNCT}]{{0,10}}{PA}|{PA}[^{PUNCT}]{{0,10}}{QUANTITY}\",\n                window=(-10, 10),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"secondhand\",\n                regex=\"(passif)\",\n                window=5,\n                reduce_mode=\"keep_first\",\n            ),\n        ],\n    )\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"PRESENT\"</code></li> <li><code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>\"ABSENT\"</code> if the patient has no tobacco dependence</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>PA</code>: the mentioned year-pack (= paquet-ann\u00e9e)</li> <li><code>secondhand</code>: if secondhand smoking</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.tobacco())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Tabagisme \u00e9valu\u00e9 \u00e0 15 PA\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme \u00e9valu\u00e9 \u00e0 15 PA]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'PA': 15}\n</code></pre> <pre><code>text = \"Patient tabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagique]\n</code></pre> <pre><code>text = \"Tabagisme festif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"On a un tabagisme ancien\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagisme ancien]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [ancien]}\n</code></pre> <pre><code>text = \"Tabac: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: 0]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'zero_after': [0]}\n</code></pre> <pre><code>text = \"Tabagisme passif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme passif]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'secondhand': passif}\n</code></pre> <pre><code>text = \"Tabac: sevr\u00e9 depuis 5 ans\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevr\u00e9]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'tobacco', 'regex': ['tabagi', 'tab...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tobacco</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tobacco': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tobacco</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/patterns/","title":"<code>edsnlp.pipes.ner.behaviors.tobacco.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/","title":"<code>edsnlp.pipes.ner.behaviors.tobacco.tobacco</code>","text":"<p><code>eds.tobacco</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher","title":"<code>TobaccoMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.tobacco</code> pipeline component extracts mentions of tobacco consumption.</p> Details of the used patterns <pre><code># fmt: off\nPA = r\"(?:\\bp/?a\\b|paquets?.?annee)\"\nQUANTITY = r\"(?P&lt;quantity&gt;[\\d]{1,3})\"\nPUNCT = r\"\\.,-;\\(\\)\"\n\ndefault_patterns = [\n    dict(\n        source=\"tobacco\",\n        regex=[\n            r\"tabagi\",\n            r\"tabac\",\n            r\"\\bfume\\b\",\n            r\"\\bfumeu\",\n            r\"\\bpipes?\\b\",\n        ],\n        exclude=dict(\n            regex=[\n                \"occasion\",\n                \"moder\",\n                \"quelqu\",\n                \"festi\",\n                \"rare\",\n                \"sujet\",  # Example : Chez le sujet fumeur ... generic sentences\n            ],\n            window=(-3, 5),\n        ),\n        regex_attr=\"NORM\",\n        assign=[\n            dict(\n                name=\"stopped\",\n                regex=r\"(?&lt;!non )(?&lt;!pas )(\\bex\\b|sevr|arret|stop|ancien)\",\n                window=(-3, 15),\n            ),\n            dict(\n                name=\"zero_after\",\n                regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non(?! sevr)))\",\n                window=6,\n            ),\n            dict(\n                name=\"PA\",\n                regex=rf\"{QUANTITY}[^{PUNCT}]{{0,10}}{PA}|{PA}[^{PUNCT}]{{0,10}}{QUANTITY}\",\n                window=(-10, 10),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"secondhand\",\n                regex=\"(passif)\",\n                window=5,\n                reduce_mode=\"keep_first\",\n            ),\n        ],\n    )\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"PRESENT\"</code></li> <li><code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>\"ABSENT\"</code> if the patient has no tobacco dependence</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>PA</code>: the mentioned year-pack (= paquet-ann\u00e9e)</li> <li><code>secondhand</code>: if secondhand smoking</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.tobacco())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Tabagisme \u00e9valu\u00e9 \u00e0 15 PA\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme \u00e9valu\u00e9 \u00e0 15 PA]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'PA': 15}\n</code></pre> <pre><code>text = \"Patient tabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagique]\n</code></pre> <pre><code>text = \"Tabagisme festif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"On a un tabagisme ancien\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagisme ancien]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [ancien]}\n</code></pre> <pre><code>text = \"Tabac: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: 0]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'zero_after': [0]}\n</code></pre> <pre><code>text = \"Tabagisme passif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme passif]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSENT\n\nspan._.assigned\n# Out: {'secondhand': passif}\n</code></pre> <pre><code>text = \"Tabac: sevr\u00e9 depuis 5 ans\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': [sevr\u00e9]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'tobacco', 'regex': ['tabagi', 'tab...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tobacco</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tobacco': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tobacco</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/cim10/","title":"<code>edsnlp.pipes.ner.cim10</code>","text":""},{"location":"reference/edsnlp/pipes/ner/cim10/factory/","title":"<code>edsnlp.pipes.ner.cim10.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/cim10/factory/#edsnlp.pipes.ner.cim10.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.cim10</code> pipeline component extract terms from documents using the CIM10 (French-language ICD) terminology as a reference.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"reference/edsnlp/pipes/ner/cim10/factory/#edsnlp.pipes.ner.cim10.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.cim10(term_matcher=\"simstring\"))\n\ntext = \"Le patient est suivi pour fi\u00e8vres typho\u00efde et paratypho\u00efde.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (fi\u00e8vres typho\u00efde et paratypho\u00efde,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: cim10\n\nent.kb_id_\n# Out: A01\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/cim10/factory/#edsnlp.pipes.ner.cim10.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cim10'</code> </p> <code>attr</code> <p>The default attribute to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cim10'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cim10': True}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/cim10/factory/#edsnlp.pipes.ner.cim10.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cim10</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/ner/cim10/patterns/","title":"<code>edsnlp.pipes.ner.cim10.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/covid/","title":"<code>edsnlp.pipes.ner.covid</code>","text":""},{"location":"reference/edsnlp/pipes/ner/covid/factory/","title":"<code>edsnlp.pipes.ner.covid.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/covid/factory/#edsnlp.pipes.ner.covid.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.covid</code> pipeline component detects mentions of COVID19.</p>"},{"location":"reference/edsnlp/pipes/ner/covid/factory/#edsnlp.pipes.ner.covid.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n\ntext = \"Le patient est admis pour une infection au coronavirus.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (infection au coronavirus,)\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/covid/factory/#edsnlp.pipes.ner.covid.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'covid'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'LOWER'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patterns</code> <p>The regex pattern to use</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns</code> </p> <code>label</code> <p>Label to use for matches</p> <p> TYPE: <code>str</code> DEFAULT: <code>'covid'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'covid': True}</code> </p> RETURNS DESCRIPTION <code>GenericMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/covid/factory/#edsnlp.pipes.ner.covid.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.covid</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/ner/covid/patterns/","title":"<code>edsnlp.pipes.ner.covid.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/","title":"<code>edsnlp.pipes.ner.disorders</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/aids/","title":"<code>edsnlp.pipes.ner.disorders.aids</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/","title":"<code>edsnlp.pipes.ner.disorders.aids.aids</code>","text":"<p><code>eds.aids</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher","title":"<code>AIDSMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.aids</code> pipeline component extracts mentions of AIDS. It will notably match:</p> <ul> <li>Mentions of VIH/HIV at the SIDA/AIDS stage</li> <li>Mentions of VIH/HIV with opportunistic(s) infection(s)</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre> <p>On HIV infection</p> <p>pre-AIDS HIV infection are not extracted, only AIDS.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>opportunist</code>: list of opportunist infections extracted around the HIV mention</li> <li><code>stage</code>: stage of the HIV infection</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.aids\")\n</code></pre> <p>Below are a few examples:</p> SIDAVIHCoinfectionVIH stade SIDA <pre><code>text = \"Patient atteint du VIH au stade SIDA.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH au stade SIDA]\n</code></pre> <pre><code>text = \"Patient atteint du VIH.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a un VIH avec coinfection pneumocystose\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'opportunist': [coinfection, pneumocystose]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un VIH stade C\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': [C]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'aids'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'aids', 'regex': ['(vih.{1,5}stade....</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>aids</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'aids': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.aids</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/","title":"<code>edsnlp.pipes.ner.disorders.aids.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component","title":"<code>create_component = registry.factory.register('eds.aids', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.AIDS'])(AIDSMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.aids</code> pipeline component extracts mentions of AIDS. It will notably match:</p> <ul> <li>Mentions of VIH/HIV at the SIDA/AIDS stage</li> <li>Mentions of VIH/HIV with opportunistic(s) infection(s)</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre> <p>On HIV infection</p> <p>pre-AIDS HIV infection are not extracted, only AIDS.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>opportunist</code>: list of opportunist infections extracted around the HIV mention</li> <li><code>stage</code>: stage of the HIV infection</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.aids\")\n</code></pre> <p>Below are a few examples:</p> SIDAVIHCoinfectionVIH stade SIDA <pre><code>text = \"Patient atteint du VIH au stade SIDA.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH au stade SIDA]\n</code></pre> <pre><code>text = \"Patient atteint du VIH.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a un VIH avec coinfection pneumocystose\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'opportunist': [coinfection, pneumocystose]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un VIH stade C\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': [C]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'aids'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'aids', 'regex': ['(vih.{1,5}stade....</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>aids</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'aids': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.aids</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/patterns/","title":"<code>edsnlp.pipes.ner.disorders.aids.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/base/","title":"<code>edsnlp.pipes.ner.disorders.base</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/base/#edsnlp.pipes.ner.disorders.base.DisorderMatcher","title":"<code>DisorderMatcher</code>","text":"<p>           Bases: <code>ContextualMatcher</code></p> <p>Base class used to implement various disorders or behaviors extraction pipes</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/base/#edsnlp.pipes.ner.disorders.base.DisorderMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> </p> <code>patterns</code> <p>The configuration dictionary</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>detailed_status_mapping</code> <p>Mapping from integer status (0, 1 or 2) to human-readable string</p> <p> TYPE: <code>Dict[int, str]</code> DEFAULT: <code>{0: 'ABSENT', 1: 'PRESENT'}</code> </p> <p>alignment_mode : str     Overwrite alignment mode. regex_flags : Union[re.RegexFlag, int]     RegExp flags to use when matching, filtering and assigning (See     the re docs)</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/base/#edsnlp.pipes.ner.disorders.base.DisorderMatcher.__call__","title":"<code>__call__</code>","text":"<p>Tags entities.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/base/#edsnlp.pipes.ner.disorders.base.DisorderMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>annotated spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/","title":"<code>edsnlp.pipes.ner.disorders.cerebrovascular_accident</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/","title":"<code>edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident</code>","text":"<p><code>eds.cerebrovascular_accident</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher","title":"<code>CerebrovascularAccidentMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.cerebrovascular_accident</code> pipeline component extracts mentions of cerebrovascular accident. It will notably match:</p> <ul> <li>Mentions of AVC/AIT</li> <li>Mentions of bleeding, hemorrhage, thrombus, ischemia, etc., localized in the brain</li> </ul> Details of the used patterns <pre><code># fmt: off\nimport re\n\nfrom edsnlp.utils.resources import get_AVC_care_site\n\nfrom ..terms import BRAIN, HEART, PERIPHERAL\n\nAVC_CARE_SITES_REGEX = [\n    r\"\\b\" + re.escape(cs.strip()) + r\"\\b\" for cs in get_AVC_care_site(prefix=True)\n] + [\n    r\"h[o\u00f4]p\",\n    r\"\\brcp\",\n    r\"service\",\n    r\"\\bsau\",\n    r\"ap.?hp\",\n    r\"\\burg\",\n    r\"finess\",\n    r\"\\bsiret\",\n    r\"[\u00e0a] avc\",\n    r\"consult\",\n]\n\navc = dict(\n    source=\"avc\",\n    regex=[\n        r\"\\bavc\\b\",\n    ],\n    exclude=[\n        dict(\n            regex=AVC_CARE_SITES_REGEX,\n            window=(-5, 5),\n            regex_flags=re.S | re.I,\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=r\"\\b[a-z]\\.\",\n            window=2,\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"(hemorr?agie|hematome)\",\n        r\"angiopath\",\n        r\"angioplasti\",\n        r\"infarctus\",\n        r\"occlusion\",\n        r\"saignement\",\n        r\"embol\",\n        r\"vascularite\",\n        r\"\\bhsd\\b\",\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"phleb\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=[\n        dict(\n            regex=r\"pulmo|poumon\",\n            window=4,\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain_localized\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-15, 15),\n            limit_to_sentence=False,\n            include_assigned=False,\n        ),\n    ],\n)\n\ngeneral = dict(\n    source=\"general\",\n    regex=[\n        r\"accident.{1,5}vasculaires.{1,5}cereb\",\n        r\"accident.{1,5}vasculaire.{1,5}ischemi\",\n        r\"accident.{1,5}ischemi\",\n        r\"moya.?moya\",\n        r\"occlusion.{1,5}(artere|veine).{1,20}retine\",\n        r\"vasculopathies?.cerebrales?.ischemique\",\n        r\"maladies?.des.petites.arteres\",\n        r\"maladies?.des.petits.vaisseaux\",\n        r\"thrombolyse\",\n        r\"\\bsusac\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAIT = dict(\n    source=\"AIT\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bAIT\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=PERIPHERAL + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-10, 15),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    avc,\n    with_localization,\n    general,\n    acronym,\n    AIT,\n    ischemia,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.cerebrovascular_accident())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Patient hospitalis\u00e9 \u00e0 AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Hospitalisation pour un AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [AVC]\n</code></pre> <pre><code>text = \"Saignement intracranien\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Saignement]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [intracranien]}\n</code></pre> <pre><code>text = \"Thrombose p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Thrombose sylvienne\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Thrombose]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [sylvienne]}\n</code></pre> <pre><code>text = \"Infarctus c\u00e9r\u00e9bral\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Infarctus]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [c\u00e9r\u00e9bral]}\n</code></pre> <pre><code>text = \"Soign\u00e9 via un thrombolyse\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [thrombolyse]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'cerebrovascular_accident'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'avc', 'regex': ['\\\\bavc\\\\b'], 'exc...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>cerebrovascular_accident</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cerebrovascular_accident': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cerebrovascular_accident</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/","title":"<code>edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component","title":"<code>create_component = registry.factory.register('eds.cerebrovascular_accident', assigns=['doc.ents', 'doc.spans'])(CerebrovascularAccidentMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.cerebrovascular_accident</code> pipeline component extracts mentions of cerebrovascular accident. It will notably match:</p> <ul> <li>Mentions of AVC/AIT</li> <li>Mentions of bleeding, hemorrhage, thrombus, ischemia, etc., localized in the brain</li> </ul> Details of the used patterns <pre><code># fmt: off\nimport re\n\nfrom edsnlp.utils.resources import get_AVC_care_site\n\nfrom ..terms import BRAIN, HEART, PERIPHERAL\n\nAVC_CARE_SITES_REGEX = [\n    r\"\\b\" + re.escape(cs.strip()) + r\"\\b\" for cs in get_AVC_care_site(prefix=True)\n] + [\n    r\"h[o\u00f4]p\",\n    r\"\\brcp\",\n    r\"service\",\n    r\"\\bsau\",\n    r\"ap.?hp\",\n    r\"\\burg\",\n    r\"finess\",\n    r\"\\bsiret\",\n    r\"[\u00e0a] avc\",\n    r\"consult\",\n]\n\navc = dict(\n    source=\"avc\",\n    regex=[\n        r\"\\bavc\\b\",\n    ],\n    exclude=[\n        dict(\n            regex=AVC_CARE_SITES_REGEX,\n            window=(-5, 5),\n            regex_flags=re.S | re.I,\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=r\"\\b[a-z]\\.\",\n            window=2,\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"(hemorr?agie|hematome)\",\n        r\"angiopath\",\n        r\"angioplasti\",\n        r\"infarctus\",\n        r\"occlusion\",\n        r\"saignement\",\n        r\"embol\",\n        r\"vascularite\",\n        r\"\\bhsd\\b\",\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"phleb\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=[\n        dict(\n            regex=r\"pulmo|poumon\",\n            window=4,\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain_localized\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-15, 15),\n            limit_to_sentence=False,\n            include_assigned=False,\n        ),\n    ],\n)\n\ngeneral = dict(\n    source=\"general\",\n    regex=[\n        r\"accident.{1,5}vasculaires.{1,5}cereb\",\n        r\"accident.{1,5}vasculaire.{1,5}ischemi\",\n        r\"accident.{1,5}ischemi\",\n        r\"moya.?moya\",\n        r\"occlusion.{1,5}(artere|veine).{1,20}retine\",\n        r\"vasculopathies?.cerebrales?.ischemique\",\n        r\"maladies?.des.petites.arteres\",\n        r\"maladies?.des.petits.vaisseaux\",\n        r\"thrombolyse\",\n        r\"\\bsusac\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAIT = dict(\n    source=\"AIT\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bAIT\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=PERIPHERAL + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-10, 15),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    avc,\n    with_localization,\n    general,\n    acronym,\n    AIT,\n    ischemia,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.cerebrovascular_accident())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Patient hospitalis\u00e9 \u00e0 AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Hospitalisation pour un AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [AVC]\n</code></pre> <pre><code>text = \"Saignement intracranien\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Saignement]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [intracranien]}\n</code></pre> <pre><code>text = \"Thrombose p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Thrombose sylvienne\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Thrombose]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [sylvienne]}\n</code></pre> <pre><code>text = \"Infarctus c\u00e9r\u00e9bral\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Infarctus]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [c\u00e9r\u00e9bral]}\n</code></pre> <pre><code>text = \"Soign\u00e9 via un thrombolyse\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [thrombolyse]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'cerebrovascular_accident'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'avc', 'regex': ['\\\\bavc\\\\b'], 'exc...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>cerebrovascular_accident</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cerebrovascular_accident': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cerebrovascular_accident</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/patterns/","title":"<code>edsnlp.pipes.ner.disorders.cerebrovascular_accident.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/","title":"<code>edsnlp.pipes.ner.disorders.ckd</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/","title":"<code>edsnlp.pipes.ner.disorders.ckd.ckd</code>","text":"<p><code>eds.ckd</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher","title":"<code>CKDMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.CKD</code> pipeline component extracts mentions of CKD (Chronic Kidney Disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Kidney transplantation</li> <li>Chronic dialysis</li> <li>Renal failure from stage 3 to 5. The stage is extracted by trying 3 methods:<ul> <li>Extracting the mentioned stage directly (\"IRC stade IV\")</li> <li>Extracting the severity directly (\"IRC terminale\")</li> <li>Extracting the mentioned GFR (DFG in french) (\"IRC avec DFG estim\u00e9 \u00e0 30   mL/min/1,73m2)\")</li> </ul> </li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: mentioned renal failure stage</li> <li><code>status</code>: mentioned renal failure severity (e.g. mod\u00e9r\u00e9e, s\u00e9v\u00e8re, terminale,   etc.)</li> <li><code>dfg</code>: mentioned DFG</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.ckd())\n</code></pre> <p>Below are a few examples:</p> 1234567891011 <pre><code>text = \"Patient atteint d'une glom\u00e9rulopathie.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [glom\u00e9rulopathie]\n</code></pre> <pre><code>text = \"Patient atteint d'une tubulopathie aig\u00fce.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient transplant\u00e9 r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [transplant\u00e9 r\u00e9nal]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une insuffisance r\u00e9nale aig\u00fce sur chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [insuffisance r\u00e9nale aig\u00fce sur chronique]\n</code></pre> <pre><code>text = \"Le patient a \u00e9t\u00e9 dialys\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est dialys\u00e9 chaque lundi\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [dialys\u00e9 chaque lundi]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'chronic': [lundi]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC s\u00e9v\u00e8re\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC s\u00e9v\u00e8re]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'status': s\u00e9v\u00e8re}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC au stade IV\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC au stade IV]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': IV}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC avec DFG \u00e0 30\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC avec DFG \u00e0 30]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'dfg': 30}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une maladie r\u00e9nale avec DFG \u00e0 110\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'ckd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['glomerulonephrit...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>ckd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'ckd': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.CKD</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/","title":"<code>edsnlp.pipes.ner.disorders.ckd.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component","title":"<code>create_component = registry.factory.register('eds.ckd', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.CKD'])(CKDMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.CKD</code> pipeline component extracts mentions of CKD (Chronic Kidney Disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Kidney transplantation</li> <li>Chronic dialysis</li> <li>Renal failure from stage 3 to 5. The stage is extracted by trying 3 methods:<ul> <li>Extracting the mentioned stage directly (\"IRC stade IV\")</li> <li>Extracting the severity directly (\"IRC terminale\")</li> <li>Extracting the mentioned GFR (DFG in french) (\"IRC avec DFG estim\u00e9 \u00e0 30   mL/min/1,73m2)\")</li> </ul> </li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: mentioned renal failure stage</li> <li><code>status</code>: mentioned renal failure severity (e.g. mod\u00e9r\u00e9e, s\u00e9v\u00e8re, terminale,   etc.)</li> <li><code>dfg</code>: mentioned DFG</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.ckd())\n</code></pre> <p>Below are a few examples:</p> 1234567891011 <pre><code>text = \"Patient atteint d'une glom\u00e9rulopathie.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [glom\u00e9rulopathie]\n</code></pre> <pre><code>text = \"Patient atteint d'une tubulopathie aig\u00fce.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient transplant\u00e9 r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [transplant\u00e9 r\u00e9nal]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une insuffisance r\u00e9nale aig\u00fce sur chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [insuffisance r\u00e9nale aig\u00fce sur chronique]\n</code></pre> <pre><code>text = \"Le patient a \u00e9t\u00e9 dialys\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est dialys\u00e9 chaque lundi\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [dialys\u00e9 chaque lundi]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'chronic': [lundi]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC s\u00e9v\u00e8re\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC s\u00e9v\u00e8re]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'status': s\u00e9v\u00e8re}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC au stade IV\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC au stade IV]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': IV}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC avec DFG \u00e0 30\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC avec DFG \u00e0 30]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'dfg': 30}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une maladie r\u00e9nale avec DFG \u00e0 110\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'ckd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['glomerulonephrit...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>ckd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'ckd': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.CKD</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/patterns/","title":"<code>edsnlp.pipes.ner.disorders.ckd.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/","title":"<code>edsnlp.pipes.ner.disorders.congestive_heart_failure</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/","title":"<code>edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure</code>","text":"<p><code>eds.congestive_heart_failure</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure.CongestiveHeartFailureMatcher","title":"<code>CongestiveHeartFailureMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.congestive_heart_failure</code> pipeline component extracts mentions of congestive heart failure. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Heart transplantation</li> <li>AF (Atrial Fibrillation)</li> <li>Pacemaker</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"defaillance.{1,10}cardi\",\n        r\"(\u0153|oe)deme.{1,10}pulmon\",\n        r\"(\u0153|oe)deme.{1,10}poumon\",\n        r\"decompensation.{1,10}card\",\n        r\"choc.{1,30}cardio\",\n        r\"greffe.{1,10}c(\u0153|oe)ur\",\n        r\"greffe.{1,10}cardia\",\n        r\"transplantation.{1,10}c(\u0153|oe)ur\",\n        r\"transplantation.{1,10}cardia\",\n        r\"arret.{1,10}cardi\",\n        r\"c(\u0153|oe)ur pulmo\",\n        r\"foie.card\",\n        r\"pace.?maker\",\n        r\"stimulateur.cardiaque\",\n        r\"valve.{1,30}(meca|artific)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nsymptomatic = dict(\n    source=\"symptomatic\",\n    regex=[\n        r\"cardiopathi\",\n        r\"cardiomyopathi\",\n        r\"d(i|y)sfonction.{1,15}(ventricul|\\bvg|cardiaque)\",\n        r\"valvulopathie\",\n        r\"\\bic\\b.{1,10}(droite|gauche)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [r\"(?&lt;!\\bnon.)ischem\"],  # Exclusion of ischemic events\n        window=5,\n    ),\n)\n\nwith_minimum_severity = dict(\n    source=\"min_severity\",\n    regex=[\n        r\"insuffisance.{1,10}(\\bcardi|\\bdiasto|\\bventri|\\bmitral|tri.?cusp)\",\n        r\"(retrecissement|stenose).(aortique|mitral)\",\n        r\"\\brac\\b\",\n        r\"\\brm\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [\"minime\", \"modere\", r\"non.serre\"],\n        window=5,\n    ),\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bOAP\\b\",\n        r\"\\bCMH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAF_main_pattern = dict(\n    source=\"AF_main\",\n    regex=[\n        r\"fibrill?ation.{1,3}(atriale|auriculaire|ventriculaire)\",\n        r\"flutter\",\n        r\"brady.?arythmie\",\n        r\"pace.?maker\",\n    ],\n)\n\nAF_acronym = dict(\n    source=\"AF_acronym\",\n    regex=[\n        r\"\\bFA\\b\",\n        r\"\\bAC.?FA\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    symptomatic,\n    acronym,\n    AF_main_pattern,\n    AF_acronym,\n    with_minimum_severity,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure.CongestiveHeartFailureMatcher--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.congestive_heart_failure())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'un oed\u00e8me pulmonaire\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [oed\u00e8me pulmonaire]\n</code></pre> <pre><code>text = \"Le patient est \u00e9quip\u00e9 d'un pace-maker\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [pace-maker]\n</code></pre> <pre><code>text = \"Un cardiopathie non d\u00e9compens\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Insuffisance cardiaque\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [Insuffisance cardiaque]\n</code></pre> <pre><code>text = \"Insuffisance cardiaque minime\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure.CongestiveHeartFailureMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>(str)</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['defaillance.{1,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>congestive_heart_failure</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'congestive_heart_failure': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure.CongestiveHeartFailureMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.congestive_heart_failure</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/","title":"<code>edsnlp.pipes.ner.disorders.congestive_heart_failure.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component","title":"<code>create_component = registry.factory.register('eds.congestive_heart_failure', assigns=['doc.ents', 'doc.spans'])(CongestiveHeartFailureMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.congestive_heart_failure</code> pipeline component extracts mentions of congestive heart failure. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Heart transplantation</li> <li>AF (Atrial Fibrillation)</li> <li>Pacemaker</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"defaillance.{1,10}cardi\",\n        r\"(\u0153|oe)deme.{1,10}pulmon\",\n        r\"(\u0153|oe)deme.{1,10}poumon\",\n        r\"decompensation.{1,10}card\",\n        r\"choc.{1,30}cardio\",\n        r\"greffe.{1,10}c(\u0153|oe)ur\",\n        r\"greffe.{1,10}cardia\",\n        r\"transplantation.{1,10}c(\u0153|oe)ur\",\n        r\"transplantation.{1,10}cardia\",\n        r\"arret.{1,10}cardi\",\n        r\"c(\u0153|oe)ur pulmo\",\n        r\"foie.card\",\n        r\"pace.?maker\",\n        r\"stimulateur.cardiaque\",\n        r\"valve.{1,30}(meca|artific)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nsymptomatic = dict(\n    source=\"symptomatic\",\n    regex=[\n        r\"cardiopathi\",\n        r\"cardiomyopathi\",\n        r\"d(i|y)sfonction.{1,15}(ventricul|\\bvg|cardiaque)\",\n        r\"valvulopathie\",\n        r\"\\bic\\b.{1,10}(droite|gauche)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [r\"(?&lt;!\\bnon.)ischem\"],  # Exclusion of ischemic events\n        window=5,\n    ),\n)\n\nwith_minimum_severity = dict(\n    source=\"min_severity\",\n    regex=[\n        r\"insuffisance.{1,10}(\\bcardi|\\bdiasto|\\bventri|\\bmitral|tri.?cusp)\",\n        r\"(retrecissement|stenose).(aortique|mitral)\",\n        r\"\\brac\\b\",\n        r\"\\brm\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [\"minime\", \"modere\", r\"non.serre\"],\n        window=5,\n    ),\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bOAP\\b\",\n        r\"\\bCMH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAF_main_pattern = dict(\n    source=\"AF_main\",\n    regex=[\n        r\"fibrill?ation.{1,3}(atriale|auriculaire|ventriculaire)\",\n        r\"flutter\",\n        r\"brady.?arythmie\",\n        r\"pace.?maker\",\n    ],\n)\n\nAF_acronym = dict(\n    source=\"AF_acronym\",\n    regex=[\n        r\"\\bFA\\b\",\n        r\"\\bAC.?FA\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    symptomatic,\n    acronym,\n    AF_main_pattern,\n    AF_acronym,\n    with_minimum_severity,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.congestive_heart_failure())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'un oed\u00e8me pulmonaire\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [oed\u00e8me pulmonaire]\n</code></pre> <pre><code>text = \"Le patient est \u00e9quip\u00e9 d'un pace-maker\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [pace-maker]\n</code></pre> <pre><code>text = \"Un cardiopathie non d\u00e9compens\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Insuffisance cardiaque\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [Insuffisance cardiaque]\n</code></pre> <pre><code>text = \"Insuffisance cardiaque minime\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>(str)</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['defaillance.{1,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>congestive_heart_failure</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'congestive_heart_failure': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.congestive_heart_failure</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/patterns/","title":"<code>edsnlp.pipes.ner.disorders.congestive_heart_failure.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/","title":"<code>edsnlp.pipes.ner.disorders.connective_tissue_disease</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/","title":"<code>edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease</code>","text":"<p><code>eds.connective_tissue_disease</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher","title":"<code>ConnectiveTissueDiseaseMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.connective_tissue_disease</code> pipeline component extracts mentions of connective tissue diseases.</p> Details of the used patterns <pre><code># fmt: off\nTO_EXCLUDE = r\"(?&lt;!a )((\\bacc\\b)|anti.?coag|anti.?corps|buschke|(\\bac\\b)|(\\bbio))\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"arthrites.{1,5}juveniles.{1,5}idiopa\",\n        r\"myosite\",\n        r\"myopathie.{1,5}inflammatoire\",\n        r\"polyarthrite.{1,5}chronique.{1,5}evol\",\n        r\"polymyosie\",\n        r\"polyarthrites.{1,5}(rhizo|rhuma)\",\n        r\"sclerodermie\",\n        r\"connectivite\",\n        r\"sarcoidose\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nlupus = dict(\n    source=\"lupus\",\n    regex=[\n        r\"\\blupus\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nlupique = dict(\n    source=\"lupique\",\n    regex=[\n        r\"\\blupique\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronyms\",\n    regex=[\n        r\"\\bAJI\\b\",\n        r\"\\bLED\\b\",\n        r\"\\bPCE\\b\",\n        r\"\\bCREST\\b\",\n        r\"\\bPPR\\b\",\n        r\"\\bMICI\\b\",\n        r\"\\bMNAI\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nnamed_disease = dict(\n    source=\"named_disease\",\n    regex=[\n        r\"libman.?lack\",\n        r\"\\bstill\",\n        r\"felty\",\n        r\"forestier.?certon\",\n        r\"gou(g|j)erot\",\n        r\"raynaud\",\n        r\"thibierge.?weiss\",\n        r\"sjogren\",\n        r\"gou(g|j)erot.?sjogren\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    lupus,\n    lupique,\n    acronym,\n    named_disease,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.connective_tissue_disease())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'une scl\u00e9rodermie.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [scl\u00e9rodermie]\n</code></pre> <pre><code>text = \"Patient atteint d'un lupus.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [lupus]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'anticoagulants lupiques,\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a une MICI.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [MICI]\n</code></pre> <pre><code>text = \"Syndrome de Raynaud\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [Raynaud]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'connective_tissue_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['arthrites.{1,5}j...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>connective_tissue_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'connective_tissue_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.connective_tissue_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/","title":"<code>edsnlp.pipes.ner.disorders.connective_tissue_disease.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component","title":"<code>create_component = registry.factory.register('eds.connective_tissue_disease', assigns=['doc.ents', 'doc.spans'])(ConnectiveTissueDiseaseMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.connective_tissue_disease</code> pipeline component extracts mentions of connective tissue diseases.</p> Details of the used patterns <pre><code># fmt: off\nTO_EXCLUDE = r\"(?&lt;!a )((\\bacc\\b)|anti.?coag|anti.?corps|buschke|(\\bac\\b)|(\\bbio))\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"arthrites.{1,5}juveniles.{1,5}idiopa\",\n        r\"myosite\",\n        r\"myopathie.{1,5}inflammatoire\",\n        r\"polyarthrite.{1,5}chronique.{1,5}evol\",\n        r\"polymyosie\",\n        r\"polyarthrites.{1,5}(rhizo|rhuma)\",\n        r\"sclerodermie\",\n        r\"connectivite\",\n        r\"sarcoidose\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nlupus = dict(\n    source=\"lupus\",\n    regex=[\n        r\"\\blupus\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nlupique = dict(\n    source=\"lupique\",\n    regex=[\n        r\"\\blupique\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronyms\",\n    regex=[\n        r\"\\bAJI\\b\",\n        r\"\\bLED\\b\",\n        r\"\\bPCE\\b\",\n        r\"\\bCREST\\b\",\n        r\"\\bPPR\\b\",\n        r\"\\bMICI\\b\",\n        r\"\\bMNAI\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nnamed_disease = dict(\n    source=\"named_disease\",\n    regex=[\n        r\"libman.?lack\",\n        r\"\\bstill\",\n        r\"felty\",\n        r\"forestier.?certon\",\n        r\"gou(g|j)erot\",\n        r\"raynaud\",\n        r\"thibierge.?weiss\",\n        r\"sjogren\",\n        r\"gou(g|j)erot.?sjogren\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    lupus,\n    lupique,\n    acronym,\n    named_disease,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.connective_tissue_disease())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'une scl\u00e9rodermie.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [scl\u00e9rodermie]\n</code></pre> <pre><code>text = \"Patient atteint d'un lupus.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [lupus]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'anticoagulants lupiques,\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a une MICI.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [MICI]\n</code></pre> <pre><code>text = \"Syndrome de Raynaud\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [Raynaud]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'connective_tissue_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['arthrites.{1,5}j...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>connective_tissue_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'connective_tissue_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.connective_tissue_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/patterns/","title":"<code>edsnlp.pipes.ner.disorders.connective_tissue_disease.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/copd/","title":"<code>edsnlp.pipes.ner.disorders.copd</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/","title":"<code>edsnlp.pipes.ner.disorders.copd.copd</code>","text":"<p><code>eds.copd</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher","title":"<code>COPDMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.copd</code> pipeline component extracts mentions of COPD (Chronic obstructive pulmonary disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Pulmonary hypertension</li> <li>Long-term oxygen therapy</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.copd())\n</code></pre> <p>Below are a few examples:</p> 123456 <pre><code>text = \"Une fibrose interstitielle diffuse idiopathique\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [fibrose interstitielle diffuse idiopathique]\n</code></pre> <pre><code>text = \"Patient atteint de pneumoconiose\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [pneumoconiose]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une HTAP.\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [HTAP]\n</code></pre> <pre><code>text = \"On voit une hypertension pulmonaire minime\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente a \u00e9t\u00e9 mis sous oxyg\u00e9norequ\u00e9rance\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente est sous oxyg\u00e9norequ\u00e9rance au long cours\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [oxyg\u00e9norequ\u00e9rance au long cours]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'long': [long cours]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'copd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['alveolites.{1,5}...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>copd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'copd': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.copd</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/","title":"<code>edsnlp.pipes.ner.disorders.copd.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component","title":"<code>create_component = registry.factory.register('eds.copd', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.COPD'])(COPDMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.copd</code> pipeline component extracts mentions of COPD (Chronic obstructive pulmonary disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Pulmonary hypertension</li> <li>Long-term oxygen therapy</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.copd())\n</code></pre> <p>Below are a few examples:</p> 123456 <pre><code>text = \"Une fibrose interstitielle diffuse idiopathique\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [fibrose interstitielle diffuse idiopathique]\n</code></pre> <pre><code>text = \"Patient atteint de pneumoconiose\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [pneumoconiose]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une HTAP.\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [HTAP]\n</code></pre> <pre><code>text = \"On voit une hypertension pulmonaire minime\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente a \u00e9t\u00e9 mis sous oxyg\u00e9norequ\u00e9rance\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente est sous oxyg\u00e9norequ\u00e9rance au long cours\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [oxyg\u00e9norequ\u00e9rance au long cours]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'long': [long cours]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'copd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['alveolites.{1,5}...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>copd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'copd': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.copd</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/patterns/","title":"<code>edsnlp.pipes.ner.disorders.copd.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/","title":"<code>edsnlp.pipes.ner.disorders.dementia</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/","title":"<code>edsnlp.pipes.ner.disorders.dementia.dementia</code>","text":"<p><code>eds.dementia</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher","title":"<code>DementiaMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.dementia</code> pipeline component extracts mentions of dementia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"demence\",\n        r\"dementiel\",\n        r\"corps de le[vw]y\",\n        r\"deficits?.chroniques?.cognitif\",\n        r\"troubles?.mnesique?\",\n        r\"troubles?.praxique\",\n        r\"troubles?.attentionel\",\n        r\"troubles?.degeneratif.{1,15}fonctions.{1,5}sup\",\n        r\"maladies?.cerebrales?.degen\",\n        r\"troubles?.neurocogn\",\n        r\"deficits?.cognitif\",\n        r\"(trouble|dysfonction).{1,20} cogniti\",\n        r\"atteinte.{1,7}spheres?cogniti\",\n        r\"syndrome.{1,10}(frontal|neuro.deg)\",\n        r\"dysfonction.{1,25}cogni\",\n        r\"(?&lt;!specialisee )alzheimer\",\n        r\"demence.{1,20}(\\balz|\\bpark)\",\n        r\"binswanger\",\n        r\"gehring\",\n        r\"\\bpick\",\n        r\"de guam\",\n        r\"[kc]reutzfeld.{1,5}ja[ck]ob\",\n        r\"huntington\",\n        r\"korsako[fv]\",\n        r\"atrophie.{1,10}(cortico|hippocamp|cereb|lobe)\",\n    ],\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bSLA\\b\",\n        r\"\\bDFT\\b\",\n        r\"\\bDFT\",\n        r\"\\bTNC\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=r\"\\banti\",  # anticorps\n        window=-15,\n        regex_attr=\"NORM\",\n    ),\n)\n\ncharcot = dict(\n    source=\"charcot\",\n    regex=[\n        r\"maladie.{1,10}charcot\",\n    ],\n    exclude=dict(\n        regex=[\n            \"pied de\",\n            \"marie.?tooth\",\n        ],\n        window=(-3, 3),\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    charcot,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.dementia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"D'importants d\u00e9ficits cognitifs\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9ficits cognitifs]\n</code></pre> <pre><code>text = \"Patient atteint de d\u00e9mence\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9mence]\n</code></pre> <pre><code>text = \"On retrouve des anti-SLA\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une maladie de Charcot\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [maladie de Charcot]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['demence', 'demen...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>dementia</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'dementia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dementia</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/","title":"<code>edsnlp.pipes.ner.disorders.dementia.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component","title":"<code>create_component = registry.factory.register('eds.dementia', assigns=['doc.ents', 'doc.spans'])(DementiaMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.dementia</code> pipeline component extracts mentions of dementia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"demence\",\n        r\"dementiel\",\n        r\"corps de le[vw]y\",\n        r\"deficits?.chroniques?.cognitif\",\n        r\"troubles?.mnesique?\",\n        r\"troubles?.praxique\",\n        r\"troubles?.attentionel\",\n        r\"troubles?.degeneratif.{1,15}fonctions.{1,5}sup\",\n        r\"maladies?.cerebrales?.degen\",\n        r\"troubles?.neurocogn\",\n        r\"deficits?.cognitif\",\n        r\"(trouble|dysfonction).{1,20} cogniti\",\n        r\"atteinte.{1,7}spheres?cogniti\",\n        r\"syndrome.{1,10}(frontal|neuro.deg)\",\n        r\"dysfonction.{1,25}cogni\",\n        r\"(?&lt;!specialisee )alzheimer\",\n        r\"demence.{1,20}(\\balz|\\bpark)\",\n        r\"binswanger\",\n        r\"gehring\",\n        r\"\\bpick\",\n        r\"de guam\",\n        r\"[kc]reutzfeld.{1,5}ja[ck]ob\",\n        r\"huntington\",\n        r\"korsako[fv]\",\n        r\"atrophie.{1,10}(cortico|hippocamp|cereb|lobe)\",\n    ],\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bSLA\\b\",\n        r\"\\bDFT\\b\",\n        r\"\\bDFT\",\n        r\"\\bTNC\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=r\"\\banti\",  # anticorps\n        window=-15,\n        regex_attr=\"NORM\",\n    ),\n)\n\ncharcot = dict(\n    source=\"charcot\",\n    regex=[\n        r\"maladie.{1,10}charcot\",\n    ],\n    exclude=dict(\n        regex=[\n            \"pied de\",\n            \"marie.?tooth\",\n        ],\n        window=(-3, 3),\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    charcot,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.dementia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"D'importants d\u00e9ficits cognitifs\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9ficits cognitifs]\n</code></pre> <pre><code>text = \"Patient atteint de d\u00e9mence\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9mence]\n</code></pre> <pre><code>text = \"On retrouve des anti-SLA\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une maladie de Charcot\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [maladie de Charcot]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['demence', 'demen...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>dementia</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'dementia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dementia</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/patterns/","title":"<code>edsnlp.pipes.ner.disorders.dementia.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/","title":"<code>edsnlp.pipes.ner.disorders.diabetes</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/","title":"<code>edsnlp.pipes.ner.disorders.diabetes.diabetes</code>","text":"<p><code>eds.diabetes</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher","title":"<code>DiabetesMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.diabetes</code> pipeline component extracts mentions of diabetes.</p> Details of the used patterns <pre><code># fmt: off\nCOMPLICATIONS = [\n    r\"nephropat\",\n    r\"neuropat\",\n    r\"retinopat\",\n    r\"glomerulopathi\",\n    r\"glomeruloscleros\",\n    r\"angiopathi\",\n    r\"origine\",\n]\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bds?n?id\\b\",\n        r\"\\bdiabet[^o]\",\n        r\"\\bdb\\b\",\n        r\"\\bdt.?(i|ii|1|2)\\b\",\n    ],\n    exclude=dict(\n        regex=[\n            \"insipide\",\n            \"nephrogenique\",\n            \"aigu\",\n            r\"\\bdr\\b\",  # Dr. ...\n            \"endocrino\",  # Section title\n            \"soins aux pieds\",  # Section title\n            \"nutrition\",  # Section title\n            r\"\\s?:\\n+\\W+(?!oui|non|\\W)\",  # General pattern for section title\n        ],\n        window=(-5, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"complicated_before\",\n            regex=r\"(\" + r\"|\".join(COMPLICATIONS + [\"origine\"]) + r\")\",\n            window=-3,\n        ),\n        dict(\n            name=\"complicated_after\",\n            regex=r\"(\"\n            + r\"|\".join([r\"(?&lt;!sans )compli\", r\"(?&lt;!a)symptomatique\"] + COMPLICATIONS)\n            + r\")\",\n            window=12,\n        ),\n        dict(\n            name=\"type\",\n            regex=r\"type.(i|ii|1|2)\",\n            window=6,\n        ),\n        dict(\n            name=\"insulin\",\n            regex=r\"insulino.?(dep|req)\",\n            window=6,\n        ),\n        dict(\n            name=\"corticoid\",\n            regex=r\"(bctc\\b|cortico(?:.?induit)?)\",\n            window=6,\n        ),\n    ],\n)\n\ncomplicated_pattern = dict(\n    source=\"complicated\",\n    regex=[\n        r\"(mal|maux).perforants?(.plantaire)?\",\n        r\"pieds? diabeti\",\n    ],\n    exclude=dict(\n        regex=\"soins aux\",  # Section title\n        window=-2,\n    ),\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    complicated_pattern,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"WITH_COMPLICATION\"</code> if the diabetes is  complicated (e.g., via organ    damages)</li> <li><code>\"WITHOUT_COMPLICATION\"</code> otherwise</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>type</code>: type of diabetes (I or II)</li> <li><code>insulin</code>: if the diabetes is insulin-dependent</li> <li><code>corticoid</code>: if the diabetes is corticoid-induced</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.diabetes())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un DT2\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DT2]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un DNID\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DNID]\n</code></pre> <pre><code>text = \"Patient diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [diab\u00e9tique]\n</code></pre> <pre><code>text = \"Un diab\u00e8te insipide\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Atteinte neurologique d'origine diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [origine diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [origine]}\n</code></pre> <pre><code>text = \"Une r\u00e9tinopathie diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [r\u00e9tinopathie diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [r\u00e9tinopathie]}\n</code></pre> <pre><code>text = \"Il y a un mal perforant plantaire\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [mal perforant plantaire]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'diabetes'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['\\\\bds?n?id\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>diabetes</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'diabetes': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.diabetes</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher.has_far_complications","title":"<code>has_far_complications</code>","text":"<p>Handles the common case where complications are listed as bullet points, sometimes fairly far from the anchor.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/","title":"<code>edsnlp.pipes.ner.disorders.diabetes.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component","title":"<code>create_component = registry.factory.register('eds.diabetes', assigns=['doc.ents', 'doc.spans'])(DiabetesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.diabetes</code> pipeline component extracts mentions of diabetes.</p> Details of the used patterns <pre><code># fmt: off\nCOMPLICATIONS = [\n    r\"nephropat\",\n    r\"neuropat\",\n    r\"retinopat\",\n    r\"glomerulopathi\",\n    r\"glomeruloscleros\",\n    r\"angiopathi\",\n    r\"origine\",\n]\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bds?n?id\\b\",\n        r\"\\bdiabet[^o]\",\n        r\"\\bdb\\b\",\n        r\"\\bdt.?(i|ii|1|2)\\b\",\n    ],\n    exclude=dict(\n        regex=[\n            \"insipide\",\n            \"nephrogenique\",\n            \"aigu\",\n            r\"\\bdr\\b\",  # Dr. ...\n            \"endocrino\",  # Section title\n            \"soins aux pieds\",  # Section title\n            \"nutrition\",  # Section title\n            r\"\\s?:\\n+\\W+(?!oui|non|\\W)\",  # General pattern for section title\n        ],\n        window=(-5, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"complicated_before\",\n            regex=r\"(\" + r\"|\".join(COMPLICATIONS + [\"origine\"]) + r\")\",\n            window=-3,\n        ),\n        dict(\n            name=\"complicated_after\",\n            regex=r\"(\"\n            + r\"|\".join([r\"(?&lt;!sans )compli\", r\"(?&lt;!a)symptomatique\"] + COMPLICATIONS)\n            + r\")\",\n            window=12,\n        ),\n        dict(\n            name=\"type\",\n            regex=r\"type.(i|ii|1|2)\",\n            window=6,\n        ),\n        dict(\n            name=\"insulin\",\n            regex=r\"insulino.?(dep|req)\",\n            window=6,\n        ),\n        dict(\n            name=\"corticoid\",\n            regex=r\"(bctc\\b|cortico(?:.?induit)?)\",\n            window=6,\n        ),\n    ],\n)\n\ncomplicated_pattern = dict(\n    source=\"complicated\",\n    regex=[\n        r\"(mal|maux).perforants?(.plantaire)?\",\n        r\"pieds? diabeti\",\n    ],\n    exclude=dict(\n        regex=\"soins aux\",  # Section title\n        window=-2,\n    ),\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    complicated_pattern,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"WITH_COMPLICATION\"</code> if the diabetes is  complicated (e.g., via organ    damages)</li> <li><code>\"WITHOUT_COMPLICATION\"</code> otherwise</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>type</code>: type of diabetes (I or II)</li> <li><code>insulin</code>: if the diabetes is insulin-dependent</li> <li><code>corticoid</code>: if the diabetes is corticoid-induced</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.diabetes())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un DT2\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DT2]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un DNID\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DNID]\n</code></pre> <pre><code>text = \"Patient diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [diab\u00e9tique]\n</code></pre> <pre><code>text = \"Un diab\u00e8te insipide\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Atteinte neurologique d'origine diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [origine diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [origine]}\n</code></pre> <pre><code>text = \"Une r\u00e9tinopathie diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [r\u00e9tinopathie diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [r\u00e9tinopathie]}\n</code></pre> <pre><code>text = \"Il y a un mal perforant plantaire\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [mal perforant plantaire]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'diabetes'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['\\\\bds?n?id\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>diabetes</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'diabetes': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.diabetes</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/patterns/","title":"<code>edsnlp.pipes.ner.disorders.diabetes.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/","title":"<code>edsnlp.pipes.ner.disorders.hemiplegia</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/","title":"<code>edsnlp.pipes.ner.disorders.hemiplegia.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component","title":"<code>create_component = registry.factory.register('eds.hemiplegia', assigns=['doc.ents', 'doc.spans'])(HemiplegiaMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.hemiplegia</code> pipeline component extracts mentions of hemiplegia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"hemiplegi\",\n        r\"tetraplegi\",\n        r\"quadriplegi\",\n        r\"paraplegi\",\n        r\"neuropathie.{1,25}motrice.{1,30}type [5V]\",\n        r\"charcot.?marie.?tooth\",\n        r\"locked.?in\",\n        r\"syndrome.{1,5}(enfermement|verrouillage)|(desafferen)\",\n        r\"paralysie.{1,10}hemicorps\",\n        r\"paralysie.{1,10}jambe\",\n        r\"paralysie.{1,10}membre\",\n        r\"paralysie.{1,10}cote\",\n        r\"paralysie.{1,5}cerebrale.{1,5}spastique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLIS\\b\",\n        r\"\\bNMSH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.hemiplegia())\n</code></pre> <p>Below are a few examples:</p> 123 <pre><code>text = \"Patient h\u00e9mipl\u00e9gique\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [h\u00e9mipl\u00e9gique]\n</code></pre> <pre><code>text = \"Paralysie des membres inf\u00e9rieurs\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [Paralysie des membres]\n</code></pre> <pre><code>text = \"Patient en LIS\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [LIS]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['hemiplegi', 'tet...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>hemiplegia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'hemiplegia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hemiplegia</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/","title":"<code>edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia</code>","text":"<p><code>eds.hemiplegia</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher","title":"<code>HemiplegiaMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.hemiplegia</code> pipeline component extracts mentions of hemiplegia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"hemiplegi\",\n        r\"tetraplegi\",\n        r\"quadriplegi\",\n        r\"paraplegi\",\n        r\"neuropathie.{1,25}motrice.{1,30}type [5V]\",\n        r\"charcot.?marie.?tooth\",\n        r\"locked.?in\",\n        r\"syndrome.{1,5}(enfermement|verrouillage)|(desafferen)\",\n        r\"paralysie.{1,10}hemicorps\",\n        r\"paralysie.{1,10}jambe\",\n        r\"paralysie.{1,10}membre\",\n        r\"paralysie.{1,10}cote\",\n        r\"paralysie.{1,5}cerebrale.{1,5}spastique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLIS\\b\",\n        r\"\\bNMSH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.hemiplegia())\n</code></pre> <p>Below are a few examples:</p> 123 <pre><code>text = \"Patient h\u00e9mipl\u00e9gique\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [h\u00e9mipl\u00e9gique]\n</code></pre> <pre><code>text = \"Paralysie des membres inf\u00e9rieurs\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [Paralysie des membres]\n</code></pre> <pre><code>text = \"Patient en LIS\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [LIS]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['hemiplegi', 'tet...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>hemiplegia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'hemiplegia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hemiplegia</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/patterns/","title":"<code>edsnlp.pipes.ner.disorders.hemiplegia.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/","title":"<code>edsnlp.pipes.ner.disorders.leukemia</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/","title":"<code>edsnlp.pipes.ner.disorders.leukemia.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component","title":"<code>create_component = registry.factory.register('eds.leukemia', assigns=['doc.ents', 'doc.spans'])(LeukemiaMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.leukemia</code> pipeline component extracts mentions of leukemia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"leucemie\",\n        r\"(syndrome.)?myeloproliferatif\",\n        r\"m[yi]eloprolifer\",\n    ],\n    exclude=dict(\n        regex=[\n            \"plasmocyte\",\n            \"benin\",\n            \"benign\",\n        ],\n        window=5,\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLAM\\b\",\n        r\"\\bLAM.?[0-9]\",\n        r\"\\bLAL\\b\",\n        r\"\\bLMC\\b\",\n        r\"\\bLCE\\b\",\n        r\"\\bLMM[JC]\\b\",\n        r\"\\bLCN\\b\",\n        r\"\\bAREB\\b\",\n        r\"\\bAPMF\\b\",\n        r\"\\bLLC\\b\",\n        r\"\\bSMD\\b\",\n        r\"LA my[\u00e9\u00e8e]lomonocytaire\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=\"anti\",\n        window=-20,\n    ),\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"myelofibrose\",\n        r\"vaquez\",\n        r\"thrombocytemie.{1,3}essentielle\",\n        r\"splenomegalie.{1,3}myeloide\",\n        r\"mastocytose.{1,5}maligne\",\n        r\"polyglobulie.{1,10}essentielle\",\n        r\"letterer.?siwe\",\n        r\"anemie.refractaire.{1,20}blaste\",\n        r\"m[iy]elod[iy]splasi\",\n        r\"syndrome.myelo.?dysplasique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    other,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.leukemia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [my\u00e9loprolif\u00e9ratif]\n</code></pre> <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif b\u00e9nin\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient atteint d'une LAM\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [LAM]\n</code></pre> <pre><code>text = \"Une maladie de Vaquez\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [Vaquez]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'leukemia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['leucemie', '(syn...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>leukemia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'leukemia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.leukemia</code> component was developed by AP-HP's Data Science team with a team  of medical experts. A paper describing in details the development of those  components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/","title":"<code>edsnlp.pipes.ner.disorders.leukemia.leukemia</code>","text":"<p><code>eds.leukemia</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher","title":"<code>LeukemiaMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.leukemia</code> pipeline component extracts mentions of leukemia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"leucemie\",\n        r\"(syndrome.)?myeloproliferatif\",\n        r\"m[yi]eloprolifer\",\n    ],\n    exclude=dict(\n        regex=[\n            \"plasmocyte\",\n            \"benin\",\n            \"benign\",\n        ],\n        window=5,\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLAM\\b\",\n        r\"\\bLAM.?[0-9]\",\n        r\"\\bLAL\\b\",\n        r\"\\bLMC\\b\",\n        r\"\\bLCE\\b\",\n        r\"\\bLMM[JC]\\b\",\n        r\"\\bLCN\\b\",\n        r\"\\bAREB\\b\",\n        r\"\\bAPMF\\b\",\n        r\"\\bLLC\\b\",\n        r\"\\bSMD\\b\",\n        r\"LA my[\u00e9\u00e8e]lomonocytaire\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=\"anti\",\n        window=-20,\n    ),\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"myelofibrose\",\n        r\"vaquez\",\n        r\"thrombocytemie.{1,3}essentielle\",\n        r\"splenomegalie.{1,3}myeloide\",\n        r\"mastocytose.{1,5}maligne\",\n        r\"polyglobulie.{1,10}essentielle\",\n        r\"letterer.?siwe\",\n        r\"anemie.refractaire.{1,20}blaste\",\n        r\"m[iy]elod[iy]splasi\",\n        r\"syndrome.myelo.?dysplasique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    other,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.leukemia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [my\u00e9loprolif\u00e9ratif]\n</code></pre> <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif b\u00e9nin\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient atteint d'une LAM\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [LAM]\n</code></pre> <pre><code>text = \"Une maladie de Vaquez\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [Vaquez]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'leukemia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['leucemie', '(syn...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>leukemia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'leukemia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.leukemia</code> component was developed by AP-HP's Data Science team with a team  of medical experts. A paper describing in details the development of those  components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/patterns/","title":"<code>edsnlp.pipes.ner.disorders.leukemia.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/","title":"<code>edsnlp.pipes.ner.disorders.liver_disease</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/","title":"<code>edsnlp.pipes.ner.disorders.liver_disease.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component","title":"<code>create_component = registry.factory.register('eds.liver_disease', assigns=['doc.ents', 'doc.spans'])(LiverDiseaseMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.liver_disease</code> pipeline component extracts mentions of liver disease.</p> Details of the used patterns <pre><code># fmt: off\nmild = dict(\n    source=\"mild\",\n    regex=[\n        r\"cholangites?.{1,10}(sclero|secondaire)\",\n        r\"fibrose.{1,10}(hepatique|foie)\",\n        r\"hepatite.{1,15}chronique\",\n        r\"hepatopathie\",\n        r\"\\bnash\\b\",\n        r\"(maladie|sydrome).{1,10}Hanot\",\n        r\"surinfections.{1,5}delta\",\n        r\"\\bcbp\\b\",\n        r\"\\bmaf\\b\",\n        r\"(maladie|syndrome).{1,8}hanot\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"\\bdots?\\b\",\n        window=-5,\n    ),\n)\n\nmoderate_severe = dict(\n    source=\"moderate_severe\",\n    regex=[\n        r\"cirrhose\",\n        r\"necrose.{1,10}(hepati|foie)\",\n        r\"varice.{1,10}(estomac|oesopha|gastr)\",\n        r\"\\bvo\\b.{1,5}(stade|grade).(1|2|3|i{1,3})\",\n        r\"hypertension.{1,5}portale\",\n        r\"scleroses.{1,5}hepatoportale\",\n        r\"sydrome.{1,10}hepato.?ren\",\n        r\"insuffisance.{1,5}hepa\",\n        r\"encephalopathie.{1,5}hepa\",\n        r\"\\btips\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ntransplant = dict(\n    source=\"transplant\",\n    regex=[\n        r\"(?&lt;!pre.?)(greffe|transplant).{1,12}(hepatique|foie)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"chc\",\n        window=(-5, 5),\n    ),\n)\n\ndefault_patterns = [\n    mild,\n    moderate_severe,\n    transplant,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"MILD\"</code> for mild liver diseases</li> <li><code>\"MODERATE_TO_SEVERE\"</code> else</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.liver_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Il y a une fibrose h\u00e9patique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [fibrose h\u00e9patique]\n</code></pre> <pre><code>text = \"Une h\u00e9patite B chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [h\u00e9patite B chronique]\n</code></pre> <pre><code>text = \"Le patient consulte pour une cirrhose\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [cirrhose]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre> <pre><code>text = \"Greffe h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [Greffe h\u00e9patique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'mild', 'regex': ['cholangites?.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>liver_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'liver_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.liver_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/","title":"<code>edsnlp.pipes.ner.disorders.liver_disease.liver_disease</code>","text":"<p><code>eds.liver_disease</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher","title":"<code>LiverDiseaseMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.liver_disease</code> pipeline component extracts mentions of liver disease.</p> Details of the used patterns <pre><code># fmt: off\nmild = dict(\n    source=\"mild\",\n    regex=[\n        r\"cholangites?.{1,10}(sclero|secondaire)\",\n        r\"fibrose.{1,10}(hepatique|foie)\",\n        r\"hepatite.{1,15}chronique\",\n        r\"hepatopathie\",\n        r\"\\bnash\\b\",\n        r\"(maladie|sydrome).{1,10}Hanot\",\n        r\"surinfections.{1,5}delta\",\n        r\"\\bcbp\\b\",\n        r\"\\bmaf\\b\",\n        r\"(maladie|syndrome).{1,8}hanot\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"\\bdots?\\b\",\n        window=-5,\n    ),\n)\n\nmoderate_severe = dict(\n    source=\"moderate_severe\",\n    regex=[\n        r\"cirrhose\",\n        r\"necrose.{1,10}(hepati|foie)\",\n        r\"varice.{1,10}(estomac|oesopha|gastr)\",\n        r\"\\bvo\\b.{1,5}(stade|grade).(1|2|3|i{1,3})\",\n        r\"hypertension.{1,5}portale\",\n        r\"scleroses.{1,5}hepatoportale\",\n        r\"sydrome.{1,10}hepato.?ren\",\n        r\"insuffisance.{1,5}hepa\",\n        r\"encephalopathie.{1,5}hepa\",\n        r\"\\btips\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ntransplant = dict(\n    source=\"transplant\",\n    regex=[\n        r\"(?&lt;!pre.?)(greffe|transplant).{1,12}(hepatique|foie)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"chc\",\n        window=(-5, 5),\n    ),\n)\n\ndefault_patterns = [\n    mild,\n    moderate_severe,\n    transplant,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"MILD\"</code> for mild liver diseases</li> <li><code>\"MODERATE_TO_SEVERE\"</code> else</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.liver_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Il y a une fibrose h\u00e9patique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [fibrose h\u00e9patique]\n</code></pre> <pre><code>text = \"Une h\u00e9patite B chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [h\u00e9patite B chronique]\n</code></pre> <pre><code>text = \"Le patient consulte pour une cirrhose\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [cirrhose]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre> <pre><code>text = \"Greffe h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [Greffe h\u00e9patique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'mild', 'regex': ['cholangites?.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>liver_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'liver_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.liver_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/patterns/","title":"<code>edsnlp.pipes.ner.disorders.liver_disease.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/","title":"<code>edsnlp.pipes.ner.disorders.lymphoma</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/","title":"<code>edsnlp.pipes.ner.disorders.lymphoma.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component","title":"<code>create_component = registry.factory.register('eds.lymphoma', assigns=['doc.ents', 'doc.spans'])(LymphomaMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.lymphoma</code> pipeline component extracts mentions of lymphoma.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"lymphom(?:.{1,10}hodgkin)\",\n        r\"lymphom\",\n        r\"lymphangio\",\n        r\"sezary\",\n        r\"burkitt\",\n        r\"kaposi\",\n        r\"hodgkin\",\n        r\"amylose\",\n        r\"plasm[ao]cytome\",\n        r\"lympho.{1,3}sarcome\",\n        r\"lympho.?prolif\",\n        r\"hemopathie.{1,10}lymphoide\",\n        r\"macroglobulinemie\",\n        r\"immunocytome\",\n        r\"maladie.des.chaine\",\n        r\"histiocytose.{1,5}(maligne|langerhans)\",\n        r\"waldenst(ro|or)m\",\n        r\"mycos.{1,10}fongoide\",\n        r\"myelome\",\n        r\"maladie.{1,5}immunoproliferative.{1,5}maligne\",\n        r\"leucemie.{1,10}plasmocyte\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLNH\\b\",\n        r\"\\bLH\\b\",\n        r\"\\bEATL\\b\",\n        r\"\\bLAGC\\b\",\n        r\"\\bLDGCB\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=[\"/L\", \"/mL\"],\n        window=10,\n    ),\n)\n\n\ngammapathy = dict(\n    source=\"gammapathy\",\n    regex=[\n        r\"gammapathie monoclonale\",\n    ],\n    exclude=dict(\n        regex=[\n            \"benin\",\n            \"benign\",\n            \"signification.indeter\",\n            \"NMSI\",\n            \"MGUS\",\n        ],\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    # gammapathy,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul> <p>Monoclonal gammapathy</p> <p>Monoclonal gammapathies are not extracted by this pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.lymphoma())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Un lymphome de Hodgkin.\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [lymphome de Hodgkin]\n</code></pre> <pre><code>text = \"Atteint d'un Waldenst\u00f6rm\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [Waldenst\u00f6rm]\n</code></pre> <pre><code>text = \"Un LAGC\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [LAGC]\n</code></pre> <pre><code>text = \"anti LAGC: 10^4/mL\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'lymphoma'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['lymphom(?:.{1,10...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>lymphoma</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'lymphoma': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.lymphoma</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/","title":"<code>edsnlp.pipes.ner.disorders.lymphoma.lymphoma</code>","text":"<p><code>eds.lymphoma</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher","title":"<code>LymphomaMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.lymphoma</code> pipeline component extracts mentions of lymphoma.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"lymphom(?:.{1,10}hodgkin)\",\n        r\"lymphom\",\n        r\"lymphangio\",\n        r\"sezary\",\n        r\"burkitt\",\n        r\"kaposi\",\n        r\"hodgkin\",\n        r\"amylose\",\n        r\"plasm[ao]cytome\",\n        r\"lympho.{1,3}sarcome\",\n        r\"lympho.?prolif\",\n        r\"hemopathie.{1,10}lymphoide\",\n        r\"macroglobulinemie\",\n        r\"immunocytome\",\n        r\"maladie.des.chaine\",\n        r\"histiocytose.{1,5}(maligne|langerhans)\",\n        r\"waldenst(ro|or)m\",\n        r\"mycos.{1,10}fongoide\",\n        r\"myelome\",\n        r\"maladie.{1,5}immunoproliferative.{1,5}maligne\",\n        r\"leucemie.{1,10}plasmocyte\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLNH\\b\",\n        r\"\\bLH\\b\",\n        r\"\\bEATL\\b\",\n        r\"\\bLAGC\\b\",\n        r\"\\bLDGCB\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=[\"/L\", \"/mL\"],\n        window=10,\n    ),\n)\n\n\ngammapathy = dict(\n    source=\"gammapathy\",\n    regex=[\n        r\"gammapathie monoclonale\",\n    ],\n    exclude=dict(\n        regex=[\n            \"benin\",\n            \"benign\",\n            \"signification.indeter\",\n            \"NMSI\",\n            \"MGUS\",\n        ],\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    # gammapathy,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul> <p>Monoclonal gammapathy</p> <p>Monoclonal gammapathies are not extracted by this pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.lymphoma())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Un lymphome de Hodgkin.\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [lymphome de Hodgkin]\n</code></pre> <pre><code>text = \"Atteint d'un Waldenst\u00f6rm\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [Waldenst\u00f6rm]\n</code></pre> <pre><code>text = \"Un LAGC\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [LAGC]\n</code></pre> <pre><code>text = \"anti LAGC: 10^4/mL\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'lymphoma'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['lymphom(?:.{1,10...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>lymphoma</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'lymphoma': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.lymphoma</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/patterns/","title":"<code>edsnlp.pipes.ner.disorders.lymphoma.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/","title":"<code>edsnlp.pipes.ner.disorders.myocardial_infarction</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/","title":"<code>edsnlp.pipes.ner.disorders.myocardial_infarction.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component","title":"<code>create_component = registry.factory.register('eds.myocardial_infarction', assigns=['doc.ents', 'doc.spans'])(MyocardialInfarctionMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.myocardial_infarction</code> pipeline component extracts mentions of myocardial infarction. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Mentions of stents with a heart localization</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import HEART\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"coronaropathie\",\n        r\"angor.{1,5}instable\",\n        r\"cardiopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"cardio.?myopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"ischemi.{1,15}myocard\",\n        r\"syndrome.{1,5}corona.{1,10}aigu\",\n        r\"syndrome.{1,5}corona.{1,10}st\",\n        r\"pontage.{1,5}mammaire\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"\\bstent\",\n        r\"endoprothese\",\n        r\"pontage\",\n        r\"anevr[iy]sme\",\n        \"infarctus\",\n        r\"angioplasti\",\n    ],\n    assign=[\n        dict(\n            name=\"heart_localized\",\n            regex=\"(\" + r\"|\".join(HEART) + \")\",\n            window=(-10, 10),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bidm\\b\",\n        r\"\\bsca\\b\",\n        r\"\\batl\\b\",\n    ],\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"segment\",\n        regex=r\"st([+-])\",\n        window=2,\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    with_localization,\n    acronym,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>heart_localized</code>: localization of the stent or bypass</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.myocardial_infarction())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Une cardiopathie isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [cardiopathie isch\u00e9mique]\n</code></pre> <pre><code>text = \"Une cardiopathie non-isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent sur la marginale\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [stent sur la marginale]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [marginale]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"infarctus du myocarde\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [infarctus du myocarde]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [myocarde]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['coronaropathie',...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>myocardial_infarction</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'myocardial_infarction': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.myocardial_infarction</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/","title":"<code>edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction</code>","text":"<p><code>eds.myocardial_infarction</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher","title":"<code>MyocardialInfarctionMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.myocardial_infarction</code> pipeline component extracts mentions of myocardial infarction. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Mentions of stents with a heart localization</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import HEART\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"coronaropathie\",\n        r\"angor.{1,5}instable\",\n        r\"cardiopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"cardio.?myopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"ischemi.{1,15}myocard\",\n        r\"syndrome.{1,5}corona.{1,10}aigu\",\n        r\"syndrome.{1,5}corona.{1,10}st\",\n        r\"pontage.{1,5}mammaire\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"\\bstent\",\n        r\"endoprothese\",\n        r\"pontage\",\n        r\"anevr[iy]sme\",\n        \"infarctus\",\n        r\"angioplasti\",\n    ],\n    assign=[\n        dict(\n            name=\"heart_localized\",\n            regex=\"(\" + r\"|\".join(HEART) + \")\",\n            window=(-10, 10),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bidm\\b\",\n        r\"\\bsca\\b\",\n        r\"\\batl\\b\",\n    ],\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"segment\",\n        regex=r\"st([+-])\",\n        window=2,\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    with_localization,\n    acronym,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>heart_localized</code>: localization of the stent or bypass</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.myocardial_infarction())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Une cardiopathie isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [cardiopathie isch\u00e9mique]\n</code></pre> <pre><code>text = \"Une cardiopathie non-isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent sur la marginale\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [stent sur la marginale]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [marginale]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"infarctus du myocarde\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [infarctus du myocarde]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [myocarde]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['coronaropathie',...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>myocardial_infarction</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'myocardial_infarction': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.myocardial_infarction</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/patterns/","title":"<code>edsnlp.pipes.ner.disorders.myocardial_infarction.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/","title":"<code>edsnlp.pipes.ner.disorders.peptic_ulcer_disease</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/","title":"<code>edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component","title":"<code>create_component = registry.factory.register('eds.peptic_ulcer_disease', assigns=['doc.ents', 'doc.spans'])(PepticUlcerDiseaseMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.peptic_ulcer_disease</code> pipeline component extracts mentions of peptic ulcer disease.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"ulcere.{1,10}gastr\",\n        r\"ulcere.{1,10}duoden\",\n        r\"ulcere.{1,10}antra\",\n        r\"ulcere.{1,10}pept\",\n        r\"ulcere.{1,10}estomac\",\n        r\"ulcere.{1,10}curling\",\n        r\"ulcere.{1,10}bulb\",\n        r\"(\u0153|oe)sophagites.{1,5}pepti.{1,10}ulcer\",\n        r\"gastrite.{1,20}ulcer\",\n        r\"antrite.{1,5}ulcer\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bUGD\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ngeneric = dict(\n    source=\"generic\",\n    regex=r\"ulcere\",\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"is_peptic\",\n        regex=r\"\\b(gastr|digest)\",\n        window=(-20, 20),\n        limit_to_sentence=False,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    generic,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that matches, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peptic_ulcer_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Beaucoup d'ulc\u00e8res gastriques\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res gastriques]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'UGD\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [UGD]\n</code></pre> <pre><code>text = \"La patient \u00e0 des ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Au niveau gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'is_peptic': [gastrique]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'peptic_ulcer_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['ulcere.{1,10}gas...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peptic_ulcer_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peptic_ulcer_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peptic_ulcer_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/patterns/","title":"<code>edsnlp.pipes.ner.disorders.peptic_ulcer_disease.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/","title":"<code>edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease</code>","text":"<p><code>eds.peptic_ulcer_disease</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher","title":"<code>PepticUlcerDiseaseMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.peptic_ulcer_disease</code> pipeline component extracts mentions of peptic ulcer disease.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"ulcere.{1,10}gastr\",\n        r\"ulcere.{1,10}duoden\",\n        r\"ulcere.{1,10}antra\",\n        r\"ulcere.{1,10}pept\",\n        r\"ulcere.{1,10}estomac\",\n        r\"ulcere.{1,10}curling\",\n        r\"ulcere.{1,10}bulb\",\n        r\"(\u0153|oe)sophagites.{1,5}pepti.{1,10}ulcer\",\n        r\"gastrite.{1,20}ulcer\",\n        r\"antrite.{1,5}ulcer\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bUGD\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ngeneric = dict(\n    source=\"generic\",\n    regex=r\"ulcere\",\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"is_peptic\",\n        regex=r\"\\b(gastr|digest)\",\n        window=(-20, 20),\n        limit_to_sentence=False,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    generic,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that matches, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peptic_ulcer_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Beaucoup d'ulc\u00e8res gastriques\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res gastriques]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'UGD\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [UGD]\n</code></pre> <pre><code>text = \"La patient \u00e0 des ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Au niveau gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'is_peptic': [gastrique]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'peptic_ulcer_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'main', 'regex': ['ulcere.{1,10}gas...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peptic_ulcer_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peptic_ulcer_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peptic_ulcer_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/","title":"<code>edsnlp.pipes.ner.disorders.peripheral_vascular_disease</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/","title":"<code>edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component","title":"<code>create_component = registry.factory.register('eds.peripheral_vascular_disease', assigns=['doc.ents', 'doc.spans'])(PeripheralVascularDiseaseMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.peripheral_vascular_disease</code> pipeline component extracts mentions of peripheral vascular disease.</p> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC, BRAIN, HEART, PERIPHERAL\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAOMI\\b\",\n        r\"\\bACOM\\b\",\n        r\"\\bTAO\\b\",\n        r\"\\bSAPL\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bSCS\\b\",\n        r\"\\bTVP\\b\",\n        r\"\\bCAPS\\b\",\n        r\"\\bMTEV\\b\",\n        r\"\\bPTT\\b\",\n        r\"\\bMAT\\b\",\n        r\"\\bSHU\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"\\bbuerger\",\n        r\"takayasu\",\n        r\"\\bhorton\",\n        r\"wegener\",\n        r\"churg.{1,10}strauss\",\n        r\"\\bsneddon\",\n        r\"budd.chiari\",\n        r\"infarctus.{1,5}(renal|splenique|polaire|pulmo)\",\n        r\"ulcere.{1,5}arter\",\n        r\"syndrome.?hemolytique.{1,8}uremique\",\n        r\"granulomatose.{1,10}polyangeite\",\n        r\"occlusion.{1,10}(artere|veine).{1,20}retine\",\n        r\"syndrome.{1,20}anti.?phospho\",\n        r\"embolie.{1,5}pulm\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"angiopathie\",\n        r\"arteriopathies.{1,5}obliterante\",\n        r\"gangren\",\n        r\"claudication\",\n        r\"dissection.{1,10}(aort|arter)\",\n        r\"tromboangeit\",\n        r\"tromboarterit\",\n        r\"(pontage|angioplastie).{1,10}(\\bfem|\\bpop|\\bren|\\bjamb)\",\n        r\"arterite\",\n        r\"(ischemie|infarctus).{1,10}mesenterique\",\n        r\"endarteriectomie\",\n        r\"vascularite\",\n        r\"occlusion.{1,10}terminaisons? carotid\",\n        r\"cryoglobulinemie\",\n        r\"colite.{1,5}ischemi\",\n        r\"embole.{1,10}cholesterol\",\n        r\"purpura.?thrombopenique.?idiopa\",\n        r\"micro.?angiopathie.?thrombotique\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + ASYMPTOMATIC + [r\"inr\\srecommande\\ssous\\savk\"],\n            window=(-8, 8),\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nthrombosis = dict(\n    source=\"thrombosis\",\n    regex=[\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"thrombo.?embo\",\n        r\"phlebit\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + [\"superficiel\", \"\\biv\\b\", \"intra.?vein\"],\n            window=(-15, 15),\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=[\n                \"pre\",\n                \"anti\",\n                \"bilan\",\n            ],\n            window=-4,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"peripheral\",\n            regex=\"(\" + r\"|\".join(PERIPHERAL) + \")\",\n            window=15,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nep = dict(\n    source=\"ep\",\n    regex=r\"\\bEP(?![\\w\\./-])\",\n    regex_attr=\"TEXT\",\n    exclude=[\n        dict(\n            regex=[\n                r\"fibreux\",\n                r\"retin\",\n                r\"\\bfove\",\n                r\"\\boct\\b\",\n                r\"\\bmacula\",\n                r\"prosta\",\n                r\"\\bip\\b\",\n                r\"protocole\",\n                r\"seance\",\n                r\"echange\",\n                r\"ritux\",\n                r\"ivig\",\n                r\"ig.?iv\",\n                r\"\\bctc\",\n                r\"corticoide\",\n                r\"serum\",\n                r\"\\bcure\",\n                r\"plasma\",\n                r\"mensuel\",\n                r\"semaine\",\n                r\"serologi\",\n                r\"espaces.porte\",\n                r\"projet\",\n                r\"bolus\",\n            ],\n            window=(-25, 25),\n            limit_to_sentence=False,\n            regex_attr=\"NORM\",\n        ),\n        dict(\n            regex=[r\"rdv\", r\"les\", r\"des\", r\"angine\"],\n            window=(-3, 0),\n            regex_attr=\"NORM\",\n        ),\n    ],\n)\n\nhypertension = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bhta\\b\",\n        r\"hyper.?tension.?arte\",\n        r\"hyper.?tendu\",\n        r\"hyper.?tension.?essenti\",\n        r\"hypertensi\",\n    ],\n    exclude=dict(\n        regex=\"(pulmo|porta)\",\n        window=3,\n    ),\n)\n\ndefault_patterns = [\n    acronym,\n    other,\n    with_localization,\n    thrombosis,\n    ep,\n    ischemia,\n    hypertension,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peripheral_vascular_disease())\n</code></pre> <p>Below are a few examples:</p> 12345678910111213 <pre><code>text = \"Un AOMI\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [AOMI]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un infarctus r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [infarctus r\u00e9nal]\n</code></pre> <pre><code>text = \"Une angiopathie c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une angiopathie\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [angiopathie]\n</code></pre> <pre><code>text = \"Une thrombose c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose des veines superficielles\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [thrombose]\n</code></pre> <pre><code>text = \"Effectuer un bilan pre-trombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une isch\u00e9mie des MI est remarqu\u00e9e.\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [isch\u00e9mie des MI]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'peripheral': [MI]}\n</code></pre> <pre><code>text = \"Plusieurs cas d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [EP]\n</code></pre> <pre><code>text = \"Effectuer des cures d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est hypertendu\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [hypertendu]\n</code></pre> <pre><code>text = \"Une hypertension portale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'peripheral_vascular_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'acronym', 'regex': ['\\\\bAOMI\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peripheral_vascular_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peripheral_vascular_disease': T...</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peripheral_vascular_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/patterns/","title":"<code>edsnlp.pipes.ner.disorders.peripheral_vascular_disease.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/","title":"<code>edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease</code>","text":"<p><code>eds.peripheral_vascular_disease</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher","title":"<code>PeripheralVascularDiseaseMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.peripheral_vascular_disease</code> pipeline component extracts mentions of peripheral vascular disease.</p> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC, BRAIN, HEART, PERIPHERAL\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAOMI\\b\",\n        r\"\\bACOM\\b\",\n        r\"\\bTAO\\b\",\n        r\"\\bSAPL\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bSCS\\b\",\n        r\"\\bTVP\\b\",\n        r\"\\bCAPS\\b\",\n        r\"\\bMTEV\\b\",\n        r\"\\bPTT\\b\",\n        r\"\\bMAT\\b\",\n        r\"\\bSHU\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"\\bbuerger\",\n        r\"takayasu\",\n        r\"\\bhorton\",\n        r\"wegener\",\n        r\"churg.{1,10}strauss\",\n        r\"\\bsneddon\",\n        r\"budd.chiari\",\n        r\"infarctus.{1,5}(renal|splenique|polaire|pulmo)\",\n        r\"ulcere.{1,5}arter\",\n        r\"syndrome.?hemolytique.{1,8}uremique\",\n        r\"granulomatose.{1,10}polyangeite\",\n        r\"occlusion.{1,10}(artere|veine).{1,20}retine\",\n        r\"syndrome.{1,20}anti.?phospho\",\n        r\"embolie.{1,5}pulm\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"angiopathie\",\n        r\"arteriopathies.{1,5}obliterante\",\n        r\"gangren\",\n        r\"claudication\",\n        r\"dissection.{1,10}(aort|arter)\",\n        r\"tromboangeit\",\n        r\"tromboarterit\",\n        r\"(pontage|angioplastie).{1,10}(\\bfem|\\bpop|\\bren|\\bjamb)\",\n        r\"arterite\",\n        r\"(ischemie|infarctus).{1,10}mesenterique\",\n        r\"endarteriectomie\",\n        r\"vascularite\",\n        r\"occlusion.{1,10}terminaisons? carotid\",\n        r\"cryoglobulinemie\",\n        r\"colite.{1,5}ischemi\",\n        r\"embole.{1,10}cholesterol\",\n        r\"purpura.?thrombopenique.?idiopa\",\n        r\"micro.?angiopathie.?thrombotique\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + ASYMPTOMATIC + [r\"inr\\srecommande\\ssous\\savk\"],\n            window=(-8, 8),\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nthrombosis = dict(\n    source=\"thrombosis\",\n    regex=[\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"thrombo.?embo\",\n        r\"phlebit\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + [\"superficiel\", \"\\biv\\b\", \"intra.?vein\"],\n            window=(-15, 15),\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=[\n                \"pre\",\n                \"anti\",\n                \"bilan\",\n            ],\n            window=-4,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"peripheral\",\n            regex=\"(\" + r\"|\".join(PERIPHERAL) + \")\",\n            window=15,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nep = dict(\n    source=\"ep\",\n    regex=r\"\\bEP(?![\\w\\./-])\",\n    regex_attr=\"TEXT\",\n    exclude=[\n        dict(\n            regex=[\n                r\"fibreux\",\n                r\"retin\",\n                r\"\\bfove\",\n                r\"\\boct\\b\",\n                r\"\\bmacula\",\n                r\"prosta\",\n                r\"\\bip\\b\",\n                r\"protocole\",\n                r\"seance\",\n                r\"echange\",\n                r\"ritux\",\n                r\"ivig\",\n                r\"ig.?iv\",\n                r\"\\bctc\",\n                r\"corticoide\",\n                r\"serum\",\n                r\"\\bcure\",\n                r\"plasma\",\n                r\"mensuel\",\n                r\"semaine\",\n                r\"serologi\",\n                r\"espaces.porte\",\n                r\"projet\",\n                r\"bolus\",\n            ],\n            window=(-25, 25),\n            limit_to_sentence=False,\n            regex_attr=\"NORM\",\n        ),\n        dict(\n            regex=[r\"rdv\", r\"les\", r\"des\", r\"angine\"],\n            window=(-3, 0),\n            regex_attr=\"NORM\",\n        ),\n    ],\n)\n\nhypertension = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bhta\\b\",\n        r\"hyper.?tension.?arte\",\n        r\"hyper.?tendu\",\n        r\"hyper.?tension.?essenti\",\n        r\"hypertensi\",\n    ],\n    exclude=dict(\n        regex=\"(pulmo|porta)\",\n        window=3,\n    ),\n)\n\ndefault_patterns = [\n    acronym,\n    other,\n    with_localization,\n    thrombosis,\n    ep,\n    ischemia,\n    hypertension,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to <code>\"PRESENT\"</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peripheral_vascular_disease())\n</code></pre> <p>Below are a few examples:</p> 12345678910111213 <pre><code>text = \"Un AOMI\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [AOMI]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un infarctus r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [infarctus r\u00e9nal]\n</code></pre> <pre><code>text = \"Une angiopathie c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une angiopathie\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [angiopathie]\n</code></pre> <pre><code>text = \"Une thrombose c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose des veines superficielles\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [thrombose]\n</code></pre> <pre><code>text = \"Effectuer un bilan pre-trombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une isch\u00e9mie des MI est remarqu\u00e9e.\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [isch\u00e9mie des MI]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'peripheral': [MI]}\n</code></pre> <pre><code>text = \"Plusieurs cas d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [EP]\n</code></pre> <pre><code>text = \"Effectuer des cures d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est hypertendu\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [hypertendu]\n</code></pre> <pre><code>text = \"Une hypertension portale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'peripheral_vascular_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> DEFAULT: <code>[{'source': 'acronym', 'regex': ['\\\\bAOMI\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peripheral_vascular_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peripheral_vascular_disease': T...</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peripheral_vascular_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/","title":"<code>edsnlp.pipes.ner.disorders.solid_tumor</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/","title":"<code>edsnlp.pipes.ner.disorders.solid_tumor.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component","title":"<code>create_component = registry.factory.register('eds.solid_tumor', assigns=['doc.ents', 'doc.spans'])(SolidTumorMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.solid_tumor</code> pipeline component extracts mentions of solid tumors. It will notably match:</p> Details of the used patterns <pre><code># fmt: off\nBENINE = r\"benign|benin|(grade.?\\b[i1]\\b)\"\nSTAGE = r\"stade ([^\\s]*)\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"carcinom(?!.{0,10}in.?situ)\",\n        r\"seminome\",\n        r\"(?&lt;!lympho)(?&lt;!lympho-)sarcome\",\n        r\"blastome\",\n        r\"cancer([^o]|\\s|\\b)\",\n        r\"adamantinome\",\n        r\"chordome\",\n        r\"craniopharyngiome\",\n        r\"melanome\",\n        r\"neoplasie\",\n        r\"neoplasme\",\n        r\"linite\",\n        r\"melanome\",\n        r\"mesoteliome\",\n        r\"mesotheliome\",\n        r\"seminome\",\n        r\"myxome\",\n        r\"paragangliome\",\n        r\"craniopharyngiome\",\n        r\"k .{0,5}(prostate|sein)\",\n        r\"pancoast.?tobias\",\n        r\"syndrome.{1,10}lynch\",\n        r\"li.?fraumeni\",\n        r\"germinome\",\n        r\"adeno[\\s-]?k\",\n        r\"thymome\",\n        r\"\\bnut\\b\",\n        r\"\\bgist\\b\",\n        r\"\\bchc\\b\",\n        r\"\\badk\\b\",\n        r\"\\btves\\b\",\n        r\"\\btv.tves\\b\",\n        r\"lesion.{1,20}tumor\",\n        r\"tumeur\",\n        r\"carcinoid\",\n        r\"histiocytome\",\n        r\"ependymome\",\n        # r\"primitif\", Trop de FP\n    ],\n    exclude=dict(\n        regex=BENINE,\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"metastasis\",\n            regex=r\"(metasta|multinodul)\",\n            window=(-3, 7),\n            reduce_mode=\"keep_last\",\n        ),\n        dict(\n            name=\"stage\",\n            regex=STAGE,\n            window=7,\n            reduce_mode=\"keep_last\",\n        ),\n    ],\n)\n\nmetastasis_pattern = dict(\n    source=\"metastasis\",\n    regex=[\n        r\"cellule.{1,5}tumorale.{1,5}circulantes\",\n        r\"metasta\",\n        r\"multinodul\",\n        r\"carcinose\",\n        r\"ruptures.{1,5}corticale\",\n        r\"envahissement.{0,15}parties\\smolle\",\n        r\"(localisation|lesion)s?.{0,20}second\",\n        r\"(lymphangite|meningite).{1,5}carcinomateuse\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=r\"goitre\",\n        window=-3,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    metastasis_pattern,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"METASTASIS\"</code> for tumors at the metastatic stage</li> <li><code>\"LOCALIZED\"</code> else</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: stage of the tumor</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.solid_tumor())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un carcinome intra-h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [carcinome]\n</code></pre> <pre><code>text = \"Patient avec un K sein.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [K sein]\n</code></pre> <pre><code>text = \"Il y a une tumeur b\u00e9nigne\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Tumeur m\u00e9tastas\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Tumeur m\u00e9tastas\u00e9e]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'metastasis': m\u00e9tastas\u00e9e}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 4\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 4]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'stage': 4}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 2\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 2]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': 2}\n</code></pre> <pre><code>text = \"Pr\u00e9sence de nombreuses l\u00e9sions secondaires\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [l\u00e9sions secondaires]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['carcinom(?!.{0,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>solid_tumor</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'solid_tumor': True}</code> </p> <code>use_tnm</code> <p>Whether to use TNM scores matching as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.solid_tumor</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/patterns/","title":"<code>edsnlp.pipes.ner.disorders.solid_tumor.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/","title":"<code>edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor</code>","text":"<p><code>eds.solid_tumor</code> pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher","title":"<code>SolidTumorMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.solid_tumor</code> pipeline component extracts mentions of solid tumors. It will notably match:</p> Details of the used patterns <pre><code># fmt: off\nBENINE = r\"benign|benin|(grade.?\\b[i1]\\b)\"\nSTAGE = r\"stade ([^\\s]*)\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"carcinom(?!.{0,10}in.?situ)\",\n        r\"seminome\",\n        r\"(?&lt;!lympho)(?&lt;!lympho-)sarcome\",\n        r\"blastome\",\n        r\"cancer([^o]|\\s|\\b)\",\n        r\"adamantinome\",\n        r\"chordome\",\n        r\"craniopharyngiome\",\n        r\"melanome\",\n        r\"neoplasie\",\n        r\"neoplasme\",\n        r\"linite\",\n        r\"melanome\",\n        r\"mesoteliome\",\n        r\"mesotheliome\",\n        r\"seminome\",\n        r\"myxome\",\n        r\"paragangliome\",\n        r\"craniopharyngiome\",\n        r\"k .{0,5}(prostate|sein)\",\n        r\"pancoast.?tobias\",\n        r\"syndrome.{1,10}lynch\",\n        r\"li.?fraumeni\",\n        r\"germinome\",\n        r\"adeno[\\s-]?k\",\n        r\"thymome\",\n        r\"\\bnut\\b\",\n        r\"\\bgist\\b\",\n        r\"\\bchc\\b\",\n        r\"\\badk\\b\",\n        r\"\\btves\\b\",\n        r\"\\btv.tves\\b\",\n        r\"lesion.{1,20}tumor\",\n        r\"tumeur\",\n        r\"carcinoid\",\n        r\"histiocytome\",\n        r\"ependymome\",\n        # r\"primitif\", Trop de FP\n    ],\n    exclude=dict(\n        regex=BENINE,\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"metastasis\",\n            regex=r\"(metasta|multinodul)\",\n            window=(-3, 7),\n            reduce_mode=\"keep_last\",\n        ),\n        dict(\n            name=\"stage\",\n            regex=STAGE,\n            window=7,\n            reduce_mode=\"keep_last\",\n        ),\n    ],\n)\n\nmetastasis_pattern = dict(\n    source=\"metastasis\",\n    regex=[\n        r\"cellule.{1,5}tumorale.{1,5}circulantes\",\n        r\"metasta\",\n        r\"multinodul\",\n        r\"carcinose\",\n        r\"ruptures.{1,5}corticale\",\n        r\"envahissement.{0,15}parties\\smolle\",\n        r\"(localisation|lesion)s?.{0,20}second\",\n        r\"(lymphangite|meningite).{1,5}carcinomateuse\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=r\"goitre\",\n        window=-3,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    metastasis_pattern,\n]\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"METASTASIS\"</code> for tumors at the metastatic stage</li> <li><code>\"LOCALIZED\"</code> else</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: stage of the tumor</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.solid_tumor())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un carcinome intra-h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [carcinome]\n</code></pre> <pre><code>text = \"Patient avec un K sein.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [K sein]\n</code></pre> <pre><code>text = \"Il y a une tumeur b\u00e9nigne\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Tumeur m\u00e9tastas\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Tumeur m\u00e9tastas\u00e9e]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'metastasis': m\u00e9tastas\u00e9e}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 4\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 4]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'stage': 4}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 2\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 2]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': 2}\n</code></pre> <pre><code>text = \"Pr\u00e9sence de nombreuses l\u00e9sions secondaires\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [l\u00e9sions secondaires]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['carcinom(?!.{0,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>solid_tumor</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'solid_tumor': True}</code> </p> <code>use_tnm</code> <p>Whether to use TNM scores matching as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.solid_tumor</code> component was developed by AP-HP's Data Science team with a team of medical experts. A paper describing in details the development of those components is being drafted and will soon be available.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/terms/","title":"<code>edsnlp.pipes.ner.disorders.terms</code>","text":""},{"location":"reference/edsnlp/pipes/ner/drugs/","title":"<code>edsnlp.pipes.ner.drugs</code>","text":""},{"location":"reference/edsnlp/pipes/ner/drugs/factory/","title":"<code>edsnlp.pipes.ner.drugs.factory</code>","text":"<ol><li><p><p>Cossin S., Lebrun L., Lobre G., Loustau R., Jouhet V., Griffier R., Mougin F., Diallo G. and Thiessard F., 2019. Romedi: An Open Data Source About French Drugs on the Semantic Web. {Studies in Health Technology and Informatics}. 264, pp.79-82. 10.3233/SHTI190187</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/drugs/factory/#edsnlp.pipes.ner.drugs.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.drugs</code> pipeline component detects mentions of French drugs (brand names and active ingredients) and adds them to <code>doc.ents</code>. Each drug is mapped to an ATC code through the Romedi terminology (Cossin et al., 2019). The ATC classifies drugs into groups.</p>"},{"location":"reference/edsnlp/pipes/ner/drugs/factory/#edsnlp.pipes.ner.drugs.factory.create_component--examples","title":"Examples","text":"<p>In this example, we are looking for an oral antidiabetic medication (ATC code: A10B).</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.drugs(term_matcher=\"exact\"))\n\ntext = \"Traitement habituel: Kard\u00e9gic, cardensiel (bisoprolol), glucophage, lasilix\"\n\ndoc = nlp(text)\n\ndrugs_detected = [(x.text, x.kb_id_) for x in doc.ents]\n\ndrugs_detected[0]\n# Out: ('Kard\u00e9gic', 'B01AC06')\n\nlen(drugs_detected)\n# Out: 5\n\noral_antidiabetics_detected = list(\n    filter(lambda x: (x[1].startswith(\"A10B\")), drugs_detected)\n)\noral_antidiabetics_detected\n# Out: [('glucophage', 'A10BA02')]\n</code></pre> <p>Glucophage is the brand name of a medication that contains metformine, the first-line medication for the treatment of type 2 diabetes.</p>"},{"location":"reference/edsnlp/pipes/ner/drugs/factory/#edsnlp.pipes.ner.drugs.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'drugs'</code> </p> <code>attr</code> <p>The default attribute to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'drug'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'drug': True}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/drugs/factory/#edsnlp.pipes.ner.drugs.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.drugs</code> pipeline was developed by the IAM team and CHU de Bordeaux's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/ner/drugs/patterns/","title":"<code>edsnlp.pipes.ner.drugs.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/","title":"<code>edsnlp.pipes.ner.scores</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/base_score/","title":"<code>edsnlp.pipes.ner.scores.base_score</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/base_score/#edsnlp.pipes.ner.scores.base_score.SimpleScoreMatcher","title":"<code>SimpleScoreMatcher</code>","text":"<p>           Bases: <code>ContextualMatcher</code></p> <p>Matcher component to extract a numeric score</p>"},{"location":"reference/edsnlp/pipes/ner/scores/base_score/#edsnlp.pipes.ner.scores.base_score.SimpleScoreMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>label</code> <p>The name of the extracted score</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>None</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>score_name</code> <p>Deprecated, use <code>label</code> instead. The name of the extracted score</p> <p> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/ner/scores/base_score/#edsnlp.pipes.ner.scores.base_score.SimpleScoreMatcher.process","title":"<code>process</code>","text":"<p>Extracts, if available, the value of the score. Normalizes the score via the provided <code>self.score_normalization</code> method.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/base_score/#edsnlp.pipes.ner.scores.base_score.SimpleScoreMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to process</p> <p> TYPE: <code>Doc</code> </p> YIELDS DESCRIPTION <code>Span</code> <p>Matches with, if found, an added <code>score_value</code> extension</p>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/","title":"<code>edsnlp.pipes.ner.scores.charlson</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/","title":"<code>edsnlp.pipes.ner.scores.charlson.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/#edsnlp.pipes.ner.scores.charlson.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.charlson</code> component extracts the Charlson Comorbidity Index.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/#edsnlp.pipes.ner.scores.charlson.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.charlson())\n\ntext = \"\"\"\nCharlson \u00e0 l'admission: 7.\nCharlson:\nOMS:\n\"\"\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (Charlson \u00e0 l'admission: 7,)\n</code></pre> <p>We can see that only one occurrence was extracted. The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/#edsnlp.pipes.ner.scores.charlson.factory.create_component--extensions","title":"Extensions","text":"<p>Each extraction exposes 2 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'charlson'\n\nent._.score_value\n# Out: 7\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/#edsnlp.pipes.ner.scores.charlson.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'charlson'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'charlson'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'charlson': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/patterns/","title":"<code>edsnlp.pipes.ner.scores.charlson.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/charlson/patterns/#edsnlp.pipes.ner.scores.charlson.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>Charlson score normalization. If available, returns the integer value of the Charlson score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/","title":"<code>edsnlp.pipes.ner.scores.elston_ellis</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/factory/","title":"<code>edsnlp.pipes.ner.scores.elston_ellis.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/factory/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component","title":"<code>create_component</code>","text":"<p>Matcher for the Elston-Ellis score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/factory/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.elston_ellis())\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/factory/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elston_ellis'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elston_ellis'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'elston_ellis': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/patterns/","title":"<code>edsnlp.pipes.ner.scores.elston_ellis.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/patterns/#edsnlp.pipes.ner.scores.elston_ellis.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>Elston and Ellis score normalization. If available, returns the integer value of the Elston and Ellis score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/","title":"<code>edsnlp.pipes.ner.scores.emergency</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/","title":"<code>edsnlp.pipes.ner.scores.emergency.ccmu</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/factory/","title":"<code>edsnlp.pipes.ner.scores.emergency.ccmu.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/factory/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component","title":"<code>create_component</code>","text":"<p>Matcher for explicit mentions of the French CCMU emergency score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/factory/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_ccmu())\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/factory/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'emergency_ccmu'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value otherwise</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_ccmu'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_ccmu': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/patterns/","title":"<code>edsnlp.pipes.ner.scores.emergency.ccmu.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/patterns/#edsnlp.pipes.ner.scores.emergency.ccmu.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>CCMU score normalization. If available, returns the integer value of the CCMU score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/","title":"<code>edsnlp.pipes.ner.scores.emergency.gemsa</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/factory/","title":"<code>edsnlp.pipes.ner.scores.emergency.gemsa.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/factory/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component","title":"<code>create_component</code>","text":"<p>Matcher for explicit mentions of the French GEMSA emergency score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/factory/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_gemsa())\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/factory/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_gemsa'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value otherwise</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_gemsa'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_gemsa': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/patterns/","title":"<code>edsnlp.pipes.ner.scores.emergency.gemsa.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/patterns/#edsnlp.pipes.ner.scores.emergency.gemsa.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>GEMSA score normalization. If available, returns the integer value of the GEMSA score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/","title":"<code>edsnlp.pipes.ner.scores.emergency.priority</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/factory/","title":"<code>edsnlp.pipes.ner.scores.emergency.priority.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/factory/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component","title":"<code>create_component</code>","text":"<p>Matcher for explicit mentions of the French priority emergency score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/factory/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_priority())\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/factory/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_priority'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_priority'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_priority': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/patterns/","title":"<code>edsnlp.pipes.ner.scores.emergency.priority.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/patterns/#edsnlp.pipes.ner.scores.emergency.priority.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>Priority score normalization. If available, returns the integer value of the priority score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/factory/","title":"<code>edsnlp.pipes.ner.scores.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/factory/#edsnlp.pipes.ner.scores.factory.create_component","title":"<code>create_component = registry.factory.register('eds.score', assigns=['doc.ents', 'doc.spans'], deprecated=['score'])(SimpleScoreMatcher)</code>  <code>module-attribute</code>","text":"<p>Matcher component to extract a numeric score</p>"},{"location":"reference/edsnlp/pipes/ner/scores/factory/#edsnlp.pipes.ner.scores.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>label</code> <p>The name of the extracted score</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>None</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>score_name</code> <p>Deprecated, use <code>label</code> instead. The name of the extracted score</p> <p> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/","title":"<code>edsnlp.pipes.ner.scores.sofa</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/","title":"<code>edsnlp.pipes.ner.scores.sofa.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/#edsnlp.pipes.ner.scores.sofa.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.sofa</code> component extracts Sequential Organ Failure Assessment (SOFA) scores, used to track a person's status during the stay in an intensive care unit to determine the extent of a person's organ function or rate failure.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/#edsnlp.pipes.ner.scores.sofa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sofa())\n\ntext = \"\"\"\nSOFA (\u00e0 24H) : 12.\nOMS:\n\"\"\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (SOFA (\u00e0 24H) : 12,)\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/#edsnlp.pipes.ner.scores.sofa.factory.create_component--extensions","title":"Extensions","text":"<p>Each extraction exposes 3 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'sofa'\n\nent._.score_value\n# Out: 12\n\nent._.score_method\n# Out: '24H'\n</code></pre> <p>Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\"</p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/#edsnlp.pipes.ner.scores.sofa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'sofa'</code> </p> <code>regex</code> <p>A list of regexes to identify the SOFA score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex to extract the score value</p> <p> TYPE: <code>Dict[str, str]</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex, and should return - None if no score could be extracted - The desired score value else</p> <p> TYPE: <code>Callable[[Union[str, None]], Any]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Flags to pass to the regex</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sofa'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'sofa': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/patterns/","title":"<code>edsnlp.pipes.ner.scores.sofa.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/sofa/patterns/#edsnlp.pipes.ner.scores.sofa.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>Sofa score normalization. If available, returns the integer value of the SOFA score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/sofa/","title":"<code>edsnlp.pipes.ner.scores.sofa.sofa</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/sofa/sofa/#edsnlp.pipes.ner.scores.sofa.sofa.SofaMatcher","title":"<code>SofaMatcher</code>","text":"<p>           Bases: <code>SimpleScoreMatcher</code></p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/sofa/#edsnlp.pipes.ner.scores.sofa.sofa.SofaMatcher.process","title":"<code>process</code>","text":"<p>Extracts, if available, the value of the score. Normalizes the score via the provided <code>self.score_normalization</code> method.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/sofa/#edsnlp.pipes.ner.scores.sofa.sofa.SofaMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to process</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>ents</code> <p>List of spaCy's spans, with, if found, an added <code>score_value</code> extension</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/tnm/","title":"<code>edsnlp.pipes.ner.tnm</code>","text":""},{"location":"reference/edsnlp/pipes/ner/tnm/factory/","title":"<code>edsnlp.pipes.ner.tnm.factory</code>","text":"<ol><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/tnm/factory/#edsnlp.pipes.ner.tnm.factory.create_component","title":"<code>create_component = registry.factory.register('eds.tnm', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.TNM'])(TNMMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.tnm</code> component extracts TNM mentions from clinical documents.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/factory/#edsnlp.pipes.ner.tnm.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.tnm())\n\ntext = \"TNM: pTx N1 M1\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (pTx N1 M1,)\n\nent = doc.ents[0]\nent._.tnm.dict()\n# {'modifier': 'p',\n#  'tumour': None,\n#  'tumour_specification': 'x',\n#  'node': '1',\n#  'node_specification': None,\n#  'metastasis': '1',\n#  'resection_completeness': None,\n#  'version': None,\n#  'version_year': None}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/tnm/factory/#edsnlp.pipes.ner.tnm.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tnm'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?:\\b|^)(?&lt;=\\(?(?P&lt;version&gt;uicc|accj|tnm|UICC|A...</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tnm</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tnm': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/tnm/factory/#edsnlp.pipes.ner.tnm.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The TNM score is based on the development of S. Priou, B. Rance and E. Kempf (Kempf et al., 2022).</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/model/","title":"<code>edsnlp.pipes.ner.tnm.model</code>","text":""},{"location":"reference/edsnlp/pipes/ner/tnm/model/#edsnlp.pipes.ner.tnm.model.TNM","title":"<code>TNM</code>","text":"<p>           Bases: <code>BaseModel</code></p>"},{"location":"reference/edsnlp/pipes/ner/tnm/model/#edsnlp.pipes.ner.tnm.model.TNM.dict","title":"<code>dict</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/patterns/","title":"<code>edsnlp.pipes.ner.tnm.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/","title":"<code>edsnlp.pipes.ner.tnm.tnm</code>","text":"<p><code>eds.tnm</code> pipeline.</p> <ol><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher","title":"<code>TNMMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>The <code>eds.tnm</code> component extracts TNM mentions from clinical documents.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.tnm())\n\ntext = \"TNM: pTx N1 M1\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (pTx N1 M1,)\n\nent = doc.ents[0]\nent._.tnm.dict()\n# {'modifier': 'p',\n#  'tumour': None,\n#  'tumour_specification': 'x',\n#  'node': '1',\n#  'node_specification': None,\n#  'metastasis': '1',\n#  'resection_completeness': None,\n#  'version': None,\n#  'version_year': None}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tnm'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?:\\b|^)(?&lt;=\\(?(?P&lt;version&gt;uicc|accj|tnm|UICC|A...</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tnm</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tnm': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The TNM score is based on the development of S. Priou, B. Rance and E. Kempf (Kempf et al., 2022).</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set spaCy extensions</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.process","title":"<code>process</code>","text":"<p>Find TNM mentions in doc.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>list of tnm spans</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.parse","title":"<code>parse</code>","text":"<p>Parse dates using the groupdict returned by the matcher.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.parse--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>spans</code> <p>List of tuples containing the spans and groupdict returned by the matcher.</p> <p> TYPE: <code>List[Tuple[Span, Dict[str, str]]]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of processed spans, with the date parsed.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.__call__","title":"<code>__call__</code>","text":"<p>Tags TNM mentions.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for TNM</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/","title":"<code>edsnlp.pipes.ner.umls</code>","text":""},{"location":"reference/edsnlp/pipes/ner/umls/factory/","title":"<code>edsnlp.pipes.ner.umls.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/umls/factory/#edsnlp.pipes.ner.umls.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.umls</code> pipeline component matches the UMLS (Unified Medical Language System from NIH) terminology.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/factory/#edsnlp.pipes.ner.umls.factory.create_component--examples","title":"Examples","text":"<p><code>eds.umls</code> is an additional module that needs to be setup by:</p> <ol> <li><code>pip install -U umls_downloader</code></li> <li>Signing up for a UMLS Terminology    Services Account. After filling a short form, you will receive your token API    within a few days.</li> <li>Set <code>UMLS_API_KEY</code> locally: <code>export UMLS_API_KEY=your_api_key</code></li> </ol> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.umls())\n\ntext = \"Grosse toux: le malade a \u00e9t\u00e9 mordu par des Amphibiens \" \"sous le genou\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (toux, a, par, Amphibiens, genou)\n\nent = doc.ents[0]\n\nent.label_\n# Out: umls\n\nent._.umls\n# Out: C0010200\n</code></pre> <p>You can easily change the default languages and sources with the <code>pattern_config</code> argument:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# Enable the French and English languages, through the French MeSH and LOINC\npattern_config = dict(languages=[\"FRE\", \"ENG\"], sources=[\"MSHFRE\", \"LNC\"])\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.umls(pattern_config=pattern_config))\n</code></pre> <p>See more options of languages and sources here.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/factory/#edsnlp.pipes.ner.umls.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'umls'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either \"exact\" or \"simstring\"</p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>pattern_config</code> <p>The pattern retriever configuration</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>dict(languages=['FRE'], sources=None)</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'umls'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'umls': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/factory/#edsnlp.pipes.ner.umls.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.umls</code> pipeline was developed by AP-HP's Data Science team and INRIA SODA's team.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/","title":"<code>edsnlp.pipes.ner.umls.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_patterns","title":"<code>get_patterns</code>","text":"<p>Load the UMLS terminology patterns.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_patterns--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_patterns--return","title":"Return","text":"<p>patterns : dict[list]     The mapping between CUI codes and their synonyms.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_patterns--notes","title":"Notes","text":"<p>When run for the first time, this method will download the entire UMLS file and store it at ~/.data/bio/umls/2022AA/. Therefore the second run will be significantly faster than the first one.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path","title":"<code>get_path</code>","text":"<p>Get the path, module and filename of the UMLS file.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path--return","title":"Return","text":"<p>path, module, filename : pathlib.Path, pystow.module, str</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path--notes","title":"Notes","text":"<p><code>get_path</code> will convert the config dict into a pretty filename.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path--examples","title":"Examples","text":"<p>config = {\"languages\": [\"FRE\", \"ENG\"], \"sources\": None} print(get_path(config)) .data/bio/umls/2022AA/languagesFRE-ENG_sourcesNone.pkl\"</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.download_and_agg_umls","title":"<code>download_and_agg_umls</code>","text":"<p>Download the UMLS if not exist and create a mapping between CUI code and synonyms.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.download_and_agg_umls--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.download_and_agg_umls--return","title":"Return","text":"<p>patterns : dict[list]     The mapping between CUI codes and their synonyms.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.download_and_agg_umls--notes","title":"Notes","text":"<p>Performs filtering on the returned mapping only, not the downloaded resource.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/","title":"<code>edsnlp.pipes.qualifiers</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/base/","title":"<code>edsnlp.pipes.qualifiers.base</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.RuleBasedQualifier","title":"<code>RuleBasedQualifier</code>","text":"<p>           Bases: <code>BaseSpanAttributeClassifierComponent</code></p> <p>Implements the ConText algorithm (eq. NegEx for negations) for detecting contextual attributes text.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.RuleBasedQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> </p> <code>**terms</code> <p>Terms to look for.</p> <p> TYPE: <code>Dict[str, Optional[List[str]]]</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.RuleBasedQualifier.get_matches","title":"<code>get_matches</code>","text":"<p>Extract matches.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.RuleBasedQualifier.get_matches--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy <code>Doc</code> object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected spans</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/","title":"<code>edsnlp.pipes.qualifiers.family</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/","title":"<code>edsnlp.pipes.qualifiers.family.factory</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component","title":"<code>create_component = registry.factory.register('eds.family', assigns=['span._.family'], deprecated=['family'])(FamilyContextQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.family</code> component uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(\n    eds.matcher(terms=dict(douleur=\"douleur\", osteoporose=\"ost\u00e9oporose\")),\n)\nnlp.add_pipe(eds.family())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents familiaux d'ost\u00e9oporose\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, ost\u00e9oporose)\n\ndoc.ents[0]._.family\n# Out: False\n\ndoc.ents[1]._.family\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.family</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>family</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token relates to a family member.</li> <li>The <code>family_</code> property is a human-readable string, computed from the <code>family</code>    attribute. It implements a simple getter function that outputs <code>PATIENT</code> or    <code>FAMILY</code>, depending on the value of <code>family</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>family</code> <p>List of terms indicating family reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_sections</code> <p>Whether to use annotated sections (namely <code>ant\u00e9c\u00e9dents familiaux</code>).</p> <p> TYPE: <code>bool, by default `False`</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.family</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/","title":"<code>edsnlp.pipes.qualifiers.family.family</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier","title":"<code>FamilyContextQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.family</code> component uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(\n    eds.matcher(terms=dict(douleur=\"douleur\", osteoporose=\"ost\u00e9oporose\")),\n)\nnlp.add_pipe(eds.family())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents familiaux d'ost\u00e9oporose\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, ost\u00e9oporose)\n\ndoc.ents[0]._.family\n# Out: False\n\ndoc.ents[1]._.family\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.family</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>family</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token relates to a family member.</li> <li>The <code>family_</code> property is a human-readable string, computed from the <code>family</code>    attribute. It implements a simple getter function that outputs <code>PATIENT</code> or    <code>FAMILY</code>, depending on the value of <code>family</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>family</code> <p>List of terms indicating family reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_sections</code> <p>Whether to use annotated sections (namely <code>ant\u00e9c\u00e9dents familiaux</code>).</p> <p> TYPE: <code>bool, by default `False`</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.family</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/patterns/","title":"<code>edsnlp.pipes.qualifiers.family.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/history/","title":"<code>edsnlp.pipes.qualifiers.history</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/","title":"<code>edsnlp.pipes.qualifiers.history.factory</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component","title":"<code>create_component = registry.factory.register('eds.history', assigns=['span._.history'], deprecated=['history', 'antecedents', 'eds.antecedents'])(HistoryQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.history</code> pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit.</p> <p>The mere definition of a medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history, e.g., preceded by a synonym of \"medical history\".</p> <p>This component may also use the output of:</p> <ul> <li>the <code>eds.sections</code> component In that case, the entire <code>ant\u00e9c\u00e9dent</code> section is tagged as a medical history.</li> </ul> <p>Sections</p> <p>Be careful, the <code>eds.sections</code> component may oversize the <code>ant\u00e9c\u00e9dents</code> section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the <code>ant\u00e9c\u00e9dents</code> title, some parts of the document will erroneously be tagged as a medical history.</p> <p>To curb that possibility, using the output of the <code>eds.sections</code> component is deactivated by default.</p> <ul> <li>the <code>eds.dates</code> component. In that case, it will take the   dates into account to tag extracted entities as a medical history or not.</li> </ul> <p>Dates</p> <p>To take the most of the <code>eds.dates</code> component, you may add the <code>note_datetime</code> context (cf. Adding context). It allows the component to compute the duration of absolute dates (e.g., le 28 ao\u00fbt 2022/August 28, 2022). The <code>birth_datetime</code> context allows the component to exclude the birthdate from the extracted dates.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", malaise=\"malaises\")))\nnlp.add_pipe(\n    eds.history(\n        use_sections=True,\n        use_dates=True,\n    ),\n)\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents de malaises.\"\n    \"ANT\u00c9C\u00c9DENTS : \"\n    \"- le patient a d\u00e9j\u00e0 eu des malaises. \"\n    \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, malaises, malaises, douleur)\n\ndoc.ents[0]._.history\n# Out: False\n\ndoc.ents[1]._.history\n# Out: True\n\ndoc.ents[2]._.history  # (1)\n# Out: True\n\ndoc.ents[3]._.history  # (2)\n# Out: False\n</code></pre> <ol> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>.</li> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>, however the extracted <code>relative_date</code> refers to an event that took place within 14 days.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.history</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>history</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a medical history.</li> <li>The <code>history_</code> property is a human-readable string, computed from the <code>history</code>    attribute. It implements a simple getter function that outputs <code>CURRENT</code> or    <code>ATCD</code>, depending on the value of <code>history</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'history'</code> </p> <code>history</code> <p>List of terms indicating medical history reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_sections</code> <p>Whether to use section pipeline to detect medical history section.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_dates</code> <p>Whether to use dates pipeline to detect if the event occurs  a long time before the document date.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>history_limit</code> <p>The number of days after which the event is considered as history.</p> <p> TYPE: <code>Union[int, timedelta]</code> DEFAULT: <code>14</code> </p> <code>exclude_birthdate</code> <p>Whether to exclude the birthdate from history dates.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>closest_dates_only</code> <p>Whether to include the closest dates only.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tz</code> <p>The timezone to use. Defaults to \"Europe/Paris\".</p> <p> TYPE: <code>Optional[Union[str, tzinfo]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.history</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/","title":"<code>edsnlp.pipes.qualifiers.history.history</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier","title":"<code>HistoryQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.history</code> pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit.</p> <p>The mere definition of a medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history, e.g., preceded by a synonym of \"medical history\".</p> <p>This component may also use the output of:</p> <ul> <li>the <code>eds.sections</code> component In that case, the entire <code>ant\u00e9c\u00e9dent</code> section is tagged as a medical history.</li> </ul> <p>Sections</p> <p>Be careful, the <code>eds.sections</code> component may oversize the <code>ant\u00e9c\u00e9dents</code> section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the <code>ant\u00e9c\u00e9dents</code> title, some parts of the document will erroneously be tagged as a medical history.</p> <p>To curb that possibility, using the output of the <code>eds.sections</code> component is deactivated by default.</p> <ul> <li>the <code>eds.dates</code> component. In that case, it will take the   dates into account to tag extracted entities as a medical history or not.</li> </ul> <p>Dates</p> <p>To take the most of the <code>eds.dates</code> component, you may add the <code>note_datetime</code> context (cf. Adding context). It allows the component to compute the duration of absolute dates (e.g., le 28 ao\u00fbt 2022/August 28, 2022). The <code>birth_datetime</code> context allows the component to exclude the birthdate from the extracted dates.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", malaise=\"malaises\")))\nnlp.add_pipe(\n    eds.history(\n        use_sections=True,\n        use_dates=True,\n    ),\n)\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents de malaises.\"\n    \"ANT\u00c9C\u00c9DENTS : \"\n    \"- le patient a d\u00e9j\u00e0 eu des malaises. \"\n    \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, malaises, malaises, douleur)\n\ndoc.ents[0]._.history\n# Out: False\n\ndoc.ents[1]._.history\n# Out: True\n\ndoc.ents[2]._.history  # (1)\n# Out: True\n\ndoc.ents[3]._.history  # (2)\n# Out: False\n</code></pre> <ol> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>.</li> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>, however the extracted <code>relative_date</code> refers to an event that took place within 14 days.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.history</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>history</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a medical history.</li> <li>The <code>history_</code> property is a human-readable string, computed from the <code>history</code>    attribute. It implements a simple getter function that outputs <code>CURRENT</code> or    <code>ATCD</code>, depending on the value of <code>history</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'history'</code> </p> <code>history</code> <p>List of terms indicating medical history reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_sections</code> <p>Whether to use section pipeline to detect medical history section.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_dates</code> <p>Whether to use dates pipeline to detect if the event occurs  a long time before the document date.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>history_limit</code> <p>The number of days after which the event is considered as history.</p> <p> TYPE: <code>Union[int, timedelta]</code> DEFAULT: <code>14</code> </p> <code>exclude_birthdate</code> <p>Whether to exclude the birthdate from history dates.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>closest_dates_only</code> <p>Whether to include the closest dates only.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tz</code> <p>The timezone to use. Defaults to \"Europe/Paris\".</p> <p> TYPE: <code>Optional[Union[str, tzinfo]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.history</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/patterns/","title":"<code>edsnlp.pipes.qualifiers.history.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/","title":"<code>edsnlp.pipes.qualifiers.hypothesis</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/","title":"<code>edsnlp.pipes.qualifiers.hypothesis.factory</code>","text":"<ol><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component","title":"<code>create_component = registry.factory.register('eds.hypothesis', assigns=['span._.hypothesis'], deprecated=['hypothesis'])(HypothesisQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.hypothesis</code> pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding hypothesis, ie cues that precede a hypothetical expression</li> <li>following hypothesis, ie cues that follow a hypothetical expression</li> <li>pseudo hypothesis : contain a hypothesis cue, but are not hypothesis   (eg \"pas de doute\"/\"no doubt\")</li> <li>hypothetical verbs : verbs indicating hypothesis (eg \"douter\")</li> <li>classic verbs conjugated to the conditional, thus indicating hypothesis</li> </ul>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", fracture=\"fracture\")))\nnlp.add_pipe(eds.hypothesis())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Possible fracture du radius.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, fracture)\n\ndoc.ents[0]._.hypothesis\n# Out: False\n\ndoc.ents[1]._.hypothesis\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.hypothesis</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>hypothesis</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a speculation.</li> <li>The <code>hypothesis_</code> property is a human-readable string, computed from the    <code>hypothesis</code> attribute. It implements a simple getter function that outputs    <code>HYP</code> or <code>CERT</code>, depending on the value of <code>hypothesis</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at APHP's CDW to test the   component on actual clinical notes, using pseudonymised notes from the APHP's CDW.</li> </ul> Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding hypothesis cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_hyp</code> <p>List of hypothetical verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_eds</code> <p>List of mainstream verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hypothesis</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/","title":"<code>edsnlp.pipes.qualifiers.hypothesis.hypothesis</code>","text":"<ol><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier","title":"<code>HypothesisQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.hypothesis</code> pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding hypothesis, ie cues that precede a hypothetical expression</li> <li>following hypothesis, ie cues that follow a hypothetical expression</li> <li>pseudo hypothesis : contain a hypothesis cue, but are not hypothesis   (eg \"pas de doute\"/\"no doubt\")</li> <li>hypothetical verbs : verbs indicating hypothesis (eg \"douter\")</li> <li>classic verbs conjugated to the conditional, thus indicating hypothesis</li> </ul>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", fracture=\"fracture\")))\nnlp.add_pipe(eds.hypothesis())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Possible fracture du radius.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, fracture)\n\ndoc.ents[0]._.hypothesis\n# Out: False\n\ndoc.ents[1]._.hypothesis\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.hypothesis</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>hypothesis</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a speculation.</li> <li>The <code>hypothesis_</code> property is a human-readable string, computed from the    <code>hypothesis</code> attribute. It implements a simple getter function that outputs    <code>HYP</code> or <code>CERT</code>, depending on the value of <code>hypothesis</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at APHP's CDW to test the   component on actual clinical notes, using pseudonymised notes from the APHP's CDW.</li> </ul> Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding hypothesis cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_hyp</code> <p>List of hypothetical verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_eds</code> <p>List of mainstream verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hypothesis</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier.load_verbs","title":"<code>load_verbs</code>","text":"<p>Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier.load_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs_hyp</code> <p> TYPE: <code>List[str]</code> </p> <code>verbs_eds</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list of hypothesis verbs conjugated at all tenses and classic</code> <code>verbs conjugated to conditional.</code>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/patterns/","title":"<code>edsnlp.pipes.qualifiers.hypothesis.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/negation/","title":"<code>edsnlp.pipes.qualifiers.negation</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/","title":"<code>edsnlp.pipes.qualifiers.negation.factory</code>","text":"<ol><li><p><p>Chapman W.W., Bridewell W., Hanbury P., Cooper G.F. and Buchanan B.G., 2001. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics. 34, pp.301--310. 10.1006/jbin.2001.1029</p></p></li><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component","title":"<code>create_component = registry.factory.register('eds.negation', assigns=['span._.negation'], deprecated=['negation'])(NegationQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.negation</code> component uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al., 2001.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding negations, i.e., cues that precede a negated expression</li> <li>following negations, i.e., cues that follow a negated expression</li> <li>pseudo negations : contain a negation cue, but are not negations   (eg \"pas de doute\"/\"no doubt\")</li> <li>negation verbs, i.e., verbs that indicate a negation</li> <li>terminations, i.e., words that delimit propositions.   The negation spans from the preceding cue to the termination.</li> </ul>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", fracture=\"fracture\")))\nnlp.add_pipe(eds.negation())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Le scanner ne d\u00e9tecte aucune fracture.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, fracture)\n\ndoc.ents[0]._.negation  # (1)\n# Out: False\n\ndoc.ents[1]._.negation\n# Out: True\n</code></pre> <ol> <li>The result of the component is kept in the <code>negation</code> custom extension.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.negation</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>negation</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is negated.</li> <li>The <code>negation_</code> property is a human-readable string, computed from the <code>negation</code>    attribute. It implements a simple getter function that outputs <code>AFF</code> or <code>NEG</code>,    depending on the value of <code>negation</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at AP-HP to test the component   on actual clinical notes, using pseudonymised notes from the AP-HP.</li> </ul> Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding negation cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding_regex</code> <p>List of preceding negation cues, but as regexes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of negation verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.negation</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/","title":"<code>edsnlp.pipes.qualifiers.negation.negation</code>","text":"<ol><li><p><p>Chapman W.W., Bridewell W., Hanbury P., Cooper G.F. and Buchanan B.G., 2001. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics. 34, pp.301--310. 10.1006/jbin.2001.1029</p></p></li><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier","title":"<code>NegationQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.negation</code> component uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al., 2001.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding negations, i.e., cues that precede a negated expression</li> <li>following negations, i.e., cues that follow a negated expression</li> <li>pseudo negations : contain a negation cue, but are not negations   (eg \"pas de doute\"/\"no doubt\")</li> <li>negation verbs, i.e., verbs that indicate a negation</li> <li>terminations, i.e., words that delimit propositions.   The negation spans from the preceding cue to the termination.</li> </ul>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", fracture=\"fracture\")))\nnlp.add_pipe(eds.negation())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Le scanner ne d\u00e9tecte aucune fracture.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, fracture)\n\ndoc.ents[0]._.negation  # (1)\n# Out: False\n\ndoc.ents[1]._.negation\n# Out: True\n</code></pre> <ol> <li>The result of the component is kept in the <code>negation</code> custom extension.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.negation</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>negation</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is negated.</li> <li>The <code>negation_</code> property is a human-readable string, computed from the <code>negation</code>    attribute. It implements a simple getter function that outputs <code>AFF</code> or <code>NEG</code>,    depending on the value of <code>negation</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at AP-HP to test the component   on actual clinical notes, using pseudonymised notes from the AP-HP.</li> </ul> Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding negation cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding_regex</code> <p>List of preceding negation cues, but as regexes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of negation verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.negation</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier.load_verbs","title":"<code>load_verbs</code>","text":"<p>Conjugate negating verbs to specific tenses.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier.load_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list_neg_verbs_preceding</code> <p> TYPE: <code>List of conjugated negating verbs preceding entities.</code> </p> <code>list_neg_verbs_following</code> <p> TYPE: <code>List of conjugated negating verbs following entities.</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/patterns/","title":"<code>edsnlp.pipes.qualifiers.negation.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/","title":"<code>edsnlp.pipes.qualifiers.reported_speech</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/","title":"<code>edsnlp.pipes.qualifiers.reported_speech.factory</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component","title":"<code>create_component = registry.factory.register('eds.reported_speech', assigns=['span._.reported_speech'], deprecated=['reported_speech', 'rspeech'])(ReportedSpeechQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.reported_speech</code> component uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", alcool=\"alcoolis\u00e9\")))\nnlp.add_pipe(eds.reported_speech())\n\ntext = (\n    \"Le patient est admis aux urgences ce soir pour une douleur au bras. \"\n    \"Il nie \u00eatre alcoolis\u00e9.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, alcoolis\u00e9)\n\ndoc.ents[0]._.reported_speech\n# Out: False\n\ndoc.ents[1]._.reported_speech\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.reported_speech</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>reported_speech</code> attribute is a boolean, set to <code>True</code> if the component    predicts that the span/token is reported.</li> <li>The <code>reported_speech_</code> property is a human-readable string, computed from the    <code>reported_speech</code> attribute. It implements a simple getter function that outputs    <code>DIRECT</code> or <code>REPORTED</code>, depending on the value of <code>reported_speech</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>quotation</code> <p>String gathering all quotation cues.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of reported speech verbs.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of terms following a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of terms preceding a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reported_speech</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/patterns/","title":"<code>edsnlp.pipes.qualifiers.reported_speech.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/","title":"<code>edsnlp.pipes.qualifiers.reported_speech.reported_speech</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier","title":"<code>ReportedSpeechQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.reported_speech</code> component uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", alcool=\"alcoolis\u00e9\")))\nnlp.add_pipe(eds.reported_speech())\n\ntext = (\n    \"Le patient est admis aux urgences ce soir pour une douleur au bras. \"\n    \"Il nie \u00eatre alcoolis\u00e9.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, alcoolis\u00e9)\n\ndoc.ents[0]._.reported_speech\n# Out: False\n\ndoc.ents[1]._.reported_speech\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.reported_speech</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>reported_speech</code> attribute is a boolean, set to <code>True</code> if the component    predicts that the span/token is reported.</li> <li>The <code>reported_speech_</code> property is a human-readable string, computed from the    <code>reported_speech</code> attribute. It implements a simple getter function that outputs    <code>DIRECT</code> or <code>REPORTED</code>, depending on the value of <code>reported_speech</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>quotation</code> <p>String gathering all quotation cues.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of reported speech verbs.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of terms following a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of terms preceding a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reported_speech</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier.load_verbs","title":"<code>load_verbs</code>","text":"<p>Conjugate reporting verbs to specific tenses (trhid person)</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier.load_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list_rep_verbs</code> <p> TYPE: <code>List of reporting verbs conjugated to specific tenses.</code> </p>"},{"location":"reference/edsnlp/pipes/terminations/","title":"<code>edsnlp.pipes.terminations</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/","title":"<code>edsnlp.pipes.trainable</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/","title":"<code>edsnlp.pipes.trainable.embeddings</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/","title":"<code>edsnlp.pipes.trainable.embeddings.span_pooler</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/factory/","title":"<code>edsnlp.pipes.trainable.embeddings.span_pooler.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/factory/#edsnlp.pipes.trainable.embeddings.span_pooler.factory.create_component","title":"<code>create_component = registry.factory.register('eds.span_pooler', assigns=[], deprecated=[])(SpanPooler)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.span_pooler</code> component is a trainable span embedding component. It generates span embeddings from a word embedding component and a span getter. It can be used to train a span classifier, as in <code>eds.span_classifier</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/factory/#edsnlp.pipes.trainable.embeddings.span_pooler.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_pooler'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>pooling_mode</code> <p>How word embeddings are aggregated into a single embedding per span.</p> <p> TYPE: <code>Literal['max', 'sum', 'mean']</code> DEFAULT: <code>mean</code> </p> <code>hidden_size</code> <p>The size of the hidden layer. If None, no projection is done and the output of the span pooler is used directly.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/","title":"<code>edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPoolerBatchInput","title":"<code>SpanPoolerBatchInput = TypedDict('SpanPoolerBatchInput', {'embedding': BatchInput, 'begins': ft.FoldedTensor, 'ends': ft.FoldedTensor, 'sequence_idx': torch.Tensor})</code>  <code>module-attribute</code>","text":"<p>embeds: torch.FloatTensor     Token embeddings to predict the tags from begins: torch.LongTensor     Begin offsets of the spans ends: torch.LongTensor     End offsets of the spans sequence_idx: torch.LongTensor     Sequence (cf Embedding spans) index of the spans</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPooler","title":"<code>SpanPooler</code>","text":"<p>           Bases: <code>SpanEmbeddingComponent</code>, <code>BaseComponent</code></p> <p>The <code>eds.span_pooler</code> component is a trainable span embedding component. It generates span embeddings from a word embedding component and a span getter. It can be used to train a span classifier, as in <code>eds.span_classifier</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPooler--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_pooler'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>pooling_mode</code> <p>How word embeddings are aggregated into a single embedding per span.</p> <p> TYPE: <code>Literal['max', 'sum', 'mean']</code> DEFAULT: <code>mean</code> </p> <code>hidden_size</code> <p>The size of the hidden layer. If None, no projection is done and the output of the span pooler is used directly.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPooler.forward","title":"<code>forward</code>","text":"<p>Apply the span classifier module to the document embeddings and given spans to: - compute the loss - and/or predict the labels of spans If labels are predicted, they are assigned to the <code>additional_outputs</code> dictionary.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPooler.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>The input batch</p> <p> TYPE: <code>SpanPoolerBatchInput</code> </p> RETURNS DESCRIPTION <code>BatchOutput</code>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/","title":"<code>edsnlp.pipes.trainable.embeddings.text_cnn</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/factory/","title":"<code>edsnlp.pipes.trainable.embeddings.text_cnn.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/factory/#edsnlp.pipes.trainable.embeddings.text_cnn.factory.create_component","title":"<code>create_component = registry.factory.register('eds.text_cnn', assigns=[], deprecated=[])(TextCnnEncoder)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.text_cnn</code> component is a simple 1D convolutional network to contextualize word embeddings (as computed by the <code>embedding</code> component passed as argument).</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/factory/#edsnlp.pipes.trainable.embeddings.text_cnn.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>embedding</code> <p>Embedding module to apply to the input</p> <p> TYPE: <code>TorchComponent[WordEmbeddingBatchOutput, BatchInput]</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>relu</code> </p> <code>residual</code> <p>Whether to use residual connections</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize</code> <p>Whether to normalize before or after the residual connection</p> <p> TYPE: <code>Literal['pre', 'post', 'none']</code> DEFAULT: <code>pre</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/","title":"<code>edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn.TextCnnEncoder","title":"<code>TextCnnEncoder</code>","text":"<p>           Bases: <code>WordContextualizerComponent</code></p> <p>The <code>eds.text_cnn</code> component is a simple 1D convolutional network to contextualize word embeddings (as computed by the <code>embedding</code> component passed as argument).</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn.TextCnnEncoder--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>embedding</code> <p>Embedding module to apply to the input</p> <p> TYPE: <code>TorchComponent[WordEmbeddingBatchOutput, BatchInput]</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>relu</code> </p> <code>residual</code> <p>Whether to use residual connections</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize</code> <p>Whether to normalize before or after the residual connection</p> <p> TYPE: <code>Literal['pre', 'post', 'none']</code> DEFAULT: <code>pre</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn.TextCnnEncoder.forward","title":"<code>forward</code>","text":"<p>Encode embeddings with a 1d convolutional network</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn.TextCnnEncoder.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <ul> <li>embeddings: embeddings of shape (batch_size, seq_len, input_size)</li> <li>mask: mask of shape (batch_size, seq_len)</li> </ul> <p> TYPE: <code>BatchInput</code> </p> RETURNS DESCRIPTION <code>WordEmbeddingBatchOutput</code> <ul> <li>embeddings: encoded embeddings of shape (batch_size, seq_len, input_size)</li> <li>mask: (same) mask of shape (batch_size, seq_len)</li> </ul>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/","title":"<code>edsnlp.pipes.trainable.embeddings.transformer</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/","title":"<code>edsnlp.pipes.trainable.embeddings.transformer.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component","title":"<code>create_component = registry.factory.register('eds.transformer', assigns=[], deprecated=[])(Transformer)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.transformer</code> component is a wrapper around HuggingFace's transformers library. If you are not familiar with transformers, a good way to start is the Illustrated Transformer tutorial.</p> <p>Compared to using the raw Huggingface model, we offer a simple mechanism to split long documents into strided windows before feeding them to the model.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--windowing","title":"Windowing","text":"<p>EDS-NLP's Transformer component splits long documents into smaller windows before feeding them to the model. This is done to avoid hitting the maximum number of tokens that can be processed by the model on a single device. The window size and stride can be configured using the <code>window</code> and <code>stride</code> parameters. The default values are 512 and 256 respectively, which means that the model will process windows of 512 tokens, each separated by 256 tokens. Whenever a token appears in multiple windows, the embedding of the \"most contextualized\" occurrence is used, i.e. the occurrence that is the closest to the center of its window.</p> <p>Here is an overview how this works to produce embeddings (shown in red) for each word of the document :</p> <p></p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--examples","title":"Examples","text":"<p>Here is an example of how to define a pipeline with a Transformer component:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.transformer(\n        model=\"prajjwal1/bert-tiny\",\n        window=128,\n        stride=96,\n    ),\n)\n</code></pre> <p>You can then compose this embedding with a task specific component such as <code>eds.ner_crf</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline instance</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name</p> <p> TYPE: <code>str</code> DEFAULT: <code>'transformer'</code> </p> <code>model</code> <p>The Huggingface model name or path</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>window</code> <p>The window size to use when splitting long documents into smaller windows before feeding them to the Transformer model (default: 512 = 512 - 2)</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>stride</code> <p>The stride (distance between windows) to use when splitting long documents into smaller windows: (default: 96)</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>training_stride</code> <p>If False, the stride will be set to the window size during training, meaning that there will be no overlap between windows. If True, the stride will be set to the <code>stride</code> parameter during training, just like during inference.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_tokens_per_device</code> <p>The maximum number of tokens that can be processed by the model on a single device. This does not affect the results but can be used to reduce the memory usage of the model, at the cost of a longer processing time.</p> <p>If \"auto\", the component will try to estimate the maximum number of tokens that can be processed by the model on the current device at a given time.</p> <p> TYPE: <code>Union[int, Literal['auto']]</code> DEFAULT: <code>auto</code> </p> <code>span_getter</code> <p>Which spans of the document should be embedded. Defaults to the full document if None.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/","title":"<code>edsnlp.pipes.trainable.embeddings.transformer.transformer</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.TransformerBatchInput","title":"<code>TransformerBatchInput = TypedDict('TransformerBatchInput', {'input_ids': ft.FoldedTensor, 'word_indices': torch.Tensor, 'word_offsets': ft.FoldedTensor, 'empty_word_indices': torch.Tensor})</code>  <code>module-attribute</code>","text":"<p>input_ids: FoldedTensor     Tokenized input (prompt + text) to embed word_indices: torch.LongTensor     Flattened indices of the word's wordpieces in the flattened input_ids word_offsets: FoldedTensor     Offsets of the word's wordpieces in the flattened input_ids empty_word_indices: torch.LongTensor     Indices of empty words in the flattened input_ids</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.TransformerBatchOutput","title":"<code>TransformerBatchOutput = TypedDict('TransformerBatchOutput', {'embeddings': ft.FoldedTensor})</code>  <code>module-attribute</code>","text":"<p>embeddings: FoldedTensor     The embeddings of the words</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer","title":"<code>Transformer</code>","text":"<p>           Bases: <code>WordEmbeddingComponent[TransformerBatchInput]</code></p> <p>The <code>eds.transformer</code> component is a wrapper around HuggingFace's transformers library. If you are not familiar with transformers, a good way to start is the Illustrated Transformer tutorial.</p> <p>Compared to using the raw Huggingface model, we offer a simple mechanism to split long documents into strided windows before feeding them to the model.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer--windowing","title":"Windowing","text":"<p>EDS-NLP's Transformer component splits long documents into smaller windows before feeding them to the model. This is done to avoid hitting the maximum number of tokens that can be processed by the model on a single device. The window size and stride can be configured using the <code>window</code> and <code>stride</code> parameters. The default values are 512 and 256 respectively, which means that the model will process windows of 512 tokens, each separated by 256 tokens. Whenever a token appears in multiple windows, the embedding of the \"most contextualized\" occurrence is used, i.e. the occurrence that is the closest to the center of its window.</p> <p>Here is an overview how this works to produce embeddings (shown in red) for each word of the document :</p> <p></p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer--examples","title":"Examples","text":"<p>Here is an example of how to define a pipeline with a Transformer component:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.transformer(\n        model=\"prajjwal1/bert-tiny\",\n        window=128,\n        stride=96,\n    ),\n)\n</code></pre> <p>You can then compose this embedding with a task specific component such as <code>eds.ner_crf</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline instance</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name</p> <p> TYPE: <code>str</code> DEFAULT: <code>'transformer'</code> </p> <code>model</code> <p>The Huggingface model name or path</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>window</code> <p>The window size to use when splitting long documents into smaller windows before feeding them to the Transformer model (default: 512 = 512 - 2)</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>stride</code> <p>The stride (distance between windows) to use when splitting long documents into smaller windows: (default: 96)</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>training_stride</code> <p>If False, the stride will be set to the window size during training, meaning that there will be no overlap between windows. If True, the stride will be set to the <code>stride</code> parameter during training, just like during inference.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_tokens_per_device</code> <p>The maximum number of tokens that can be processed by the model on a single device. This does not affect the results but can be used to reduce the memory usage of the model, at the cost of a longer processing time.</p> <p>If \"auto\", the component will try to estimate the maximum number of tokens that can be processed by the model on the current device at a given time.</p> <p> TYPE: <code>Union[int, Literal['auto']]</code> DEFAULT: <code>auto</code> </p> <code>span_getter</code> <p>Which spans of the document should be embedded. Defaults to the full document if None.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer.collate","title":"<code>collate</code>","text":"<p>How this works: 1. Iterate over samples, and in each sample over spans of text to embed    independently, and extract their input ids (and optionally prompts)    in a list <code>input_ids</code> that will be passed to the transformer.</p> <p><code>embeds = self.embedding(input_ids)</code> 2. Since we want to aggregate over words, and have overlapping spans, we need    to process indices carefully. Once the individual spans are embedded, we    will flatten them...</p> <p><code>flat_embeds = embeds.view(-1, embeds.size(2))[indexer]</code></p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer.collate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/typing/","title":"<code>edsnlp.pipes.trainable.embeddings.typing</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/","title":"<code>edsnlp.pipes.trainable.layers</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/crf/","title":"<code>edsnlp.pipes.trainable.layers.crf</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF","title":"<code>LinearChainCRF</code>","text":"<p>           Bases: <code>Module</code></p> <p>A linear chain CRF in Pytorch</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>forbidden_transitions</code> <p>Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1</p> <p> </p> <code>start_forbidden_transitions</code> <p>Shape: n_tags Impossible transitions at the start of a sequence</p> <p> DEFAULT: <code>None</code> </p> <code>end_forbidden_transitions</code> <p>Shape: n_tags Impossible transitions at the end of a sequence</p> <p> DEFAULT: <code>None</code> </p> <code>learnable_transitions</code> <p>Should we learn transition scores to complete the constraints ?</p> <p> DEFAULT: <code>True</code> </p> <code>with_start_end_transitions</code> <p>Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores</p> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.decode","title":"<code>decode</code>","text":"<p>Decodes a sequence of tag scores using the Viterbi algorithm</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.decode--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: ... * n_tokens * n_tags</p> <p> </p> <code>mask</code> <p>Shape: ... * n_tokens</p> <p> </p> RETURNS DESCRIPTION <code>LongTensor</code> <p>Backtrack indices (= argmax), ie best tag sequence</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.marginal","title":"<code>marginal</code>","text":"<p>Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF</p> <p>We could use the <code>propagate</code> method but this implementation is faster.</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.marginal--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: ... * n_tokens * n_tags</p> <p> </p> <code>mask</code> <p>Shape: ... * n_tokens</p> <p> </p> RETURNS DESCRIPTION <code>FloatTensor</code> <p>Shape: ... * n_tokens * n_tags</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.forward","title":"<code>forward</code>","text":"<p>Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss.</p> <p>We could use the <code>propagate</code> method but this implementation is faster.</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: n_samples * n_tokens * ... * n_tags</p> <p> </p> <code>mask</code> <p>Shape: n_samples * n_tokens * ...</p> <p> </p> <code>target</code> <p>Shape: n_samples * n_tokens * ... * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training.</p> <p> </p> RETURNS DESCRIPTION <code>FloatTensor</code> <p>Shape: ... The loss</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder","title":"<code>MultiLabelBIOULDecoder</code>","text":"<p>           Bases: <code>LinearChainCRF</code></p> <p>Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>num_labels</code> <code>with_start_end_transitions</code> <p> DEFAULT: <code>True</code> </p> <code>learnable_transitions</code> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder.tags_to_spans","title":"<code>tags_to_spans</code>  <code>staticmethod</code>","text":"<p>Convert a sequence of multiple label BIOUL tags to a sequence of spans</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder.tags_to_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>tags</code> <p>Shape: n_samples * n_tokens * n_labels</p> <p> </p> RETURNS DESCRIPTION <code>LongTensor</code> <p>Shape: n_spans *  4 (doc_idx, begin, end, label_idx)</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/metric/","title":"<code>edsnlp.pipes.trainable.layers.metric</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/metric/#edsnlp.pipes.trainable.layers.metric.Metric","title":"<code>Metric</code>","text":"<p>           Bases: <code>Module</code></p> <p>Metric layer, used for computing similarities between two sets of vectors. A typical use case is to compute the similarity between a set of query vectors (input embeddings) and a set of concept vectors (output embeddings).</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/metric/#edsnlp.pipes.trainable.layers.metric.Metric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>in_features</code> <p>Size of the input embeddings</p> <p> TYPE: <code>int</code> </p> <code>out_features</code> <p>Size of the output embeddings</p> <p> TYPE: <code>int</code> </p> <code>num_groups</code> <p>Number of groups for the output embeddings, that can be used to filter out certain concepts that are not relevant for a given query (e.g. do not compare a drug with concepts for diseases)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>metric</code> <p>Whether to compute the cosine similarity between the input and output embeddings or the dot product.</p> <p> TYPE: <code>Literal['cosine', 'dot']</code> DEFAULT: <code>'cosine'</code> </p> <code>rescale</code> <p>Rescale the output cosine similarities by a constant factor.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/layers/text_cnn/","title":"<code>edsnlp.pipes.trainable.layers.text_cnn</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/text_cnn/#edsnlp.pipes.trainable.layers.text_cnn.TextCnn","title":"<code>TextCnn</code>","text":"<p>           Bases: <code>Module</code></p>"},{"location":"reference/edsnlp/pipes/trainable/layers/text_cnn/#edsnlp.pipes.trainable.layers.text_cnn.TextCnn--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>input_size</code> <p>Size of the input embeddings</p> <p> TYPE: <code>int</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'relu'</code> </p> <code>residual</code> <p>Whether to use residual connections</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize</code> <p>Whether to normalize before or after the residual connection</p> <p> TYPE: <code>Literal['pre', 'post', 'none']</code> DEFAULT: <code>'pre'</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/","title":"<code>edsnlp.pipes.trainable.ner_crf</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/","title":"<code>edsnlp.pipes.trainable.ner_crf.factory</code>","text":"<ol><li><p><p>Wajsb\u00fcrt P., 2021. Extraction and normalization of simple and structured entities in medical documents. https://hal.archives-ouvertes.fr/tel-03624928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component","title":"<code>create_component = registry.factory.register('eds.ner_crf', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.nested_ner', 'nested_ner'])(TrainableNerCrf)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.ner_crf</code> component is a general purpose trainable named entity recognizer. It can extract:</p> <ul> <li>flat entities</li> <li>overlapping entities of different labels</li> </ul> <p>However, at the moment, the model cannot currently extract entities that are nested inside larger entities of the same label.</p> <p>It is based on a CRF (Conditional Random Field) layer and should therefore work well on dataset composed of entities will ill-defined boundaries. We offer a compromise between speed and performance by allowing the user to specify a window size for the CRF layer. The smaller the window, the faster the model will be, but at the cost of degraded performance.</p> <p>The pipeline assigns both <code>doc.ents</code> (in which overlapping entities are filtered out) and <code>doc.spans</code>. These destinations can be inferred from the <code>target_span_getter</code> parameter, combined with the <code>post_init</code> step.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--architecture","title":"Architecture","text":"<p>The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts <code>n_labels</code> sequences of O, I, B, L, U tags. The architecture is displayed in the figure below.</p> <p>To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction.</p> <p> </p> Nested NER architecture"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--examples","title":"Examples","text":"<p>Let us define a pipeline composed of a transformer, and a NER component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.ner_crf(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        window=10,\n    ),\n    name=\"ner\"\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ner_crf'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code></p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ner_crf</code> pipeline was developed by AP-HP's Data Science team.</p> <p>The deep learning model was adapted from Wajsb\u00fcrt, 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/","title":"<code>edsnlp.pipes.trainable.ner_crf.ner_crf</code>","text":"<ol><li><p><p>Wajsb\u00fcrt P., 2021. Extraction and normalization of simple and structured entities in medical documents. https://hal.archives-ouvertes.fr/tel-03624928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf","title":"<code>TrainableNerCrf</code>","text":"<p>           Bases: <code>TorchComponent[NERBatchOutput, NERBatchInput]</code>, <code>BaseNERComponent</code></p> <p>The <code>eds.ner_crf</code> component is a general purpose trainable named entity recognizer. It can extract:</p> <ul> <li>flat entities</li> <li>overlapping entities of different labels</li> </ul> <p>However, at the moment, the model cannot currently extract entities that are nested inside larger entities of the same label.</p> <p>It is based on a CRF (Conditional Random Field) layer and should therefore work well on dataset composed of entities will ill-defined boundaries. We offer a compromise between speed and performance by allowing the user to specify a window size for the CRF layer. The smaller the window, the faster the model will be, but at the cost of degraded performance.</p> <p>The pipeline assigns both <code>doc.ents</code> (in which overlapping entities are filtered out) and <code>doc.spans</code>. These destinations can be inferred from the <code>target_span_getter</code> parameter, combined with the <code>post_init</code> step.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--architecture","title":"Architecture","text":"<p>The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts <code>n_labels</code> sequences of O, I, B, L, U tags. The architecture is displayed in the figure below.</p> <p>To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction.</p> <p> </p> Nested NER architecture"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--examples","title":"Examples","text":"<p>Let us define a pipeline composed of a transformer, and a NER component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.ner_crf(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        window=10,\n    ),\n    name=\"ner\"\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ner_crf'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code></p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ner_crf</code> pipeline was developed by AP-HP's Data Science team.</p> <p>The deep learning model was adapted from Wajsb\u00fcrt, 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf.post_init","title":"<code>post_init</code>","text":"<p>Update the labels based on the data and the span getter, and fills in the to_ents and to_span_groups if necessary</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf.post_init--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>The documents to use to infer the labels</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>exclude</code> <p>Components to exclude from the post initialization</p> <p> TYPE: <code>Set[str]</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/","title":"<code>edsnlp.pipes.trainable.span_classifier</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/","title":"<code>edsnlp.pipes.trainable.span_classifier.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/#edsnlp.pipes.trainable.span_classifier.factory.create_component","title":"<code>create_component = registry.factory.register('eds.span_classifier', assigns=[], deprecated=['eds.span_qualifier'])(TrainableSpanClassifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.span_classifier</code> component is a trainable attribute predictor. In this context, the span classification task consists in assigning values (boolean, strings or any object) to attributes/extensions of spans such as:</p> <ul> <li><code>span._.negation</code>,</li> <li><code>span._.date.mode</code></li> <li><code>span._.cui</code></li> </ul> <p>In the rest of this page, we will refer to a pair of (attribute, value) as a \"binding\". For instance, the binding <code>(\"_.negation\", True)</code> means that the attribute <code>negation</code> of the span is (or should be, when predicted) set to <code>True</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/#edsnlp.pipes.trainable.span_classifier.factory.create_component--architecture","title":"Architecture","text":"<p>The model performs span classification by:</p> <ol> <li>Calling a word pooling embedding such as <code>eds.span_pooler</code> to compute a single embedding for each span</li> <li>Computing logits for each possible binding using a linear layer</li> <li> <p>Splitting these bindings into groups of exclusive values such as</p> <ul> <li><code>event=start</code> and <code>event=stop</code></li> <li><code>negated=False</code> and <code>negated=True</code></li> </ul> <p>Note that the above groups are not exclusive, but the values within each group are.</p> </li> <li> <p>Applying the best scoring binding in each group to each span</p> </li> </ol>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/#edsnlp.pipes.trainable.span_classifier.factory.create_component--examples","title":"Examples","text":"<p>To create a span classifier component, you can use the following code:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_classifier(\n        # To embed the spans, we will use a span pooler\n        embedding=eds.span_pooler(\n            pooling_mode=\"mean\",  # mean pooling\n            # that will use a transformer to embed the doc words\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n        span_getter=[\"ents\", \"sc\"],\n        # For every span embedded by the span pooler\n        # (doc.ents and doc.spans[\"sc\"]), we will predict both\n        # span._.negation and span._.event_type\n        attributes=[\"_.negation\", \"_.event_type\"],\n    ),\n    name=\"span_classifier\",\n)\n</code></pre> <p>To infer the values of the attributes, you can use the pipeline <code>post_init</code> method:</p> <pre><code>nlp.post_init(gold_data)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>You can inspect the bindings that will be used for training and prediction <pre><code>print(nlp.pipes.attr.bindings)\n# list of (attr name, span labels or True if all, values)\n# Out: [\n#   ('_.negation', True, [True, False]),\n#   ('_.event_type', True, ['start', 'stop'])\n# ]\n</code></pre></p> <p>You can also change these values and update the bindings by calling the <code>update_bindings</code> method. Don't forget to retrain the model if new values are added !</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/#edsnlp.pipes.trainable.span_classifier.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_classifier'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>span_getter</code> <p>How to extract the candidate spans and the attributes to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). This can be:</p> <ul> <li>a <code>SpanGetterArg</code> to retrieve contexts from a whole document. For example   <code>{\"section\": \"conclusion\"}</code> to only use the conclusion as context (you   must ensure that all spans produced by the <code>span_getter</code> argument do fall   in the conclusion in this case)</li> <li>a callable, that gets a span and should return a context for this span.   For instance, <code>lambda span: span.sent</code> to use the sentence as context.</li> </ul> <p> TYPE: <code>Optional[Union[Callable, SpanGetterArg]]</code> DEFAULT: <code>None</code> </p> <code>attributes</code> <p>The attributes to predict or train on. If a dict is given, keys are the attributes and values are the labels for which the attr is allowed, or True if the attr is allowed for all labels.</p> <p> TYPE: <code>AttributesArg</code> DEFAULT: <code>None</code> </p> <code>keep_none</code> <p>If False, skip spans for which a attr returns None. If True (default), the None values will be learned and predicted, just as any other value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/","title":"<code>edsnlp.pipes.trainable.span_classifier.span_classifier</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.SpanClassifierBatchInput","title":"<code>SpanClassifierBatchInput = TypedDict('SpanClassifierBatchInput', {'embedding': BatchInput, 'targets': NotRequired[torch.Tensor]})</code>  <code>module-attribute</code>","text":"<p>embeds: torch.FloatTensor     Token embeddings to predict the tags from mask: torch.BoolTensor     Mask of the sequences spans: torch.Tensor     2d tensor of n_spans * (doc_idx, ner_label_idx, begin, end) targets: NotRequired[List[torch.Tensor]]     list of 2d tensor of n_spans * n_combinations (1 hot)</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.SpanClassifierBatchOutput","title":"<code>SpanClassifierBatchOutput = TypedDict('SpanClassifierBatchOutput', {'loss': Optional[torch.Tensor], 'labels': Optional[List[torch.Tensor]]})</code>  <code>module-attribute</code>","text":"<p>loss: Optional[torch.Tensor]     The loss of the model labels: Optional[List[torch.Tensor]]     The predicted labels</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier","title":"<code>TrainableSpanClassifier</code>","text":"<p>           Bases: <code>TorchComponent[BatchOutput, SpanClassifierBatchInput]</code>, <code>BaseSpanAttributeClassifierComponent</code></p> <p>The <code>eds.span_classifier</code> component is a trainable attribute predictor. In this context, the span classification task consists in assigning values (boolean, strings or any object) to attributes/extensions of spans such as:</p> <ul> <li><code>span._.negation</code>,</li> <li><code>span._.date.mode</code></li> <li><code>span._.cui</code></li> </ul> <p>In the rest of this page, we will refer to a pair of (attribute, value) as a \"binding\". For instance, the binding <code>(\"_.negation\", True)</code> means that the attribute <code>negation</code> of the span is (or should be, when predicted) set to <code>True</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier--architecture","title":"Architecture","text":"<p>The model performs span classification by:</p> <ol> <li>Calling a word pooling embedding such as <code>eds.span_pooler</code> to compute a single embedding for each span</li> <li>Computing logits for each possible binding using a linear layer</li> <li> <p>Splitting these bindings into groups of exclusive values such as</p> <ul> <li><code>event=start</code> and <code>event=stop</code></li> <li><code>negated=False</code> and <code>negated=True</code></li> </ul> <p>Note that the above groups are not exclusive, but the values within each group are.</p> </li> <li> <p>Applying the best scoring binding in each group to each span</p> </li> </ol>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier--examples","title":"Examples","text":"<p>To create a span classifier component, you can use the following code:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_classifier(\n        # To embed the spans, we will use a span pooler\n        embedding=eds.span_pooler(\n            pooling_mode=\"mean\",  # mean pooling\n            # that will use a transformer to embed the doc words\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n        span_getter=[\"ents\", \"sc\"],\n        # For every span embedded by the span pooler\n        # (doc.ents and doc.spans[\"sc\"]), we will predict both\n        # span._.negation and span._.event_type\n        attributes=[\"_.negation\", \"_.event_type\"],\n    ),\n    name=\"span_classifier\",\n)\n</code></pre> <p>To infer the values of the attributes, you can use the pipeline <code>post_init</code> method:</p> <pre><code>nlp.post_init(gold_data)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>You can inspect the bindings that will be used for training and prediction <pre><code>print(nlp.pipes.attr.bindings)\n# list of (attr name, span labels or True if all, values)\n# Out: [\n#   ('_.negation', True, [True, False]),\n#   ('_.event_type', True, ['start', 'stop'])\n# ]\n</code></pre></p> <p>You can also change these values and update the bindings by calling the <code>update_bindings</code> method. Don't forget to retrain the model if new values are added !</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_classifier'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>span_getter</code> <p>How to extract the candidate spans and the attributes to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). This can be:</p> <ul> <li>a <code>SpanGetterArg</code> to retrieve contexts from a whole document. For example   <code>{\"section\": \"conclusion\"}</code> to only use the conclusion as context (you   must ensure that all spans produced by the <code>span_getter</code> argument do fall   in the conclusion in this case)</li> <li>a callable, that gets a span and should return a context for this span.   For instance, <code>lambda span: span.sent</code> to use the sentence as context.</li> </ul> <p> TYPE: <code>Optional[Union[Callable, SpanGetterArg]]</code> DEFAULT: <code>None</code> </p> <code>attributes</code> <p>The attributes to predict or train on. If a dict is given, keys are the attributes and values are the labels for which the attr is allowed, or True if the attr is allowed for all labels.</p> <p> TYPE: <code>AttributesArg</code> DEFAULT: <code>None</code> </p> <code>keep_none</code> <p>If False, skip spans for which a attr returns None. If True (default), the None values will be learned and predicted, just as any other value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier.forward","title":"<code>forward</code>","text":"<p>Apply the span classifier module to the document embeddings and given spans to: - compute the loss - and/or predict the labels of spans</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>The input batch</p> <p> TYPE: <code>SpanClassifierBatchInput</code> </p> RETURNS DESCRIPTION <code>BatchOutput</code>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/","title":"<code>edsnlp.pipes.trainable.span_linker</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/","title":"<code>edsnlp.pipes.trainable.span_linker.factory</code>","text":"<ol><li><p><p>Wajsb\u00fcrt P., Sarfati A. and Tannier X., 2021. Medical concept normalization in French using multilingual terminologies and contextual embeddings. Journal of Biomedical Informatics. 114, pp.103684. https://doi.org/10.1016/j.jbi.2021.103684</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component","title":"<code>create_component = registry.factory.register('eds.span_linker', assigns=[], deprecated=[])(TrainableSpanLinker)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.span_linker</code> component is a trainable span concept predictor, typically used to match spans in the text with concepts in a knowledge base. This task is known as \"Entity Linking\", \"Named Entity Disambiguation\" or \"Normalization\" (the latter is mostly used in the biomedical machine learning community).</p> <p>Entity Linking vs Named Entity Recognition</p> <p>Entity Linking is the task of linking existing entities to their concept in a knowledge base, while Named Entity Recognition is the task of detecting spans in the text that correspond to entities. The <code>eds.span_linker</code> component should therefore be used after the Named Entity Recognition step (e.g. using the <code>eds.ner_crf</code> component).</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--how-it-works","title":"How it works","text":"<p>To perform this task, this components compare the embedding of a given query span (e.g. \"aspirin\") with the embeddings in the knowledge base, where each embedding represents a concept (e.g. \"B01AC06\"), and selects the most similar embedding and returns its concept id. This comparison is done using either:</p> <ul> <li>the cosine similarity between the input and output embeddings (recommended)</li> <li>a simple dot product</li> </ul> <p>We filter out the concepts that are not relevant for a given query by using groups. For each span to link, we use its label to select a group of concepts to compare with. For example, if the span is labeled as \"drug\", we only compare it with concepts that are drugs. These concepts groups are inferred from the training data when running the <code>post_init</code> method, or can be provided manually using the <code>pipe.update_concepts(concepts, mapping, [embeddings])</code> method. If a label is not found in the mapping, the span is compared with all concepts.</p> <p>We support comparing entity queries against two kind of references : either the embeddings of the concepts themselves (<code>reference_mode = \"concept\"</code>), or the embeddings of the synonyms of the concepts (<code>reference_mode = \"synonym\"</code>).</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--synonym-similarity","title":"Synonym similarity","text":"<p>When performing span linking in <code>synonym</code> mode, the span linker embedding matrix contains one embedding vector per concept per synonym, and each embedding maps to the concept of its synonym. This mode is slower and more memory intensive, since you have to store multiple embeddings per concept, but it can yield good results in zero-shot scenarios (see example below).</p> <p> </p> Entity linking based on synonym similarity"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--concept-similarity","title":"Concept similarity","text":"<p>In <code>concept</code> mode, the span linker embedding matrix contains one embedding vector per concept : imagine a single vector that approximately averages all the synonyms of a concept (e.g. B01AC06 = average of \"aspirin\", \"acetyl-salicylic acid\", etc.). This mode is faster and more memory efficient, but usually requires that the concept weights are fine-tuned.</p> <p> </p> Entity linking based on concept similarity"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--examples","title":"Examples","text":"<p>Here is how you can use the <code>eds.span_linker</code> component to link spans without training, in <code>synonym</code> mode. You will still need to pre-compute the embeddings of the target synonyms.</p> <p>First, initialize the component:</p> <pre><code>import pandas as pd\nimport edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_linker(\n        rescale=20.0,\n        threshold=0.8,\n        metric=\"cosine\",\n        reference_mode=\"synonym\",\n        probability_mode=\"sigmoid\",\n        span_getter=[\"ents\"],\n        embedding=eds.span_pooler(\n            hidden_size=128,\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"linker\",\n)\n</code></pre> <p>We will assume you have a list of synonyms with their concept and label with the columns:</p> <ul> <li><code>STR</code>: synonym text</li> <li><code>CUI</code>: concept id</li> <li><code>GRP</code>: label.</li> </ul> <p>All we need to do is to initialize the component with the synonyms and that's it ! Since we have set <code>init_weights</code> to True, and we are in <code>synonym</code> mode, the embeddings of the synonyms will be stored in the component and used to compute the similarity scores</p> <pre><code>synonyms_df = pd.read_csv(\"synonyms.csv\")\n\ndef make_doc(row):\n    doc = nlp.make_doc(row[\"STR\"])\n    span = doc[:]\n    span.label_ = row[\"GRP\"]\n    doc.ents = [span]\n    span._.cui = row[\"CUI\"]\n    return doc\n\nnlp.post_init(\n    edsnlp.data.from_pandas(\n        synonyms_df,\n        converter=make_doc,\n    )\n)\n</code></pre> <p>Now, you can now use it in a text: <pre><code>doc = nlp.make_doc(\"Aspirin is a drug\")\nspan = doc[0:1]  # \"Aspirin\"\nspan.label_ = \"Drug\"\ndoc.ents = [span]\n\ndoc = nlp(doc)\nprint(doc.ents[0]._.cui)\n# \"B01AC06\"\n</code></pre></p> <p>To use the <code>eds.span_linker</code> component in <code>class</code> mode, we refer to the following repository: deep_multilingual_normalization based on the work of Wajsb\u00fcrt et al., 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Spacy vocabulary</p> <p> </p> <code>name</code> <p>Name of the component</p> <p> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>metric</code> <p>Whether to compute the cosine similarity between the input and output embeddings or the dot product.</p> <p> TYPE: <code>Literal[\"cosine\", \"dot\"] = \"cosine\"</code> DEFAULT: <code>cosine</code> </p> <code>rescale</code> <p>Rescale the output cosine similarities by a constant factor.</p> <p> TYPE: <code>float</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>Threshold probability to consider a concept as valid</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>attribute</code> <p>The attribute to store the concept id</p> <p> TYPE: <code>str</code> DEFAULT: <code>cui</code> </p> <code>reference_mode</code> <p>Whether to compare the embeddings with the concepts embeddings (one per concept) or the synonyms embeddings (one per concept per synonym). See above for more details.</p> <p> TYPE: <code>Literal['concept', 'synonym']</code> DEFAULT: <code>concept</code> </p> <code>span_getter</code> <p>How to extract the candidate spans to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the entity only, so no context)</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>probability_mode</code> <p>Whether to compute the probabilities using a softmax or a sigmoid function. This will also determine the loss function to use, either cross-entropy or binary cross-entropy.</p> <p>Subsetting the concepts</p> <p>The probabilities returned in <code>softmax</code> mode depend on the number of concepts (as an extreme cas, if you have only one concept, its softmax probability will always be 1). This is why we recommend using the <code>sigmoid</code> mode in which the probabilities are computed independently for each concept.</p> <p> TYPE: <code>Literal['softmax', 'sigmoid']</code> DEFAULT: <code>sigmoid</code> </p> <code>init_weights</code> <p>Whether to initialize the weights of the component with the embeddings of the entities of the docs provided to the <code>post_init</code> method. How this is done depends on the <code>reference_mode</code> parameter:</p> <ul> <li><code>concept</code>: the embeddings are averaged</li> <li><code>synonym</code>: the embeddings are stored as is</li> </ul> <p>By default, this is set to <code>True</code> if <code>reference_mode</code> is <code>synonym</code>, and <code>False</code> otherwise.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.span_linker</code> component was developed by AP-HP's Data Science team.</p> <p>The deep learning concept-based architecture was adapted from Wajsb\u00fcrt et al., 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/","title":"<code>edsnlp.pipes.trainable.span_linker.span_linker</code>","text":"<ol><li><p><p>Wajsb\u00fcrt P., Sarfati A. and Tannier X., 2021. Medical concept normalization in French using multilingual terminologies and contextual embeddings. Journal of Biomedical Informatics. 114, pp.103684. https://doi.org/10.1016/j.jbi.2021.103684</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.SpanLinkerBatchInput","title":"<code>SpanLinkerBatchInput = TypedDict('SpanLinkerBatchInput', {'embedding': BatchInput, 'span_labels': torch.Tensor, 'concepts': NotRequired[torch.Tensor]})</code>  <code>module-attribute</code>","text":"<p>embeds: torch.FloatTensor     Token embeddings to predict the tags from mask: torch.BoolTensor     Mask of the sequences spans: torch.LongTensor     2d tensor of n_spans * (doc_idx, ner_label_idx, begin, end) targets: NotRequired[List[torch.LongTensor]]     list of 2d tensor of n_spans * n_combinations (1 hot)</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker","title":"<code>TrainableSpanLinker</code>","text":"<p>           Bases: <code>TorchComponent[BatchOutput, SpanLinkerBatchInput]</code>, <code>BaseSpanAttributeClassifierComponent</code></p> <p>The <code>eds.span_linker</code> component is a trainable span concept predictor, typically used to match spans in the text with concepts in a knowledge base. This task is known as \"Entity Linking\", \"Named Entity Disambiguation\" or \"Normalization\" (the latter is mostly used in the biomedical machine learning community).</p> <p>Entity Linking vs Named Entity Recognition</p> <p>Entity Linking is the task of linking existing entities to their concept in a knowledge base, while Named Entity Recognition is the task of detecting spans in the text that correspond to entities. The <code>eds.span_linker</code> component should therefore be used after the Named Entity Recognition step (e.g. using the <code>eds.ner_crf</code> component).</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--how-it-works","title":"How it works","text":"<p>To perform this task, this components compare the embedding of a given query span (e.g. \"aspirin\") with the embeddings in the knowledge base, where each embedding represents a concept (e.g. \"B01AC06\"), and selects the most similar embedding and returns its concept id. This comparison is done using either:</p> <ul> <li>the cosine similarity between the input and output embeddings (recommended)</li> <li>a simple dot product</li> </ul> <p>We filter out the concepts that are not relevant for a given query by using groups. For each span to link, we use its label to select a group of concepts to compare with. For example, if the span is labeled as \"drug\", we only compare it with concepts that are drugs. These concepts groups are inferred from the training data when running the <code>post_init</code> method, or can be provided manually using the <code>pipe.update_concepts(concepts, mapping, [embeddings])</code> method. If a label is not found in the mapping, the span is compared with all concepts.</p> <p>We support comparing entity queries against two kind of references : either the embeddings of the concepts themselves (<code>reference_mode = \"concept\"</code>), or the embeddings of the synonyms of the concepts (<code>reference_mode = \"synonym\"</code>).</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--synonym-similarity","title":"Synonym similarity","text":"<p>When performing span linking in <code>synonym</code> mode, the span linker embedding matrix contains one embedding vector per concept per synonym, and each embedding maps to the concept of its synonym. This mode is slower and more memory intensive, since you have to store multiple embeddings per concept, but it can yield good results in zero-shot scenarios (see example below).</p> <p> </p> Entity linking based on synonym similarity"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--concept-similarity","title":"Concept similarity","text":"<p>In <code>concept</code> mode, the span linker embedding matrix contains one embedding vector per concept : imagine a single vector that approximately averages all the synonyms of a concept (e.g. B01AC06 = average of \"aspirin\", \"acetyl-salicylic acid\", etc.). This mode is faster and more memory efficient, but usually requires that the concept weights are fine-tuned.</p> <p> </p> Entity linking based on concept similarity"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--examples","title":"Examples","text":"<p>Here is how you can use the <code>eds.span_linker</code> component to link spans without training, in <code>synonym</code> mode. You will still need to pre-compute the embeddings of the target synonyms.</p> <p>First, initialize the component:</p> <pre><code>import pandas as pd\nimport edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_linker(\n        rescale=20.0,\n        threshold=0.8,\n        metric=\"cosine\",\n        reference_mode=\"synonym\",\n        probability_mode=\"sigmoid\",\n        span_getter=[\"ents\"],\n        embedding=eds.span_pooler(\n            hidden_size=128,\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"linker\",\n)\n</code></pre> <p>We will assume you have a list of synonyms with their concept and label with the columns:</p> <ul> <li><code>STR</code>: synonym text</li> <li><code>CUI</code>: concept id</li> <li><code>GRP</code>: label.</li> </ul> <p>All we need to do is to initialize the component with the synonyms and that's it ! Since we have set <code>init_weights</code> to True, and we are in <code>synonym</code> mode, the embeddings of the synonyms will be stored in the component and used to compute the similarity scores</p> <pre><code>synonyms_df = pd.read_csv(\"synonyms.csv\")\n\ndef make_doc(row):\n    doc = nlp.make_doc(row[\"STR\"])\n    span = doc[:]\n    span.label_ = row[\"GRP\"]\n    doc.ents = [span]\n    span._.cui = row[\"CUI\"]\n    return doc\n\nnlp.post_init(\n    edsnlp.data.from_pandas(\n        synonyms_df,\n        converter=make_doc,\n    )\n)\n</code></pre> <p>Now, you can now use it in a text: <pre><code>doc = nlp.make_doc(\"Aspirin is a drug\")\nspan = doc[0:1]  # \"Aspirin\"\nspan.label_ = \"Drug\"\ndoc.ents = [span]\n\ndoc = nlp(doc)\nprint(doc.ents[0]._.cui)\n# \"B01AC06\"\n</code></pre></p> <p>To use the <code>eds.span_linker</code> component in <code>class</code> mode, we refer to the following repository: deep_multilingual_normalization based on the work of Wajsb\u00fcrt et al., 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Spacy vocabulary</p> <p> </p> <code>name</code> <p>Name of the component</p> <p> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>metric</code> <p>Whether to compute the cosine similarity between the input and output embeddings or the dot product.</p> <p> TYPE: <code>Literal[\"cosine\", \"dot\"] = \"cosine\"</code> DEFAULT: <code>cosine</code> </p> <code>rescale</code> <p>Rescale the output cosine similarities by a constant factor.</p> <p> TYPE: <code>float</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>Threshold probability to consider a concept as valid</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>attribute</code> <p>The attribute to store the concept id</p> <p> TYPE: <code>str</code> DEFAULT: <code>cui</code> </p> <code>reference_mode</code> <p>Whether to compare the embeddings with the concepts embeddings (one per concept) or the synonyms embeddings (one per concept per synonym). See above for more details.</p> <p> TYPE: <code>Literal['concept', 'synonym']</code> DEFAULT: <code>concept</code> </p> <code>span_getter</code> <p>How to extract the candidate spans to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the entity only, so no context)</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>probability_mode</code> <p>Whether to compute the probabilities using a softmax or a sigmoid function. This will also determine the loss function to use, either cross-entropy or binary cross-entropy.</p> <p>Subsetting the concepts</p> <p>The probabilities returned in <code>softmax</code> mode depend on the number of concepts (as an extreme cas, if you have only one concept, its softmax probability will always be 1). This is why we recommend using the <code>sigmoid</code> mode in which the probabilities are computed independently for each concept.</p> <p> TYPE: <code>Literal['softmax', 'sigmoid']</code> DEFAULT: <code>sigmoid</code> </p> <code>init_weights</code> <p>Whether to initialize the weights of the component with the embeddings of the entities of the docs provided to the <code>post_init</code> method. How this is done depends on the <code>reference_mode</code> parameter:</p> <ul> <li><code>concept</code>: the embeddings are averaged</li> <li><code>synonym</code>: the embeddings are stored as is</li> </ul> <p>By default, this is set to <code>True</code> if <code>reference_mode</code> is <code>synonym</code>, and <code>False</code> otherwise.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.span_linker</code> component was developed by AP-HP's Data Science team.</p> <p>The deep learning concept-based architecture was adapted from Wajsb\u00fcrt et al., 2021.</p>"},{"location":"reference/edsnlp/processing/","title":"<code>edsnlp.processing</code>","text":""},{"location":"reference/edsnlp/processing/deprecated_pipe/","title":"<code>edsnlp.processing.deprecated_pipe</code>","text":""},{"location":"reference/edsnlp/processing/deprecated_pipe/#edsnlp.processing.deprecated_pipe.slugify","title":"<code>slugify</code>","text":"<p>Slugify a chained attribute name</p>"},{"location":"reference/edsnlp/processing/deprecated_pipe/#edsnlp.processing.deprecated_pipe.slugify--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>chained_attr</code> <p>The string to slugify (replace dots by _)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The slugified string</p>"},{"location":"reference/edsnlp/processing/deprecated_pipe/#edsnlp.processing.deprecated_pipe.pipe","title":"<code>pipe</code>","text":"<p>Helper to process a pandas, koalas or spark dataframe. This function is deprecated. Prefer using the following instead:</p> <pre><code>import edsnlp\n\ndocs = edsnlp.data.from_***(\n    df,\n    converter='omop',\n    doc_attributes=context,\n)\ndocs = docs.map_pipeline(nlp)\nres = edsnlp.data.to_***(\n    docs,\n    converter='ents',  # or custom extractor\n    span_getter=\"ents\",\n    span_attributes=span_attributes,\n    **kwargs\n)\n</code></pre> <p>You can also call this function to get a migration suggestion.</p>"},{"location":"reference/edsnlp/processing/deprecated_pipe/#edsnlp.processing.deprecated_pipe.pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>df</code> <p>The dataframe to process, can be a pandas, spark or koalas dataframe</p> <p> TYPE: <code>Union[DataFrame, DataFrame, DataFrame]</code> </p> <code>nlp</code> <p>The pipeline to use</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>n_jobs</code> <p>Number of CPU workers to use</p> <p> TYPE: <code>int</code> DEFAULT: <code>-2</code> </p> <code>context</code> <p>List of context attributes to keep</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>[]</code> </p> <code>results_extractor</code> <p>Function to extract results from the pipeline. Defaults to one row per entities.</p> <p> TYPE: <code>Optional[Callable[[Doc], List[Dict[str, Any]]]]</code> DEFAULT: <code>None</code> </p> <code>additional_spans</code> <p>Additional spans groups to keep, defaults to <code>ents</code> (doc.ents)</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>[]</code> </p> <code>extensions</code> <p>Span extensions to export as a column. Can be a list of extension names, a dict of extension names to types, or a string</p> <p> TYPE: <code>ExtensionSchema</code> DEFAULT: <code>[]</code> </p> <code>dtypes</code> <p>Spark schema to use for the output dataframe. This is only used if the input dataframe is a spark dataframe.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>edsnlp.data.to_*</code> function</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, DataFrame, DataFrame]</code> <p>The processed dataframe</p>"},{"location":"reference/edsnlp/processing/multiprocessing/","title":"<code>edsnlp.processing.multiprocessing</code>","text":""},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.ForkingPickler","title":"<code>ForkingPickler</code>","text":"<p>           Bases: <code>Pickler</code></p> <p>ForkingPickler that uses dill instead of pickle to transfer objects between processes.</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.ForkingPickler.register","title":"<code>register</code>  <code>classmethod</code>","text":"<p>Register a reduce function for a type.</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.replace_pickler","title":"<code>replace_pickler</code>","text":"<p>Replace the default pickler used by multiprocessing with dill. \"multiprocess\" didn't work for obscure reasons (maybe the reducers / dispatchers are not propagated between multiprocessing and multiprocess =&gt; torch specific reducers might be missing ?), so this patches multiprocessing directly. directly.</p> <p>For some reason I do not explain, this has a massive impact on the performance of the multiprocessing backend. With the original pickler, the performance can be up to 2x slower than with our custom one.</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.cpu_count","title":"<code>cpu_count</code>","text":"<p>Heavily inspired (partially copied) from joblib's loky (https://github.com/joblib/loky/blob/2c21e/loky/backend/context.py#L83) by Thomas Moreau and Olivier Grisel.</p> <p>Return the number of CPUs we can use to process data in parallel.</p> <p>The returned number of CPUs returns the minimum of:  * <code>os.cpu_count()</code>  * the CPU affinity settings  * cgroup CPU bandwidth limit (share of total CPU time allowed in a given job)    typically used in containerized environments like Docker</p> <p>Note that on Windows, the returned number of CPUs cannot exceed 61 (or 60 for Python &lt; 3.10), see: https://bugs.python.org/issue26903.</p> <p>It is also always larger or equal to 1.</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.execute_multiprocessing_backend","title":"<code>execute_multiprocessing_backend</code>","text":"<p>If you have multiple CPU cores, and optionally multiple GPUs, we provide the <code>multiprocessing</code> backend that allows to run the inference on multiple processes.</p> <p>This accelerator dispatches the batches between multiple workers (data-parallelism), and distribute the computation of a given batch on one or two workers (model-parallelism):</p> <ul> <li>a <code>CPUWorker</code> which handles the non deep-learning components and the   preprocessing, collating and postprocessing of deep-learning components</li> <li>a <code>GPUWorker</code> which handles the forward call of the deep-learning components</li> </ul> <p>If no GPU is available, no <code>GPUWorker</code> is started, and the <code>CPUWorkers</code> handle the forward call of the deep-learning components as well.</p> <p>The advantage of dedicating a worker to the deep-learning components is that it allows to prepare multiple batches in parallel in multiple <code>CPUWorker</code>, and ensure that the <code>GPUWorker</code> never wait for a batch to be ready.</p> <p>The overall architecture described in the following figure, for 3 CPU workers and 2 GPU workers.</p> <p>Here is how a small pipeline with rule-based components and deep-learning components is distributed between the workers:</p> <p>Caveat</p> <p>Since workers can produce their results in any order, the order of the results may not be the same as the order of the input tasks.</p>"},{"location":"reference/edsnlp/processing/simple/","title":"<code>edsnlp.processing.simple</code>","text":""},{"location":"reference/edsnlp/processing/simple/#edsnlp.processing.simple.execute_simple_backend","title":"<code>execute_simple_backend</code>","text":"<p>This is the default execution mode which batches the documents and processes each batch on the current process in a sequential manner.</p>"},{"location":"reference/edsnlp/processing/spark/","title":"<code>edsnlp.processing.spark</code>","text":""},{"location":"reference/edsnlp/processing/spark/#edsnlp.processing.spark.execute_spark_backend","title":"<code>execute_spark_backend</code>","text":"<p>This execution mode uses Spark to parallelize the processing of the documents. The documents are first stored in a Spark DataFrame (if it was not already the case) and then processed in parallel using Spark.</p> <p>Beware, if the original reader was not a SparkReader (<code>edsnlp.data.from_spark</code>), the local docs \u2192 spark dataframe conversion might take some time, and the whole process might be slower than using the <code>multiprocessing</code> backend.</p>"},{"location":"reference/edsnlp/processing/utils/","title":"<code>edsnlp.processing.utils</code>","text":""},{"location":"reference/edsnlp/reducers/","title":"<code>edsnlp.reducers</code>","text":""},{"location":"reference/edsnlp/scorers/","title":"<code>edsnlp.scorers</code>","text":""},{"location":"reference/edsnlp/scorers/ner/","title":"<code>edsnlp.scorers.ner</code>","text":""},{"location":"reference/edsnlp/scorers/ner/#edsnlp.scorers.ner.ner_exact_scorer","title":"<code>ner_exact_scorer</code>","text":"<p>Scores the extracted entities that may be overlapping or nested by looking in the spans returned by a given SpanGetter object.</p>"},{"location":"reference/edsnlp/scorers/ner/#edsnlp.scorers.ner.ner_exact_scorer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>examples</code> <p>The examples to score, either a tuple of (golds, preds) or a list of spacy.training.Example objects</p> <p> TYPE: <code>Examples</code> </p> <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetter</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code>"},{"location":"reference/edsnlp/scorers/ner/#edsnlp.scorers.ner.ner_token_scorer","title":"<code>ner_token_scorer</code>","text":"<p>Scores the extracted entities that may be overlapping or nested by looking in <code>doc.ents</code>, and <code>doc.spans</code>, and comparing the predicted and gold entities at the TOKEN level.</p>"},{"location":"reference/edsnlp/scorers/ner/#edsnlp.scorers.ner.ner_token_scorer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>examples</code> <p>The examples to score, either a tuple of (golds, preds) or a list of spacy.training.Example objects</p> <p> TYPE: <code>Examples</code> </p> <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetter</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code>"},{"location":"reference/edsnlp/scorers/ner/#edsnlp.scorers.ner.dice","title":"<code>dice</code>","text":"<p>Compute the Dice coefficient between two spans</p>"},{"location":"reference/edsnlp/scorers/ner/#edsnlp.scorers.ner.ner_overlap_scorer","title":"<code>ner_overlap_scorer</code>","text":"<p>Scores the extracted entities that may be overlapping or nested by looking in <code>doc.ents</code>, and <code>doc.spans</code>, and comparing the predicted and gold entities and counting true when a predicted entity overlaps with a gold entity of the same label</p>"},{"location":"reference/edsnlp/scorers/ner/#edsnlp.scorers.ner.ner_overlap_scorer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>examples</code> <p>The examples to score, either a tuple of (golds, preds) or a list of spacy.training.Example objects</p> <p> TYPE: <code>Examples</code> </p> <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetter</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>threshold</code> <p>The threshold to use to consider that two spans overlap</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code>"},{"location":"reference/edsnlp/scorers/span_attributes/","title":"<code>edsnlp.scorers.span_attributes</code>","text":""},{"location":"reference/edsnlp/scorers/span_attributes/#edsnlp.scorers.span_attributes.span_attribute_scorer","title":"<code>span_attribute_scorer</code>","text":"<p>Scores the attributes predictions between a list of gold and predicted spans.</p>"},{"location":"reference/edsnlp/scorers/span_attributes/#edsnlp.scorers.span_attributes.span_attribute_scorer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>args</code> <p>The examples to score, either a tuple of (golds, preds) or a list of spacy.training.Example objects</p> <p> TYPE: <code>Examples</code> </p> <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>attributes</code> <p>The attributes to use to score the spans</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>default_values</code> <p>Values to dismiss when computing the micro-average per label. This is useful to compute precision and recall for certain attributes that have imbalanced value repartitions, such as \"negation\", \"family related\" or \"certainty\" attributes.</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> <code>include_falsy</code> <p>Whether to count predicted or gold occurrences of falsy values when computing the metrics. If <code>False</code>, only the non-falsy values will be counted and matched together.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, float]</code>"},{"location":"reference/edsnlp/train/","title":"<code>edsnlp.train</code>","text":""},{"location":"reference/edsnlp/train/#edsnlp.train.BatchSizeArg","title":"<code>BatchSizeArg</code>","text":"<p>Batch size argument validator / caster for confit/pydantic</p>"},{"location":"reference/edsnlp/train/#edsnlp.train.BatchSizeArg--examples","title":"Examples","text":"<pre><code>def fn(batch_size: BatchSizeArg):\n    return batch_size\n\n\nprint(fn(\"10 samples\"))\n# Out: (10, \"samples\")\n\nprint(fn(\"10 words\"))\n# Out: (10, \"words\")\n\nprint(fn(10))\n# Out: (10, \"samples\")\n</code></pre>"},{"location":"reference/edsnlp/train/#edsnlp.train.LengthSortedBatchSampler","title":"<code>LengthSortedBatchSampler</code>","text":"<p>Batch sampler that sorts the dataset by length and then batches sequences of similar length together. This is useful for transformer models that can then be padded more efficiently.</p>"},{"location":"reference/edsnlp/train/#edsnlp.train.LengthSortedBatchSampler--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>dataset</code> <p>The dataset to sample from (can be a generator or a fixed size collection)</p> <p> </p> <code>batch_size</code> <p>The batch size</p> <p> TYPE: <code>int</code> </p> <code>batch_unit</code> <p>The unit of the batch size, either \"words\" or \"samples\"</p> <p> TYPE: <code>str</code> </p> <code>noise</code> <p>The amount of noise to add to the sequence length before sorting (uniformly sampled in [-noise, noise])</p> <p> DEFAULT: <code>1</code> </p> <code>drop_last</code> <p>Whether to drop the last batch if it is smaller than the batch size</p> <p> DEFAULT: <code>True</code> </p> <code>buffer_size</code> <p>The size of the buffer to use to shuffle the batches. If None, the buffer will be approximately the size of the dataset.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/train/#edsnlp.train.SubBatchCollater","title":"<code>SubBatchCollater</code>","text":"<p>Collater that splits batches into sub-batches of a maximum size</p>"},{"location":"reference/edsnlp/train/#edsnlp.train.SubBatchCollater--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> </p> <code>embedding</code> <p>The transformer embedding pipe</p> <p> </p> <code>grad_accumulation_max_tokens</code> <p>The maximum number of tokens (word pieces) to accumulate in a single batch</p> <p> </p>"},{"location":"reference/edsnlp/train/#edsnlp.train.Reader","title":"<code>Reader</code>","text":"<p>           Bases: <code>BaseModel</code></p> <p>Reader that reads docs from a file or a generator, and adapts them to the pipeline.</p>"},{"location":"reference/edsnlp/train/#edsnlp.train.Reader--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>reader</code> <p>The reader object</p> <p> </p> <code>limit</code> <p>The maximum number of docs to read</p> <p> </p> <code>max_length</code> <p>The maximum length of the resulting docs</p> <p> </p> <code>randomize</code> <p>Whether to randomize the split</p> <p> </p> <code>multi_sentence</code> <p>Whether to split sentences across multiple docs</p> <p> </p> <code>filter_expr</code> <p>An expression to filter the docs to generate</p> <p> </p>"},{"location":"reference/edsnlp/train/#edsnlp.train.Reader.split_doc","title":"<code>split_doc</code>","text":"<p>Split a doc into multiple docs of max_length tokens.</p>"},{"location":"reference/edsnlp/train/#edsnlp.train.Reader.split_doc--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The doc to split</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Iterable[Doc]</code>"},{"location":"reference/edsnlp/train/#edsnlp.train.subset_doc","title":"<code>subset_doc</code>","text":"<p>Subset a doc given a start and end index.</p>"},{"location":"reference/edsnlp/train/#edsnlp.train.subset_doc--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The doc to subset</p> <p> TYPE: <code>Doc</code> </p> <code>start</code> <p>The start index</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>The end index</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/utils/","title":"<code>edsnlp.utils</code>","text":""},{"location":"reference/edsnlp/utils/batching/","title":"<code>edsnlp.utils.batching</code>","text":""},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_words","title":"<code>batchify_by_words</code>","text":"<p>Yields batch that contain at most <code>batch_size</code> words. If an item contains more than <code>batch_size</code> words, it will be yielded as a single batch.</p>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_words--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>iterable</code> <p> TYPE: <code>Iterable[T]</code> </p> <code>batch_size</code> <p> TYPE: <code>int</code> </p> <code>drop_last</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/utils/bindings/","title":"<code>edsnlp.utils.bindings</code>","text":""},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.AttributesArg","title":"<code>AttributesArg</code>","text":"<p>Valid values for the <code>attributes</code> argument of a component can be :</p> <ul> <li>a (span) -&gt; attribute callable</li> <li>a attribute name (\"_.negated\")</li> <li>a list of attribute names ([\".negated\", \".event\"])</li> <li>a dict of attribute name to True or list of labels, to filter the attributes</li> </ul>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.AttributesArg--examples","title":"Examples","text":"<ul> <li><code>attributes=\"_.negated\"</code> will use the <code>negated</code> extention of the span</li> <li><code>attributes=[\"_.negated\", \"_.past\"]</code> will use the <code>negated</code> and <code>past</code>    extensions of the span</li> <li><code>attributes={\"_.negated\": True, \"_.past\": \"DATE\"}</code> will use the <code>negated</code>    extension of any span, and the <code>past</code> extension of spans with the <code>DATE</code> label</li> </ul>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.make_binding_getter","title":"<code>make_binding_getter</code>","text":"<p>Make a attribute getter</p>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.make_binding_getter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>attribute</code> <p>Either one of the following: - a path to a nested attributes of the span, such as \"attribute_\" or \"_.negated\" - a tuple of (key, value) equality, such as <code>(\"_.date.mode\", \"PASSED\")</code></p> <p> TYPE: <code>Union[str, Binding]</code> </p> RETURNS DESCRIPTION <code>Callable[[Span], bool]</code> <p>The attribute getter</p>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.make_binding_setter","title":"<code>make_binding_setter</code>","text":"<p>Make a attribute setter</p>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.make_binding_setter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>binding</code> <p>A pair of - a path to a nested attributes of the span, such as <code>attribute_</code> or <code>_.negated</code> - a value assignment</p> <p> TYPE: <code>Binding</code> </p> RETURNS DESCRIPTION <code>Callable[[Span]]</code> <p>The attribute setter</p>"},{"location":"reference/edsnlp/utils/collections/","title":"<code>edsnlp.utils.collections</code>","text":""},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.batch_compress_dict","title":"<code>batch_compress_dict</code>","text":"<p>Compress a sequence of dictionaries in which values that occur multiple times are deduplicated. The corresponding keys will be merged into a single string using the \"|\" character as a separator. This is useful to preserve referential identities when decompressing the dictionary after it has been serialized and deserialized.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.batch_compress_dict--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>seq</code> <p>Sequence of dictionaries to compress</p> <p> TYPE: <code>Optional[Iterable[Dict[str, Any]]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.multi_tee","title":"<code>multi_tee</code>","text":"<p>Makes copies of an iterable such that every iteration over it starts from 0. If the iterable is a sequence (list, tuple), just returns it since every iter() over the object restart from the beginning</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.FrozenDict","title":"<code>FrozenDict</code>","text":"<p>           Bases: <code>dict</code></p> <p>Copied from <code>spacy.util.SimpleFrozenDict</code> to ensure compatibility.</p> <p>Initialize the frozen dict. Can be initialized with pre-defined values.</p> <p>error (str): The error message when user tries to assign to dict.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.FrozenList","title":"<code>FrozenList</code>","text":"<p>           Bases: <code>list</code></p> <p>Copied from <code>spacy.util.SimpleFrozenDict</code> to ensure compatibility</p> <p>Initialize the frozen list.</p> <p>error (str): The error message when user tries to mutate the list.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.ld_to_dl","title":"<code>ld_to_dl</code>","text":"<p>Convert a list of dictionaries to a dictionary of lists</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.ld_to_dl--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>ld</code> <p>The list of dictionaries</p> <p> TYPE: <code>Iterable[Mapping[str, T]]</code> </p> RETURNS DESCRIPTION <code>Dict[str, List[T]]</code> <p>The dictionary of lists</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.dl_to_ld","title":"<code>dl_to_ld</code>","text":"<p>Convert a dictionary of lists to a list of dictionaries</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.dl_to_ld--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>dl</code> <p>The dictionary of lists</p> <p> TYPE: <code>Mapping[str, Sequence[Any]]</code> </p> RETURNS DESCRIPTION <code>List[Dict[str, Any]]</code> <p>The list of dictionaries</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.decompress_dict","title":"<code>decompress_dict</code>","text":"<p>Decompress a dictionary of lists into a sequence of dictionaries. This function assumes that the dictionary structure was obtained using the <code>batch_compress_dict</code> class. Keys that were merged into a single string using the \"|\" character as a separator will be split into a nested dictionary structure.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.decompress_dict--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>seq</code> <p>The dictionary to decompress or a sequence of dictionaries to decompress</p> <p> TYPE: <code>Union[Iterable[Dict[str, Any]], Dict[str, Any]]</code> </p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.batchify","title":"<code>batchify</code>","text":"<p>Yields batch that contain at most <code>batch_size</code> elements. If an item contains more than <code>batch_size</code> elements, it will be yielded as a single batch.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.batchify--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>iterable</code> <p> TYPE: <code>Iterable[T]</code> </p> <code>batch_size</code> <p> TYPE: <code>int</code> </p> <code>drop_last</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/utils/deprecation/","title":"<code>edsnlp.utils.deprecation</code>","text":""},{"location":"reference/edsnlp/utils/doc_to_text/","title":"<code>edsnlp.utils.doc_to_text</code>","text":""},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.aggregate_tokens","title":"<code>aggregate_tokens</code>  <code>cached</code>","text":"<p>Aggregate tokens strings, computed from their <code>attr</code> attribute, into a single string, possibly ignoring excluded tokens (like pollution tokens) and/or space tokens. This also returns the start and end offsets of each token in the aggregated string, as well as a bytes array indicating which tokens were kept. The reason for the bytes array is that it is faster to index, and allows reverse indexing as well.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.aggregate_tokens--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> <code>attr</code> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tuple[str, List[int], List[int], bytes]</code> <p>The aggregated text, the start offsets, the end offsets, and the bytes array indicating which tokens were kept.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.get_text","title":"<code>get_text</code>","text":"<p>Get text using a custom attribute, possibly ignoring excluded tokens.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.get_text--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>Doc or Span to get text from.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>attr</code> <p>Attribute to use.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens, by default False</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Extracted text.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.get_char_offsets","title":"<code>get_char_offsets</code>","text":"<p>Get char offsets of the doc tokens in the \"cleaned\" text.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.get_char_offsets--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>Doc or Span to get text from.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>attr</code> <p>Attribute to use.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens, by default False</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tuple[List[int], List[int]]</code> <p>An alignment tuple: clean start/end offsets lists.</p>"},{"location":"reference/edsnlp/utils/examples/","title":"<code>edsnlp.utils.examples</code>","text":""},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.find_matches","title":"<code>find_matches</code>","text":"<p>Finds entities within the example.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.find_matches--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>example</code> <p>Example to process.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Match]</code> <p>List of matches for entities.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.parse_match","title":"<code>parse_match</code>","text":"<p>Parse a regex match representing an entity.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.parse_match--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>match</code> <p>Match for an entity.</p> <p> TYPE: <code>Match</code> </p> RETURNS DESCRIPTION <code>Match</code> <p>Usable representation for the entity match.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.parse_example","title":"<code>parse_example</code>","text":"<p>Parses an example : finds examples and removes the tags.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.parse_example--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>example</code> <p>Example to process.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Tuple[str, List[Entity]]</code> <p>Cleaned text and extracted entities.</p>"},{"location":"reference/edsnlp/utils/extensions/","title":"<code>edsnlp.utils.extensions</code>","text":""},{"location":"reference/edsnlp/utils/extensions/#edsnlp.utils.extensions.rgetattr","title":"<code>rgetattr</code>","text":"<p>Get attribute recursively</p>"},{"location":"reference/edsnlp/utils/extensions/#edsnlp.utils.extensions.rgetattr--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>obj</code> <p>An object</p> <p> TYPE: <code>Any</code> </p> <code>attr</code> <p>The name of the attribute to get. Can contain dots.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edsnlp/utils/file_system/","title":"<code>edsnlp.utils.file_system</code>","text":""},{"location":"reference/edsnlp/utils/filter/","title":"<code>edsnlp.utils.filter</code>","text":""},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.default_sort_key","title":"<code>default_sort_key</code>","text":"<p>Returns the sort key for filtering spans.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.default_sort_key--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to sort.</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>key</code> <p>Sort key.</p> <p> TYPE: <code>Tuple(int, int)</code> </p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.start_sort_key","title":"<code>start_sort_key</code>","text":"<p>Returns the sort key for filtering spans by start order.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.start_sort_key--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to sort.</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>key</code> <p>Sort key.</p> <p> TYPE: <code>Tuple(int, int)</code> </p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.filter_spans","title":"<code>filter_spans</code>","text":"<p>Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones.</p> <p>Can also accept a <code>label_to_remove</code> argument, useful for filtering out pseudo cues. If set, <code>results</code> can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues.</p> <p>It can handle an iterable of tuples instead of an iterable of <code>Span</code>s. The primary use-case is the use with the <code>RegexMatcher</code>'s capacity to return the span's <code>groupdict</code>.</p> <p>The spaCy documentation states:</p> <p>Filter a sequence of spans and remove duplicates or overlaps. Useful for creating named entities (where one token can only be part of one entity) or when merging spans with <code>Retokenizer.merge</code>. When spans overlap, the (first) longest span is preferred over shorter spans.</p> <p>Filtering out spans</p> <p>If the <code>label_to_remove</code> argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove.</p> <p>The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede and follow a marked entity. Hence we need to keep every example.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.filter_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>spans</code> <p>Spans to filter.</p> <p> TYPE: <code>Iterable[Union[Span, Tuple[Span, Any]]]</code> </p> <code>return_discarded</code> <p>Whether to return discarded spans.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_to_remove</code> <p>Label to remove. If set, results can contain overlapping spans.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>sort_key</code> <p>Key to sorting spans before applying overlap conflict resolution. A span with a higher key will have precedence over another span. By default, the largest, leftmost spans are selected first.</p> <p> TYPE: <code>Callable[Span, Any]</code> DEFAULT: <code>default_sort_key</code> </p> RETURNS DESCRIPTION <code>results</code> <p>Filtered spans</p> <p> TYPE: <code>List[Union[Span, Tuple[Span, Any]]]</code> </p> <code>discarded</code> <p>Discarded spans</p> <p> TYPE: <code>(List[Union[Span, Tuple[Span, Any]]], optional)</code> </p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.consume_spans","title":"<code>consume_spans</code>","text":"<p>Consume a list of span, according to a filter.</p> <p>Warning</p> <p>This method makes the hard hypothesis that:</p> <ol> <li>Spans are sorted.</li> <li>Spans are consumed in sequence and only once.</li> </ol> <p>The second item is problematic for the way we treat long entities, hence the <code>second_chance</code> parameter, which lets entities be seen more than once.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.consume_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>spans</code> <p>List of spans to filter</p> <p> TYPE: <code>List of spans</code> </p> <code>filter</code> <p>Filtering function. Should return True when the item is to be included.</p> <p> TYPE: <code>Callable</code> </p> <code>second_chance</code> <p>Optional list of spans to include again (useful for long entities), by default None</p> <p> TYPE: <code>List of spans</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>matches</code> <p>List of spans consumed by the filter.</p> <p> TYPE: <code>List of spans</code> </p> <code>remainder</code> <p>List of remaining spans in the original <code>spans</code> parameter.</p> <p> TYPE: <code>List of spans</code> </p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.get_spans","title":"<code>get_spans</code>","text":"<p>Extracts spans with a given label. Prefer using hash label for performance reasons.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.get_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>spans</code> <p>List of spans to filter.</p> <p> TYPE: <code>List[Span]</code> </p> <code>label</code> <p>Label to filter on.</p> <p> TYPE: <code>Union[int, str]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>Filtered spans.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.span_f1","title":"<code>span_f1</code>","text":"<p>Computes the F1 overlap between two spans.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.span_f1--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>a</code> <p>First span</p> <p> TYPE: <code>Span</code> </p> <code>b</code> <p>Second span</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>float</code> <p>F1 overlap</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.align_spans","title":"<code>align_spans</code>","text":"<p>Aligns two lists of spans, by matching source spans that overlap target spans. This function is optimized to avoid quadratic complexity.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.align_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>source_spans</code> <p>List of spans to align.</p> <p> TYPE: <code>List[Span]</code> </p> <code>target_spans</code> <p>List of spans to align.</p> <p> TYPE: <code>List[Span]</code> </p> <code>sort_by_overlap</code> <p>Whether to sort the aligned spans by maximum dice/f1 overlap with the target span.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[List[Span]]</code> <p>Subset of <code>source</code> spans for each target span</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.get_span_group","title":"<code>get_span_group</code>","text":"<p>Get the spans of a span group that are contained inside a doclike object.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.get_span_group--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>Doclike object to act as a mask.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>group</code> <p>Group name from which to get the spans.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of spans.</p>"},{"location":"reference/edsnlp/utils/inclusion/","title":"<code>edsnlp.utils.inclusion</code>","text":""},{"location":"reference/edsnlp/utils/inclusion/#edsnlp.utils.inclusion.check_inclusion","title":"<code>check_inclusion</code>","text":"<p>Checks whether the span overlaps the boundaries.</p>"},{"location":"reference/edsnlp/utils/inclusion/#edsnlp.utils.inclusion.check_inclusion--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to check.</p> <p> TYPE: <code>Span</code> </p> <code>start</code> <p>Start of the boundary</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>End of the boundary</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the span overlaps the boundaries.</p>"},{"location":"reference/edsnlp/utils/inclusion/#edsnlp.utils.inclusion.check_sent_inclusion","title":"<code>check_sent_inclusion</code>","text":"<p>Checks whether the span overlaps the boundaries.</p>"},{"location":"reference/edsnlp/utils/inclusion/#edsnlp.utils.inclusion.check_sent_inclusion--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to check.</p> <p> TYPE: <code>Span</code> </p> <code>start</code> <p>Start of the boundary</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>End of the boundary</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the span overlaps the boundaries.</p>"},{"location":"reference/edsnlp/utils/lazy_module/","title":"<code>edsnlp.utils.lazy_module</code>","text":""},{"location":"reference/edsnlp/utils/numbers/","title":"<code>edsnlp.utils.numbers</code>","text":""},{"location":"reference/edsnlp/utils/regex/","title":"<code>edsnlp.utils.regex</code>","text":""},{"location":"reference/edsnlp/utils/regex/#edsnlp.utils.regex.make_pattern","title":"<code>make_pattern</code>","text":"<p>Create OR pattern from a list of patterns.</p>"},{"location":"reference/edsnlp/utils/regex/#edsnlp.utils.regex.make_pattern--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to merge.</p> <p> TYPE: <code>List[str]</code> </p> <code>with_breaks</code> <p>Whether to add breaks (<code>\\b</code>) on each side, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>name</code> <p>Name of the group, using regex <code>?P&lt;&gt;</code> directive.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Merged pattern.</p>"},{"location":"reference/edsnlp/utils/regex/#edsnlp.utils.regex.compile_regex","title":"<code>compile_regex</code>","text":"<p>This function tries to compile <code>reg</code>  using the <code>re</code> module, and fallbacks to the <code>regex</code> module that is more permissive.</p>"},{"location":"reference/edsnlp/utils/regex/#edsnlp.utils.regex.compile_regex--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>reg</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Union[Pattern, Pattern]</code>"},{"location":"reference/edsnlp/utils/resources/","title":"<code>edsnlp.utils.resources</code>","text":""},{"location":"reference/edsnlp/utils/resources/#edsnlp.utils.resources.get_verbs","title":"<code>get_verbs</code>","text":"<p>Extract verbs from the resources, as a pandas dataframe.</p>"},{"location":"reference/edsnlp/utils/resources/#edsnlp.utils.resources.get_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to keep. Returns all verbs by default.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>check_contains</code> <p>Whether to check that no verb is missing if a list of verbs was provided. By default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>DataFrame containing conjugated verbs.</p>"},{"location":"reference/edsnlp/utils/resources/#edsnlp.utils.resources.get_adicap_dict","title":"<code>get_adicap_dict</code>  <code>cached</code>","text":"RETURNS DESCRIPTION <code>Dict</code>"},{"location":"reference/edsnlp/utils/span_getters/","title":"<code>edsnlp.utils.span_getters</code>","text":""},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.SpanSetterArg","title":"<code>SpanSetterArg</code>","text":"<p>Valid values for the <code>span_setter</code> argument of a component can be :</p> <ul> <li>a (doc, matches) -&gt; None callable</li> <li>a span group name</li> <li>a list of span group names</li> <li>a dict of group name to True or list of labels</li> </ul> <p>The group name <code>\"ents\"</code> is a special case, and will add the matches to <code>doc.ents</code></p>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.SpanSetterArg--examples","title":"Examples","text":"<ul> <li><code>span_setter=[\"ents\", \"ckd\"]</code> will add the matches to both <code>doc.ents</code> and <code>doc.spans[\"ckd\"]</code>. It is equivalent to <code>{\"ents\": True, \"ckd\": True}</code>.</li> <li><code>span_setter={\"ents\": [\"foo\", \"bar\"]}</code> will add the matches with label \"foo\" and \"bar\" to <code>doc.ents</code>.</li> <li><code>span_setter=\"ents\"</code> will add all matches only to <code>doc.ents</code>.</li> <li><code>span_setter=\"ckd\"</code> will add all matches only to <code>doc.spans[\"ckd\"]</code>.</li> </ul>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.SpanGetterArg","title":"<code>SpanGetterArg</code>","text":"<p>Valid values for the <code>span_getter</code> argument of a component can be :</p> <ul> <li>a (doc) -&gt; spans callable</li> <li>a span group name</li> <li>a list of span group names</li> <li>a dict of group name to True or list of labels</li> </ul> <p>The group name <code>\"ents\"</code> is a special case, and will get the matches from <code>doc.ents</code></p>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.SpanGetterArg--examples","title":"Examples","text":"<ul> <li><code>span_getter=[\"ents\", \"ckd\"]</code> will get the matches from both <code>doc.ents</code> and <code>doc.spans[\"ckd\"]</code>. It is equivalent to <code>{\"ents\": True, \"ckd\": True}</code>.</li> <li><code>span_getter={\"ents\": [\"foo\", \"bar\"]}</code> will get the matches with label \"foo\" and \"bar\" from <code>doc.ents</code>.</li> <li><code>span_getter=\"ents\"</code> will get all matches from <code>doc.ents</code>.</li> <li><code>span_getter=\"ckd\"</code> will get all matches from <code>doc.spans[\"ckd\"]</code>.</li> </ul>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.make_span_context_getter","title":"<code>make_span_context_getter</code>","text":"<p>Create a span context getter.</p>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.make_span_context_getter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>context_words</code> <p>Minimum number of words to include on each side of the span.</p> <p> TYPE: <code>NonNegativeInt</code> DEFAULT: <code>0</code> </p> <code>context_sents</code> <p>Minimum number of sentences to include on each side of the span:</p> <ul> <li>0: don't use sentences to build the context.</li> <li>1: include the sentence of the span.</li> <li>n: include n sentences on each side of the span.</li> </ul> <p>By default, 0 if the document has no sentence annotations, 1 otherwise.</p> <p> TYPE: <code>Optional[NonNegativeInt]</code> DEFAULT: <code>1</code> </p>"},{"location":"reference/edsnlp/utils/spark_dtypes/","title":"<code>edsnlp.utils.spark_dtypes</code>","text":""},{"location":"reference/edsnlp/utils/spark_dtypes/#edsnlp.utils.spark_dtypes.infer_type","title":"<code>infer_type</code>","text":"<p>Infer the DataType from obj</p>"},{"location":"reference/edsnlp/utils/spark_dtypes/#edsnlp.utils.spark_dtypes.infer_schema","title":"<code>infer_schema</code>","text":"<p>Infer the schema from dict/namedtuple/object</p>"},{"location":"reference/edsnlp/utils/torch/","title":"<code>edsnlp.utils.torch</code>","text":""},{"location":"reference/edsnlp/utils/torch/#edsnlp.utils.torch.mask_to_triangle","title":"<code>mask_to_triangle</code>","text":"<p>Convert a boolean mask to a tensor containing distance to the nearest edge of the mask, in each direction. For example, if the mask is <pre><code>[\n    [1, 1, 1, 1, 1],\n    [1, 1, 1, 0, 0],\n]\n</code></pre></p> <p>The output will be <pre><code>[\n    [0, 1, 2, 1, 0],\n    [0, 1, 0, -1, -2]\n]\n</code></pre></p>"},{"location":"reference/edsnlp/utils/torch/#edsnlp.utils.torch.mask_to_triangle--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>mask</code> <p>A boolean mask</p> <p> </p> RETURNS DESCRIPTION <code>Tensor</code>"},{"location":"reference/edsnlp/utils/typing/","title":"<code>edsnlp.utils.typing</code>","text":""},{"location":"reference/edsnlp/viz/","title":"<code>edsnlp.viz</code>","text":""},{"location":"contributing/","title":"Contributing to EDS-NLP","text":"<p>We welcome contributions ! There are many ways to help. For example, you can:</p> <ol> <li>Help us track bugs by filing issues</li> <li>Suggest and help prioritise new functionalities</li> <li>Develop a new pipe ! Fork the project and propose a new functionality through a pull request</li> <li>Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you.</li> </ol>"},{"location":"contributing/#development-installation","title":"Development installation","text":"<p>To be able to run the test suite, run the example notebooks and develop your own pipeline component, you should clone the repo and install it locally.</p> <pre><code># Clone the repository and change directory\n$ git clone https://github.com/aphp/edsnlp.git\n---&gt; 100%\n$ cd edsnlp\n\n# Optional: create a virtual environment\n$ python -m venv venv\n$ source venv/bin/activate\n\n# Install the package with common, dev, setup dependencies in editable mode\n$ pip install -e '.[dev,setup]'\n# And build resources\n$ python scripts/conjugate_verbs.py\n</code></pre> <p>To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the <code>pre-commit</code> Python library. To use it, simply install it:</p> <pre><code>$ pre-commit install\n</code></pre> <p>The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong.</p> <p>The hooks only run on staged changes. To force-run it on all files, run:</p> <pre><code>$ pre-commit run --all-files\n---&gt; 100%\ncolor:green All good !\n</code></pre>"},{"location":"contributing/#proposing-a-merge-request","title":"Proposing a merge request","text":"<p>At the very least, your changes should :</p> <ul> <li>Be well-documented ;</li> <li>Pass every tests, and preferably implement its own ;</li> <li>Follow the style guide.</li> </ul>"},{"location":"contributing/#testing-your-code","title":"Testing your code","text":"<p>We use the Pytest test suite.</p> <p>The following command will run the test suite. Writing your own tests is encouraged !</p> <pre><code>python -m pytest\n</code></pre> <p>Testing Cython code</p> <p>Make sure the package is installed in editable mode. Otherwise <code>Pytest</code> won't be able to find the Cython modules.</p> <p>Should your contribution propose a bug fix, we require the bug be thoroughly tested.</p>"},{"location":"contributing/#architecture-of-a-pipeline-component","title":"Architecture of a pipeline component","text":"<p>Pipes should follow the same pattern :</p> <pre><code>edsnlp/pipes/&lt;pipe&gt;\n   |-- &lt;pipe&gt;.py                # Defines the component logic\n   |-- patterns.py                  # Defines matched patterns\n   |-- factory.py                   # Declares the component to spaCy\n</code></pre>"},{"location":"contributing/#style-guide","title":"Style Guide","text":"<p>We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short :</p> <p>Black reformats entire files in place. It is not configurable.</p> <p>Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use <code>pre-commit</code> to keep our codebase clean.</p> <p>Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be.</p> <p>We use <code>MkDocs</code> for EDS-NLP's documentation. You can checkout the changes you make with:</p> <pre><code># Install the requirements\n$ pip install -e '.[dev]'\n---&gt; 100%\ncolor:green Installation successful\n\n# Run the documentation\n$ mkdocs serve\n</code></pre> <p>Go to <code>localhost:8000</code> to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v0123","title":"v0.12.3","text":""},{"location":"changelog/#changed","title":"Changed","text":"<p>Packages:</p> <ul> <li>Pip-installable models are now built with <code>hatch</code> instead of poetry, which allows us to expose <code>artifacts</code> (weights)   at the root of the sdist package (uploadable to HF) and move them inside the package upon installation to avoid conflicts.</li> <li>Dependencies are no longer inferred with dill-magic (this didn't work well before anyway)</li> <li>Option to perform substitutions in the model's README.md file (e.g., for the model's name, metrics, ...)</li> <li>Huggingface models are now installed with pip editable installations, which is faster since it doesn't copy around the weights</li> </ul>"},{"location":"changelog/#v0121","title":"v0.12.1","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Added binary distribution for linux aarch64 (Streamlit's environment)</li> <li>Added new separator option in eds.table and new input check</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Make catalogue &amp; entrypoints compatible with py37-py312</li> <li>Check that a data has a doc before trying to use the document's <code>note_datetime</code></li> </ul>"},{"location":"changelog/#v0120","title":"v0.12.0","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>The <code>eds.transformer</code> component now accepts <code>prompts</code> (passed to its <code>preprocess</code> method, see breaking change below) to add before each window of text to embed.</li> <li><code>LazyCollection.map</code> / <code>map_batches</code> now support generator functions as arguments.</li> <li>Window stride can now be disabled (i.e., stride = window) during training in the <code>eds.transformer</code> component by <code>training_stride = False</code></li> <li>Added a new <code>eds.ner_overlap_scorer</code> to evaluate matches between two lists of entities, counting true when the dice overlap is above a given threshold</li> <li><code>edsnlp.load</code> now accepts EDS-NLP models from the huggingface hub \ud83e\udd17 !</li> <li>New <code>python -m edsnlp.package</code> command to package a model for the huggingface hub or pypi-like registries</li> <li>Improve table detection in <code>eds.tables</code> and support new options in <code>table._.to_pd_table(...)</code>:</li> <li><code>header=True</code> to use first row as header</li> <li><code>index=True</code> to use first column as index</li> <li><code>as_spans=True</code> to fill cells as document spans instead of strings</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li> Major breaking change in trainable components, moving towards a more \"task-centric\" design:</li> <li>the <code>eds.transformer</code> component is no longer responsible for deciding which spans of text (\"contexts\") should be embedded. These contexts are now passed via the <code>preprocess</code> method, which now accepts more arguments than just the docs to process.</li> <li>similarly the <code>eds.span_pooler</code> is now longer responsible for deciding which spans to pool, and instead pools all spans passed to it in the <code>preprocess</code> method.</li> </ul> <p>Consequently, the <code>eds.transformer</code> and <code>eds.span_pooler</code> no longer accept their <code>span_getter</code> argument, and the <code>eds.ner_crf</code>, <code>eds.span_classifier</code>, <code>eds.span_linker</code> and <code>eds.span_qualifier</code> components now accept a <code>context_getter</code> argument instead, as well as a <code>span_getter</code> argument for the latter two. This refactoring can be summarized as follows:</p> <pre><code>```diff\n- eds.transformer.span_getter\n+ eds.ner_crf.context_getter\n+ eds.span_classifier.context_getter\n+ eds.span_linker.context_getter\n\n- eds.span_pooler.span_getter\n+ eds.span_qualifier.span_getter\n+ eds.span_linker.span_getter\n```\n\nand as an example for the `eds.span_linker` component:\n\n```diff\nnlp.add_pipe(\n    eds.span_linker(\n        metric=\"cosine\",\n        probability_mode=\"sigmoid\",\n+       span_getter=\"ents\",\n+       # context_getter=\"ents\",  -&gt; by default, same as span_getter\n        embedding=eds.span_pooler(\n            hidden_size=128,\n-           span_getter=\"ents\",\n            embedding=eds.transformer(\n-               span_getter=\"ents\",\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"linker\",\n)\n```\n</code></pre> <ul> <li>Trainable embedding components now all use <code>foldedtensor</code> to return embeddings, instead of returning a tensor of floats and a mask tensor.</li> <li> TorchComponent <code>__call__</code> no longer applies the end to end method, and instead calls the <code>forward</code> method directly, like all torch modules.</li> <li>The trainable <code>eds.span_qualifier</code> component has been renamed to <code>eds.span_classifier</code> to reflect its general purpose (it doesn't only predict qualifiers, but any attribute of a span using its context or not).</li> <li><code>omop</code> converter now takes the <code>note_datetime</code> field into account by default when building a document</li> <li><code>span._.date.to_datetime()</code> and <code>span._.date.to_duration()</code> now automatically take the <code>note_datetime</code> into account</li> <li><code>nlp.vocab</code> is no longer serialized when saving a model, as it may contain sensitive information and can be recomputed during inference anyway</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li><code>edsnlp.data.read_json</code> now correctly read the files from the directory passed as an argument, and not from the parent directory.</li> <li>Overwrite spacy's Doc, Span and Token pickling utils to allow recursively storing Doc, Span and Token objects in the extension values (in particular, span._.date.doc)</li> <li>Removed pendulum dependency, solving various pickling, multiprocessing and missing attributes errors</li> </ul>"},{"location":"changelog/#v0112","title":"v0.11.2","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fix <code>edsnlp.utils.file_system.normalize_fs_path</code> file system detection not working correctly</li> <li>Improved performance of <code>edsnlp.data</code> methods over a filesystem (<code>fs</code> parameter)</li> </ul>"},{"location":"changelog/#v0111-2024-04-02","title":"v0.11.1 (2024-04-02)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Automatic estimation of cpu count when using multiprocessing</li> <li><code>optim.initialize()</code> method to create optim state before the first backward pass</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li><code>nlp.post_init</code> will not tee lazy collections anymore (use <code>edsnlp.utils.collections.multi_tee</code> yourself if needed)</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Corrected inconsistencies in <code>eds.span_linker</code></li> </ul>"},{"location":"changelog/#v0110-2024-03-29","title":"v0.11.0 (2024-03-29)","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Support for a <code>filesystem</code> parameter in every <code>edsnlp.data.read_*</code> and <code>edsnlp.data.write_*</code> functions</li> <li>Pipes of a pipeline are now easily accessible with <code>nlp.pipes.xxx</code> instead of <code>nlp.get_pipe(\"xxx\")</code></li> <li>Support builtin Span attributes in converters <code>span_attributes</code> parameter, e.g.   <pre><code>import edsnlp\n\nnlp = ...\nnlp.add_pipe(\"eds.sentences\")\n\ndata = edsnlp.data.from_xxx(...)\ndata = data.map_pipeline(nlp)\ndata.to_pandas(converters={\"ents\": {\"span_attributes\": [\"sent.text\", \"start\", \"end\"]}})\n</code></pre></li> <li>Support assigning Brat AnnotatorNotes as span attributes: <code>edsnlp.data.read_standoff(...,  notes_as_span_attribute=\"cui\")</code></li> <li>Support for mapping full batches in <code>edsnlp.processing</code> pipelines with <code>map_batches</code> lazy collection method:   <pre><code>import edsnlp\n\ndata = edsnlp.data.from_xxx(...)\ndata = data.map_batches(lambda batch: do_something(batch))\ndata.to_pandas()\n</code></pre></li> <li>New <code>data.map_gpu</code> method to map a deep learning operation on some data and take advantage of edsnlp multi-gpu inference capabilities</li> <li>Added average precision computation in edsnlp span_classification scorer</li> <li>You can now add pipes to your pipeline by instantiating them directly, which comes with many advantages, such as auto-completion, introspection and type checking !</li> </ul> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# instead of nlp.add_pipe(\"eds.sentences\")\n</code></pre> <p>The previous way of adding pipes is still supported. - New <code>eds.span_linker</code> deep-learning component to match entities with their concepts in a knowledge base, in synonym-similarity or concept-similarity mode.</p>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li><code>nlp.preprocess_many</code> now uses lazy collections to enable parallel processing</li> <li> Breaking change. Improved and simplified <code>eds.span_qualifier</code>: we didn't support combination groups before, so this feature was scrapped for now. We now also support splitting values of a single qualifier between different span labels.</li> <li>Optimized edsnlp.data batching, especially for large batch sizes (removed a quadratic loop)</li> <li> Breaking change. By default, the name of components added to a pipeline is now the default name defined in their class <code>__init__</code> signature. For most components of EDS-NLP, this will change the name from \"eds.xxx\" to \"xxx\".</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Flatten list outputs (such as \"ents\" converter) when iterating: <code>nlp.map(data).to_iterable(\"ents\")</code> is now a list of entities, and not a list of lists of entities</li> <li>Allow span pooler to choose between multiple base embedding spans (as likely produced by <code>eds.transformer</code>) by sorting them by Dice overlap score.</li> <li>EDS-NLP does not raise an error anymore when saving a model to an already existing, but empty directory</li> </ul>"},{"location":"changelog/#v0107-2024-03-12","title":"v0.10.7 (2024-03-12)","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Support empty writer converter by default in <code>edsnlp.data</code> readers / writers (do not convert by default)</li> <li>Add support for polars data import / export</li> <li>Allow kwargs in <code>eds.transformer</code> to pass to the transformer model</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Saving pipelines now longer saves the <code>disabled</code> status of the pipes (i.e., all pipes are considered \"enabled\" when saved). This feature was not used and causing issues when saving a model wrapped in a <code>nlp.select_pipes</code> context.</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Allow missing <code>meta.json</code>, <code>tokenizer</code> and <code>vocab</code> paths when loading saved models</li> <li>Save torch buffers when dumping machine learning models to disk (previous versions only saved the model parameters)</li> <li>Fix automatic <code>batch_size</code> estimation in <code>eds.transformer</code> when <code>max_tokens_per_device</code> is set to <code>auto</code> and multiple GPUs are used</li> <li>Fix JSONL file parsing</li> </ul>"},{"location":"changelog/#v0106-2024-02-24","title":"v0.10.6 (2024-02-24)","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Added <code>batch_by</code>, <code>split_into_batches_after</code>, <code>sort_chunks</code>, <code>chunk_size</code>, <code>disable_implicit_parallelism</code> parameters to processing (<code>simple</code> and <code>multiprocessing</code>) backends to improve performance   and memory usage. Sorting chunks can improve yield up to twice the speed in some cases.</li> <li>The deep learning cache mechanism now supports multitask models with weight sharing in multiprocessing mode.</li> <li>Added <code>max_tokens_per_device=\"auto\"</code> parameter to <code>eds.transformer</code> to estimate memory usage and automatically split the input into chunks that fit into the GPU.</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Improved speed and memory usage of the <code>eds.text_cnn</code> pipe by running the CNN on a non-padded version of its input: expect a speedup up to 1.3x in real-world use cases.</li> <li>Deprecate the converters' (especially for BRAT/Standoff data) <code>bool_attributes</code>   parameter in favor of general <code>default_attributes</code>. This new mapping describes how to   set attributes on spans for which no attribute value was found in the input format.   This is especially useful for negation, or frequent attributes values (e.g. \"negated\"   is often False, \"temporal\" is often \"present\"), that annotators may not want to   annotate every time.</li> <li>Default <code>eds.ner_crf</code> window is now set to 40 and stride set to 20, as it doesn't   affect throughput (compared to before, window set to 20) and improves accuracy.</li> <li>New default <code>overlap_policy='merge'</code> option and parameter renaming in   <code>eds.span_context_getter</code> (which replaces <code>eds.span_sentence_getter</code>)</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Improved error handling in <code>multiprocessing</code> backend (e.g., no more deadlock)</li> <li>Various improvements to the data processing related documentation pages</li> <li>Begin of sentence / end of sentence transitions of the <code>eds.ner_crf</code> component are now   disabled when windows are used (e.g., neither <code>window=1</code> equivalent to softmax and   <code>window=0</code>equivalent to default full sequence Viterbi decoding)</li> <li><code>eds</code> tokenizer nows inherits from <code>spacy.Tokenizer</code> to avoid typing errors</li> <li>Only match 'ne' negation pattern when not part of another word to avoid false positives cases like <code>u[ne] cure de 10 jours</code></li> <li>Disabled pipes are now correctly ignored in the <code>Pipeline.preprocess</code> method</li> <li>Add \"eventuel*\" patterns to <code>eds.hyphothesis</code></li> </ul>"},{"location":"changelog/#v0105-2024-01-29","title":"v0.10.5 (2024-01-29)","text":""},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Allow non-url paths when parquet filesystem is given</li> </ul>"},{"location":"changelog/#v0104-2024-01-19","title":"v0.10.4 (2024-01-19)","text":""},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Assigning <code>doc._.note_datetime</code> will now automatically cast the value to a <code>pendulum.DateTime</code> object</li> </ul>"},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Support loading model from package name (e.g., <code>edsnlp.load(\"eds_pseudo_aphp\")</code>)</li> <li>Support filesystem parameter in <code>edsnlp.data.read_parquet</code> and <code>edsnlp.data.write_parquet</code></li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Support doc -&gt; list converters with parquet files writer</li> <li>Fixed some OOM errors when writing many outputs to parquet files</li> <li>Both edsnlp &amp; spacy factories are now listed when a factory lookup fails</li> <li>Fixed some GPU OOM errors with the <code>eds.transformer</code> pipe when processing really long documents</li> </ul>"},{"location":"changelog/#v0103-2024-01-11","title":"v0.10.3 (2024-01-11)","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>By default, <code>edsnlp.data.write_json</code> will infer if the data should be written as a single JSONL   file or as a directory of JSON files, based on the <code>path</code> argument being a file or not.</li> </ul>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Measurements now correctly match \"0.X\", \"0.XX\", ... numbers</li> <li>Typo in \"celsius\" measurement unit</li> <li>Spaces and digits are now supported in BRAT entity labels</li> <li>Fixed missing 'permet pas + verb' false positive negation patterns</li> </ul>"},{"location":"changelog/#v0102-2023-12-20","title":"v0.10.2 (2023-12-20)","text":""},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li><code>eds.span_qualifier</code> qualifiers argument now automatically adds the underscore prefix if not present</li> </ul>"},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Fix imports of components declared in <code>spacy_factories</code> entry points</li> <li>Support <code>pendulum</code> v3</li> <li><code>AsList</code> errors are now correctly reported</li> <li><code>eds.span_qualifier</code> saved configuration during <code>to_disk</code> is now longer null</li> </ul>"},{"location":"changelog/#v0101-2023-12-15","title":"v0.10.1 (2023-12-15)","text":""},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Small regex matching performance improvement, up to 1.25x faster (e.g. <code>eds.measurements</code>)</li> </ul>"},{"location":"changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Microgram scale is now correctly 1/1000g and inverse meter now 1/100 inverse cm.</li> <li>We now isolate some of edsnlp components (trainable pipes that require ml dependencies)   in a new <code>edsnlp_factories</code> entry points to prevent spacy from auto-importing them.</li> <li>TNM scores followed by a space are now correctly detected</li> <li>Removed various short TNM false positives (e.g., \"PT\" or \"a T\") and false negatives</li> <li>The Span value extension is not more forcibly overwritten, and user assigned values are returned by <code>Span._.value</code> in priority, before the aggregated <code>span._.get(span.label_)</code> getter result (#220)</li> <li>Enable mmap during multiprocessing model transfers</li> <li><code>RegexMatcher</code> now supports all alignment modes (<code>strict</code>, <code>expand</code>, <code>contract</code>) and better handles partial doc matching (#201).</li> <li><code>on_ent_only=False/True</code> is now supported again in qualifier pipes (e.g., \"eds.negation\", \"eds.hypothesis\", ...)</li> </ul>"},{"location":"changelog/#v0100-2023-12-04","title":"v0.10.0 (2023-12-04)","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>New add unified <code>edsnlp.data</code> api (json, brat, spark, pandas) and LazyCollection object   to efficiently read / write data from / to different formats &amp; sources.</li> <li>New unified processing API to select the execution execution backends via <code>data.set_processing(...)</code></li> <li>The training scripts can now use data from multiple concatenated adapters</li> <li>Support quantized transformers (compatible with multiprocessing as well !)</li> </ul>"},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li><code>edsnlp.pipelines</code> has been renamed to <code>edsnlp.pipes</code>, but the old name is still available for backward compatibility</li> <li>Pipes (in <code>edsnlp/pipes</code>) are now lazily loaded, which should improve the loading time of the library.</li> <li><code>to_disk</code> methods can now return a config to override the initial config of the pipeline (e.g., to load a transformer directly from the path storing its fine-tuned weights)</li> <li>The <code>eds.tokenizer</code> tokenizer has been added to entry points, making it accessible from the outside</li> <li>Deprecate old connectors (e.g. BratDataConnector) in favor of the new <code>edsnlp.data</code> API</li> <li>Deprecate old <code>pipe</code> wrapper in favor of the new processing API</li> </ul>"},{"location":"changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Support for pydantic v2</li> <li>Support for python 3.11 (not ci-tested yet)</li> </ul>"},{"location":"changelog/#v0100beta1-2023-12-04","title":"v0.10.0beta1 (2023-12-04)","text":"<p>Large refacto of EDS-NLP to allow training models and performing inference using PyTorch as the deep-learning backend. Rather than a mere wrapper of Pytorch using spaCy, this is a new framework to build hybrid multi-task models.</p> <p>To achieve this, instead of patching spaCy's pipeline, a new pipeline was implemented in a similar fashion to aphp/edspdf#12. The new pipeline tries to preserve the existing API, especially for non-machine learning uses such as rule-based components. This means that users can continue to use the library in the same way as before, while also having the option to train models using PyTorch. We still use spaCy data structures such as Doc and Span to represent the texts and their annotations.</p> <p>Otherwise, changes should be transparent for users that still want to use spacy pipelines with <code>nlp = spacy.blank('eds')</code>. To benefit from the new features, users should use <code>nlp = edsnlp.blank('eds')</code> instead.</p>"},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>New pipeline system available via <code>edsnlp.blank('eds')</code> (instead of <code>spacy.blank('eds')</code>)</li> <li>Use the confit package to instantiate components</li> <li>Training script with Pytorch only (<code>tests/training/</code>) and tutorial</li> <li>New trainable embeddings: <code>eds.transformer</code>, <code>eds.text_cnn</code>, <code>eds.span_pooler</code>   embedding contextualizer pipes</li> <li>Re-implemented the trainable NER component and trainable Span qualifier with the new   system under <code>eds.ner_crf</code> and <code>eds.span_classifier</code></li> <li>New efficient implementation for eds.transformer (to be used in place of   spacy-transformer)</li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>Pipe registering: <code>Language.factory</code> -&gt; <code>edsnlp.registry.factory.register</code> via confit</li> <li>Lazy loading components from their entry point (had to patch spacy.Language.init)   to avoid having to wrap every import torch statement for pure rule-based use cases.   Hence, torch is not a required dependency</li> </ul>"},{"location":"changelog/#v092-2023-12-04","title":"v0.9.2 (2023-12-04)","text":""},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>Fix matchers to skip pipes with assigned extensions that are not required by the matcher during the initialization</li> </ul>"},{"location":"changelog/#v091-2023-09-22","title":"v0.9.1 (2023-09-22)","text":""},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>Improve negation patterns</li> <li>Abstent disorders now set the negation to True when matched as <code>ABSENT</code></li> <li>Default qualifier is now <code>None</code> instead of <code>False</code> (empty string)</li> </ul>"},{"location":"changelog/#fixed_13","title":"Fixed","text":"<ul> <li><code>span_getter</code> is not incompatible with on_ents_only anymore</li> <li><code>ContextualMatcher</code> now supports empty matches (e.g. lookahead/lookbehind) in <code>assign</code> patterns</li> </ul>"},{"location":"changelog/#v090-2023-09-15","title":"v0.9.0 (2023-09-15)","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>New <code>to_duration</code> method to convert an absolute date into a date relative to the note_datetime (or None)</li> </ul>"},{"location":"changelog/#changes","title":"Changes","text":"<ul> <li>Input and output of components are now specified by <code>span_getter</code> and <code>span_setter</code> arguments.</li> <li> Score / disorders / behaviors entities now have a fixed label (passed as an argument), instead of being dynamically set from the component name. The following scores may have a different name   than the current one in your pipelines:<ul> <li><code>eds.emergency.gemsa</code> \u2192 <code>emergency_gemsa</code></li> <li><code>eds.emergency.ccmu</code> \u2192 <code>emergency_ccmu</code></li> <li><code>eds.emergency.priority</code> \u2192 <code>emergency_priority</code></li> <li><code>eds.charlson</code> \u2192 <code>charlson</code></li> <li><code>eds.elston_ellis</code> \u2192 <code>elston_ellis</code></li> <li><code>eds.SOFA</code> \u2192 <code>sofa</code></li> <li><code>eds.adicap</code> \u2192 <code>adicap</code></li> <li><code>eds.measuremets</code> \u2192 <code>size</code>, <code>weight</code>, ... instead of <code>eds.size</code>, <code>eds.weight</code>, ...</li> </ul> </li> <li><code>eds.dates</code> now separate dates from durations. Each entity has its own label:<ul> <li><code>spans[\"dates\"]</code> \u2192 entities labelled as <code>date</code> with a <code>span._.date</code> parsed object</li> <li><code>spans[\"durations\"]</code> \u2192 entities labelled as <code>duration</code> with a <code>span._.duration</code> parsed object</li> </ul> </li> <li>the \"relative\" / \"absolute\" / \"duration\" mode of the time entity is now stored in   the <code>mode</code> attribute of the <code>span._.date/duration</code></li> <li>the \"from\" / \"until\" period bound, if any, is now stored in the <code>span._.date.bound</code> attribute</li> <li><code>to_datetime</code> now only return absolute dates, converts relative dates into absolute if <code>doc._.note_datetime</code> is given, and None otherwise</li> </ul>"},{"location":"changelog/#fixed_14","title":"Fixed","text":"<ul> <li><code>export_to_brat</code> issue with spans of entities on multiple lines.</li> </ul>"},{"location":"changelog/#v081-2023-05-31","title":"v0.8.1 (2023-05-31)","text":"<p>Fix release to allow installation from source</p>"},{"location":"changelog/#v080-2023-05-24","title":"v0.8.0 (2023-05-24)","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>New trainable component for multi-label, multi-class span qualification (any attribute/extension)</li> <li>Add range measurements (like <code>la tumeur fait entre 1 et 2 cm</code>) to <code>eds.measurements</code> matcher</li> <li>Add <code>eds.CKD</code> component</li> <li>Add <code>eds.COPD</code> component</li> <li>Add <code>eds.alcohol</code> component</li> <li>Add <code>eds.cerebrovascular_accident</code> component</li> <li>Add <code>eds.congestive_heart_failure</code> component</li> <li>Add <code>eds.connective_tissue_disease</code> component</li> <li>Add <code>eds.dementia</code> component</li> <li>Add <code>eds.diabetes</code> component</li> <li>Add <code>eds.hemiplegia</code> component</li> <li>Add <code>eds.leukemia</code> component</li> <li>Add <code>eds.liver_disease</code> component</li> <li>Add <code>eds.lymphoma</code> component</li> <li>Add <code>eds.myocardial_infarction</code> component</li> <li>Add <code>eds.peptic_ulcer_disease</code> component</li> <li>Add <code>eds.peripheral_vascular_disease</code> component</li> <li>Add <code>eds.solid_tumor</code> component</li> <li>Add <code>eds.tobacco</code> component</li> <li>Add <code>eds.spaces</code> (or <code>eds.normalizer</code> with <code>spaces=True</code>) to detect space tokens, and add <code>ignore_space_tokens</code> to <code>EDSPhraseMatcher</code> and <code>SimstringMatcher</code> to skip them</li> <li>Add <code>ignore_space_tokens</code> option in most components</li> <li><code>eds.tables</code>: new pipeline to identify formatted tables</li> <li>New <code>merge_mode</code> parameter in <code>eds.measurements</code> to normalize existing entities or detect   measures only inside existing entities</li> <li>Tokenization exceptions (<code>Mr.</code>, <code>Dr.</code>, <code>Mrs.</code>) and non end-of-sentence periods are now tokenized with the next letter in the <code>eds</code> tokenizer</li> </ul>"},{"location":"changelog/#changed_13","title":"Changed","text":"<ul> <li>Disable <code>EDSMatcher</code> preprocessing auto progress tracking by default</li> <li>Moved dependencies to a single pyproject.toml: support for <code>pip install -e '.[dev,docs,setup]'</code></li> <li>ADICAP matcher now allow dot separators (e.g. <code>B.H.HP.A7A0</code>)</li> </ul>"},{"location":"changelog/#fixed_15","title":"Fixed","text":"<ul> <li>Abbreviation and number tokenization issues in the <code>eds</code> tokenizer</li> <li><code>eds.adicap</code> : reparsed the dictionnary used to decode the ADICAP codes (some of them were wrongly decoded)</li> <li>Fix build for python 3.9 on Mac M1/M2 machines.</li> </ul>"},{"location":"changelog/#v074-2022-12-12","title":"v0.7.4 (2022-12-12)","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li><code>eds.history</code> : Add the option to consider only the closest dates in the sentence (dates inside the boundaries and if there is not, it takes the closest date in the entire sentence).</li> <li><code>eds.negation</code> : It takes into account following past participates and preceding infinitives.</li> <li><code>eds.hypothesis</code>: It takes into account following past participates hypothesis verbs.</li> <li><code>eds.negation</code> &amp; <code>eds.hypothesis</code> : Introduce new patterns and remove unnecessary patterns.</li> <li><code>eds.dates</code> : Add a pattern for preceding relative dates (ex: l'embolie qui est survenue \u00e0 10 jours).</li> <li>Improve patterns in the <code>eds.pollution</code> component to account for multiline footers</li> <li>Add <code>QuickExample</code> object to quickly try a pipeline.</li> <li>Add UMLS terminology matcher <code>eds.umls</code></li> <li>New <code>RegexMatcher</code> method to create spans from groupdicts</li> <li>New <code>eds.dates</code> option to disable time detection</li> </ul>"},{"location":"changelog/#changed_14","title":"Changed","text":"<ul> <li>Improve date detection by removing false positives</li> </ul>"},{"location":"changelog/#fixed_16","title":"Fixed","text":"<ul> <li><code>eds.hypothesis</code> : Remove too generic patterns.</li> <li><code>EDSTokenizer</code> : It now tokenizes <code>\"rechereche d'\"</code> as <code>[\"recherche\", \"d'\"]</code>, instead of <code>[\"recherche\", \"d\", \"'\"]</code>.</li> <li>Fix small typos in the documentation and in the docstring.</li> <li>Harmonize processing utils (distributed custom_pipe) to have the same API for Pandas and Pyspark</li> <li>Fix BratConnector file loading issues with complex file hierarchies</li> </ul>"},{"location":"changelog/#v072-2022-10-26","title":"v0.7.2 (2022-10-26)","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>Improve the <code>eds.history</code> component by taking into account the date extracted from <code>eds.dates</code> component.</li> <li>New pop up when you click on the copy icon in the termynal widget (docs).</li> <li>Add NER <code>eds.elston-ellis</code> pipeline to identify Elston Ellis scores</li> <li>Add flags=re.MULTILINE to <code>eds.pollution</code> and change pattern of footer</li> </ul>"},{"location":"changelog/#fixed_17","title":"Fixed","text":"<ul> <li>Remove the warning in the <code>eds.sections</code> when <code>eds.normalizer</code> is in the pipe.</li> <li>Fix filter_spans for strictly nested entities</li> <li>Fill eds.remove-lowercase \"assign\" metadata to run the pipeline during EDSPhraseMatcher preprocessing</li> <li>Allow back spaCy components whose name contains a dot (forbidden since spaCy v3.4.2) for backward compatibility.</li> </ul>"},{"location":"changelog/#v071-2022-10-13","title":"v0.7.1 (2022-10-13)","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>Add new patterns (footer, web entities, biology tables, coding sections) to pipeline normalisation (pollution)</li> </ul>"},{"location":"changelog/#changed_15","title":"Changed","text":"<ul> <li>Improved TNM detection algorithm</li> <li>Account for more modifiers in ADICAP codes detection</li> </ul>"},{"location":"changelog/#fixed_18","title":"Fixed","text":"<ul> <li>Add nephew, niece and daughter to family qualifier patterns</li> <li>EDSTokenizer (<code>spacy.blank('eds')</code>) now recognizes non-breaking whitespaces as spaces and does not split float numbers</li> <li><code>eds.dates</code> pipeline now allows new lines as space separators in dates</li> </ul>"},{"location":"changelog/#v070-2022-09-06","title":"v0.7.0 (2022-09-06)","text":""},{"location":"changelog/#added_15","title":"Added","text":"<ul> <li>New nested NER trainable <code>nested_ner</code> pipeline component</li> <li>Support for nested entities and attributes in BratDataConnector</li> <li>Pytorch wrappers and experimental training utils</li> <li>Add attribute <code>section</code> to entities</li> <li>Add new cases for separator pattern when components of the TNM score are separated by a forward slash</li> <li>Add NER <code>eds.adicap</code> pipeline to identify ADICAP codes</li> <li>Add patterns to <code>pollution</code> pipeline and simplifies activating or deactivating specific patterns</li> </ul>"},{"location":"changelog/#changed_16","title":"Changed","text":"<ul> <li>Simplified the configuration scheme of the <code>pollution</code> pipeline</li> <li>Update of the <code>ContextualMatcher</code> (and all pipelines depending on it), rendering it more flexible to use</li> <li>Rename R component of score TNM as \"resection_completeness\"</li> </ul>"},{"location":"changelog/#fixed_19","title":"Fixed","text":"<ul> <li>Prevent section titles from capturing surrounding tokens, causing overlaps (#113)</li> <li>Enhance existing patterns for section detection and add patterns for previously ignored sections (introduction, evolution, modalites de sortie, vaccination) .</li> <li>Fix explain mode, which was always triggered, in <code>eds.history</code> factory.</li> <li>Fix test in <code>eds.sections</code>. Previously, no check was done</li> <li>Remove SOFA scores spurious span suffixes</li> </ul>"},{"location":"changelog/#v062-2022-08-02","title":"v0.6.2 (2022-08-02)","text":""},{"location":"changelog/#added_16","title":"Added","text":"<ul> <li>New <code>SimstringMatcher</code> matcher to perform fuzzy term matching, and <code>algorithm</code> parameter in terminology components and <code>eds.matcher</code> component</li> <li>Makefile to install,test the application and see the documentation</li> </ul>"},{"location":"changelog/#changed_17","title":"Changed","text":"<ul> <li>Add consultation date pattern \"CS\", and False Positive patterns for dates (namely phone numbers and pagination).</li> <li>Update the pipeline score <code>eds.TNM</code>. Now it is possible to return a dictionary where the results are either <code>str</code> or <code>int</code> values</li> </ul>"},{"location":"changelog/#fixed_20","title":"Fixed","text":"<ul> <li>Add new patterns to the negation qualifier</li> <li>Numpy header issues with binary distributed packages</li> <li>Simstring dependency on Windows</li> </ul>"},{"location":"changelog/#v061-2022-07-11","title":"v0.6.1 (2022-07-11)","text":""},{"location":"changelog/#added_17","title":"Added","text":"<ul> <li>Now possible to provide regex flags when using the RegexMatcher</li> <li>New <code>ContextualMatcher</code> pipe, aiming at replacing the <code>AdvancedRegex</code> pipe.</li> <li>New <code>as_ents</code> parameter for <code>eds.dates</code>, to save detected dates as entities</li> </ul>"},{"location":"changelog/#changed_18","title":"Changed","text":"<ul> <li>Faster <code>eds.sentences</code> pipeline component with Cython</li> <li>Bump version of Pydantic in <code>requirements.txt</code> to 1.8.2 to handle an incompatibility with the ContextualMatcher</li> <li>Optimise space requirements by using <code>.csv.gz</code> compression for verbs</li> </ul>"},{"location":"changelog/#fixed_21","title":"Fixed","text":"<ul> <li><code>eds.sentences</code> behaviour with dot-delimited dates (eg <code>02.07.2022</code>, which counted as three sentences)</li> </ul>"},{"location":"changelog/#v060-2022-06-17","title":"v0.6.0 (2022-06-17)","text":""},{"location":"changelog/#added_18","title":"Added","text":"<ul> <li>Complete revamp of the measurements detection pipeline, with better parsing and more exhaustive matching</li> <li>Add new functionality to the method <code>Span._.date.to_datetime()</code> to return a result infered from context for those cases with missing information.</li> <li>Force a batch size of 2000 when distributing a pipeline with Spark</li> <li>New patterns to pipeline <code>eds.dates</code> to identify cases where only the month is mentioned</li> <li>New <code>eds.terminology</code> component for generic terminology matching, using the <code>kb_id_</code> attribute to store fine-grained entity label</li> <li>New <code>eds.cim10</code> terminology matching pipeline</li> <li>New <code>eds.drugs</code> terminology pipeline that maps brand names and active ingredients to a unique ATC code</li> </ul>"},{"location":"changelog/#v053-2022-05-04","title":"v0.5.3 (2022-05-04)","text":""},{"location":"changelog/#added_19","title":"Added","text":"<ul> <li>Support for strings in the example utility</li> <li>TNM detection and normalisation with the <code>eds.TNM</code> pipeline</li> <li>Support for arbitrary callback for Pandas multiprocessing, with the <code>callback</code> argument</li> </ul>"},{"location":"changelog/#v052-2022-05-04","title":"v0.5.2 (2022-05-04)","text":""},{"location":"changelog/#added_20","title":"Added","text":"<ul> <li>Support for chained attributes in the <code>processing</code> pipelines</li> <li>Colour utility with the category20 colour palette</li> </ul>"},{"location":"changelog/#fixed_22","title":"Fixed","text":"<ul> <li>Correct a REGEX on the date detector (both <code>nov</code> and <code>nov.</code> are now detected, as all other months)</li> </ul>"},{"location":"changelog/#v051-2022-04-11","title":"v0.5.1 (2022-04-11)","text":""},{"location":"changelog/#fixed_23","title":"Fixed","text":"<ul> <li>Updated Numpy requirements to be compatible with the <code>EDSPhraseMatcher</code></li> </ul>"},{"location":"changelog/#v050-2022-04-08","title":"v0.5.0 (2022-04-08)","text":""},{"location":"changelog/#added_21","title":"Added","text":"<ul> <li>New <code>eds</code> language to better fit French clinical documents and improve speed</li> <li>Testing for markdown codeblocks to make sure the documentation is actually executable</li> </ul>"},{"location":"changelog/#changed_19","title":"Changed","text":"<ul> <li>Complete revamp of the date detection pipeline, with better parsing and more exhaustive matching</li> <li>Reimplementation of the EDSPhraseMatcher in Cython, leading to a x15 speed increase</li> </ul>"},{"location":"changelog/#v044-2022-03-31","title":"v0.4.4 (2022-03-31)","text":"<ul> <li>Add <code>measures</code> pipeline</li> <li>Cap Jinja2 version to fix mkdocs</li> <li>Adding the possibility to add context in the processing module</li> <li>Improve the speed of char replacement pipelines (accents and quotes)</li> <li>Improve the speed of the regex matcher</li> </ul>"},{"location":"changelog/#v043-2022-03-18","title":"v0.4.3 (2022-03-18)","text":"<ul> <li>Fix regex matching on spans.</li> <li>Add fast_parse in date pipeline.</li> <li>Add relative_date information parsing</li> </ul>"},{"location":"changelog/#v042-2022-03-16","title":"v0.4.2 (2022-03-16)","text":"<ul> <li>Fix issue with <code>dateparser</code> library (see scrapinghub/dateparser#1045)</li> <li>Fix <code>attr</code> issue in the <code>advanced-regex</code> pipelin</li> <li>Add documentation for <code>eds.covid</code></li> <li>Update the demo with an explanation for the regex</li> </ul>"},{"location":"changelog/#v041-2022-03-14","title":"v0.4.1 (2022-03-14)","text":"<ul> <li>Added support to Koalas DataFrames in the <code>edsnlp.processing</code> pipe.</li> <li>Added <code>eds.covid</code> NER pipeline for detecting COVID19 mentions.</li> </ul>"},{"location":"changelog/#v040-2022-02-22","title":"v0.4.0 (2022-02-22)","text":"<ul> <li>Profound re-write of the normalisation :<ul> <li>The custom attribute <code>CUSTOM_NORM</code> is completely abandoned in favour of a more spacyfic alternative</li> <li>The <code>normalizer</code> pipeline modifies the <code>NORM</code> attribute in place</li> <li>Other pipelines can modify the <code>Token._.excluded</code> custom attribute</li> </ul> </li> <li>EDS regex and term matchers can ignore excluded tokens during matching, effectively adding a second dimension to normalisation (choice of the attribute and possibility to skip pollution tokens   regardless of the attribute)</li> <li>Matching can be performed on custom attributes more easily</li> <li>Qualifiers are regrouped together within the <code>edsnlp.qualifiers</code> submodule, the inheritance from the <code>GenericMatcher</code> is dropped.</li> <li><code>edsnlp.utils.filter.filter_spans</code> now accepts a <code>label_to_remove</code> parameter. If set, only corresponding spans are removed, along with overlapping spans. Primary use-case: removing pseudo cues for   qualifiers.</li> <li>Generalise the naming convention for extensions, which keep the same name as the pipeline that created them (eg <code>Span._.negation</code> for the <code>eds.negation</code> pipeline). The previous convention is kept   for now, but calling it issues a warning.</li> <li>The <code>dates</code> pipeline underwent some light formatting to increase robustness and fix a few issues</li> <li>A new <code>consultation_dates</code> pipeline was added, which looks for dates preceded by expressions specific to consultation dates</li> <li>In rule-based processing, the <code>terms.py</code> submodule is replaced by <code>patterns.py</code> to reflect the possible presence of regular expressions</li> <li>Refactoring of the architecture :<ul> <li>pipelines are now regrouped by type (<code>core</code>, <code>ner</code>, <code>misc</code>, <code>qualifiers</code>)</li> <li><code>matchers</code> submodule contains <code>RegexMatcher</code> and <code>PhraseMatcher</code> classes, which interact with the normalisation</li> <li><code>multiprocessing</code> submodule contains <code>spark</code> and <code>local</code> multiprocessing tools</li> <li><code>connectors</code> contains <code>Brat</code>, <code>OMOP</code> and <code>LabelTool</code> connectors</li> <li><code>utils</code> contains various utilities</li> </ul> </li> <li>Add entry points to make pipeline usable directly, removing the need to import <code>edsnlp.components</code>.</li> <li>Add a <code>eds</code> namespace for components: for instance, <code>negation</code> becomes <code>eds.negation</code>. Using the former pipeline name still works, but issues a deprecation warning.</li> <li>Add 3 score pipelines related to emergency</li> <li>Add a helper function to use a spaCy pipeline as a Spark UDF.</li> <li>Fix alignment issues in RegexMatcher</li> <li>Change the alignment procedure, dropping clumsy <code>numpy</code> dependency in favour of <code>bisect</code></li> <li>Change the name of <code>eds.antecedents</code> to <code>eds.history</code>.   Calling <code>eds.antecedents</code> still works, but issues a deprecation warning and support will be removed in a future version.</li> <li>Add a <code>eds.covid</code> component, that identifies mentions of COVID</li> <li>Change the demo, to include NER components</li> </ul>"},{"location":"changelog/#v032-2021-11-24","title":"v0.3.2 (2021-11-24)","text":"<ul> <li>Major revamp of the normalisation.<ul> <li>The <code>normalizer</code> pipeline now adds atomic components (<code>lowercase</code>, <code>accents</code>, <code>quotes</code>, <code>pollution</code> &amp; <code>endlines</code>) to the processing pipeline, and compiles the results into a   new <code>Doc._.normalized</code> extension. The latter is itself a spaCy <code>Doc</code> object, wherein tokens are normalised and pollution tokens are removed altogether. Components that match on the <code>CUSTOM_NORM</code>   attribute process the <code>normalized</code> document, and matches are brought back to the original document using a token-wise mapping.</li> <li>Update the <code>RegexMatcher</code> to use the <code>CUSTOM_NORM</code> attribute</li> <li>Add an <code>EDSPhraseMatcher</code>, wrapping spaCy's <code>PhraseMatcher</code> to enable matching on <code>CUSTOM_NORM</code>.</li> <li>Update the <code>matcher</code> and <code>advanced</code> pipelines to enable matching on the <code>CUSTOM_NORM</code> attribute.</li> </ul> </li> <li>Add an OMOP connector, to help go back and forth between OMOP-formatted pandas dataframes and spaCy documents.</li> <li>Add a <code>reason</code> pipeline, that extracts the reason for visit.</li> <li>Add an <code>endlines</code> pipeline, that classifies newline characters between spaces and actual ends of line.</li> <li>Add possibility to annotate within entities for qualifiers (<code>negation</code>, <code>hypothesis</code>, etc), ie if the cue is within the entity. Disabled by default.</li> </ul>"},{"location":"changelog/#v031-2021-10-13","title":"v0.3.1 (2021-10-13)","text":"<ul> <li>Update <code>dates</code> to remove miscellaneous bugs.</li> <li>Add <code>isort</code> pre-commit hook.</li> <li>Improve performance for <code>negation</code>, <code>hypothesis</code>, <code>antecedents</code>, <code>family</code> and <code>rspeech</code> by using spaCy's <code>filter_spans</code> and our <code>consume_spans</code> methods.</li> <li>Add proposition segmentation to <code>hypothesis</code> and <code>family</code>, enhancing results.</li> </ul>"},{"location":"changelog/#v030-2021-09-29","title":"v0.3.0 (2021-09-29)","text":"<ul> <li>Renamed <code>generic</code> to <code>matcher</code>. This is a non-breaking change for the average user, adding the pipeline is still :</li> </ul> <pre><code>nlp.add_pipe(\"matcher\", config=dict(terms=dict(maladie=\"maladie\")))\n</code></pre> <ul> <li>Removed <code>quickumls</code> pipeline. It was untested, unmaintained. Will be added back in a future release.</li> <li>Add <code>score</code> pipeline, and <code>charlson</code>.</li> <li>Add <code>advanced-regex</code> pipeline</li> <li>Corrected bugs in the <code>negation</code> pipeline</li> </ul>"},{"location":"changelog/#v020-2021-09-13","title":"v0.2.0 (2021-09-13)","text":"<ul> <li>Add <code>negation</code> pipeline</li> <li>Add <code>family</code> pipeline</li> <li>Add <code>hypothesis</code> pipeline</li> <li>Add <code>antecedents</code> pipeline</li> <li>Add <code>rspeech</code> pipeline</li> <li>Refactor the library :<ul> <li>Remove the <code>rules</code> folder</li> <li>Add a <code>pipelines</code> folder, containing one subdirectory per component</li> <li>Every component subdirectory contains a module defining the component, and a module defining a factory, plus any other utilities (eg <code>terms.py</code>)</li> </ul> </li> </ul>"},{"location":"changelog/#v010-2021-09-29","title":"v0.1.0 (2021-09-29)","text":"<p>First working version. Available pipelines :</p> <ul> <li><code>section</code></li> <li><code>sentences</code></li> <li><code>normalization</code></li> <li><code>pollution</code></li> </ul>"}]}