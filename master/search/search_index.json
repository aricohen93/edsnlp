{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started EDS-NLP provides a set of spaCy components that are used to extract information from clinical notes written in French. If it's your first time with spaCy, we recommend you familiarise yourself with some of their key concepts by looking at the \" spaCy 101 \" page. Quick start Installation You can install EDS-NLP via pip : $ pip install edsnlp ---> 100% color:green Successfully installed! We recommend pinning the library version in your projects, or use a strict package manager like Poetry . pip install edsnlp==0.7.2 A first pipeline Once you've installed the library, let's begin with a very simple example that extracts mentions of COVID19 in a text, and detects whether they are negated. import spacy nlp = spacy . blank ( \"fr\" ) # (1) terms = dict ( covid = [ \"covid\" , \"coronavirus\" ], # (2) ) # Sentencizer component, needed for negation detection nlp . add_pipe ( \"eds.sentences\" ) # (3) # Matcher component nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms )) # (4) # Negation detection nlp . add_pipe ( \"eds.negation\" ) # Process your text in one call ! doc = nlp ( \"Le patient est atteint de covid\" ) doc . ents # (5) # Out: (covid,) doc . ents [ 0 ] . _ . negation # (6) # Out: False We only need spaCy's French tokenizer. This example terminology provides a very simple, and by no means exhaustive, list of synonyms for COVID19. In spaCy, pipelines are added via the nlp.add_pipe method . EDS-NLP pipelines are automatically discovered by spaCy. See the matching tutorial for mode details. spaCy stores extracted entities in the Doc.ents attribute . The eds.negation pipeline has added a negation custom attribute. This example is complete, it should run as-is. Check out the spaCy 101 page if you're not familiar with spaCy. Available pipeline components Core Qualifiers Miscellaneous NER Trainable Pipeline Description eds.normalizer Non-destructive input text normalisation eds.sentences Better sentence boundary detection eds.matcher A simple yet powerful entity extractor eds.terminology A simple yet powerful terminology matcher eds.contextual-matcher A conditional entity extractor eds.endlines An unsupervised model to classify each end line Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based medical history detection Pipeline Description eds.dates Date extraction and normalisation eds.measurements Measure extraction and normalisation eds.sections Section detection eds.reason Rule-based hospitalisation reason detection Pipeline Description eds.covid A COVID mentions detector eds.charlson A Charlson score extractor eds.sofa A SOFA score extractor eds.emergency.priority A priority score extractor eds.emergency.ccmu A CCMU score extractor eds.emergency.gemsa A GEMSA score extractor eds.TNM A TNM score extractor eds.cim10 A CIM10 terminology matcher eds.drugs A Drug mentions extractor eds.adicap A ADICAP codes extractor Pipeline Description eds.nested-ner Nested and overlapping named entity recogntion Disclaimer The performances of an extraction pipeline may depend on the population and documents that are considered. Contributing to EDS-NLP We welcome contributions ! Fork the project and propose a pull request. Take a look at the dedicated page for detail. Citation If you use EDS-NLP, please cite us as below. @misc { edsnlp , author = {Dura, Basile and Wajsburt, Perceval and Petit-Jean, Thomas and Cohen, Ariel and Jean, Charline and Bey, Romain} , doi = {10.5281/zenodo.6424993} , title = {EDS-NLP: efficient information extraction from French clinical notes} , url = {http://aphp.github.io/edsnlp} }","title":"Getting started"},{"location":"#getting-started","text":"EDS-NLP provides a set of spaCy components that are used to extract information from clinical notes written in French. If it's your first time with spaCy, we recommend you familiarise yourself with some of their key concepts by looking at the \" spaCy 101 \" page.","title":"Getting started"},{"location":"#quick-start","text":"","title":"Quick start"},{"location":"#installation","text":"You can install EDS-NLP via pip : $ pip install edsnlp ---> 100% color:green Successfully installed! We recommend pinning the library version in your projects, or use a strict package manager like Poetry . pip install edsnlp==0.7.2","title":"Installation"},{"location":"#a-first-pipeline","text":"Once you've installed the library, let's begin with a very simple example that extracts mentions of COVID19 in a text, and detects whether they are negated. import spacy nlp = spacy . blank ( \"fr\" ) # (1) terms = dict ( covid = [ \"covid\" , \"coronavirus\" ], # (2) ) # Sentencizer component, needed for negation detection nlp . add_pipe ( \"eds.sentences\" ) # (3) # Matcher component nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms )) # (4) # Negation detection nlp . add_pipe ( \"eds.negation\" ) # Process your text in one call ! doc = nlp ( \"Le patient est atteint de covid\" ) doc . ents # (5) # Out: (covid,) doc . ents [ 0 ] . _ . negation # (6) # Out: False We only need spaCy's French tokenizer. This example terminology provides a very simple, and by no means exhaustive, list of synonyms for COVID19. In spaCy, pipelines are added via the nlp.add_pipe method . EDS-NLP pipelines are automatically discovered by spaCy. See the matching tutorial for mode details. spaCy stores extracted entities in the Doc.ents attribute . The eds.negation pipeline has added a negation custom attribute. This example is complete, it should run as-is. Check out the spaCy 101 page if you're not familiar with spaCy.","title":"A first pipeline"},{"location":"#available-pipeline-components","text":"Core Qualifiers Miscellaneous NER Trainable Pipeline Description eds.normalizer Non-destructive input text normalisation eds.sentences Better sentence boundary detection eds.matcher A simple yet powerful entity extractor eds.terminology A simple yet powerful terminology matcher eds.contextual-matcher A conditional entity extractor eds.endlines An unsupervised model to classify each end line Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based medical history detection Pipeline Description eds.dates Date extraction and normalisation eds.measurements Measure extraction and normalisation eds.sections Section detection eds.reason Rule-based hospitalisation reason detection Pipeline Description eds.covid A COVID mentions detector eds.charlson A Charlson score extractor eds.sofa A SOFA score extractor eds.emergency.priority A priority score extractor eds.emergency.ccmu A CCMU score extractor eds.emergency.gemsa A GEMSA score extractor eds.TNM A TNM score extractor eds.cim10 A CIM10 terminology matcher eds.drugs A Drug mentions extractor eds.adicap A ADICAP codes extractor Pipeline Description eds.nested-ner Nested and overlapping named entity recogntion","title":"Available pipeline components"},{"location":"#disclaimer","text":"The performances of an extraction pipeline may depend on the population and documents that are considered.","title":"Disclaimer"},{"location":"#contributing-to-eds-nlp","text":"We welcome contributions ! Fork the project and propose a pull request. Take a look at the dedicated page for detail.","title":"Contributing to EDS-NLP"},{"location":"#citation","text":"If you use EDS-NLP, please cite us as below. @misc { edsnlp , author = {Dura, Basile and Wajsburt, Perceval and Petit-Jean, Thomas and Cohen, Ariel and Jean, Charline and Bey, Romain} , doi = {10.5281/zenodo.6424993} , title = {EDS-NLP: efficient information extraction from French clinical notes} , url = {http://aphp.github.io/edsnlp} }","title":"Citation"},{"location":"changelog/","text":"Changelog Unreleased Added eds.history : Add the option to consider only the closest dates in the sentence (dates inside the boundaries and if there is not, it takes the closest date in the entire sentence). eds.negation : It takes into account following past participates and preceding infinitives. eds.hypothesis : It takes into account following past participates hypothesis verbs. eds.negation & eds.hypothesis : Introduce new patterns and remove unnecessary patterns. eds.dates : Add a pattern for preceding relative dates (ex: l'embolie qui est survenue \u00e0 10 jours ). Improve patterns in the eds.pollution component to account for multiline footers Add UMLS terminology matcher eds.umls Fixed eds.hypothesis : Remove too generic patterns. EDSTokenizer : It now tokenizes \"rechereche d'\" as [\"recherche\", \"d'\"] , instead of [\"recherche\", \"d\", \"'\"] . Fix small typos in the documentation and in the docstring. Harmonize processing utils (distributed custom_pipe) to have the same API for Pandas and Pyspark v0.7.2 (2022-10-26) Added Improve the eds.history component by taking into account the date extracted from eds.dates component. New pop up when you click on the copy icon in the termynal widget (docs). Add NER eds.elston-ellis pipeline to identify Elston Ellis scores Add flags=re.MULTILINE to eds.pollution and change pattern of footer Fixed Remove the warning in the eds.sections when eds.normalizer is in the pipe. Fix filter_spans for strictly nested entities Fill eds.remove-lowercase \"assign\" metadata to run the pipeline during EDSPhraseMatcher preprocessing v0.7.1 (2022-10-13) Added Add new patterns (footer, web entities, biology tables, coding sections) to pipeline normalisation (pollution) Changed Improved TNM detection algorithm Account for more modifiers in ADICAP codes detection Fixed Add nephew, niece and daughter to family qualifier patterns EDSTokenizer ( spacy.blank('eds') ) now recognizes non-breaking whitespaces as spaces and does not split float numbers eds.dates pipeline now allows new lines as space separators in dates v0.7.0 (2022-09-06) Added New nested NER trainable nested_ner pipeline component Support for nested entities and attributes in BratDataConnector Pytorch wrappers and experimental training utils Add attribute section to entities Add new cases for separator pattern when components of the TNM score are separated by a forward slash Add NER eds.adicap pipeline to identify ADICAP codes Add patterns to pollution pipeline and simplifies activating or deactivating specific patterns Changed Simplified the configuration scheme of the pollution pipeline Update of the ContextualMatcher (and all pipelines depending on it), rendering it more flexible to use Rename R component of score TNM as \"resection_completeness\" Fixed Prevent section titles from capturing surrounding tokens, causing overlaps (#113) Enhance existing patterns for section detection and add patterns for previously ignored sections (introduction, evolution, modalites de sortie, vaccination) . Fix explain mode, which was always triggered, in eds.history factory. Fix test in eds.sections . Previously, no check was done Remove SOFA scores spurious span suffixes v0.6.2 (2022-08-02) Added New SimstringMatcher matcher to perform fuzzy term matching, and algorithm parameter in terminology components and eds.matcher component Makefile to install,test the application and see the documentation Changed Add consultation date pattern \"CS\", and False Positive patterns for dates (namely phone numbers and pagination). Update the pipeline score eds.TNM . Now it is possible to return a dictionary where the results are either str or int values Fixed Add new patterns to the negation qualifier Numpy header issues with binary distributed packages Simstring dependency on Windows v0.6.1 (2022-07-11) Added Now possible to provide regex flags when using the RegexMatcher New ContextualMatcher pipe, aiming at replacing the AdvancedRegex pipe. New as_ents parameter for eds.dates , to save detected dates as entities Changed Faster eds.sentences pipeline component with Cython Bump version of Pydantic in requirements.txt to 1.8.2 to handle an incompatibility with the ContextualMatcher Optimise space requirements by using .csv.gz compression for verbs Fixed eds.sentences behaviour with dot-delimited dates (eg 02.07.2022 , which counted as three sentences) v0.6.0 (2022-06-17) Added Complete revamp of the measurements detection pipeline, with better parsing and more exhaustive matching Add new functionality to the method Span._.date.to_datetime() to return a result infered from context for those cases with missing information. Force a batch size of 2000 when distributing a pipeline with Spark New patterns to pipeline eds.dates to identify cases where only the month is mentioned New eds.terminology component for generic terminology matching, using the kb_id_ attribute to store fine-grained entity label New eds.cim10 terminology matching pipeline New eds.drugs terminology pipeline that maps brand names and active ingredients to a unique ATC code v0.5.3 (2022-05-04) Added Support for strings in the example utility TNM detection and normalisation with the eds.TNM pipeline Support for arbitrary callback for Pandas multiprocessing, with the callback argument v0.5.2 (2022-04-29) Added Support for chained attributes in the processing pipelines Colour utility with the category20 colour palette Fixed Correct a REGEX on the date detector (both nov and nov. are now detected, as all other months) v0.5.1 (2022-04-11) Fixed Updated Numpy requirements to be compatible with the EDSPhraseMatcher v0.5.0 (2022-04-08) Added New eds language to better fit French clinical documents and improve speed Testing for markdown codeblocks to make sure the documentation is actually executable Changed Complete revamp of the date detection pipeline, with better parsing and more exhaustive matching Reimplementation of the EDSPhraseMatcher in Cython, leading to a x15 speed increase v0.4.4 Add measures pipeline Cap Jinja2 version to fix mkdocs Adding the possibility to add context in the processing module Improve the speed of char replacement pipelines (accents and quotes) Improve the speed of the regex matcher v0.4.3 Fix regex matching on spans. Add fast_parse in date pipeline. Add relative_date information parsing v0.4.2 Fix issue with dateparser library (see scrapinghub/dateparser#1045) Fix attr issue in the advanced-regex pipelin Add documentation for eds.covid Update the demo with an explanation for the regex v0.4.1 Added support to Koalas DataFrames in the edsnlp.processing pipe. Added eds.covid NER pipeline for detecting COVID19 mentions. v0.4.0 Profound re-write of the normalisation : The custom attribute CUSTOM_NORM is completely abandoned in favour of a more spacyfic alternative The normalizer pipeline modifies the NORM attribute in place Other pipelines can modify the Token._.excluded custom attribute EDS regex and term matchers can ignore excluded tokens during matching, effectively adding a second dimension to normalisation (choice of the attribute and possibility to skip pollution tokens regardless of the attribute) Matching can be performed on custom attributes more easily Qualifiers are regrouped together within the edsnlp.qualifiers submodule, the inheritance from the GenericMatcher is dropped. edsnlp.utils.filter.filter_spans now accepts a label_to_remove parameter. If set, only corresponding spans are removed, along with overlapping spans. Primary use-case: removing pseudo cues for qualifiers. Generalise the naming convention for extensions, which keep the same name as the pipeline that created them (eg Span._.negation for the eds.negation pipeline). The previous convention is kept for now, but calling it issues a warning. The dates pipeline underwent some light formatting to increase robustness and fix a few issues A new consultation_dates pipeline was added, which looks for dates preceded by expressions specific to consultation dates In rule-based processing, the terms.py submodule is replaced by patterns.py to reflect the possible presence of regular expressions Refactoring of the architecture : pipelines are now regrouped by type ( core , ner , misc , qualifiers ) matchers submodule contains RegexMatcher and PhraseMatcher classes, which interact with the normalisation multiprocessing submodule contains spark and local multiprocessing tools connectors contains Brat , OMOP and LabelTool connectors utils contains various utilities Add entry points to make pipeline usable directly, removing the need to import edsnlp.components . Add a eds namespace for components: for instance, negation becomes eds.negation . Using the former pipeline name still works, but issues a deprecation warning. Add 3 score pipelines related to emergency Add a helper function to use a spaCy pipeline as a Spark UDF. Fix alignment issues in RegexMatcher Change the alignment procedure, dropping clumsy numpy dependency in favour of bisect Change the name of eds.antecedents to eds.history . Calling eds.antecedents still works, but issues a deprecation warning and support will be removed in a future version. Add a eds.covid component, that identifies mentions of COVID Change the demo, to include NER components v0.3.2 Major revamp of the normalisation. The normalizer pipeline now adds atomic components ( lowercase , accents , quotes , pollution & endlines ) to the processing pipeline, and compiles the results into a new Doc._.normalized extension. The latter is itself a spaCy Doc object, wherein tokens are normalised and pollution tokens are removed altogether. Components that match on the CUSTOM_NORM attribute process the normalized document, and matches are brought back to the original document using a token-wise mapping. Update the RegexMatcher to use the CUSTOM_NORM attribute Add an EDSPhraseMatcher , wrapping spaCy's PhraseMatcher to enable matching on CUSTOM_NORM . Update the matcher and advanced pipelines to enable matching on the CUSTOM_NORM attribute. Add an OMOP connector, to help go back and forth between OMOP-formatted pandas dataframes and spaCy documents. Add a reason pipeline, that extracts the reason for visit. Add an endlines pipeline, that classifies newline characters between spaces and actual ends of line. Add possibility to annotate within entities for qualifiers ( negation , hypothesis , etc), ie if the cue is within the entity. Disabled by default. v0.3.1 Update dates to remove miscellaneous bugs. Add isort pre-commit hook. Improve performance for negation , hypothesis , antecedents , family and rspeech by using spaCy's filter_spans and our consume_spans methods. Add proposition segmentation to hypothesis and family , enhancing results. v0.3.0 Renamed generic to matcher . This is a non-breaking change for the average user, adding the pipeline is still : nlp . add_pipe ( \"matcher\" , config = dict ( terms = dict ( maladie = \"maladie\" ))) Removed quickumls pipeline. It was untested, unmaintained. Will be added back in a future release. Add score pipeline, and charlson . Add advanced-regex pipeline Corrected bugs in the negation pipeline v0.2.0 Add negation pipeline Add family pipeline Add hypothesis pipeline Add antecedents pipeline Add rspeech pipeline Refactor the library : Remove the rules folder Add a pipelines folder, containing one subdirectory per component Every component subdirectory contains a module defining the component, and a module defining a factory, plus any other utilities (eg terms.py ) v0.1.0 First working version. Available pipelines : section sentences normalization pollution","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#added","text":"eds.history : Add the option to consider only the closest dates in the sentence (dates inside the boundaries and if there is not, it takes the closest date in the entire sentence). eds.negation : It takes into account following past participates and preceding infinitives. eds.hypothesis : It takes into account following past participates hypothesis verbs. eds.negation & eds.hypothesis : Introduce new patterns and remove unnecessary patterns. eds.dates : Add a pattern for preceding relative dates (ex: l'embolie qui est survenue \u00e0 10 jours ). Improve patterns in the eds.pollution component to account for multiline footers Add UMLS terminology matcher eds.umls","title":"Added"},{"location":"changelog/#fixed","text":"eds.hypothesis : Remove too generic patterns. EDSTokenizer : It now tokenizes \"rechereche d'\" as [\"recherche\", \"d'\"] , instead of [\"recherche\", \"d\", \"'\"] . Fix small typos in the documentation and in the docstring. Harmonize processing utils (distributed custom_pipe) to have the same API for Pandas and Pyspark","title":"Fixed"},{"location":"changelog/#v072-2022-10-26","text":"","title":"v0.7.2 (2022-10-26)"},{"location":"changelog/#added_1","text":"Improve the eds.history component by taking into account the date extracted from eds.dates component. New pop up when you click on the copy icon in the termynal widget (docs). Add NER eds.elston-ellis pipeline to identify Elston Ellis scores Add flags=re.MULTILINE to eds.pollution and change pattern of footer","title":"Added"},{"location":"changelog/#fixed_1","text":"Remove the warning in the eds.sections when eds.normalizer is in the pipe. Fix filter_spans for strictly nested entities Fill eds.remove-lowercase \"assign\" metadata to run the pipeline during EDSPhraseMatcher preprocessing","title":"Fixed"},{"location":"changelog/#v071-2022-10-13","text":"","title":"v0.7.1 (2022-10-13)"},{"location":"changelog/#added_2","text":"Add new patterns (footer, web entities, biology tables, coding sections) to pipeline normalisation (pollution)","title":"Added"},{"location":"changelog/#changed","text":"Improved TNM detection algorithm Account for more modifiers in ADICAP codes detection","title":"Changed"},{"location":"changelog/#fixed_2","text":"Add nephew, niece and daughter to family qualifier patterns EDSTokenizer ( spacy.blank('eds') ) now recognizes non-breaking whitespaces as spaces and does not split float numbers eds.dates pipeline now allows new lines as space separators in dates","title":"Fixed"},{"location":"changelog/#v070-2022-09-06","text":"","title":"v0.7.0 (2022-09-06)"},{"location":"changelog/#added_3","text":"New nested NER trainable nested_ner pipeline component Support for nested entities and attributes in BratDataConnector Pytorch wrappers and experimental training utils Add attribute section to entities Add new cases for separator pattern when components of the TNM score are separated by a forward slash Add NER eds.adicap pipeline to identify ADICAP codes Add patterns to pollution pipeline and simplifies activating or deactivating specific patterns","title":"Added"},{"location":"changelog/#changed_1","text":"Simplified the configuration scheme of the pollution pipeline Update of the ContextualMatcher (and all pipelines depending on it), rendering it more flexible to use Rename R component of score TNM as \"resection_completeness\"","title":"Changed"},{"location":"changelog/#fixed_3","text":"Prevent section titles from capturing surrounding tokens, causing overlaps (#113) Enhance existing patterns for section detection and add patterns for previously ignored sections (introduction, evolution, modalites de sortie, vaccination) . Fix explain mode, which was always triggered, in eds.history factory. Fix test in eds.sections . Previously, no check was done Remove SOFA scores spurious span suffixes","title":"Fixed"},{"location":"changelog/#v062-2022-08-02","text":"","title":"v0.6.2 (2022-08-02)"},{"location":"changelog/#added_4","text":"New SimstringMatcher matcher to perform fuzzy term matching, and algorithm parameter in terminology components and eds.matcher component Makefile to install,test the application and see the documentation","title":"Added"},{"location":"changelog/#changed_2","text":"Add consultation date pattern \"CS\", and False Positive patterns for dates (namely phone numbers and pagination). Update the pipeline score eds.TNM . Now it is possible to return a dictionary where the results are either str or int values","title":"Changed"},{"location":"changelog/#fixed_4","text":"Add new patterns to the negation qualifier Numpy header issues with binary distributed packages Simstring dependency on Windows","title":"Fixed"},{"location":"changelog/#v061-2022-07-11","text":"","title":"v0.6.1 (2022-07-11)"},{"location":"changelog/#added_5","text":"Now possible to provide regex flags when using the RegexMatcher New ContextualMatcher pipe, aiming at replacing the AdvancedRegex pipe. New as_ents parameter for eds.dates , to save detected dates as entities","title":"Added"},{"location":"changelog/#changed_3","text":"Faster eds.sentences pipeline component with Cython Bump version of Pydantic in requirements.txt to 1.8.2 to handle an incompatibility with the ContextualMatcher Optimise space requirements by using .csv.gz compression for verbs","title":"Changed"},{"location":"changelog/#fixed_5","text":"eds.sentences behaviour with dot-delimited dates (eg 02.07.2022 , which counted as three sentences)","title":"Fixed"},{"location":"changelog/#v060-2022-06-17","text":"","title":"v0.6.0 (2022-06-17)"},{"location":"changelog/#added_6","text":"Complete revamp of the measurements detection pipeline, with better parsing and more exhaustive matching Add new functionality to the method Span._.date.to_datetime() to return a result infered from context for those cases with missing information. Force a batch size of 2000 when distributing a pipeline with Spark New patterns to pipeline eds.dates to identify cases where only the month is mentioned New eds.terminology component for generic terminology matching, using the kb_id_ attribute to store fine-grained entity label New eds.cim10 terminology matching pipeline New eds.drugs terminology pipeline that maps brand names and active ingredients to a unique ATC code","title":"Added"},{"location":"changelog/#v053-2022-05-04","text":"","title":"v0.5.3 (2022-05-04)"},{"location":"changelog/#added_7","text":"Support for strings in the example utility TNM detection and normalisation with the eds.TNM pipeline Support for arbitrary callback for Pandas multiprocessing, with the callback argument","title":"Added"},{"location":"changelog/#v052-2022-04-29","text":"","title":"v0.5.2 (2022-04-29)"},{"location":"changelog/#added_8","text":"Support for chained attributes in the processing pipelines Colour utility with the category20 colour palette","title":"Added"},{"location":"changelog/#fixed_6","text":"Correct a REGEX on the date detector (both nov and nov. are now detected, as all other months)","title":"Fixed"},{"location":"changelog/#v051-2022-04-11","text":"","title":"v0.5.1 (2022-04-11)"},{"location":"changelog/#fixed_7","text":"Updated Numpy requirements to be compatible with the EDSPhraseMatcher","title":"Fixed"},{"location":"changelog/#v050-2022-04-08","text":"","title":"v0.5.0 (2022-04-08)"},{"location":"changelog/#added_9","text":"New eds language to better fit French clinical documents and improve speed Testing for markdown codeblocks to make sure the documentation is actually executable","title":"Added"},{"location":"changelog/#changed_4","text":"Complete revamp of the date detection pipeline, with better parsing and more exhaustive matching Reimplementation of the EDSPhraseMatcher in Cython, leading to a x15 speed increase","title":"Changed"},{"location":"changelog/#v044","text":"Add measures pipeline Cap Jinja2 version to fix mkdocs Adding the possibility to add context in the processing module Improve the speed of char replacement pipelines (accents and quotes) Improve the speed of the regex matcher","title":"v0.4.4"},{"location":"changelog/#v043","text":"Fix regex matching on spans. Add fast_parse in date pipeline. Add relative_date information parsing","title":"v0.4.3"},{"location":"changelog/#v042","text":"Fix issue with dateparser library (see scrapinghub/dateparser#1045) Fix attr issue in the advanced-regex pipelin Add documentation for eds.covid Update the demo with an explanation for the regex","title":"v0.4.2"},{"location":"changelog/#v041","text":"Added support to Koalas DataFrames in the edsnlp.processing pipe. Added eds.covid NER pipeline for detecting COVID19 mentions.","title":"v0.4.1"},{"location":"changelog/#v040","text":"Profound re-write of the normalisation : The custom attribute CUSTOM_NORM is completely abandoned in favour of a more spacyfic alternative The normalizer pipeline modifies the NORM attribute in place Other pipelines can modify the Token._.excluded custom attribute EDS regex and term matchers can ignore excluded tokens during matching, effectively adding a second dimension to normalisation (choice of the attribute and possibility to skip pollution tokens regardless of the attribute) Matching can be performed on custom attributes more easily Qualifiers are regrouped together within the edsnlp.qualifiers submodule, the inheritance from the GenericMatcher is dropped. edsnlp.utils.filter.filter_spans now accepts a label_to_remove parameter. If set, only corresponding spans are removed, along with overlapping spans. Primary use-case: removing pseudo cues for qualifiers. Generalise the naming convention for extensions, which keep the same name as the pipeline that created them (eg Span._.negation for the eds.negation pipeline). The previous convention is kept for now, but calling it issues a warning. The dates pipeline underwent some light formatting to increase robustness and fix a few issues A new consultation_dates pipeline was added, which looks for dates preceded by expressions specific to consultation dates In rule-based processing, the terms.py submodule is replaced by patterns.py to reflect the possible presence of regular expressions Refactoring of the architecture : pipelines are now regrouped by type ( core , ner , misc , qualifiers ) matchers submodule contains RegexMatcher and PhraseMatcher classes, which interact with the normalisation multiprocessing submodule contains spark and local multiprocessing tools connectors contains Brat , OMOP and LabelTool connectors utils contains various utilities Add entry points to make pipeline usable directly, removing the need to import edsnlp.components . Add a eds namespace for components: for instance, negation becomes eds.negation . Using the former pipeline name still works, but issues a deprecation warning. Add 3 score pipelines related to emergency Add a helper function to use a spaCy pipeline as a Spark UDF. Fix alignment issues in RegexMatcher Change the alignment procedure, dropping clumsy numpy dependency in favour of bisect Change the name of eds.antecedents to eds.history . Calling eds.antecedents still works, but issues a deprecation warning and support will be removed in a future version. Add a eds.covid component, that identifies mentions of COVID Change the demo, to include NER components","title":"v0.4.0"},{"location":"changelog/#v032","text":"Major revamp of the normalisation. The normalizer pipeline now adds atomic components ( lowercase , accents , quotes , pollution & endlines ) to the processing pipeline, and compiles the results into a new Doc._.normalized extension. The latter is itself a spaCy Doc object, wherein tokens are normalised and pollution tokens are removed altogether. Components that match on the CUSTOM_NORM attribute process the normalized document, and matches are brought back to the original document using a token-wise mapping. Update the RegexMatcher to use the CUSTOM_NORM attribute Add an EDSPhraseMatcher , wrapping spaCy's PhraseMatcher to enable matching on CUSTOM_NORM . Update the matcher and advanced pipelines to enable matching on the CUSTOM_NORM attribute. Add an OMOP connector, to help go back and forth between OMOP-formatted pandas dataframes and spaCy documents. Add a reason pipeline, that extracts the reason for visit. Add an endlines pipeline, that classifies newline characters between spaces and actual ends of line. Add possibility to annotate within entities for qualifiers ( negation , hypothesis , etc), ie if the cue is within the entity. Disabled by default.","title":"v0.3.2"},{"location":"changelog/#v031","text":"Update dates to remove miscellaneous bugs. Add isort pre-commit hook. Improve performance for negation , hypothesis , antecedents , family and rspeech by using spaCy's filter_spans and our consume_spans methods. Add proposition segmentation to hypothesis and family , enhancing results.","title":"v0.3.1"},{"location":"changelog/#v030","text":"Renamed generic to matcher . This is a non-breaking change for the average user, adding the pipeline is still : nlp . add_pipe ( \"matcher\" , config = dict ( terms = dict ( maladie = \"maladie\" ))) Removed quickumls pipeline. It was untested, unmaintained. Will be added back in a future release. Add score pipeline, and charlson . Add advanced-regex pipeline Corrected bugs in the negation pipeline","title":"v0.3.0"},{"location":"changelog/#v020","text":"Add negation pipeline Add family pipeline Add hypothesis pipeline Add antecedents pipeline Add rspeech pipeline Refactor the library : Remove the rules folder Add a pipelines folder, containing one subdirectory per component Every component subdirectory contains a module defining the component, and a module defining a factory, plus any other utilities (eg terms.py )","title":"v0.2.0"},{"location":"changelog/#v010","text":"First working version. Available pipelines : section sentences normalization pollution","title":"v0.1.0"},{"location":"contributing/","text":"Contributing to EDS-NLP We welcome contributions ! There are many ways to help. For example, you can: Help us track bugs by filing issues Suggest and help prioritise new functionalities Develop a new pipeline ! Fork the project and propose a new functionality through a pull request Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you. Development installation To be able to run the test suite, run the example notebooks and develop your own pipeline, you should clone the repo and install it locally. # Clone the repository and change directory $ git clone https://github.com/aphp/edsnlp.git ---> 100% $ cd edsnlp # Optional: create a virtual environment $ python -m venv venv $ source venv/bin/activate # Install setup dependencies and build resources $ pip install -r requirements.txt $ pip install -r requirements-setup.txt $ python scripts/conjugate_verbs.py # Install development dependencies $ pip install -r requirements-dev.txt $ pip install -r requirements-docs.txt # Finally, install the package in editable mode $ pip install -e . To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: $ pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: $ pre-commit run --all-files ---> 100% color:green All good ! Proposing a merge request At the very least, your changes should : Be well-documented ; Pass every tests, and preferably implement its own ; Follow the style guide. Testing your code We use the Pytest test suite. The following command will run the test suite. Writing your own tests is encouraged ! python -m pytest Testing Cython code Make sure the package is installed in editable mode . Otherwise Pytest won't be able to find the Cython modules. Should your contribution propose a bug fix, we require the bug be thoroughly tested. Architecture of a pipeline Pipelines should follow the same pattern : edsnlp/pipelines/<pipeline> |-- <pipeline>.py # Defines the component logic |-- patterns.py # Defines matched patterns |-- factory.py # Declares the pipeline to spaCy Style Guide We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short : Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use pre-commit to keep our codebase clean. Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save. Documentation Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for EDS-NLP's documentation. You can checkout the changes you make with: # Install the requirements $ pip install -r requirements-docs.txt ---> 100% color:green Installation successful # Run the documentation $ mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.","title":"Contributing to EDS-NLP"},{"location":"contributing/#contributing-to-eds-nlp","text":"We welcome contributions ! There are many ways to help. For example, you can: Help us track bugs by filing issues Suggest and help prioritise new functionalities Develop a new pipeline ! Fork the project and propose a new functionality through a pull request Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you.","title":"Contributing to EDS-NLP"},{"location":"contributing/#development-installation","text":"To be able to run the test suite, run the example notebooks and develop your own pipeline, you should clone the repo and install it locally. # Clone the repository and change directory $ git clone https://github.com/aphp/edsnlp.git ---> 100% $ cd edsnlp # Optional: create a virtual environment $ python -m venv venv $ source venv/bin/activate # Install setup dependencies and build resources $ pip install -r requirements.txt $ pip install -r requirements-setup.txt $ python scripts/conjugate_verbs.py # Install development dependencies $ pip install -r requirements-dev.txt $ pip install -r requirements-docs.txt # Finally, install the package in editable mode $ pip install -e . To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: $ pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: $ pre-commit run --all-files ---> 100% color:green All good !","title":"Development installation"},{"location":"contributing/#proposing-a-merge-request","text":"At the very least, your changes should : Be well-documented ; Pass every tests, and preferably implement its own ; Follow the style guide.","title":"Proposing a merge request"},{"location":"contributing/#testing-your-code","text":"We use the Pytest test suite. The following command will run the test suite. Writing your own tests is encouraged ! python -m pytest Testing Cython code Make sure the package is installed in editable mode . Otherwise Pytest won't be able to find the Cython modules. Should your contribution propose a bug fix, we require the bug be thoroughly tested.","title":"Testing your code"},{"location":"contributing/#architecture-of-a-pipeline","text":"Pipelines should follow the same pattern : edsnlp/pipelines/<pipeline> |-- <pipeline>.py # Defines the component logic |-- patterns.py # Defines matched patterns |-- factory.py # Declares the pipeline to spaCy","title":"Architecture of a pipeline"},{"location":"contributing/#style-guide","text":"We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short : Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use pre-commit to keep our codebase clean. Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save.","title":"Style Guide"},{"location":"contributing/#documentation","text":"Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for EDS-NLP's documentation. You can checkout the changes you make with: # Install the requirements $ pip install -r requirements-docs.txt ---> 100% color:green Installation successful # Run the documentation $ mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.","title":"Documentation"},{"location":"languages/","text":"Languages In addition to the standard spaCy FrenchLanguage ( fr ), EDS-NLP offers a new language better fit for French clinical documents: EDSLanguage ( eds ). Additionally, the EDSLanguage document creation should be around 5-6 times faster than the fr tokenizer. The main differences lie in the tokenization process. A comparison of the two tokenization methods is demonstrated below: Example FrenchLanguage EDSLanguage ACR 5 [ ACR5 ] [ ACR , 5 ] 26.5/ [ 26.5/ ] [ 26.5 , / ] \\n \\n CONCLUSION [ \\n \\n , CONCLUSION] [ \\n , \\n , CONCLUSION ] l'art\u00e8re [ l' , art\u00e8re ] [ l' , art\u00e8re ] (same) To instantiate one of the two languages, you can call the spacy.blank method. EDSLanguage FrenchLanguage import spacy nlp = spacy . blank ( \"eds\" ) import spacy nlp = spacy . blank ( \"fr\" )","title":"Languages"},{"location":"languages/#languages","text":"In addition to the standard spaCy FrenchLanguage ( fr ), EDS-NLP offers a new language better fit for French clinical documents: EDSLanguage ( eds ). Additionally, the EDSLanguage document creation should be around 5-6 times faster than the fr tokenizer. The main differences lie in the tokenization process. A comparison of the two tokenization methods is demonstrated below: Example FrenchLanguage EDSLanguage ACR 5 [ ACR5 ] [ ACR , 5 ] 26.5/ [ 26.5/ ] [ 26.5 , / ] \\n \\n CONCLUSION [ \\n \\n , CONCLUSION] [ \\n , \\n , CONCLUSION ] l'art\u00e8re [ l' , art\u00e8re ] [ l' , art\u00e8re ] (same) To instantiate one of the two languages, you can call the spacy.blank method. EDSLanguage FrenchLanguage import spacy nlp = spacy . blank ( \"eds\" ) import spacy nlp = spacy . blank ( \"fr\" )","title":"Languages"},{"location":"advanced-tutorials/","text":"Advanced use cases In this section, we review a few advanced use cases: Adding pre-computed word vectors to spaCy Deploying your spaCy pipeline as an API Creating your own component","title":"Advanced use cases"},{"location":"advanced-tutorials/#advanced-use-cases","text":"In this section, we review a few advanced use cases: Adding pre-computed word vectors to spaCy Deploying your spaCy pipeline as an API Creating your own component","title":"Advanced use cases"},{"location":"advanced-tutorials/fastapi/","text":"Deploying as an API In this section, we will see how you can deploy your pipeline as a REST API using the power of FastAPI . The NLP pipeline Let's create a simple NLP model, that can: match synonyms of COVID19 check for negation, speculation and reported speech. You know the drill: pipeline.py import spacy nlp = spacy . blank ( 'fr' ) nlp . add_pipe ( \"eds.sentences\" ) config = dict ( regex = dict ( covid = [ \"covid\" , r \"covid[-\\s]?19\" , r \"sars[-\\s]?cov[-\\s]?2\" , r \"corona[-\\s]?virus\" , ], ), attr = \"LOWER\" , ) nlp . add_pipe ( 'eds.matcher' , config = config ) nlp . add_pipe ( \"eds.negation\" ) nlp . add_pipe ( \"eds.family\" ) nlp . add_pipe ( \"eds.hypothesis\" ) nlp . add_pipe ( \"eds.reported_speech\" ) Creating the FastAPI app FastAPI is a incredibly efficient framework, based on Python type hints from the ground up, with the help of Pydantic (another great library for building modern Python). We won't go into too much detail about FastAPI in this tutorial. For further information on how the framework operates, go to its excellent documentation ! We'll need to create two things: A module containing the models for inputs and outputs. The script that defines the application itself. models.py from typing import List from pydantic import BaseModel class Entity ( BaseModel ): # (1) # OMOP-style attributes start : int end : int label : str lexical_variant : str normalized_variant : str # Qualifiers negated : bool hypothesis : bool family : bool reported_speech : bool class Document ( BaseModel ): # (2) text : str ents : List [ Entity ] The Entity model contains attributes that define a matched entity, as well as variables that contain the output of the qualifier components. The Document model contains the input text, and a list of detected entities Having defined the output models and the pipeline, we can move on to creating the application itself: app.py from typing import List from fastapi import FastAPI from pipeline import nlp from models import Entity , Document app = FastAPI ( title = \"EDS-NLP\" , version = edsnlp . __version__ ) @app . post ( \"/covid\" , response_model = List [ Document ]) # (1) async def process ( notes : List [ str ], # (2) ): documents = [] for doc in nlp . pipe ( notes ): entities = [] for ent in doc . ents : entity = Entity ( start = ent . start_char , end = ent . end_char , label = ent . label_ , lexical_variant = ent . text , normalized_variant = ent . _ . normalized_variant , negated = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , reported_speech = ent . _ . reported_speech , ) entities . append ( entity ) documents . append ( Document ( text = doc . text , ents = entities , ) ) return documents By telling FastAPI what output format is expected, you get automatic data validation. In FastAPI, input and output schemas are defined through Python type hinting. Here, we tell FastAPI to expect a list of strings in the POST request body. As a bonus, you get data validation for free. Running the API Our simple API is ready to launch! We'll just need to install FastAPI along with a ASGI server to run it. This can be done in one go: $ pip install 'fastapi[uvicorn]' ---> 100% color:green Successfully installed fastapi Launching the API is trivial: $ uvicorn app:app --reload Go to localhost:8000/docs to admire the automatically generated documentation! Using the API You can try the API directly from the documentation. Otherwise, you may use the requests package: import requests notes = [ \"Le p\u00e8re du patient n'est pas atteint de la covid.\" , \"Probable coronavirus.\" , ] r = requests . post ( \"http://localhost:8000/covid\" , json = notes , ) r . json () You should get something like: [ { \"text\" : \"Le p\u00e8re du patient n'est pas atteint de la covid.\" , \"ents\" : [ { \"start\" : 43 , \"end\" : 48 , \"label\" : \"covid\" , \"lexical_variant\" : \"covid\" , \"normalized_variant\" : \"covid\" , \"negated\" : true , \"hypothesis\" : false , \"family\" : true , \"reported_speech\" : false } ] }, { \"text\" : \"Probable coronavirus.\" , \"ents\" : [ { \"start\" : 9 , \"end\" : 20 , \"label\" : \"covid\" , \"lexical_variant\" : \"coronavirus\" , \"normalized_variant\" : \"coronavirus\" , \"negated\" : false , \"hypothesis\" : true , \"family\" : false , \"reported_speech\" : false } ] } ]","title":"Deploying as an API"},{"location":"advanced-tutorials/fastapi/#deploying-as-an-api","text":"In this section, we will see how you can deploy your pipeline as a REST API using the power of FastAPI .","title":"Deploying as an API"},{"location":"advanced-tutorials/fastapi/#the-nlp-pipeline","text":"Let's create a simple NLP model, that can: match synonyms of COVID19 check for negation, speculation and reported speech. You know the drill: pipeline.py import spacy nlp = spacy . blank ( 'fr' ) nlp . add_pipe ( \"eds.sentences\" ) config = dict ( regex = dict ( covid = [ \"covid\" , r \"covid[-\\s]?19\" , r \"sars[-\\s]?cov[-\\s]?2\" , r \"corona[-\\s]?virus\" , ], ), attr = \"LOWER\" , ) nlp . add_pipe ( 'eds.matcher' , config = config ) nlp . add_pipe ( \"eds.negation\" ) nlp . add_pipe ( \"eds.family\" ) nlp . add_pipe ( \"eds.hypothesis\" ) nlp . add_pipe ( \"eds.reported_speech\" )","title":"The NLP pipeline"},{"location":"advanced-tutorials/fastapi/#creating-the-fastapi-app","text":"FastAPI is a incredibly efficient framework, based on Python type hints from the ground up, with the help of Pydantic (another great library for building modern Python). We won't go into too much detail about FastAPI in this tutorial. For further information on how the framework operates, go to its excellent documentation ! We'll need to create two things: A module containing the models for inputs and outputs. The script that defines the application itself. models.py from typing import List from pydantic import BaseModel class Entity ( BaseModel ): # (1) # OMOP-style attributes start : int end : int label : str lexical_variant : str normalized_variant : str # Qualifiers negated : bool hypothesis : bool family : bool reported_speech : bool class Document ( BaseModel ): # (2) text : str ents : List [ Entity ] The Entity model contains attributes that define a matched entity, as well as variables that contain the output of the qualifier components. The Document model contains the input text, and a list of detected entities Having defined the output models and the pipeline, we can move on to creating the application itself: app.py from typing import List from fastapi import FastAPI from pipeline import nlp from models import Entity , Document app = FastAPI ( title = \"EDS-NLP\" , version = edsnlp . __version__ ) @app . post ( \"/covid\" , response_model = List [ Document ]) # (1) async def process ( notes : List [ str ], # (2) ): documents = [] for doc in nlp . pipe ( notes ): entities = [] for ent in doc . ents : entity = Entity ( start = ent . start_char , end = ent . end_char , label = ent . label_ , lexical_variant = ent . text , normalized_variant = ent . _ . normalized_variant , negated = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , reported_speech = ent . _ . reported_speech , ) entities . append ( entity ) documents . append ( Document ( text = doc . text , ents = entities , ) ) return documents By telling FastAPI what output format is expected, you get automatic data validation. In FastAPI, input and output schemas are defined through Python type hinting. Here, we tell FastAPI to expect a list of strings in the POST request body. As a bonus, you get data validation for free.","title":"Creating the FastAPI app"},{"location":"advanced-tutorials/fastapi/#running-the-api","text":"Our simple API is ready to launch! We'll just need to install FastAPI along with a ASGI server to run it. This can be done in one go: $ pip install 'fastapi[uvicorn]' ---> 100% color:green Successfully installed fastapi Launching the API is trivial: $ uvicorn app:app --reload Go to localhost:8000/docs to admire the automatically generated documentation!","title":"Running the API"},{"location":"advanced-tutorials/fastapi/#using-the-api","text":"You can try the API directly from the documentation. Otherwise, you may use the requests package: import requests notes = [ \"Le p\u00e8re du patient n'est pas atteint de la covid.\" , \"Probable coronavirus.\" , ] r = requests . post ( \"http://localhost:8000/covid\" , json = notes , ) r . json () You should get something like: [ { \"text\" : \"Le p\u00e8re du patient n'est pas atteint de la covid.\" , \"ents\" : [ { \"start\" : 43 , \"end\" : 48 , \"label\" : \"covid\" , \"lexical_variant\" : \"covid\" , \"normalized_variant\" : \"covid\" , \"negated\" : true , \"hypothesis\" : false , \"family\" : true , \"reported_speech\" : false } ] }, { \"text\" : \"Probable coronavirus.\" , \"ents\" : [ { \"start\" : 9 , \"end\" : 20 , \"label\" : \"covid\" , \"lexical_variant\" : \"coronavirus\" , \"normalized_variant\" : \"coronavirus\" , \"negated\" : false , \"hypothesis\" : true , \"family\" : false , \"reported_speech\" : false } ] } ]","title":"Using the API"},{"location":"advanced-tutorials/word-vectors/","text":"Word embeddings The only ready-to-use components in EDS-NLP are rule-based components. However, that does not prohibit you from exploiting spaCy's machine learning capabilities! You can mix and match machine learning pipelines, trainable or not, with EDS-NLP rule-based components. In this tutorial, we will explore how you can use static word vectors trained with Gensim within spaCy. Training the word embedding, however, is outside the scope of this post. You'll find very well designed resources on the subject in Gensim's documenation . Using Transformer models spaCy v3 introduced support for Transformer models through their helper library spacy-transformers that interfaces with HuggingFace's transformers library. Using transformer models can significantly increase your model's performance. Adding pre-trained word vectors spaCy provides a init vectors CLI utility that takes a Gensim-trained binary and transforms it to a spaCy-readable pipeline. Using it is straightforward : $ spacy init vectors fr /path/to/vectors /path/to/pipeline ---> 100% color:green Conversion successful! See the documentation for implementation details.","title":"Word embeddings"},{"location":"advanced-tutorials/word-vectors/#word-embeddings","text":"The only ready-to-use components in EDS-NLP are rule-based components. However, that does not prohibit you from exploiting spaCy's machine learning capabilities! You can mix and match machine learning pipelines, trainable or not, with EDS-NLP rule-based components. In this tutorial, we will explore how you can use static word vectors trained with Gensim within spaCy. Training the word embedding, however, is outside the scope of this post. You'll find very well designed resources on the subject in Gensim's documenation . Using Transformer models spaCy v3 introduced support for Transformer models through their helper library spacy-transformers that interfaces with HuggingFace's transformers library. Using transformer models can significantly increase your model's performance.","title":"Word embeddings"},{"location":"advanced-tutorials/word-vectors/#adding-pre-trained-word-vectors","text":"spaCy provides a init vectors CLI utility that takes a Gensim-trained binary and transforms it to a spaCy-readable pipeline. Using it is straightforward : $ spacy init vectors fr /path/to/vectors /path/to/pipeline ---> 100% color:green Conversion successful! See the documentation for implementation details.","title":"Adding pre-trained word vectors"},{"location":"pipelines/","text":"Pipelines overview EDS-NLP provides easy-to-use spaCy components. Core Qualifiers Miscellaneous NER Pipeline Description eds.normalizer Non-destructive input text normalisation eds.sentences Better sentence boundary detection eds.matcher A simple yet powerful entity extractor eds.terminology A simple yet powerful terminology matcher eds.contextual-matcher A conditional entity extractor eds.endlines An unsupervised model to classify each end line Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based medical history detection Pipeline Description eds.dates Date extraction and normalisation eds.measurements Measure extraction and normalisation eds.sections Section detection eds.reason Rule-based hospitalisation reason detection Pipeline Description eds.covid A COVID mentions detector eds.charlson A Charlson score extractor eds.sofa A SOFA score extractor eds.emergency.priority A priority score extractor eds.emergency.ccmu A CCMU score extractor eds.emergency.gemsa A GEMSA score extractor eds.TNM A TNM score extractor eds.cim10 A CIM10 terminology matcher eds.drugs A Drug mentions extractor eds.adicap A ADICAP codes extractor eds.umls A UMLS terminology matcher You can add them to your spaCy pipeline by simply calling add_pipe , for instance: # \u2191 Omitted code that defines the nlp object \u2191 nlp . add_pipe ( \"eds.normalizer\" )","title":"Pipelines overview"},{"location":"pipelines/#pipelines-overview","text":"EDS-NLP provides easy-to-use spaCy components. Core Qualifiers Miscellaneous NER Pipeline Description eds.normalizer Non-destructive input text normalisation eds.sentences Better sentence boundary detection eds.matcher A simple yet powerful entity extractor eds.terminology A simple yet powerful terminology matcher eds.contextual-matcher A conditional entity extractor eds.endlines An unsupervised model to classify each end line Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based medical history detection Pipeline Description eds.dates Date extraction and normalisation eds.measurements Measure extraction and normalisation eds.sections Section detection eds.reason Rule-based hospitalisation reason detection Pipeline Description eds.covid A COVID mentions detector eds.charlson A Charlson score extractor eds.sofa A SOFA score extractor eds.emergency.priority A priority score extractor eds.emergency.ccmu A CCMU score extractor eds.emergency.gemsa A GEMSA score extractor eds.TNM A TNM score extractor eds.cim10 A CIM10 terminology matcher eds.drugs A Drug mentions extractor eds.adicap A ADICAP codes extractor eds.umls A UMLS terminology matcher You can add them to your spaCy pipeline by simply calling add_pipe , for instance: # \u2191 Omitted code that defines the nlp object \u2191 nlp . add_pipe ( \"eds.normalizer\" )","title":"Pipelines overview"},{"location":"pipelines/architecture/","text":"Basic Architecture Most pipelines provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library: Implement a normaliser (see normalizer ) Add an entity recognition component (eg the simple but powerful matcher pipeline ) Add zero or more entity qualification components, such as negation , family or hypothesis . These qualifiers typically help detect false-positives. Scope Since the basic usage of EDS-NLP components is to qualify entities, most pipelines can function in two modes: Annotation of the extracted entities (this is the default). To increase throughput, only pre-extracted entities (found in doc.ents ) are processed. Full-text, token-wise annotation. This mode is activated by setting the on_ents_only parameter to False . The possibility to do full-text annotation implies that one could use the pipelines the other way around, eg detecting all negations once and for all in an ETL phase, and reusing the results consequently. However, this is not the intended use of the library, which aims to help researchers downstream as a standalone application. Result persistence Depending on their purpose (entity extraction, qualification, etc), EDS-NLP pipelines write their results to Doc.ents , Doc.spans or in a custom attribute. Extraction pipelines Extraction pipelines (matchers, the date detector or NER pipelines, for instance) keep their results to the Doc.ents attribute directly. Note that spaCy prohibits overlapping entities within the Doc.ents attribute. To circumvent this limitation, we filter spans , and keep all discarded entities within the discarded key of the Doc.spans attribute. Some pipelines write their output to the Doc.spans dictionary. We enforce the following doctrine: Should the pipe extract entities that are directly informative (typically the output of the eds.matcher component), said entities are stashed in the Doc.ents attribute. On the other hand, should the entity be useful to another pipe, but less so in itself (eg the output of the eds.sections or eds.dates component), it will be stashed in a specific key within the Doc.spans attribute. Entity tagging Moreover, most pipelines declare spaCy extensions , on the Doc , Span and/or Token objects. These extensions are especially useful for qualifier pipelines, but can also be used by other pipelines to persist relevant information. For instance, the eds.dates pipeline: Populates Doc . spans [ \"dates\" ] For each detected item, keeps the normalised date in Span . _ . date","title":"Basic Architecture"},{"location":"pipelines/architecture/#basic-architecture","text":"Most pipelines provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library: Implement a normaliser (see normalizer ) Add an entity recognition component (eg the simple but powerful matcher pipeline ) Add zero or more entity qualification components, such as negation , family or hypothesis . These qualifiers typically help detect false-positives.","title":"Basic Architecture"},{"location":"pipelines/architecture/#scope","text":"Since the basic usage of EDS-NLP components is to qualify entities, most pipelines can function in two modes: Annotation of the extracted entities (this is the default). To increase throughput, only pre-extracted entities (found in doc.ents ) are processed. Full-text, token-wise annotation. This mode is activated by setting the on_ents_only parameter to False . The possibility to do full-text annotation implies that one could use the pipelines the other way around, eg detecting all negations once and for all in an ETL phase, and reusing the results consequently. However, this is not the intended use of the library, which aims to help researchers downstream as a standalone application.","title":"Scope"},{"location":"pipelines/architecture/#result-persistence","text":"Depending on their purpose (entity extraction, qualification, etc), EDS-NLP pipelines write their results to Doc.ents , Doc.spans or in a custom attribute.","title":"Result persistence"},{"location":"pipelines/architecture/#extraction-pipelines","text":"Extraction pipelines (matchers, the date detector or NER pipelines, for instance) keep their results to the Doc.ents attribute directly. Note that spaCy prohibits overlapping entities within the Doc.ents attribute. To circumvent this limitation, we filter spans , and keep all discarded entities within the discarded key of the Doc.spans attribute. Some pipelines write their output to the Doc.spans dictionary. We enforce the following doctrine: Should the pipe extract entities that are directly informative (typically the output of the eds.matcher component), said entities are stashed in the Doc.ents attribute. On the other hand, should the entity be useful to another pipe, but less so in itself (eg the output of the eds.sections or eds.dates component), it will be stashed in a specific key within the Doc.spans attribute.","title":"Extraction pipelines"},{"location":"pipelines/architecture/#entity-tagging","text":"Moreover, most pipelines declare spaCy extensions , on the Doc , Span and/or Token objects. These extensions are especially useful for qualifier pipelines, but can also be used by other pipelines to persist relevant information. For instance, the eds.dates pipeline: Populates Doc . spans [ \"dates\" ] For each detected item, keeps the normalised date in Span . _ . date","title":"Entity tagging"},{"location":"pipelines/core/","text":"Core Pipelines This section deals with \"core\" functionalities offered by EDS-NLP: Matching a terminology Normalising a text Detecting sentence boundaries","title":"Core Pipelines"},{"location":"pipelines/core/#core-pipelines","text":"This section deals with \"core\" functionalities offered by EDS-NLP: Matching a terminology Normalising a text Detecting sentence boundaries","title":"Core Pipelines"},{"location":"pipelines/core/contextual-matcher/","text":"Contextual Matcher During feature extraction, it may be necessary to search for additional patterns in their neighborhood, namely: patterns to discard irrelevant entities patterns to enrich these entities and store some information For example, to extract mentions of non-benign cancers, we need to discard all extractions that mention \"benin\" in their immediate neighborhood. Although such a filtering is feasible using a regular expression, it essentially requires modifying each of the regular expressions. The ContextualMatcher allows to perform this extraction in a clear and concise way. The configuration file The whole ContextualMatcher pipeline is basically defined as a list of pattern dictionaries . Let us see step by step how to build such a list using the example stated just above. a. Finding mentions of cancer To do this, we can build either a set of terms or a set of regex . terms will be used to search for exact matches in the text. While less flexible, it is faster than using regex. In our case we could use the following lists (which are of course absolutely not exhaustives): terms = [ \"cancer\" , \"tumeur\" , ] regex = [ \"adeno(carcinom|[\\s-]?k)\" , \"neoplas\" , \"melanom\" , ] Maybe we want to exclude mentions of benign cancers: benign = \"benign|benin\" b. Find mention of a stage and extract its value For this we will forge a RegEx with one capturing group (basically a pattern enclosed in parentheses): stage = \"stade (I{1,3}V?|[1234])\" This will extract stage between 1 and 4 We can add a second regex to try to capture if the cancer is in a metastasis stage or not: metastase = \"(metasta)\" c. The complete configuration We can now put everything together: cancer = dict ( source = \"Cancer solide\" , regex = regex , terms = terms , regex_attr = \"NORM\" , exclude = dict ( regex = benign , window = 3 , ), assign = [ dict ( name = \"stage\" , regex = stage , window = ( - 10 , 10 ), replace_entity = True , reduce_mode = None , ), dict ( name = \"metastase\" , regex = metastase , window = 10 , replace_entity = False , reduce_mode = \"keep_last\" , ), ] ) Here the configuration consists of a single dictionary. We might want to also include lymphoma in the matcher: lymphome = dict ( source = \"Lymphome\" , regex = [ \"lymphom\" , \"lymphangio\" ], regex_attr = \"NORM\" , exclude = dict ( regex = [ \"hodgkin\" ], # (1) window = 3 , ), ) We are excluding \"Lymphome de Hodgkin\" here In this case, the configuration can be concatenated in a list: patterns = [ cancer , lymphome ] Available parameters for more flexibility 3 main parameters can be used to refine how entities will be formed The include_assigned parameter Following the previous example, you might want your extracted entities to include , if found, the cancer stage and the metastasis status. This can be achieved by setting include_assigned=True in the pipe configuration. For instance, from the sentence \"Le patient a un cancer au stade 3\", the extracted entity will be: \"cancer\" if include_assigned=False \"cancer au stade 3\" if include_assigned=True The reduce_mode parameter It may happen that an assignment matches more than once. For instance, in the (nonsensical) sentence \"Le patient a un cancer au stade 3 et au stade 4\", both \"stade 3\" and \"stade 4\" will be matched by the stage assign key. Depending on your use case, you may want to keep all the extractions, or just one. If reduce_mode=None (default), all extractions are kept in a list If reduce_mode=\"keep_first\" , only the extraction closest to the main matched entity will be kept (in this case, it would be \"stade 3\" since it is the closest to \"cancer\") If reduce_mode==\"keep_last\" , only the furthest extraction is kept. The replace_entity parameter This parameter can be se to True only for a single assign key per dictionary . This limitation comes from the purpose of this parameter: If set to True , the corresponding assign key will be returned as the entity, instead of the match itself. For clarity, let's take the same sentence \"Le patient a un cancer au stade 3\" as an example: if replace_entity=True in the stage assign key, then the extracted entity will be \"stade 3\" instead of \"cancer\" if replace_entity=False for every assign key, the returned entity will be, as expected, \"cancer\" Please notice that with replace_entity set to True, if the correponding assign key matches nothing, the entity will be discarded. Usage import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"sentences\" ) nlp . add_pipe ( \"normalizer\" ) nlp . add_pipe ( \"eds.contextual-matcher\" , name = \"Cancer\" , config = dict ( patterns = patterns , ), ) Let us see what we can get from this pipeline with a few examples Simple match Exclusion rule Extracting additional infos txt = \"Le patient a eu un cancer il y a 5 ans\" doc = nlp ( txt ) ent = doc . ents [ 0 ] ent . label_ # Out: Cancer ent . _ . source # Out: Cancer solide ent . text , ent . start , ent . end # Out: ('cancer', 5, 6) Let us check that when a benign mention is present, the extraction is excluded: txt = \"Le patient a eu un cancer relativement b\u00e9nin il y a 5 ans\" doc = nlp ( txt ) doc . ents # Out: () All informations extracted from the provided assign configuration can be found in the assigned attribute under the form of a dictionary: txt = \"Le patient a eu un cancer de stade 3.\" doc = nlp ( txt ) doc . ents [ 0 ] . _ . assigned # Out: {'stage': '3'} Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default patterns Dictionary or list of dictionaries. See below assign_as_span Whether to store eventual extractions defined via the assign key as Spans or as string False attr spaCy attribute to match on (eg NORM , LOWER ) \"TEXT\" ignore_excluded Whether to skip excluded tokens during matching False include_assigned Whether to include (eventuals) assign matches to the final entity False regex_flags RegExp flags to use when matching, filtering and assigning (See here ) 0 (use default flag) However, most of the configuration is provided in the patterns key, as a pattern dictionary or a list of pattern dictionaries The pattern dictionary Description A patterr is a nested dictionary with the following keys: source regex regex_attr terms exclude assign A label describing the pattern A single Regex or a list of Regexes An attributes to overwrite the given attr when matching with Regexes. A single term or a list of terms (for exact matches) A dictionary (or list of dictionaries) to define exclusion rules. Exclusion rules are given as Regexes, and if a match is found in the surrounding context of an extraction, the extraction is removed. Each dictionary should have the following keys: window regex Size of the context to use (in number of words). You can provide the window as: A positive integer, in this case the used context will be taken after the extraction A negative integer, in this case the used context will be taken before the extraction A tuple of integers (start, end) , in this case the used context will be the snippet from start tokens before the extraction to end tokens after the extraction A single Regex or a list of Regexes. A dictionary to refine the extraction. Similarily to the exclude key, you can provide a dictionary to use on the context before and after the extraction. name window regex replace_entity reduce_mode A name (string) Size of the context to use (in number of words). You can provide the window as: A positive integer, in this case the used context will be taken after the extraction A negative integer, in this case the used context will be taken before the extraction A tuple of integers (start, end) , in this case the used context will be the snippet from start tokens before the extraction to end tokens after the extraction A dictionary where keys are labels and values are Regexes with a single capturing group If set to True , the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph Set how multiple assign matches are handled. See the documentation of the reduce_mode parameter A full pattern dictionary example dict ( source = \"AVC\" , regex = [ \"accidents? vasculaires? cerebr\" , ], terms = \"avc\" , regex_attr = \"NORM\" , exclude = [ dict ( regex = [ \"service\" ], window = 3 , ), dict ( regex = [ \" a \" ], window =- 2 , ), ], assign = [ dict ( name = \"neo\" , regex = r \"(neonatal)\" , expand_entity = True , window = 3 , ), dict ( name = \"trans\" , regex = \"(transitoire)\" , expand_entity = True , window = 3 , ), dict ( name = \"hemo\" , regex = r \"(hemorragique)\" , expand_entity = True , window = 3 , ), dict ( name = \"risk\" , regex = r \"(risque)\" , expand_entity = False , window =- 3 , ), ] ) Authors and citation The eds.matcher pipeline was developed by AP-HP's Data Science team.","title":"Contextual Matcher"},{"location":"pipelines/core/contextual-matcher/#contextual-matcher","text":"During feature extraction, it may be necessary to search for additional patterns in their neighborhood, namely: patterns to discard irrelevant entities patterns to enrich these entities and store some information For example, to extract mentions of non-benign cancers, we need to discard all extractions that mention \"benin\" in their immediate neighborhood. Although such a filtering is feasible using a regular expression, it essentially requires modifying each of the regular expressions. The ContextualMatcher allows to perform this extraction in a clear and concise way.","title":"Contextual Matcher"},{"location":"pipelines/core/contextual-matcher/#the-configuration-file","text":"The whole ContextualMatcher pipeline is basically defined as a list of pattern dictionaries . Let us see step by step how to build such a list using the example stated just above.","title":"The configuration file"},{"location":"pipelines/core/contextual-matcher/#a-finding-mentions-of-cancer","text":"To do this, we can build either a set of terms or a set of regex . terms will be used to search for exact matches in the text. While less flexible, it is faster than using regex. In our case we could use the following lists (which are of course absolutely not exhaustives): terms = [ \"cancer\" , \"tumeur\" , ] regex = [ \"adeno(carcinom|[\\s-]?k)\" , \"neoplas\" , \"melanom\" , ] Maybe we want to exclude mentions of benign cancers: benign = \"benign|benin\"","title":"a. Finding mentions of cancer"},{"location":"pipelines/core/contextual-matcher/#b-find-mention-of-a-stage-and-extract-its-value","text":"For this we will forge a RegEx with one capturing group (basically a pattern enclosed in parentheses): stage = \"stade (I{1,3}V?|[1234])\" This will extract stage between 1 and 4 We can add a second regex to try to capture if the cancer is in a metastasis stage or not: metastase = \"(metasta)\"","title":"b. Find mention of a stage and extract its value"},{"location":"pipelines/core/contextual-matcher/#c-the-complete-configuration","text":"We can now put everything together: cancer = dict ( source = \"Cancer solide\" , regex = regex , terms = terms , regex_attr = \"NORM\" , exclude = dict ( regex = benign , window = 3 , ), assign = [ dict ( name = \"stage\" , regex = stage , window = ( - 10 , 10 ), replace_entity = True , reduce_mode = None , ), dict ( name = \"metastase\" , regex = metastase , window = 10 , replace_entity = False , reduce_mode = \"keep_last\" , ), ] ) Here the configuration consists of a single dictionary. We might want to also include lymphoma in the matcher: lymphome = dict ( source = \"Lymphome\" , regex = [ \"lymphom\" , \"lymphangio\" ], regex_attr = \"NORM\" , exclude = dict ( regex = [ \"hodgkin\" ], # (1) window = 3 , ), ) We are excluding \"Lymphome de Hodgkin\" here In this case, the configuration can be concatenated in a list: patterns = [ cancer , lymphome ]","title":"c. The complete configuration"},{"location":"pipelines/core/contextual-matcher/#available-parameters-for-more-flexibility","text":"3 main parameters can be used to refine how entities will be formed","title":"Available parameters for more flexibility"},{"location":"pipelines/core/contextual-matcher/#the-include_assigned-parameter","text":"Following the previous example, you might want your extracted entities to include , if found, the cancer stage and the metastasis status. This can be achieved by setting include_assigned=True in the pipe configuration. For instance, from the sentence \"Le patient a un cancer au stade 3\", the extracted entity will be: \"cancer\" if include_assigned=False \"cancer au stade 3\" if include_assigned=True","title":"The include_assigned parameter"},{"location":"pipelines/core/contextual-matcher/#the-reduce_mode-parameter","text":"It may happen that an assignment matches more than once. For instance, in the (nonsensical) sentence \"Le patient a un cancer au stade 3 et au stade 4\", both \"stade 3\" and \"stade 4\" will be matched by the stage assign key. Depending on your use case, you may want to keep all the extractions, or just one. If reduce_mode=None (default), all extractions are kept in a list If reduce_mode=\"keep_first\" , only the extraction closest to the main matched entity will be kept (in this case, it would be \"stade 3\" since it is the closest to \"cancer\") If reduce_mode==\"keep_last\" , only the furthest extraction is kept.","title":"The reduce_mode parameter"},{"location":"pipelines/core/contextual-matcher/#the-replace_entity-parameter","text":"This parameter can be se to True only for a single assign key per dictionary . This limitation comes from the purpose of this parameter: If set to True , the corresponding assign key will be returned as the entity, instead of the match itself. For clarity, let's take the same sentence \"Le patient a un cancer au stade 3\" as an example: if replace_entity=True in the stage assign key, then the extracted entity will be \"stade 3\" instead of \"cancer\" if replace_entity=False for every assign key, the returned entity will be, as expected, \"cancer\" Please notice that with replace_entity set to True, if the correponding assign key matches nothing, the entity will be discarded.","title":"The replace_entity parameter"},{"location":"pipelines/core/contextual-matcher/#usage","text":"import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"sentences\" ) nlp . add_pipe ( \"normalizer\" ) nlp . add_pipe ( \"eds.contextual-matcher\" , name = \"Cancer\" , config = dict ( patterns = patterns , ), ) Let us see what we can get from this pipeline with a few examples Simple match Exclusion rule Extracting additional infos txt = \"Le patient a eu un cancer il y a 5 ans\" doc = nlp ( txt ) ent = doc . ents [ 0 ] ent . label_ # Out: Cancer ent . _ . source # Out: Cancer solide ent . text , ent . start , ent . end # Out: ('cancer', 5, 6) Let us check that when a benign mention is present, the extraction is excluded: txt = \"Le patient a eu un cancer relativement b\u00e9nin il y a 5 ans\" doc = nlp ( txt ) doc . ents # Out: () All informations extracted from the provided assign configuration can be found in the assigned attribute under the form of a dictionary: txt = \"Le patient a eu un cancer de stade 3.\" doc = nlp ( txt ) doc . ents [ 0 ] . _ . assigned # Out: {'stage': '3'}","title":"Usage"},{"location":"pipelines/core/contextual-matcher/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default patterns Dictionary or list of dictionaries. See below assign_as_span Whether to store eventual extractions defined via the assign key as Spans or as string False attr spaCy attribute to match on (eg NORM , LOWER ) \"TEXT\" ignore_excluded Whether to skip excluded tokens during matching False include_assigned Whether to include (eventuals) assign matches to the final entity False regex_flags RegExp flags to use when matching, filtering and assigning (See here ) 0 (use default flag) However, most of the configuration is provided in the patterns key, as a pattern dictionary or a list of pattern dictionaries","title":"Configuration"},{"location":"pipelines/core/contextual-matcher/#the-pattern-dictionary","text":"","title":"The pattern dictionary"},{"location":"pipelines/core/contextual-matcher/#description","text":"A patterr is a nested dictionary with the following keys: source regex regex_attr terms exclude assign A label describing the pattern A single Regex or a list of Regexes An attributes to overwrite the given attr when matching with Regexes. A single term or a list of terms (for exact matches) A dictionary (or list of dictionaries) to define exclusion rules. Exclusion rules are given as Regexes, and if a match is found in the surrounding context of an extraction, the extraction is removed. Each dictionary should have the following keys: window regex Size of the context to use (in number of words). You can provide the window as: A positive integer, in this case the used context will be taken after the extraction A negative integer, in this case the used context will be taken before the extraction A tuple of integers (start, end) , in this case the used context will be the snippet from start tokens before the extraction to end tokens after the extraction A single Regex or a list of Regexes. A dictionary to refine the extraction. Similarily to the exclude key, you can provide a dictionary to use on the context before and after the extraction. name window regex replace_entity reduce_mode A name (string) Size of the context to use (in number of words). You can provide the window as: A positive integer, in this case the used context will be taken after the extraction A negative integer, in this case the used context will be taken before the extraction A tuple of integers (start, end) , in this case the used context will be the snippet from start tokens before the extraction to end tokens after the extraction A dictionary where keys are labels and values are Regexes with a single capturing group If set to True , the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph Set how multiple assign matches are handled. See the documentation of the reduce_mode parameter","title":"Description"},{"location":"pipelines/core/contextual-matcher/#a-full-pattern-dictionary-example","text":"dict ( source = \"AVC\" , regex = [ \"accidents? vasculaires? cerebr\" , ], terms = \"avc\" , regex_attr = \"NORM\" , exclude = [ dict ( regex = [ \"service\" ], window = 3 , ), dict ( regex = [ \" a \" ], window =- 2 , ), ], assign = [ dict ( name = \"neo\" , regex = r \"(neonatal)\" , expand_entity = True , window = 3 , ), dict ( name = \"trans\" , regex = \"(transitoire)\" , expand_entity = True , window = 3 , ), dict ( name = \"hemo\" , regex = r \"(hemorragique)\" , expand_entity = True , window = 3 , ), dict ( name = \"risk\" , regex = r \"(risque)\" , expand_entity = False , window =- 3 , ), ] )","title":"A full pattern dictionary example"},{"location":"pipelines/core/contextual-matcher/#authors-and-citation","text":"The eds.matcher pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/core/endlines/","text":"Endlines The eds.endlines pipeline classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document. Behind the scenes, it uses a endlinesmodel instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al 1 . Usage The following example shows a simple usage. Training import spacy from edsnlp.pipelines.core.endlines.endlinesmodel import EndLinesModel nlp = spacy . blank ( \"fr\" ) texts = [ \"\"\"Le patient est arriv\u00e9 hier soir. Il est accompagn\u00e9 par son fils ANTECEDENTS Il a fait une TS en 2010; Fumeur, il est arret\u00e9 il a 5 mois Chirurgie de coeur en 2011 CONCLUSION Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. DIAGNOSTIC : Antecedents Familiaux: - 1. P\u00e8re avec diabete \"\"\" , \"\"\"J'aime le \\nfromage...\\n\"\"\" , ] docs = list ( nlp . pipe ( texts )) # Train and predict an EndLinesModel endlines = EndLinesModel ( nlp = nlp ) df = endlines . fit_and_predict ( docs ) df . head () PATH = \"/tmp/path_to_save\" endlines . save ( PATH ) Inference import spacy from spacy.tokens import Span from spacy import displacy nlp = spacy . blank ( \"fr\" ) PATH = \"/tmp/path_to_save\" nlp . add_pipe ( \"eds.endlines\" , config = dict ( model_path = PATH )) docs = list ( nlp . pipe ( texts )) doc_exemple = docs [ 1 ] doc_exemple . ents = tuple ( Span ( doc_exemple , token . i , token . i + 1 , \"excluded\" ) for token in doc_exemple if token . tag_ == \"EXCLUDED\" ) displacy . render ( doc_exemple , style = \"ent\" , options = { \"colors\" : { \"space\" : \"red\" }}) Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default model_path Path to the pre-trained pipeline Required Declared extensions The eds.endlines pipeline declares one spaCy extensions , on both Span and Token objects. The end_line attribute is a boolean, set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. The pipeline also sets the excluded custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation ) for more detail. Authors and citation The eds.endlines pipeline was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al 1 . Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters) , 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7 . \u21a9 \u21a9","title":"Endlines"},{"location":"pipelines/core/endlines/#endlines","text":"The eds.endlines pipeline classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document. Behind the scenes, it uses a endlinesmodel instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al 1 .","title":"Endlines"},{"location":"pipelines/core/endlines/#usage","text":"The following example shows a simple usage.","title":"Usage"},{"location":"pipelines/core/endlines/#training","text":"import spacy from edsnlp.pipelines.core.endlines.endlinesmodel import EndLinesModel nlp = spacy . blank ( \"fr\" ) texts = [ \"\"\"Le patient est arriv\u00e9 hier soir. Il est accompagn\u00e9 par son fils ANTECEDENTS Il a fait une TS en 2010; Fumeur, il est arret\u00e9 il a 5 mois Chirurgie de coeur en 2011 CONCLUSION Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. DIAGNOSTIC : Antecedents Familiaux: - 1. P\u00e8re avec diabete \"\"\" , \"\"\"J'aime le \\nfromage...\\n\"\"\" , ] docs = list ( nlp . pipe ( texts )) # Train and predict an EndLinesModel endlines = EndLinesModel ( nlp = nlp ) df = endlines . fit_and_predict ( docs ) df . head () PATH = \"/tmp/path_to_save\" endlines . save ( PATH )","title":"Training"},{"location":"pipelines/core/endlines/#inference","text":"import spacy from spacy.tokens import Span from spacy import displacy nlp = spacy . blank ( \"fr\" ) PATH = \"/tmp/path_to_save\" nlp . add_pipe ( \"eds.endlines\" , config = dict ( model_path = PATH )) docs = list ( nlp . pipe ( texts )) doc_exemple = docs [ 1 ] doc_exemple . ents = tuple ( Span ( doc_exemple , token . i , token . i + 1 , \"excluded\" ) for token in doc_exemple if token . tag_ == \"EXCLUDED\" ) displacy . render ( doc_exemple , style = \"ent\" , options = { \"colors\" : { \"space\" : \"red\" }})","title":"Inference"},{"location":"pipelines/core/endlines/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default model_path Path to the pre-trained pipeline Required","title":"Configuration"},{"location":"pipelines/core/endlines/#declared-extensions","text":"The eds.endlines pipeline declares one spaCy extensions , on both Span and Token objects. The end_line attribute is a boolean, set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. The pipeline also sets the excluded custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation ) for more detail.","title":"Declared extensions"},{"location":"pipelines/core/endlines/#authors-and-citation","text":"The eds.endlines pipeline was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al 1 . Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters) , 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7 . \u21a9 \u21a9","title":"Authors and citation"},{"location":"pipelines/core/matcher/","text":"Matcher EDS-NLP simplifies the matching process by exposing a eds.matcher pipeline that can match on terms or regular expressions. Usage Let us redefine the pipeline : import spacy nlp = spacy . blank ( \"fr\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], # (1) patient = \"patient\" , # (2) ) regex = dict ( covid = r \"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\" , # (3) ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , regex = regex , attr = \"LOWER\" , term_matcher = \"exact\" , term_matcher_config = {}, ), ) Every key in the terms dictionary is mapped to a concept. The eds.matcher pipeline expects a list of expressions, or a single expression. We can also define regular expression patterns. This snippet is complete, and should run as is. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default terms Terms patterns. Expects a dictionary. None (use regex only) regex RegExp patterns. Expects a dictionary. None (use terms only) term_matcher Which algorithm should we use : exact or simstring \"exact\" term_matcher_config Config of the algorithm ( SimstringMatcher 's for simstring ) {} attr spaCy attribute to match on (eg NORM , LOWER ) \"TEXT\" ignore_excluded Whether to skip excluded tokens during matching False Patterns, be they terms or regex , are defined as dictionaries where keys become the label of the extracted entities. Dictionary values are a either a single expression or a list of expressions that match the concept (see example ). Authors and citation The eds.matcher pipeline was developed by AP-HP's Data Science team.","title":"Matcher"},{"location":"pipelines/core/matcher/#matcher","text":"EDS-NLP simplifies the matching process by exposing a eds.matcher pipeline that can match on terms or regular expressions.","title":"Matcher"},{"location":"pipelines/core/matcher/#usage","text":"Let us redefine the pipeline : import spacy nlp = spacy . blank ( \"fr\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], # (1) patient = \"patient\" , # (2) ) regex = dict ( covid = r \"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\" , # (3) ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , regex = regex , attr = \"LOWER\" , term_matcher = \"exact\" , term_matcher_config = {}, ), ) Every key in the terms dictionary is mapped to a concept. The eds.matcher pipeline expects a list of expressions, or a single expression. We can also define regular expression patterns. This snippet is complete, and should run as is.","title":"Usage"},{"location":"pipelines/core/matcher/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default terms Terms patterns. Expects a dictionary. None (use regex only) regex RegExp patterns. Expects a dictionary. None (use terms only) term_matcher Which algorithm should we use : exact or simstring \"exact\" term_matcher_config Config of the algorithm ( SimstringMatcher 's for simstring ) {} attr spaCy attribute to match on (eg NORM , LOWER ) \"TEXT\" ignore_excluded Whether to skip excluded tokens during matching False Patterns, be they terms or regex , are defined as dictionaries where keys become the label of the extracted entities. Dictionary values are a either a single expression or a list of expressions that match the concept (see example ).","title":"Configuration"},{"location":"pipelines/core/matcher/#authors-and-citation","text":"The eds.matcher pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/core/normalisation/","text":"Normalisation The normalisation scheme used by EDS-NLP adheres to the non-destructive doctrine. In other words, nlp ( text ) . text == text is always true. To achieve this, the input text is never modified. Instead, our normalisation strategy focuses on two axes: Only the NORM attribute is modified by the normalizer pipeline ; Pipelines (eg the pollution pipeline) can mark tokens as excluded by setting the extension Token._.excluded to True . It enables downstream matchers to skip excluded tokens. The normaliser can act on the input text in four dimensions : Move the text to lowercase . Remove accents . We use a deterministic approach to avoid modifying the character-length of the text, which helps for RegEx matching. Normalize apostrophes and quotation marks , which are often coded using special characters. Remove pollutions . Note We recommend you also add an end-of-line classifier to remove excess new line characters (introduced by the PDF layout). We provide a endlines pipeline, which requires training an unsupervised model. Refer to the dedicated page for more information . Usage The normalisation is handled by the single eds.normalizer pipeline. The following code snippet is complete, and should run as is. import spacy from edsnlp.matchers.utils import get_text nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) # Notice the special character used for the apostrophe and the quotes text = \"Le patient est admis \u00e0 l'h\u00f4pital le 23 ao\u00fbt 2021 pour une douleur \u02baaffreuse\u201d \u00e0 l`estomac.\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: le patient est admis a l'hopital le 23 aout 2021 pour une douleur \"affreuse\" a l'estomac. Utilities To simplify the use of the normalisation output, we provide the get_text utility function. It computes the textual representation for a Span or Doc object. Moreover, every span exposes a normalized_variant extension getter, which computes the normalised representation of an entity on the fly. Configuration Parameter Explanation Default accents Whether to strip accents True lowercase Whether to remove casing True quotes Whether to normalise quotes True pollution Whether to tag pollutions as excluded tokens True Pipelines Let's review each subcomponent. Lowercase The eds.lowercase pipeline transforms every token to lowercase. It is not configurable. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = True , accents = False , quotes = False , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: pneumopathie \u00e0 nbnbwbwbnbwbnbnbnbwbw 'coronavirus' Accents The eds.accents pipeline removes accents. To avoid edge cases, the component uses a specified list of accentuated characters and their unaccented representation, making it more predictable than using a library such as unidecode . Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = True , quotes = False , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus' Apostrophes and quotation marks Apostrophes and quotation marks can be encoded using unpredictable special characters. The eds.quotes component transforms every such special character to ' and \" , respectively. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = False , quotes = True , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW 'coronavirus' Pollution The pollution pipeline uses a set of regular expressions to detect pollutions (irrelevant non-medical text that hinders text processing). Corresponding tokens are marked as excluded (by setting Token._.excluded to True ), enabling the use of the phrase matcher. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = True , quotes = False , pollution = True , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus' get_text ( doc , attr = \"TEXT\" , ignore_excluded = True ) # Out: Pneumopathie \u00e0 `coronavirus' This example above shows that the normalisation scheme works on two axes: non-destructive text modification and exclusion of tokens. The two are independent: a matcher can use the NORM attribute but keep excluded tokens, and conversely, match on TEXT while ignoring excluded tokens. Types of pollution Pollution can come in various forms in clinical texts. We provide a small set of possible pollutions patterns that can be enabled or disabled as needed. For instance, if we consider biology tables as pollution, we only need to instantiate the normalizer pipe as follows: nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = dict ( pollution = dict ( biology = True , ), ), ) Type Description Example Included by default information Footnote present in a lot of notes, providing information to the patient about the use of its data \"L'AP-HP collecte vos donn\u00e9es administratives \u00e0 des fins ...\" True bars Barcodes wrongly parsed as text \"...NBNbWbWbNbWbNBNbNbWbW...\" True biology Parsed biology results table. It often contains disease names that often leads to false positives with NER pipelines. \"...\u00a6UI/L \u00a620 \u00a6 \u00a6 \u00a620-70 Polyarthrite rhumato\u00efde Facteur rhumatoide \u00a6UI/mL \u00a6 \u00a6<10 \u00a6 \u00a6 \u00a6 \u00a60-14...\" False doctors List of doctor names and specialities, often found in left-side note margins. Also source of potential false positives . \"... Dr ABC - Diab\u00e8te/Endocrino ...\" True web Webpages URL and email adresses. Also source of potential false positives . \"... www.vascularites.fr ...\" True coding Subsection containing ICD-10 codes along with their description. Also source of potential false positives . \"... (2) E112 + Oeil (2) E113 + Neuro (2) E114 D\u00e9mence (2) F03 MA (2) F001+G301 DCL G22+G301 Vasc (2) ...\" False footer Footer of new page \"2/2Pat : NOM Prenom le 2020/01/01 IPP 12345678 Intitul\u00e9 RCP : Urologie HMN le \" True Custom pollution If you want to exclude specific patterns, you can provide them as a RegEx (or a list of Regexes). For instance, to consider text between \"AAA\" and \"ZZZ\" as pollution you might use: nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = dict ( pollution = dict ( custom_pollution = r \"AAA.*ZZZ\" , ), ), ) Authors and citation The eds.normalizer pipeline was developed by AP-HP's Data Science team.","title":"Normalisation"},{"location":"pipelines/core/normalisation/#normalisation","text":"The normalisation scheme used by EDS-NLP adheres to the non-destructive doctrine. In other words, nlp ( text ) . text == text is always true. To achieve this, the input text is never modified. Instead, our normalisation strategy focuses on two axes: Only the NORM attribute is modified by the normalizer pipeline ; Pipelines (eg the pollution pipeline) can mark tokens as excluded by setting the extension Token._.excluded to True . It enables downstream matchers to skip excluded tokens. The normaliser can act on the input text in four dimensions : Move the text to lowercase . Remove accents . We use a deterministic approach to avoid modifying the character-length of the text, which helps for RegEx matching. Normalize apostrophes and quotation marks , which are often coded using special characters. Remove pollutions . Note We recommend you also add an end-of-line classifier to remove excess new line characters (introduced by the PDF layout). We provide a endlines pipeline, which requires training an unsupervised model. Refer to the dedicated page for more information .","title":"Normalisation"},{"location":"pipelines/core/normalisation/#usage","text":"The normalisation is handled by the single eds.normalizer pipeline. The following code snippet is complete, and should run as is. import spacy from edsnlp.matchers.utils import get_text nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) # Notice the special character used for the apostrophe and the quotes text = \"Le patient est admis \u00e0 l'h\u00f4pital le 23 ao\u00fbt 2021 pour une douleur \u02baaffreuse\u201d \u00e0 l`estomac.\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: le patient est admis a l'hopital le 23 aout 2021 pour une douleur \"affreuse\" a l'estomac.","title":"Usage"},{"location":"pipelines/core/normalisation/#utilities","text":"To simplify the use of the normalisation output, we provide the get_text utility function. It computes the textual representation for a Span or Doc object. Moreover, every span exposes a normalized_variant extension getter, which computes the normalised representation of an entity on the fly.","title":"Utilities"},{"location":"pipelines/core/normalisation/#configuration","text":"Parameter Explanation Default accents Whether to strip accents True lowercase Whether to remove casing True quotes Whether to normalise quotes True pollution Whether to tag pollutions as excluded tokens True","title":"Configuration"},{"location":"pipelines/core/normalisation/#pipelines","text":"Let's review each subcomponent.","title":"Pipelines"},{"location":"pipelines/core/normalisation/#lowercase","text":"The eds.lowercase pipeline transforms every token to lowercase. It is not configurable. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = True , accents = False , quotes = False , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: pneumopathie \u00e0 nbnbwbwbnbwbnbnbnbwbw 'coronavirus'","title":"Lowercase"},{"location":"pipelines/core/normalisation/#accents","text":"The eds.accents pipeline removes accents. To avoid edge cases, the component uses a specified list of accentuated characters and their unaccented representation, making it more predictable than using a library such as unidecode . Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = True , quotes = False , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus'","title":"Accents"},{"location":"pipelines/core/normalisation/#apostrophes-and-quotation-marks","text":"Apostrophes and quotation marks can be encoded using unpredictable special characters. The eds.quotes component transforms every such special character to ' and \" , respectively. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = False , quotes = True , pollution = False , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW 'coronavirus'","title":"Apostrophes and quotation marks"},{"location":"pipelines/core/normalisation/#pollution","text":"The pollution pipeline uses a set of regular expressions to detect pollutions (irrelevant non-medical text that hinders text processing). Corresponding tokens are marked as excluded (by setting Token._.excluded to True ), enabling the use of the phrase matcher. Consider the following example : import spacy from edsnlp.matchers.utils import get_text config = dict ( lowercase = False , accents = True , quotes = False , pollution = True , ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = config ) text = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\" doc = nlp ( text ) get_text ( doc , attr = \"NORM\" , ignore_excluded = False ) # Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus' get_text ( doc , attr = \"TEXT\" , ignore_excluded = True ) # Out: Pneumopathie \u00e0 `coronavirus' This example above shows that the normalisation scheme works on two axes: non-destructive text modification and exclusion of tokens. The two are independent: a matcher can use the NORM attribute but keep excluded tokens, and conversely, match on TEXT while ignoring excluded tokens.","title":"Pollution"},{"location":"pipelines/core/normalisation/#types-of-pollution","text":"Pollution can come in various forms in clinical texts. We provide a small set of possible pollutions patterns that can be enabled or disabled as needed. For instance, if we consider biology tables as pollution, we only need to instantiate the normalizer pipe as follows: nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = dict ( pollution = dict ( biology = True , ), ), ) Type Description Example Included by default information Footnote present in a lot of notes, providing information to the patient about the use of its data \"L'AP-HP collecte vos donn\u00e9es administratives \u00e0 des fins ...\" True bars Barcodes wrongly parsed as text \"...NBNbWbWbNbWbNBNbNbWbW...\" True biology Parsed biology results table. It often contains disease names that often leads to false positives with NER pipelines. \"...\u00a6UI/L \u00a620 \u00a6 \u00a6 \u00a620-70 Polyarthrite rhumato\u00efde Facteur rhumatoide \u00a6UI/mL \u00a6 \u00a6<10 \u00a6 \u00a6 \u00a6 \u00a60-14...\" False doctors List of doctor names and specialities, often found in left-side note margins. Also source of potential false positives . \"... Dr ABC - Diab\u00e8te/Endocrino ...\" True web Webpages URL and email adresses. Also source of potential false positives . \"... www.vascularites.fr ...\" True coding Subsection containing ICD-10 codes along with their description. Also source of potential false positives . \"... (2) E112 + Oeil (2) E113 + Neuro (2) E114 D\u00e9mence (2) F03 MA (2) F001+G301 DCL G22+G301 Vasc (2) ...\" False footer Footer of new page \"2/2Pat : NOM Prenom le 2020/01/01 IPP 12345678 Intitul\u00e9 RCP : Urologie HMN le \" True","title":"Types of pollution"},{"location":"pipelines/core/normalisation/#custom-pollution","text":"If you want to exclude specific patterns, you can provide them as a RegEx (or a list of Regexes). For instance, to consider text between \"AAA\" and \"ZZZ\" as pollution you might use: nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" , config = dict ( pollution = dict ( custom_pollution = r \"AAA.*ZZZ\" , ), ), )","title":"Custom pollution"},{"location":"pipelines/core/normalisation/#authors-and-citation","text":"The eds.normalizer pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/core/sentences/","text":"Sentences The eds.sentences pipeline provides an alternative to spaCy's default sentencizer , aiming to overcome some of its limitations. Indeed, the sentencizer merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our sentences component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances. Moreover, the eds.sentences pipeline can use the output of the eds.normalizer pipeline, and more specifically the end-of-line classification. This is activated by default. Usage EDS-NLP spaCy sentencizer import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac \\n \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\" ) doc = nlp ( text ) for sentence in doc . sents : print ( \"<s>\" , sentence , \"</s>\" ) # Out: <s> Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac # Out: <\\s> # Out: <s> Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. <\\s> import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"sentencizer\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac \\n \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\" ) doc = nlp ( text ) for sentence in doc . sents : print ( \"<s>\" , sentence , \"</s>\" ) # Out: <s> Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac # Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. <\\s> Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default punct_chars Punctuation patterns None (use pre-defined patterns) use_endlines Whether to use endlines prediction (see documentation ) True Authors and citation The eds.sentences pipeline was developed by AP-HP's Data Science team.","title":"Sentences"},{"location":"pipelines/core/sentences/#sentences","text":"The eds.sentences pipeline provides an alternative to spaCy's default sentencizer , aiming to overcome some of its limitations. Indeed, the sentencizer merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our sentences component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances. Moreover, the eds.sentences pipeline can use the output of the eds.normalizer pipeline, and more specifically the end-of-line classification. This is activated by default.","title":"Sentences"},{"location":"pipelines/core/sentences/#usage","text":"EDS-NLP spaCy sentencizer import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac \\n \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\" ) doc = nlp ( text ) for sentence in doc . sents : print ( \"<s>\" , sentence , \"</s>\" ) # Out: <s> Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac # Out: <\\s> # Out: <s> Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. <\\s> import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"sentencizer\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac \\n \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\" ) doc = nlp ( text ) for sentence in doc . sents : print ( \"<s>\" , sentence , \"</s>\" ) # Out: <s> Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac # Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. <\\s> Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings.","title":"Usage"},{"location":"pipelines/core/sentences/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default punct_chars Punctuation patterns None (use pre-defined patterns) use_endlines Whether to use endlines prediction (see documentation ) True","title":"Configuration"},{"location":"pipelines/core/sentences/#authors-and-citation","text":"The eds.sentences pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/core/terminology/","text":"Terminology EDS-NLP simplifies the terminology matching process by exposing a eds.terminology pipeline that can match on terms or regular expressions. The terminology matcher is very similar to the generic matcher , although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies. There are some key differences: It labels every matched entity to the same value, provided to the pipeline The keys provided in the regex and terms dictionaries are used as the kb_id_ of the entity, which handles fine-grained labelling For instance, a terminology matcher could detect every drug mention under the top-level label drug , and link each individual mention to a given drug through its kb_id_ attribute. Usage Let us redefine the pipeline : import spacy nlp = spacy . blank ( \"fr\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], # (1) flu = [ \"grippe saisonni\u00e8re\" ], # (2) ) regex = dict ( covid = r \"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\" , # (3) ) nlp . add_pipe ( \"eds.terminology\" , config = dict ( label = \"disease\" , terms = terms , regex = regex , attr = \"LOWER\" , ), ) Every key in the terms dictionary is mapped to a concept. The eds.matcher pipeline expects a list of expressions, or a single expression. We can also define regular expression patterns. This snippet is complete, and should run as is. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default label Top-level label. Required terms Terms patterns. Expects a dictionary. None (use regex only) regex RegExp patterns. Expects a dictionary. None (use terms only) attr spaCy attribute to match on (eg NORM , LOWER ) \"TEXT\" ignore_excluded Whether to skip excluded tokens during matching False Patterns, be they terms or regex , are defined as dictionaries where keys become the kb_id_ of the extracted entities. Dictionary values are a either a single expression or a list of expressions that match the concept (see example ). Authors and citation The eds.terminology pipeline was developed by AP-HP's Data Science team.","title":"Terminology"},{"location":"pipelines/core/terminology/#terminology","text":"EDS-NLP simplifies the terminology matching process by exposing a eds.terminology pipeline that can match on terms or regular expressions. The terminology matcher is very similar to the generic matcher , although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies. There are some key differences: It labels every matched entity to the same value, provided to the pipeline The keys provided in the regex and terms dictionaries are used as the kb_id_ of the entity, which handles fine-grained labelling For instance, a terminology matcher could detect every drug mention under the top-level label drug , and link each individual mention to a given drug through its kb_id_ attribute.","title":"Terminology"},{"location":"pipelines/core/terminology/#usage","text":"Let us redefine the pipeline : import spacy nlp = spacy . blank ( \"fr\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], # (1) flu = [ \"grippe saisonni\u00e8re\" ], # (2) ) regex = dict ( covid = r \"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\" , # (3) ) nlp . add_pipe ( \"eds.terminology\" , config = dict ( label = \"disease\" , terms = terms , regex = regex , attr = \"LOWER\" , ), ) Every key in the terms dictionary is mapped to a concept. The eds.matcher pipeline expects a list of expressions, or a single expression. We can also define regular expression patterns. This snippet is complete, and should run as is.","title":"Usage"},{"location":"pipelines/core/terminology/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default label Top-level label. Required terms Terms patterns. Expects a dictionary. None (use regex only) regex RegExp patterns. Expects a dictionary. None (use terms only) attr spaCy attribute to match on (eg NORM , LOWER ) \"TEXT\" ignore_excluded Whether to skip excluded tokens during matching False Patterns, be they terms or regex , are defined as dictionaries where keys become the kb_id_ of the extracted entities. Dictionary values are a either a single expression or a list of expressions that match the concept (see example ).","title":"Configuration"},{"location":"pipelines/core/terminology/#authors-and-citation","text":"The eds.terminology pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/","text":"Miscellaneous This section regroups pipelines that extract information that can be used by other components, but have little medical value in itself. For instance, the date detection and normalisation pipeline falls in this category.","title":"Miscellaneous"},{"location":"pipelines/misc/#miscellaneous","text":"This section regroups pipelines that extract information that can be used by other components, but have little medical value in itself. For instance, the date detection and normalisation pipeline falls in this category.","title":"Miscellaneous"},{"location":"pipelines/misc/consultation-dates/","text":"Consultation Dates This pipeline consists of two main parts: A matcher which finds mentions of consultation events (more details below) A date parser (see the corresponding pipeline) that links a date to those events Usage Note It is designed to work ONLY on consultation notes ( CR-CONS ), so please filter accordingly before proceeding. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" , config = dict ( lowercase = True , accents = True , quotes = True , pollution = False , ), ) nlp . add_pipe ( \"eds.consultation_dates\" ) text = \"XXX \\n \" \"Objet : Compte-Rendu de Consultation du 03/10/2018. \\n \" \"XXX \" doc = nlp ( text ) doc . spans [ \"consultation_dates\" ] # Out: [Consultation du 03/10/2018] doc . spans [ \"consultation_dates\" ][ 0 ] . _ . consultation_date . to_datetime () # Out: DateTime(2018, 10, 3, 0, 0, 0, tzinfo=Timezone('Europe/Paris')) Consultation events Three main families of terms are available by default to extract those events. The consultation_mention terms This list contains terms directly referring to consultations, such as \" Consultation du... \" or \" Compte rendu du... \". This list is the only one activated by default since it is fairly precise an not error-prone. The town_mention terms This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \" Paris, le 13 d\u00e9cembre 2015 \". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates. The document_date_mention terms This list contains expressions mentioning the date of creation/edition of a document, such as \" Date du rapport: 13/12/2015 \" or \" Sign\u00e9 le 13/12/2015 \". As for town_mention , it has a high recall but is prone to errors since document date and consultation date aren't necessary similar. Note By default, only the consultation_mention are used Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default consultation_mention Whether to use consultation patterns, or list of patterns True (use pre-defined patterns) town_mention Whether to use town patterns, or list of patterns False document_date_mention Whether to use document date patterns, or list of patterns False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" Declared extensions The eds.consultation_dates pipeline declares one spaCy extensions on the Span object: the consultation_date attribute, which is a Python datetime object. Authors and citation The eds.consultation_dates pipeline was developed by AP-HP's Data Science team.","title":"Consultation Dates"},{"location":"pipelines/misc/consultation-dates/#consultation-dates","text":"This pipeline consists of two main parts: A matcher which finds mentions of consultation events (more details below) A date parser (see the corresponding pipeline) that links a date to those events","title":"Consultation Dates"},{"location":"pipelines/misc/consultation-dates/#usage","text":"Note It is designed to work ONLY on consultation notes ( CR-CONS ), so please filter accordingly before proceeding. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" , config = dict ( lowercase = True , accents = True , quotes = True , pollution = False , ), ) nlp . add_pipe ( \"eds.consultation_dates\" ) text = \"XXX \\n \" \"Objet : Compte-Rendu de Consultation du 03/10/2018. \\n \" \"XXX \" doc = nlp ( text ) doc . spans [ \"consultation_dates\" ] # Out: [Consultation du 03/10/2018] doc . spans [ \"consultation_dates\" ][ 0 ] . _ . consultation_date . to_datetime () # Out: DateTime(2018, 10, 3, 0, 0, 0, tzinfo=Timezone('Europe/Paris'))","title":"Usage"},{"location":"pipelines/misc/consultation-dates/#consultation-events","text":"Three main families of terms are available by default to extract those events.","title":"Consultation events"},{"location":"pipelines/misc/consultation-dates/#the-consultation_mention-terms","text":"This list contains terms directly referring to consultations, such as \" Consultation du... \" or \" Compte rendu du... \". This list is the only one activated by default since it is fairly precise an not error-prone.","title":"The consultation_mention terms"},{"location":"pipelines/misc/consultation-dates/#the-town_mention-terms","text":"This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \" Paris, le 13 d\u00e9cembre 2015 \". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.","title":"The town_mention terms"},{"location":"pipelines/misc/consultation-dates/#the-document_date_mention-terms","text":"This list contains expressions mentioning the date of creation/edition of a document, such as \" Date du rapport: 13/12/2015 \" or \" Sign\u00e9 le 13/12/2015 \". As for town_mention , it has a high recall but is prone to errors since document date and consultation date aren't necessary similar. Note By default, only the consultation_mention are used","title":"The document_date_mention terms"},{"location":"pipelines/misc/consultation-dates/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default consultation_mention Whether to use consultation patterns, or list of patterns True (use pre-defined patterns) town_mention Whether to use town patterns, or list of patterns False document_date_mention Whether to use document date patterns, or list of patterns False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\"","title":"Configuration"},{"location":"pipelines/misc/consultation-dates/#declared-extensions","text":"The eds.consultation_dates pipeline declares one spaCy extensions on the Span object: the consultation_date attribute, which is a Python datetime object.","title":"Declared extensions"},{"location":"pipelines/misc/consultation-dates/#authors-and-citation","text":"The eds.consultation_dates pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/dates/","text":"Dates The eds.dates pipeline's role is to detect and normalise dates within a medical document. We use simple regular expressions to extract date mentions. Scope The eds.dates pipeline finds absolute (eg 23/08/2021 ) and relative (eg hier , la semaine derni\u00e8re ) dates alike. It also handles mentions of duration. Type Example absolute 3 mai , 03/05/2020 relative hier , la semaine derni\u00e8re duration pendant quatre jours See the tutorial for a presentation of a full pipeline featuring the eds.dates component. Usage import spacy import pendulum nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.dates\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \" \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\" ) doc = nlp ( text ) dates = doc . spans [ \"dates\" ] dates # Out: [23 ao\u00fbt 2021, il y a un an, pendant une semaine, mai 1995] dates [ 0 ] . _ . date . to_datetime () # Out: 2021-08-23T00:00:00+02:00 dates [ 1 ] . _ . date . to_datetime () # Out: -1 year note_datetime = pendulum . datetime ( 2021 , 8 , 27 , tz = \"Europe/Paris\" ) dates [ 1 ] . _ . date . to_datetime ( note_datetime = note_datetime ) # Out: DateTime(2020, 8, 27, 0, 0, 0, tzinfo=Timezone('Europe/Paris')) date_3_output = dates [ 3 ] . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = True , tz = \"Europe/Paris\" , default_day = 15 , ) date_3_output # Out: DateTime(1995, 5, 15, 0, 0, 0, tzinfo=Timezone('Europe/Paris')) Declared extensions The eds.dates pipeline declares one spaCy extension on the Span object: the date attribute contains a parsed version of the date. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default absolute Absolute date patterns, eg le 5 ao\u00fbt 2020 None (use pre-defined patterns) relative Relative date patterns, eg hier ) None (use pre-defined patterns) durations Duration patterns, eg pendant trois mois ) None (use pre-defined patterns) false_positive Some false positive patterns to exclude None (use pre-defined patterns) detect_periods Whether to look for dates around entities only False on_ents_only Whether to look for dates around entities only False as_ents Whether to save detected dates as entities False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" Authors and citation The eds.dates pipeline was developed by AP-HP's Data Science team.","title":"Dates"},{"location":"pipelines/misc/dates/#dates","text":"The eds.dates pipeline's role is to detect and normalise dates within a medical document. We use simple regular expressions to extract date mentions.","title":"Dates"},{"location":"pipelines/misc/dates/#scope","text":"The eds.dates pipeline finds absolute (eg 23/08/2021 ) and relative (eg hier , la semaine derni\u00e8re ) dates alike. It also handles mentions of duration. Type Example absolute 3 mai , 03/05/2020 relative hier , la semaine derni\u00e8re duration pendant quatre jours See the tutorial for a presentation of a full pipeline featuring the eds.dates component.","title":"Scope"},{"location":"pipelines/misc/dates/#usage","text":"import spacy import pendulum nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.dates\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \" \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \" \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\" ) doc = nlp ( text ) dates = doc . spans [ \"dates\" ] dates # Out: [23 ao\u00fbt 2021, il y a un an, pendant une semaine, mai 1995] dates [ 0 ] . _ . date . to_datetime () # Out: 2021-08-23T00:00:00+02:00 dates [ 1 ] . _ . date . to_datetime () # Out: -1 year note_datetime = pendulum . datetime ( 2021 , 8 , 27 , tz = \"Europe/Paris\" ) dates [ 1 ] . _ . date . to_datetime ( note_datetime = note_datetime ) # Out: DateTime(2020, 8, 27, 0, 0, 0, tzinfo=Timezone('Europe/Paris')) date_3_output = dates [ 3 ] . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = True , tz = \"Europe/Paris\" , default_day = 15 , ) date_3_output # Out: DateTime(1995, 5, 15, 0, 0, 0, tzinfo=Timezone('Europe/Paris'))","title":"Usage"},{"location":"pipelines/misc/dates/#declared-extensions","text":"The eds.dates pipeline declares one spaCy extension on the Span object: the date attribute contains a parsed version of the date.","title":"Declared extensions"},{"location":"pipelines/misc/dates/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default absolute Absolute date patterns, eg le 5 ao\u00fbt 2020 None (use pre-defined patterns) relative Relative date patterns, eg hier ) None (use pre-defined patterns) durations Duration patterns, eg pendant trois mois ) None (use pre-defined patterns) false_positive Some false positive patterns to exclude None (use pre-defined patterns) detect_periods Whether to look for dates around entities only False on_ents_only Whether to look for dates around entities only False as_ents Whether to save detected dates as entities False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\"","title":"Configuration"},{"location":"pipelines/misc/dates/#authors-and-citation","text":"The eds.dates pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/measurements/","text":"Measurements The eds.measurements pipeline's role is to detect and normalise numerical measurements within a medical document. We use simple regular expressions to extract and normalize measurements, and use Measurement classes to store them. Warning The measurements pipeline is still in active development and has not been rigorously validated. If you come across a measurement expression that goes undetected, please file an issue ! Scope The eds.measurements pipeline can extract simple (eg 3cm ) measurements. It can detect elliptic enumerations (eg 32, 33 et 34kg ) of measurements of the same type and split the measurements accordingly. The normalized value can then be accessed via the span._.value attribute and converted on the fly to a desired unit. The current pipeline annotates the following measurements out of the box: Measurement name Example eds.size 1m50 , 1.50m eds.weight 12kg , 1kg300 eds.bmi BMI: 24 , 24 kg.m-2 eds.volume 2 cac , 8ml Usage import spacy nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.measurements\" , config = dict ( measurements = [ \"eds.size\" , \"eds.weight\" , \"eds.bmi\" ]) ) text = ( \"Le patient est admis hier, fait 1m78 pour 76kg. \" \"Les deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm. \" \"BMI: 24 \" ) doc = nlp ( text ) measurements = doc . spans [ \"measurements\" ] measurements # Out: [1m78, 76kg, 1,2, 2.4mm, 24] measurements [ 0 ] # Out: 1m78 str ( measurements [ 0 ] . _ . value ) # Out: '1.78 m' measurements [ 0 ] . _ . value . cm # Out: 178.0 measurements [ 2 ] # Out: 1,2 str ( measurements [ 2 ] . _ . value ) # Out: '1.2 mm' str ( measurements [ 2 ] . _ . value . mm ) # Out: 1.2 measurements [ 4 ] # Out: 24 str ( measurements [ 4 ] . _ . value ) # Out: '24.0 kg_per_m2' str ( measurements [ 4 ] . _ . value . kg_per_m2 ) # Out: 24.0 Custom measurement You can declare custom measurements by changing the patterns import spacy nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.measurements\" , config = dict ( measurements = { # this name will be used to define the labels of the matched entities \"my_custom_surface_measurement\" : { # This measurement unit is homogenous to square meters \"unit\" : \"m2\" , # To handle cases like \"surface: 1.8\" (implied m2), we can use # unitless patterns \"unitless_patterns\" : [ { \"terms\" : [ \"surface\" , \"aire\" ], \"ranges\" : [ { \"unit\" : \"m2\" , \"min\" : 0 , \"max\" : 9 , } ], } ], }, } ), ) Declared extensions The eds.measurements pipeline declares a single spaCy extension on the Span object, the value attribute that is a Measurement instance. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default measurements A list or dict of the measurements to extract [\"eds.size\", \"eds.weight\", \"eds.angle\"] units_config A dict describing the units with lexical patterns, dimensions, scales, ... ... number_terms A dict describing the textual forms of common numbers ... stopwords A list of stopwords that do not matter when placed between a unitless trigger ... unit_divisors A list of terms used to divide two units (like: m / s) ... ignore_excluded Whether to ignore excluded tokens for matching False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" Authors and citation The eds.measurements pipeline was developed by AP-HP's Data Science team.","title":"Measurements"},{"location":"pipelines/misc/measurements/#measurements","text":"The eds.measurements pipeline's role is to detect and normalise numerical measurements within a medical document. We use simple regular expressions to extract and normalize measurements, and use Measurement classes to store them. Warning The measurements pipeline is still in active development and has not been rigorously validated. If you come across a measurement expression that goes undetected, please file an issue !","title":"Measurements"},{"location":"pipelines/misc/measurements/#scope","text":"The eds.measurements pipeline can extract simple (eg 3cm ) measurements. It can detect elliptic enumerations (eg 32, 33 et 34kg ) of measurements of the same type and split the measurements accordingly. The normalized value can then be accessed via the span._.value attribute and converted on the fly to a desired unit. The current pipeline annotates the following measurements out of the box: Measurement name Example eds.size 1m50 , 1.50m eds.weight 12kg , 1kg300 eds.bmi BMI: 24 , 24 kg.m-2 eds.volume 2 cac , 8ml","title":"Scope"},{"location":"pipelines/misc/measurements/#usage","text":"import spacy nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.measurements\" , config = dict ( measurements = [ \"eds.size\" , \"eds.weight\" , \"eds.bmi\" ]) ) text = ( \"Le patient est admis hier, fait 1m78 pour 76kg. \" \"Les deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm. \" \"BMI: 24 \" ) doc = nlp ( text ) measurements = doc . spans [ \"measurements\" ] measurements # Out: [1m78, 76kg, 1,2, 2.4mm, 24] measurements [ 0 ] # Out: 1m78 str ( measurements [ 0 ] . _ . value ) # Out: '1.78 m' measurements [ 0 ] . _ . value . cm # Out: 178.0 measurements [ 2 ] # Out: 1,2 str ( measurements [ 2 ] . _ . value ) # Out: '1.2 mm' str ( measurements [ 2 ] . _ . value . mm ) # Out: 1.2 measurements [ 4 ] # Out: 24 str ( measurements [ 4 ] . _ . value ) # Out: '24.0 kg_per_m2' str ( measurements [ 4 ] . _ . value . kg_per_m2 ) # Out: 24.0","title":"Usage"},{"location":"pipelines/misc/measurements/#custom-measurement","text":"You can declare custom measurements by changing the patterns import spacy nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.measurements\" , config = dict ( measurements = { # this name will be used to define the labels of the matched entities \"my_custom_surface_measurement\" : { # This measurement unit is homogenous to square meters \"unit\" : \"m2\" , # To handle cases like \"surface: 1.8\" (implied m2), we can use # unitless patterns \"unitless_patterns\" : [ { \"terms\" : [ \"surface\" , \"aire\" ], \"ranges\" : [ { \"unit\" : \"m2\" , \"min\" : 0 , \"max\" : 9 , } ], } ], }, } ), )","title":"Custom measurement"},{"location":"pipelines/misc/measurements/#declared-extensions","text":"The eds.measurements pipeline declares a single spaCy extension on the Span object, the value attribute that is a Measurement instance.","title":"Declared extensions"},{"location":"pipelines/misc/measurements/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default measurements A list or dict of the measurements to extract [\"eds.size\", \"eds.weight\", \"eds.angle\"] units_config A dict describing the units with lexical patterns, dimensions, scales, ... ... number_terms A dict describing the textual forms of common numbers ... stopwords A list of stopwords that do not matter when placed between a unitless trigger ... unit_divisors A list of terms used to divide two units (like: m / s) ... ignore_excluded Whether to ignore excluded tokens for matching False attr spaCy attribute to match on, eg NORM or TEXT \"NORM\"","title":"Configuration"},{"location":"pipelines/misc/measurements/#authors-and-citation","text":"The eds.measurements pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/reason/","text":"Reason The eds.reason pipeline uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS. Usage The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is . import spacy text = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018 MOTIF D'HOSPITALISATION Monsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. ANT\u00c9C\u00c9DENTS Ant\u00e9c\u00e9dents m\u00e9dicaux : Premier \u00e9pisode d'asthme en mai 2018.\"\"\" nlp = spacy . blank ( \"fr\" ) # Extraction of entities nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( respiratoire = [ \"asthmatique\" , \"asthme\" , \"toux\" , ] ) ), ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.reason\" , config = dict ( use_sections = True )) doc = nlp ( text ) reason = doc . spans [ \"reasons\" ][ 0 ] reason # Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. reason . _ . is_reason # Out: True entities = reason . _ . ents_reason entities # Out: [asthme] entities [ 0 ] . label_ # Out: 'respiratoire' ent = entities [ 0 ] ent . _ . is_reason # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default reasons Reasons patterns None (use pre-defined patterns) attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" use_sections Whether to use sections False ignore_excluded Whether to ignore excluded tokens False Declared extensions The eds.reason pipeline adds the key reasons to doc.spans and declares one spaCy extension , on the Span objects called ents_reason . The ents_reason extension is a list of named entities that overlap the Span , typically entities found in previous pipelines like matcher . It also declares the boolean extension is_reason . This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. Authors and citation The eds.reason pipeline was developed by AP-HP's Data Science team.","title":"Reason"},{"location":"pipelines/misc/reason/#reason","text":"The eds.reason pipeline uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.","title":"Reason"},{"location":"pipelines/misc/reason/#usage","text":"The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is . import spacy text = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018 MOTIF D'HOSPITALISATION Monsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. ANT\u00c9C\u00c9DENTS Ant\u00e9c\u00e9dents m\u00e9dicaux : Premier \u00e9pisode d'asthme en mai 2018.\"\"\" nlp = spacy . blank ( \"fr\" ) # Extraction of entities nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( respiratoire = [ \"asthmatique\" , \"asthme\" , \"toux\" , ] ) ), ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.reason\" , config = dict ( use_sections = True )) doc = nlp ( text ) reason = doc . spans [ \"reasons\" ][ 0 ] reason # Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. reason . _ . is_reason # Out: True entities = reason . _ . ents_reason entities # Out: [asthme] entities [ 0 ] . label_ # Out: 'respiratoire' ent = entities [ 0 ] ent . _ . is_reason # Out: True","title":"Usage"},{"location":"pipelines/misc/reason/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default reasons Reasons patterns None (use pre-defined patterns) attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" use_sections Whether to use sections False ignore_excluded Whether to ignore excluded tokens False","title":"Configuration"},{"location":"pipelines/misc/reason/#declared-extensions","text":"The eds.reason pipeline adds the key reasons to doc.spans and declares one spaCy extension , on the Span objects called ents_reason . The ents_reason extension is a list of named entities that overlap the Span , typically entities found in previous pipelines like matcher . It also declares the boolean extension is_reason . This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.","title":"Declared extensions"},{"location":"pipelines/misc/reason/#authors-and-citation","text":"The eds.reason pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/misc/sections/","text":"Sections Detected sections are : allergies ant\u00e9c\u00e9dents ant\u00e9c\u00e9dents familiaux traitements entr\u00e9e conclusion conclusion entr\u00e9e habitus correspondants diagnostic donn\u00e9es biom\u00e9triques entr\u00e9e examens examens compl\u00e9mentaires facteurs de risques histoire de la maladie actes motif prescriptions traitements sortie evolution modalites sortie vaccinations introduction | The pipeline extracts section title. A \"section\" is then defined as the span of text between two titles. Remarks : - section introduction corresponds to the span of text between the header \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document) and the title of the following detected section - this pipeline works well for hospitalization summaries (CRH), but not necessarily for all types of documents (in particular for emergency or scan summaries CR-IMAGERIE) Use at your own risks Should you rely on eds.sections for critical downstream tasks, make sure to validate the pipeline to make sure that the component works. For instance, the eds.history pipeline can use sections to make its predictions, but that possibility is deactivated by default. Usage The following snippet detects section titles. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) text = \"CRU du 10/09/2021 \\n \" \"Motif : \\n \" \"Patient admis pour suspicion de COVID\" doc = nlp ( text ) doc . spans [ \"section_titles\" ] # Out: [Motif] Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default sections Sections patterns None (use pre-defined patterns) add_patterns Whether add endlines patterns True attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" ignore_excluded Whether to ignore excluded tokens True Declared extensions The eds.sections pipeline adds two fields to the doc.spans attribute : The section_titles key contains the list of all section titles extracted using the list declared in the terms.py module. The sections key contains a list of sections, ie spans of text between two section titles (or the last title and the end of the document). If the document has entities before calling this pipeline an attribute section is added to each entity. Authors and citation The eds.sections pipeline was developed by AP-HP's Data Science team.","title":"Sections"},{"location":"pipelines/misc/sections/#sections","text":"Detected sections are : allergies ant\u00e9c\u00e9dents ant\u00e9c\u00e9dents familiaux traitements entr\u00e9e conclusion conclusion entr\u00e9e habitus correspondants diagnostic donn\u00e9es biom\u00e9triques entr\u00e9e examens examens compl\u00e9mentaires facteurs de risques histoire de la maladie actes motif prescriptions traitements sortie evolution modalites sortie vaccinations introduction | The pipeline extracts section title. A \"section\" is then defined as the span of text between two titles. Remarks : - section introduction corresponds to the span of text between the header \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document) and the title of the following detected section - this pipeline works well for hospitalization summaries (CRH), but not necessarily for all types of documents (in particular for emergency or scan summaries CR-IMAGERIE) Use at your own risks Should you rely on eds.sections for critical downstream tasks, make sure to validate the pipeline to make sure that the component works. For instance, the eds.history pipeline can use sections to make its predictions, but that possibility is deactivated by default.","title":"Sections"},{"location":"pipelines/misc/sections/#usage","text":"The following snippet detects section titles. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) text = \"CRU du 10/09/2021 \\n \" \"Motif : \\n \" \"Patient admis pour suspicion de COVID\" doc = nlp ( text ) doc . spans [ \"section_titles\" ] # Out: [Motif]","title":"Usage"},{"location":"pipelines/misc/sections/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default sections Sections patterns None (use pre-defined patterns) add_patterns Whether add endlines patterns True attr spaCy attribute to match on, eg NORM or TEXT \"NORM\" ignore_excluded Whether to ignore excluded tokens True","title":"Configuration"},{"location":"pipelines/misc/sections/#declared-extensions","text":"The eds.sections pipeline adds two fields to the doc.spans attribute : The section_titles key contains the list of all section titles extracted using the list declared in the terms.py module. The sections key contains a list of sections, ie spans of text between two section titles (or the last title and the end of the document). If the document has entities before calling this pipeline an attribute section is added to each entity.","title":"Declared extensions"},{"location":"pipelines/misc/sections/#authors-and-citation","text":"The eds.sections pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/ner/","text":"Named entity recognition We provide a few Named Entity Recognition (NER) pipelines.","title":"Named entity recognition"},{"location":"pipelines/ner/#named-entity-recognition","text":"We provide a few Named Entity Recognition (NER) pipelines.","title":"Named entity recognition"},{"location":"pipelines/ner/adicap/","text":"ADICAP The eds.adicap pipeline component matches the ADICAP codes. It was developped to work on anapathology reports. Document type It was developped to work on anapathology reports. We recommend also to use the eds language ( spacy.blank(\"eds\") ) The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes: Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\". Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor. For further details about the ADICAP code follow this link . Usage import spacy nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.adicap\" ) text = \"\"\"\" COMPTE RENDU D\u2019EXAMEN Ant\u00e9riorit\u00e9(s) : NEANT Renseignements cliniques : Contexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-externe du sein droit. La l\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion du quadrant sup\u00e9ro-externe, \u00e0 l'union des quadrants inf\u00e9rieurs. Macrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit, mesurant 4 mm, class\u00e9e ACR4 14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il n'y a pas eu d'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en totalit\u00e9 et coup\u00e9s sur plusieurs niveaux. Histologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois l\u00e9g\u00e8rement dystrophique avec quelques petits kystes. Il n'y a pas d'hyperplasie \u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration tumorale. On note quelques suffusions h\u00e9morragiques focales. Conclusion : L\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit. Absence d'atypies ou de prolif\u00e9ration tumorale. Codification : BHGS0040 \"\"\" doc = nlp ( text ) doc . ents # Out: (BHGS0040,) ent = doc . ents [ 0 ] ent . label_ # Out: adicap ent . _ . adicap . dict () # Out: {'code': 'BHGS0040', # 'sampling_mode': 'BIOPSIE CHIRURGICALE', # 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION', # 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\", # 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE', # 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE', # 'behaviour_type': 'CARACTERES GENERAUX'} Configuration The pipeline can be configured using the following parameters : Parameter Description Default window Number of tokens to look for prefix. It will never go further the start of the sentence -500 prefix The prefix to look before the ADICAP pattern r\"(?i)(codification |adicap)\" Authors and citation The eds.adicap pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' 1 (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\") Agence du num\u00e9rique en sant\u00e9. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. 05 2019. URL: http://esante.gouv.fr/terminologie-adicap . \u21a9","title":"ADICAP"},{"location":"pipelines/ner/adicap/#adicap","text":"The eds.adicap pipeline component matches the ADICAP codes. It was developped to work on anapathology reports. Document type It was developped to work on anapathology reports. We recommend also to use the eds language ( spacy.blank(\"eds\") ) The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes: Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\". Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor. For further details about the ADICAP code follow this link .","title":"ADICAP"},{"location":"pipelines/ner/adicap/#usage","text":"import spacy nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.adicap\" ) text = \"\"\"\" COMPTE RENDU D\u2019EXAMEN Ant\u00e9riorit\u00e9(s) : NEANT Renseignements cliniques : Contexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-externe du sein droit. La l\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion du quadrant sup\u00e9ro-externe, \u00e0 l'union des quadrants inf\u00e9rieurs. Macrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit, mesurant 4 mm, class\u00e9e ACR4 14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il n'y a pas eu d'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en totalit\u00e9 et coup\u00e9s sur plusieurs niveaux. Histologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois l\u00e9g\u00e8rement dystrophique avec quelques petits kystes. Il n'y a pas d'hyperplasie \u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration tumorale. On note quelques suffusions h\u00e9morragiques focales. Conclusion : L\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit. Absence d'atypies ou de prolif\u00e9ration tumorale. Codification : BHGS0040 \"\"\" doc = nlp ( text ) doc . ents # Out: (BHGS0040,) ent = doc . ents [ 0 ] ent . label_ # Out: adicap ent . _ . adicap . dict () # Out: {'code': 'BHGS0040', # 'sampling_mode': 'BIOPSIE CHIRURGICALE', # 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION', # 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\", # 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE', # 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE', # 'behaviour_type': 'CARACTERES GENERAUX'}","title":"Usage"},{"location":"pipelines/ner/adicap/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Description Default window Number of tokens to look for prefix. It will never go further the start of the sentence -500 prefix The prefix to look before the ADICAP pattern r\"(?i)(codification |adicap)\"","title":"Configuration"},{"location":"pipelines/ner/adicap/#authors-and-citation","text":"The eds.adicap pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' 1 (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\") Agence du num\u00e9rique en sant\u00e9. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. 05 2019. URL: http://esante.gouv.fr/terminologie-adicap . \u21a9","title":"Authors and citation"},{"location":"pipelines/ner/cim10/","text":"CIM10 The eds.cim10 pipeline component matches the CIM10 (French-language ICD) terminology. Very low recall When using the exact' matching mode, this component has a very poor recall performance. We can use the simstring` mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time. Usage import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.cim10\" , config = dict ( term_matcher = \"simstring\" )) text = \"Le patient est suivi pour fi\u00e8vres typho\u00efde et paratypho\u00efde.\" doc = nlp ( text ) doc . ents # Out: (fi\u00e8vres typho\u00efde et paratypho\u00efde,) ent = doc . ents [ 0 ] ent . label_ # Out: cim10 ent . kb_id_ # Out: A01 Configuration The pipeline can be configured using the following parameters : Parameter Description Default term_matcher Which algorithm should we use : exact or simstring \"exact\" term_matcher_config Config of the algorithm ( SimstringMatcher 's for simstring ) {} attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"LOWER\" ignore_excluded Whether to ignore excluded tokens for matching False Authors and citation The eds.cim10 pipeline was developed by AP-HP's Data Science team.","title":"CIM10"},{"location":"pipelines/ner/cim10/#cim10","text":"The eds.cim10 pipeline component matches the CIM10 (French-language ICD) terminology. Very low recall When using the exact' matching mode, this component has a very poor recall performance. We can use the simstring` mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.","title":"CIM10"},{"location":"pipelines/ner/cim10/#usage","text":"import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.cim10\" , config = dict ( term_matcher = \"simstring\" )) text = \"Le patient est suivi pour fi\u00e8vres typho\u00efde et paratypho\u00efde.\" doc = nlp ( text ) doc . ents # Out: (fi\u00e8vres typho\u00efde et paratypho\u00efde,) ent = doc . ents [ 0 ] ent . label_ # Out: cim10 ent . kb_id_ # Out: A01","title":"Usage"},{"location":"pipelines/ner/cim10/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Description Default term_matcher Which algorithm should we use : exact or simstring \"exact\" term_matcher_config Config of the algorithm ( SimstringMatcher 's for simstring ) {} attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"LOWER\" ignore_excluded Whether to ignore excluded tokens for matching False","title":"Configuration"},{"location":"pipelines/ner/cim10/#authors-and-citation","text":"The eds.cim10 pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/ner/covid/","text":"COVID The eds.covid pipeline component detects mentions of COVID19 and adds them to doc.ents . Usage import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.covid\" ) text = \"Le patient est admis pour une infection au coronavirus.\" doc = nlp ( text ) doc . ents # Out: (infection au coronavirus,) Configuration The pipeline can be configured using the following parameters : Parameter Description Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"LOWER\" ignore_excluded Whether to ignore excluded tokens for matching False Authors and citation The eds.covid pipeline was developed by AP-HP's Data Science team.","title":"COVID"},{"location":"pipelines/ner/covid/#covid","text":"The eds.covid pipeline component detects mentions of COVID19 and adds them to doc.ents .","title":"COVID"},{"location":"pipelines/ner/covid/#usage","text":"import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.covid\" ) text = \"Le patient est admis pour une infection au coronavirus.\" doc = nlp ( text ) doc . ents # Out: (infection au coronavirus,)","title":"Usage"},{"location":"pipelines/ner/covid/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Description Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"LOWER\" ignore_excluded Whether to ignore excluded tokens for matching False","title":"Configuration"},{"location":"pipelines/ner/covid/#authors-and-citation","text":"The eds.covid pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/ner/drugs/","text":"Drugs The eds.drugs pipeline component detects mentions of French drugs (brand names and active ingredients) and adds them to doc.ents . Each drug has an associated ATC code. ATC classifies drugs into groups. Usage In this example, we are looking for an oral antidiabetic medication (ATC code: A10B). from edsnlp.pipelines.core.terminology import TerminologyTermMatcher import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.drugs\" , config = dict ( term_matcher = TerminologyTermMatcher . exact )) text = \"Traitement habituel: Kard\u00e9gic, cardensiel (bisoprolol), glucophage, lasilix\" doc = nlp ( text ) drugs_detected = [( x . text , x . kb_id_ ) for x in doc . ents ] drugs_detected # Out: [('Kard\u00e9gic', 'B01AC06'), ('cardensiel', 'C07AB07'), ('bisoprolol', 'C07AB07'), ('glucophage', 'A10BA02'), ('lasilix', 'C03CA01')] oral_antidiabetics_detected = list ( filter ( lambda x : ( x [ 1 ] . startswith ( \"A10B\" )), drugs_detected ) ) oral_antidiabetics_detected # Out: [('glucophage', 'A10BA02')] Glucophage is the brand name of a medication that contains metformine, the first-line medication for the treatment of type 2 diabetes. Configuration The pipeline can be configured using the following parameters : Parameter Description Default term_matcher Which algorithm should we use : exact or simstring \"LOWER\" term_matcher_config Config of the algorithm ( SimstringMatcher 's for simstring ) \"LOWER\" attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" ignore_excluded Whether to ignore excluded tokens for matching False Authors and citation The eds.drugs pipeline was developed by the IAM team and CHU de Bordeaux's Data Science team.","title":"Drugs"},{"location":"pipelines/ner/drugs/#drugs","text":"The eds.drugs pipeline component detects mentions of French drugs (brand names and active ingredients) and adds them to doc.ents . Each drug has an associated ATC code. ATC classifies drugs into groups.","title":"Drugs"},{"location":"pipelines/ner/drugs/#usage","text":"In this example, we are looking for an oral antidiabetic medication (ATC code: A10B). from edsnlp.pipelines.core.terminology import TerminologyTermMatcher import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.drugs\" , config = dict ( term_matcher = TerminologyTermMatcher . exact )) text = \"Traitement habituel: Kard\u00e9gic, cardensiel (bisoprolol), glucophage, lasilix\" doc = nlp ( text ) drugs_detected = [( x . text , x . kb_id_ ) for x in doc . ents ] drugs_detected # Out: [('Kard\u00e9gic', 'B01AC06'), ('cardensiel', 'C07AB07'), ('bisoprolol', 'C07AB07'), ('glucophage', 'A10BA02'), ('lasilix', 'C03CA01')] oral_antidiabetics_detected = list ( filter ( lambda x : ( x [ 1 ] . startswith ( \"A10B\" )), drugs_detected ) ) oral_antidiabetics_detected # Out: [('glucophage', 'A10BA02')] Glucophage is the brand name of a medication that contains metformine, the first-line medication for the treatment of type 2 diabetes.","title":"Usage"},{"location":"pipelines/ner/drugs/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Description Default term_matcher Which algorithm should we use : exact or simstring \"LOWER\" term_matcher_config Config of the algorithm ( SimstringMatcher 's for simstring ) \"LOWER\" attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" ignore_excluded Whether to ignore excluded tokens for matching False","title":"Configuration"},{"location":"pipelines/ner/drugs/#authors-and-citation","text":"The eds.drugs pipeline was developed by the IAM team and CHU de Bordeaux's Data Science team.","title":"Authors and citation"},{"location":"pipelines/ner/score/","text":"Score The eds.score pipeline allows easy extraction of typical scores (Charlson, SOFA...) that can be found in clinical documents. The pipeline works by Extracting the score's name via the provided regular expressions Extracting the score's raw value via another set of RegEx Normalising the score's value via a normalising function Charlson Comorbidity Index Implementing the eds.score pipeline, the charlson pipeline will extract the Charlson Comorbidity Index : import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.charlson\" ) text = \"Charlson \u00e0 l'admission: 7. \\n \" \"Charlson: \\n \" \"OMS: \\n \" doc = nlp ( text ) doc . ents # Out: (Charlson \u00e0 l'admission: 7,) We can see that only one occurrence was extracted. The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted. Each extraction exposes 2 extensions: ent = doc . ents [ 0 ] ent . _ . score_name # Out: 'eds.charlson' ent . _ . score_value # Out: 7 SOFA score The SOFA pipe allows to extract SOFA scores. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.SOFA\" ) text = \"SOFA (\u00e0 24H) : 12. \\n \" \"OMS: \\n \" doc = nlp ( text ) doc . ents # Out: (SOFA (\u00e0 24H) : 12,) Each extraction exposes 3 extensions: ent = doc . ents [ 0 ] ent . _ . score_name # Out: 'eds.SOFA' ent . _ . score_value # Out: 12 ent . _ . score_method # Out: '24H' Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\" TNM score The eds.TNM pipe allows to extract TNM scores. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.TNM\" ) text = \"TNM: pTx N1 M1\" doc = nlp ( text ) doc . ents # Out: (pTx N1 M1,) ent = doc . ents [ 0 ] ent . _ . value . dict () # {'modifier': 'p', # 'tumour': None, # 'tumour_specification': 'x', # 'node': '1', # 'node_specification': None, # 'metastasis': '1', # 'resection_completeness': None, # 'version': None, # 'version_year': None} The TNM score is based on the developement of S. Priou, B. Rance and E. Kempf 1 . Implementing your own score Using the eds.score pipeline, you only have to change its configuration in order to implement a simple score extraction algorithm. As an example, let us see the configuration used for the eds.charlson pipe The configuration consists of 4 items: score_name : The name of the score regex : A list of regular expression to detect the score's mention value_extract : A regular expression to extract the score's value in the context of the score's mention score_normalization : A function name used to normalise the score's raw value Note spaCy doesn't allow to pass functions in the configuration of a pipeline. To circumvent this issue, functions need to be registered, which simply consists in decorating those functions The registration is done as follows: @spacy . registry . misc ( \"score_normalization.charlson\" ) def my_normalization_score ( raw_score : str ): # Implement some filtering here # Return None if you want the score to be discarded return normalized_score The values used for the eds.charlson pipe are the following: @spacy . registry . misc ( \"score_normalization.charlson\" ) def score_normalization ( extracted_score ): \"\"\" Charlson score normalization. If available, returns the integer value of the Charlson score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score ) charlson_config = dict ( score_name = \"charlson\" , regex = [ r \"charlson\" ], value_extract = r \"charlson.*[\\n\\W]*(\\d+)\" , score_normalization = \"score_normalization.charlson\" , ) Emmanuelle Kempf, Sonia Priou, Guillaume Lam\u00e9, Christel Daniel, Ali Bellamine, Daniele Sommacale, yazid Belkacemi, Romain Bey, Gilles Galula, Namik Taright, Xavier Tannier, Bastien Rance, R\u00e9mi Flicoteaux, Fran\u00e7ois Hemery, Etienne Audureau, Gilles Chatellier, and Christophe Tournigand. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. International Journal of Cancer , 150(10):1609\u20131618, 2022. URL: https://hal.archives-ouvertes.fr/hal-03519085 , doi:10.1002/ijc.33928 . \u21a9","title":"Score"},{"location":"pipelines/ner/score/#score","text":"The eds.score pipeline allows easy extraction of typical scores (Charlson, SOFA...) that can be found in clinical documents. The pipeline works by Extracting the score's name via the provided regular expressions Extracting the score's raw value via another set of RegEx Normalising the score's value via a normalising function","title":"Score"},{"location":"pipelines/ner/score/#charlson-comorbidity-index","text":"Implementing the eds.score pipeline, the charlson pipeline will extract the Charlson Comorbidity Index : import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.charlson\" ) text = \"Charlson \u00e0 l'admission: 7. \\n \" \"Charlson: \\n \" \"OMS: \\n \" doc = nlp ( text ) doc . ents # Out: (Charlson \u00e0 l'admission: 7,) We can see that only one occurrence was extracted. The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted. Each extraction exposes 2 extensions: ent = doc . ents [ 0 ] ent . _ . score_name # Out: 'eds.charlson' ent . _ . score_value # Out: 7","title":"Charlson Comorbidity Index"},{"location":"pipelines/ner/score/#sofa-score","text":"The SOFA pipe allows to extract SOFA scores. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.SOFA\" ) text = \"SOFA (\u00e0 24H) : 12. \\n \" \"OMS: \\n \" doc = nlp ( text ) doc . ents # Out: (SOFA (\u00e0 24H) : 12,) Each extraction exposes 3 extensions: ent = doc . ents [ 0 ] ent . _ . score_name # Out: 'eds.SOFA' ent . _ . score_value # Out: 12 ent . _ . score_method # Out: '24H' Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\"","title":"SOFA score"},{"location":"pipelines/ner/score/#tnm-score","text":"The eds.TNM pipe allows to extract TNM scores. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.TNM\" ) text = \"TNM: pTx N1 M1\" doc = nlp ( text ) doc . ents # Out: (pTx N1 M1,) ent = doc . ents [ 0 ] ent . _ . value . dict () # {'modifier': 'p', # 'tumour': None, # 'tumour_specification': 'x', # 'node': '1', # 'node_specification': None, # 'metastasis': '1', # 'resection_completeness': None, # 'version': None, # 'version_year': None} The TNM score is based on the developement of S. Priou, B. Rance and E. Kempf 1 .","title":"TNM score"},{"location":"pipelines/ner/score/#implementing-your-own-score","text":"Using the eds.score pipeline, you only have to change its configuration in order to implement a simple score extraction algorithm. As an example, let us see the configuration used for the eds.charlson pipe The configuration consists of 4 items: score_name : The name of the score regex : A list of regular expression to detect the score's mention value_extract : A regular expression to extract the score's value in the context of the score's mention score_normalization : A function name used to normalise the score's raw value Note spaCy doesn't allow to pass functions in the configuration of a pipeline. To circumvent this issue, functions need to be registered, which simply consists in decorating those functions The registration is done as follows: @spacy . registry . misc ( \"score_normalization.charlson\" ) def my_normalization_score ( raw_score : str ): # Implement some filtering here # Return None if you want the score to be discarded return normalized_score The values used for the eds.charlson pipe are the following: @spacy . registry . misc ( \"score_normalization.charlson\" ) def score_normalization ( extracted_score ): \"\"\" Charlson score normalization. If available, returns the integer value of the Charlson score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score ) charlson_config = dict ( score_name = \"charlson\" , regex = [ r \"charlson\" ], value_extract = r \"charlson.*[\\n\\W]*(\\d+)\" , score_normalization = \"score_normalization.charlson\" , ) Emmanuelle Kempf, Sonia Priou, Guillaume Lam\u00e9, Christel Daniel, Ali Bellamine, Daniele Sommacale, yazid Belkacemi, Romain Bey, Gilles Galula, Namik Taright, Xavier Tannier, Bastien Rance, R\u00e9mi Flicoteaux, Fran\u00e7ois Hemery, Etienne Audureau, Gilles Chatellier, and Christophe Tournigand. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. International Journal of Cancer , 150(10):1609\u20131618, 2022. URL: https://hal.archives-ouvertes.fr/hal-03519085 , doi:10.1002/ijc.33928 . \u21a9","title":"Implementing your own score"},{"location":"pipelines/ner/umls/","text":"UMLS The eds.umls pipeline component matches the UMLS (Unified Medical Language System from NIH) terminology. Very low recall When using the exact' matching mode, this component has a very poor recall performance. We can use the simstring` mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time. Usage eds.umls is an additional module that needs to be setup by: pip install -U umls_downloader Signing up for a UMLS Terminology Services Account . After filling a short form, you will receive your token API within a few days. Set UMLS_API_KEY locally: export UMLS_API_KEY=your_api_key import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.umls\" ) text = \"Grosse toux: le malade a \u00e9t\u00e9 mordu par des Amphibiens \" \"sous le genou\" doc = nlp ( text ) doc . ents # Out: (toux, a, par, Amphibiens, genou) ent = doc . ents [ 0 ] ent . label_ # Out: umls ent . _ . umls # Out: C0010200 You can easily change the default languages and sources with the pattern_config argument: import spacy # Enable the french and english languages, through the french MeSH and LOINC pattern_config = dict ( languages = [ \"FRE\" , \"ENG\" ], sources = [ \"MSHFRE\" , \"LNC\" ]) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.umls\" , config = dict ( pattern_config = pattern_config )) See more options of languages and sources here . Configuration The pipeline can be configured using the following parameters : Parameter Description Default term_matcher Which algorithm should we use : exact or simstring \"exact\" term_matcher_config Config of the algorithm ( SimstringMatcher 's for simstring ) {} pattern_config Config of the terminology patterns loader {\"languages\"=[\"FRE\"], sources=None} (sources None means all available sources) attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"LOWER\" ignore_excluded Whether to ignore excluded tokens for matching False Authors and citation The eds.umls pipeline was developed by AP-HP's Data Science team and INRIA SODA's team.","title":"UMLS"},{"location":"pipelines/ner/umls/#umls","text":"The eds.umls pipeline component matches the UMLS (Unified Medical Language System from NIH) terminology. Very low recall When using the exact' matching mode, this component has a very poor recall performance. We can use the simstring` mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.","title":"UMLS"},{"location":"pipelines/ner/umls/#usage","text":"eds.umls is an additional module that needs to be setup by: pip install -U umls_downloader Signing up for a UMLS Terminology Services Account . After filling a short form, you will receive your token API within a few days. Set UMLS_API_KEY locally: export UMLS_API_KEY=your_api_key import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.umls\" ) text = \"Grosse toux: le malade a \u00e9t\u00e9 mordu par des Amphibiens \" \"sous le genou\" doc = nlp ( text ) doc . ents # Out: (toux, a, par, Amphibiens, genou) ent = doc . ents [ 0 ] ent . label_ # Out: umls ent . _ . umls # Out: C0010200 You can easily change the default languages and sources with the pattern_config argument: import spacy # Enable the french and english languages, through the french MeSH and LOINC pattern_config = dict ( languages = [ \"FRE\" , \"ENG\" ], sources = [ \"MSHFRE\" , \"LNC\" ]) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.umls\" , config = dict ( pattern_config = pattern_config )) See more options of languages and sources here .","title":"Usage"},{"location":"pipelines/ner/umls/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Description Default term_matcher Which algorithm should we use : exact or simstring \"exact\" term_matcher_config Config of the algorithm ( SimstringMatcher 's for simstring ) {} pattern_config Config of the terminology patterns loader {\"languages\"=[\"FRE\"], sources=None} (sources None means all available sources) attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"LOWER\" ignore_excluded Whether to ignore excluded tokens for matching False","title":"Configuration"},{"location":"pipelines/ner/umls/#authors-and-citation","text":"The eds.umls pipeline was developed by AP-HP's Data Science team and INRIA SODA's team.","title":"Authors and citation"},{"location":"pipelines/qualifiers/","text":"Qualifier overview In EDS-NLP, we call qualifiers the suite of pipelines designed to qualify a pre-extracted entity for a linguistic modality. Available pipelines Name Description eds.negation Detect negated entities eds.family Detect entities that pertain to a patient's kin rather than themself eds.hypothesis Detect entities subject to speculation eds.reported_speech Detect entities that are quoted from the patient eds.history Detect entities that pertain to the patient's history Rationale In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents. Now, consider the following example: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort. Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort. Under the hood Our qualifier pipelines all follow the same basic pattern: The pipeline extracts cues. We define three (possibly overlapping) kinds : preceding , ie cues that precede modulated entities ; following , ie cues that follow modulated entities ; in some cases, verbs , ie verbs that convey a modulation (treated as preceding cues). The pipeline splits the text between sentences and propositions, using annotations from a sentencizer pipeline and termination patterns, which define syntagma/proposition terminations. For each pre-extracted entity, the pipeline checks whether there is a cue between the start of the syntagma and the start of the entity, or a following cue between the end of the entity and the end of the proposition. Albeit simple, this algorithm can achieve very good performance depending on the modality. For instance, our eds.negation pipeline reaches 88% F1-score on our dataset. Dealing with pseudo-cues The pipeline can also detect pseudo-cues , ie phrases that contain cues but that are not cues themselves . For instance: sans doute / without doubt contains sans/without , but does not convey negation. Detecting pseudo-cues lets the pipeline filter out any cue that overlaps with a pseudo-cue. Sentence boundaries are required The rule-based algorithm detects cues, and propagate their modulation on the rest of the syntagma . For that reason, a qualifier pipeline needs a sentencizer component to be defined, and will fail otherwise. You may use EDS-NLP's: nlp . add_pipe ( \"eds.sentences\" ) Persisting the results Our qualifier pipelines write their results to a custom spaCy extension , defined on both Span and Token objects. We follow the convention of naming said attribute after the pipeline itself, eg Span._.negation for the eds.negation pipeline. In most cases, that extension is a boolean. We also provide a string representation of the result, computed on the fly by declaring a getter that reads the boolean result of the pipeline. Following spaCy convention, we give this attribute the same name, followed by a _ .","title":"Qualifier overview"},{"location":"pipelines/qualifiers/#qualifier-overview","text":"In EDS-NLP, we call qualifiers the suite of pipelines designed to qualify a pre-extracted entity for a linguistic modality.","title":"Qualifier overview"},{"location":"pipelines/qualifiers/#available-pipelines","text":"Name Description eds.negation Detect negated entities eds.family Detect entities that pertain to a patient's kin rather than themself eds.hypothesis Detect entities subject to speculation eds.reported_speech Detect entities that are quoted from the patient eds.history Detect entities that pertain to the patient's history","title":"Available pipelines"},{"location":"pipelines/qualifiers/#rationale","text":"In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents. Now, consider the following example: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort. Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort.","title":"Rationale"},{"location":"pipelines/qualifiers/#under-the-hood","text":"Our qualifier pipelines all follow the same basic pattern: The pipeline extracts cues. We define three (possibly overlapping) kinds : preceding , ie cues that precede modulated entities ; following , ie cues that follow modulated entities ; in some cases, verbs , ie verbs that convey a modulation (treated as preceding cues). The pipeline splits the text between sentences and propositions, using annotations from a sentencizer pipeline and termination patterns, which define syntagma/proposition terminations. For each pre-extracted entity, the pipeline checks whether there is a cue between the start of the syntagma and the start of the entity, or a following cue between the end of the entity and the end of the proposition. Albeit simple, this algorithm can achieve very good performance depending on the modality. For instance, our eds.negation pipeline reaches 88% F1-score on our dataset. Dealing with pseudo-cues The pipeline can also detect pseudo-cues , ie phrases that contain cues but that are not cues themselves . For instance: sans doute / without doubt contains sans/without , but does not convey negation. Detecting pseudo-cues lets the pipeline filter out any cue that overlaps with a pseudo-cue. Sentence boundaries are required The rule-based algorithm detects cues, and propagate their modulation on the rest of the syntagma . For that reason, a qualifier pipeline needs a sentencizer component to be defined, and will fail otherwise. You may use EDS-NLP's: nlp . add_pipe ( \"eds.sentences\" )","title":"Under the hood"},{"location":"pipelines/qualifiers/#persisting-the-results","text":"Our qualifier pipelines write their results to a custom spaCy extension , defined on both Span and Token objects. We follow the convention of naming said attribute after the pipeline itself, eg Span._.negation for the eds.negation pipeline. In most cases, that extension is a boolean. We also provide a string representation of the result, computed on the fly by declaring a getter that reads the boolean result of the pipeline. Following spaCy convention, we give this attribute the same name, followed by a _ .","title":"Persisting the results"},{"location":"pipelines/qualifiers/family/","text":"Family The eds.family pipeline uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself. Usage The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , ostheoporose = \"osth\u00e9oporose\" )), ) nlp . add_pipe ( \"eds.family\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Il a des ant\u00e9c\u00e9dents familiaux d'osth\u00e9oporose\" ) doc = nlp ( text ) doc . ents # Out: (douleur, osth\u00e9oporose) doc . ents [ 0 ] . _ . family # Out: False doc . ents [ 1 ] . _ . family # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" family Family patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) use_sections Whether to use pre-annotated sections (requires the sections pipeline) False on_ents_only Whether to qualify pre-extracted entities only True explain Whether to keep track of the cues for each entity False Declared extensions The eds.family pipeline declares two spaCy extensions , on both Span and Token objects : The family attribute is a boolean, set to True if the pipeline predicts that the span/token relates to a family member. The family_ property is a human-readable string, computed from the family attribute. It implements a simple getter function that outputs PATIENT or FAMILY , depending on the value of family . Authors and citation The eds.family pipeline was developed by AP-HP's Data Science team.","title":"Family"},{"location":"pipelines/qualifiers/family/#family","text":"The eds.family pipeline uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.","title":"Family"},{"location":"pipelines/qualifiers/family/#usage","text":"The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , ostheoporose = \"osth\u00e9oporose\" )), ) nlp . add_pipe ( \"eds.family\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Il a des ant\u00e9c\u00e9dents familiaux d'osth\u00e9oporose\" ) doc = nlp ( text ) doc . ents # Out: (douleur, osth\u00e9oporose) doc . ents [ 0 ] . _ . family # Out: False doc . ents [ 1 ] . _ . family # Out: True","title":"Usage"},{"location":"pipelines/qualifiers/family/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" family Family patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) use_sections Whether to use pre-annotated sections (requires the sections pipeline) False on_ents_only Whether to qualify pre-extracted entities only True explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/family/#declared-extensions","text":"The eds.family pipeline declares two spaCy extensions , on both Span and Token objects : The family attribute is a boolean, set to True if the pipeline predicts that the span/token relates to a family member. The family_ property is a human-readable string, computed from the family attribute. It implements a simple getter function that outputs PATIENT or FAMILY , depending on the value of family .","title":"Declared extensions"},{"location":"pipelines/qualifiers/family/#authors-and-citation","text":"The eds.family pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/qualifiers/history/","text":"Medical History The eds.history pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit. The mere definition of an medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history , eg preceded by a synonym of \"medical history\". This component may also use the output of: the eds.sections pipeline . In that case, the entire ant\u00e9c\u00e9dent section is tagged as a medical history. Sections Be careful, the eds.sections component may oversize the ant\u00e9c\u00e9dents section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the ant\u00e9c\u00e9dents title, some parts of the document will erroneously be tagged as a medical history. To curb that possibility, using the output of the eds.sections component is deactivated by default. the eds.dates pipeline . In that case, it will take the dates into account to tag extracted entities as a medical history or not. Dates To take the most of the eds.dates component, you may add the note_datetime context (cf. Adding context ). It allows the pipeline to compute the duration of absolute dates (eg le 28 ao\u00fbt 2022/August 28, 2022). The birth_datetime context allows the pipeline to exclude the birth date from the extracted dates. Usage The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) nlp . add_pipe ( \"eds.dates\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , malaise = \"malaises\" )), ) nlp . add_pipe ( \"eds.history\" , config = dict ( use_sections = True , use_dates = True , ), ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Il a des ant\u00e9c\u00e9dents de malaises.\" \"ANT\u00c9C\u00c9DENTS : \" \"- le patient a d\u00e9j\u00e0 eu des malaises. \" \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\" ) doc = nlp ( text ) doc . ents # Out: (douleur, malaises, malaises, douleur) doc . ents [ 0 ] . _ . history # Out: False doc . ents [ 1 ] . _ . history # Out: True doc . ents [ 2 ] . _ . history # (1) # Out: True doc . ents [ 3 ] . _ . history # (2) # Out: False The entity is in the section ant\u00e9c\u00e9dent . The entity is in the section ant\u00e9c\u00e9dent , however the extracted relative_date refers to an event that took place within 14 days. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" history History patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) use_sections Whether to use pre-annotated sections (requires the sections pipeline) False use_dates Whether to use dates pipeline (requires the dates pipeline and note_datetime context is recommended) False history_limit If use_dates = True . The number of days after which the event is considered as history. 14 (2 weeks) exclude_birthdate If use_dates = True . Whether to exclude the birth date from history dates. True closest_dates_only If use_dates = True . Whether to include the closest dates only. If False , it includes all dates in the sentence. True on_ents_only Whether to qualify pre-extracted entities only True explain Whether to keep track of the cues for each entity False Declared extensions The eds.history pipeline declares two spaCy extensions , on both Span and Token objects : The history attribute is a boolean, set to True if the pipeline predicts that the span/token is a medical history. The history_ property is a human-readable string, computed from the history attribute. It implements a simple getter function that outputs CURRENT or ATCD , depending on the value of history . Authors and citation The eds.history pipeline was developed by AP-HP's Data Science team.","title":"Medical History"},{"location":"pipelines/qualifiers/history/#medical-history","text":"The eds.history pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit. The mere definition of an medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history , eg preceded by a synonym of \"medical history\". This component may also use the output of: the eds.sections pipeline . In that case, the entire ant\u00e9c\u00e9dent section is tagged as a medical history. Sections Be careful, the eds.sections component may oversize the ant\u00e9c\u00e9dents section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the ant\u00e9c\u00e9dents title, some parts of the document will erroneously be tagged as a medical history. To curb that possibility, using the output of the eds.sections component is deactivated by default. the eds.dates pipeline . In that case, it will take the dates into account to tag extracted entities as a medical history or not. Dates To take the most of the eds.dates component, you may add the note_datetime context (cf. Adding context ). It allows the pipeline to compute the duration of absolute dates (eg le 28 ao\u00fbt 2022/August 28, 2022). The birth_datetime context allows the pipeline to exclude the birth date from the extracted dates.","title":"Medical History"},{"location":"pipelines/qualifiers/history/#usage","text":"The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) nlp . add_pipe ( \"eds.dates\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , malaise = \"malaises\" )), ) nlp . add_pipe ( \"eds.history\" , config = dict ( use_sections = True , use_dates = True , ), ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Il a des ant\u00e9c\u00e9dents de malaises.\" \"ANT\u00c9C\u00c9DENTS : \" \"- le patient a d\u00e9j\u00e0 eu des malaises. \" \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\" ) doc = nlp ( text ) doc . ents # Out: (douleur, malaises, malaises, douleur) doc . ents [ 0 ] . _ . history # Out: False doc . ents [ 1 ] . _ . history # Out: True doc . ents [ 2 ] . _ . history # (1) # Out: True doc . ents [ 3 ] . _ . history # (2) # Out: False The entity is in the section ant\u00e9c\u00e9dent . The entity is in the section ant\u00e9c\u00e9dent , however the extracted relative_date refers to an event that took place within 14 days.","title":"Usage"},{"location":"pipelines/qualifiers/history/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" history History patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) use_sections Whether to use pre-annotated sections (requires the sections pipeline) False use_dates Whether to use dates pipeline (requires the dates pipeline and note_datetime context is recommended) False history_limit If use_dates = True . The number of days after which the event is considered as history. 14 (2 weeks) exclude_birthdate If use_dates = True . Whether to exclude the birth date from history dates. True closest_dates_only If use_dates = True . Whether to include the closest dates only. If False , it includes all dates in the sentence. True on_ents_only Whether to qualify pre-extracted entities only True explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/history/#declared-extensions","text":"The eds.history pipeline declares two spaCy extensions , on both Span and Token objects : The history attribute is a boolean, set to True if the pipeline predicts that the span/token is a medical history. The history_ property is a human-readable string, computed from the history attribute. It implements a simple getter function that outputs CURRENT or ATCD , depending on the value of history .","title":"Declared extensions"},{"location":"pipelines/qualifiers/history/#authors-and-citation","text":"The eds.history pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/qualifiers/hypothesis/","text":"Hypothesis The eds.hypothesis pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements. Usage The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , fracture = \"fracture\" )), ) nlp . add_pipe ( \"eds.hypothesis\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Possible fracture du radius.\" ) doc = nlp ( text ) doc . ents # Out: (douleur, fracture) doc . ents [ 0 ] . _ . hypothesis # Out: False doc . ents [ 1 ] . _ . hypothesis # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-hypothesis patterns None (use pre-defined patterns) preceding Preceding hypothesis patterns None (use pre-defined patterns) following Following hypothesis patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs_hyp Patterns for verbs that imply a hypothesis None (use pre-defined patterns) verbs_eds Common verb patterns, checked for conditional mode None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for hypothesis within entities False explain Whether to keep track of the cues for each entity False Declared extensions The eds.hypothesis pipeline declares two spaCy extensions , on both Span and Token objects : The hypothesis attribute is a boolean, set to True if the pipeline predicts that the span/token is a speculation. The hypothesis_ property is a human-readable string, computed from the hypothesis attribute. It implements a simple getter function that outputs HYP or CERT , depending on the value of hypothesis . Performance The pipeline's performance is measured on three datasets : The ESSAI 1 and CAS 2 datasets were developed at the CNRS. The two are concatenated. The NegParHyp corpus was specifically developed at EDS to test the pipeline on actual clinical notes, using pseudonymised notes from the EDS. Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% NegParHyp corpus The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context. Authors and citation The eds.hypothesis pipeline was developed by AP-HP's Data Science team. Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale , 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637 . \u21a9 Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis , Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096 . \u21a9","title":"Hypothesis"},{"location":"pipelines/qualifiers/hypothesis/#hypothesis","text":"The eds.hypothesis pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.","title":"Hypothesis"},{"location":"pipelines/qualifiers/hypothesis/#usage","text":"The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( douleur = \"douleur\" , fracture = \"fracture\" )), ) nlp . add_pipe ( \"eds.hypothesis\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Possible fracture du radius.\" ) doc = nlp ( text ) doc . ents # Out: (douleur, fracture) doc . ents [ 0 ] . _ . hypothesis # Out: False doc . ents [ 1 ] . _ . hypothesis # Out: True","title":"Usage"},{"location":"pipelines/qualifiers/hypothesis/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-hypothesis patterns None (use pre-defined patterns) preceding Preceding hypothesis patterns None (use pre-defined patterns) following Following hypothesis patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs_hyp Patterns for verbs that imply a hypothesis None (use pre-defined patterns) verbs_eds Common verb patterns, checked for conditional mode None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for hypothesis within entities False explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/hypothesis/#declared-extensions","text":"The eds.hypothesis pipeline declares two spaCy extensions , on both Span and Token objects : The hypothesis attribute is a boolean, set to True if the pipeline predicts that the span/token is a speculation. The hypothesis_ property is a human-readable string, computed from the hypothesis attribute. It implements a simple getter function that outputs HYP or CERT , depending on the value of hypothesis .","title":"Declared extensions"},{"location":"pipelines/qualifiers/hypothesis/#performance","text":"The pipeline's performance is measured on three datasets : The ESSAI 1 and CAS 2 datasets were developed at the CNRS. The two are concatenated. The NegParHyp corpus was specifically developed at EDS to test the pipeline on actual clinical notes, using pseudonymised notes from the EDS. Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% NegParHyp corpus The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.","title":"Performance"},{"location":"pipelines/qualifiers/hypothesis/#authors-and-citation","text":"The eds.hypothesis pipeline was developed by AP-HP's Data Science team. Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale , 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637 . \u21a9 Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis , Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096 . \u21a9","title":"Authors and citation"},{"location":"pipelines/qualifiers/negation/","text":"Negation The eds.negation pipeline uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al 1 . Usage The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( patient = \"patient\" , fracture = \"fracture\" )), ) nlp . add_pipe ( \"eds.negation\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Le scanner ne d\u00e9tecte aucune fracture.\" ) doc = nlp ( text ) doc . ents # Out: (patient, fracture) doc . ents [ 0 ] . _ . negation # (1) # Out: False doc . ents [ 1 ] . _ . negation # Out: True The result of the pipeline is kept in the negation custom extension. Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-negation patterns None (use pre-defined patterns) preceding Preceding negation patterns None (use pre-defined patterns) following Following negation patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs Patterns for verbs that imply a negation None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for negations within entities False explain Whether to keep track of the cues for each entity False Declared extensions The eds.negation pipeline declares two spaCy extensions , on both Span and Token objects : The negation attribute is a boolean, set to True if the pipeline predicts that the span/token is negated. The negation_ property is a human-readable string, computed from the negation attribute. It implements a simple getter function that outputs AFF or NEG , depending on the value of negation . Performance The pipeline's performance is measured on three datasets : The ESSAI 2 and CAS 3 datasets were developed at the CNRS. The two are concatenated. The NegParHyp corpus was specifically developed at AP-HP to test the pipeline on actual clinical notes, using pseudonymised notes from the AP-HP. Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% NegParHyp corpus The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context. Authors and citation The eds.negation pipeline was developed by AP-HP's Data Science team. Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics , 34(5):301\u2013310, October 2001. URL: https://linkinghub.elsevier.com/retrieve/pii/S1532046401910299 (visited on 2020-12-31), doi:10.1006/jbin.2001.1029 . \u21a9 Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale , 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637 . \u21a9 Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis , Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096 . \u21a9","title":"Negation"},{"location":"pipelines/qualifiers/negation/#negation","text":"The eds.negation pipeline uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al 1 .","title":"Negation"},{"location":"pipelines/qualifiers/negation/#usage","text":"The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( patient = \"patient\" , fracture = \"fracture\" )), ) nlp . add_pipe ( \"eds.negation\" ) text = ( \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \" \"Le scanner ne d\u00e9tecte aucune fracture.\" ) doc = nlp ( text ) doc . ents # Out: (patient, fracture) doc . ents [ 0 ] . _ . negation # (1) # Out: False doc . ents [ 1 ] . _ . negation # Out: True The result of the pipeline is kept in the negation custom extension.","title":"Usage"},{"location":"pipelines/qualifiers/negation/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-negation patterns None (use pre-defined patterns) preceding Preceding negation patterns None (use pre-defined patterns) following Following negation patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs Patterns for verbs that imply a negation None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for negations within entities False explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/negation/#declared-extensions","text":"The eds.negation pipeline declares two spaCy extensions , on both Span and Token objects : The negation attribute is a boolean, set to True if the pipeline predicts that the span/token is negated. The negation_ property is a human-readable string, computed from the negation attribute. It implements a simple getter function that outputs AFF or NEG , depending on the value of negation .","title":"Declared extensions"},{"location":"pipelines/qualifiers/negation/#performance","text":"The pipeline's performance is measured on three datasets : The ESSAI 2 and CAS 3 datasets were developed at the CNRS. The two are concatenated. The NegParHyp corpus was specifically developed at AP-HP to test the pipeline on actual clinical notes, using pseudonymised notes from the AP-HP. Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% NegParHyp corpus The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.","title":"Performance"},{"location":"pipelines/qualifiers/negation/#authors-and-citation","text":"The eds.negation pipeline was developed by AP-HP's Data Science team. Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics , 34(5):301\u2013310, October 2001. URL: https://linkinghub.elsevier.com/retrieve/pii/S1532046401910299 (visited on 2020-12-31), doi:10.1006/jbin.2001.1029 . \u21a9 Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale , 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637 . \u21a9 Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis , Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096 . \u21a9","title":"Authors and citation"},{"location":"pipelines/qualifiers/reported-speech/","text":"Reported Speech The eds.reported_speech pipeline uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS. Usage The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( patient = \"patient\" , alcool = \"alcoolis\u00e9\" )), ) nlp . add_pipe ( \"eds.reported_speech\" ) text = ( \"Le patient est admis aux urgences ce soir pour une douleur au bras. \" \"Il nie \u00eatre alcoolis\u00e9.\" ) doc = nlp ( text ) doc . ents # Out: (patient, alcoolis\u00e9) doc . ents [ 0 ] . _ . reported_speech # Out: False doc . ents [ 1 ] . _ . reported_speech # Out: True Configuration The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-reported speech patterns None (use pre-defined patterns) preceding Preceding reported speech patterns None (use pre-defined patterns) following Following reported speech patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs Patterns for verbs that imply a reported speech None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for reported speech within entities False explain Whether to keep track of the cues for each entity False Declared extensions The eds.reported_speech pipeline declares two spaCy extensions , on both Span and Token objects : The reported_speech attribute is a boolean, set to True if the pipeline predicts that the span/token is reported. The reported_speech_ property is a human-readable string, computed from the reported_speech attribute. It implements a simple getter function that outputs DIRECT or REPORTED , depending on the value of reported_speech . Authors and citation The eds.reported_speech pipeline was developed by AP-HP's Data Science team.","title":"Reported Speech"},{"location":"pipelines/qualifiers/reported-speech/#reported-speech","text":"The eds.reported_speech pipeline uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.","title":"Reported Speech"},{"location":"pipelines/qualifiers/reported-speech/#usage","text":"The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is . import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # Dummy matcher nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( patient = \"patient\" , alcool = \"alcoolis\u00e9\" )), ) nlp . add_pipe ( \"eds.reported_speech\" ) text = ( \"Le patient est admis aux urgences ce soir pour une douleur au bras. \" \"Il nie \u00eatre alcoolis\u00e9.\" ) doc = nlp ( text ) doc . ents # Out: (patient, alcoolis\u00e9) doc . ents [ 0 ] . _ . reported_speech # Out: False doc . ents [ 1 ] . _ . reported_speech # Out: True","title":"Usage"},{"location":"pipelines/qualifiers/reported-speech/#configuration","text":"The pipeline can be configured using the following parameters : Parameter Explanation Default attr spaCy attribute to match on (eg NORM , TEXT , LOWER ) \"NORM\" pseudo Pseudo-reported speech patterns None (use pre-defined patterns) preceding Preceding reported speech patterns None (use pre-defined patterns) following Following reported speech patterns None (use pre-defined patterns) termination Termination patterns (for syntagma/proposition extraction) None (use pre-defined patterns) verbs Patterns for verbs that imply a reported speech None (use pre-defined patterns) on_ents_only Whether to qualify pre-extracted entities only True within_ents Whether to look for reported speech within entities False explain Whether to keep track of the cues for each entity False","title":"Configuration"},{"location":"pipelines/qualifiers/reported-speech/#declared-extensions","text":"The eds.reported_speech pipeline declares two spaCy extensions , on both Span and Token objects : The reported_speech attribute is a boolean, set to True if the pipeline predicts that the span/token is reported. The reported_speech_ property is a human-readable string, computed from the reported_speech attribute. It implements a simple getter function that outputs DIRECT or REPORTED , depending on the value of reported_speech .","title":"Declared extensions"},{"location":"pipelines/qualifiers/reported-speech/#authors-and-citation","text":"The eds.reported_speech pipeline was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"pipelines/trainable/","text":"Trainable components overview In addition to its rule-based pipeline components, EDS-NLP offers new trainable pipelines to fit and run machine learning models for classic biomedical information extraction tasks. Available components : Name Description eds.nested_ner Recognize overlapping or nested entities (replaces spaCy's ner component) Writing custom models spaCy models can be written with Thinc (spaCy's deep learning library), Tensorflow or Pytorch. As Pytorch is predominant in the NLP research field, we recommend writing models with the latter to facilitate interactions with the NLP community. To this end, we have written some Pytorch wrapping utilities like wrap_pytorch_model to allow loss and predictions to be computed directly in the Pytorch module. Utils Training In addition to the spaCy train CLI, EDS-NLP offers a train function that can be called in Python directly with an existing spaCy pipeline. Experimental This training API is an experimental feature of edsnlp and could change at any time. Usage Let us define and train a full pipeline : from pathlib import Path import spacy from edsnlp.connectors.brat import BratConnector from edsnlp.utils.training import train , make_spacy_corpus_config tmp_path = Path ( \"/tmp/test-train\" ) nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"nested_ner\" ) # (1) # Train the model, with additional training configuration nlp = train ( nlp , output_path = tmp_path / \"model\" , config = dict ( ** make_spacy_corpus_config ( train_data = \"/path/to/the/training/set/brat/files\" , dev_data = \"/path/to/the/dev/set/brat/files\" , nlp = nlp , data_format = \"brat\" , ), training = dict ( max_steps = 4000 , ), ), ) # Finally, we can run the pipeline on a new document doc = nlp ( \"Arret du folfox si inefficace\" ) doc . spans [ \"drug\" ] # Out: [folfox] doc . spans [ \"criteria\" ] # Out: [si folfox inefficace] # And export new predictions as Brat annotations predicted_docs = BratConnector ( \"/path/to/the/new/files\" , run_pipe = True ) . brat2docs ( nlp ) BratConnector ( \"/path/to/predictions\" ) . docs2brat ( predicted_docs ) you can configure the component using the add_pipe(..., config=...) parameter","title":"Trainable components overview"},{"location":"pipelines/trainable/#trainable-components-overview","text":"In addition to its rule-based pipeline components, EDS-NLP offers new trainable pipelines to fit and run machine learning models for classic biomedical information extraction tasks.","title":"Trainable components overview"},{"location":"pipelines/trainable/#available-components","text":"Name Description eds.nested_ner Recognize overlapping or nested entities (replaces spaCy's ner component) Writing custom models spaCy models can be written with Thinc (spaCy's deep learning library), Tensorflow or Pytorch. As Pytorch is predominant in the NLP research field, we recommend writing models with the latter to facilitate interactions with the NLP community. To this end, we have written some Pytorch wrapping utilities like wrap_pytorch_model to allow loss and predictions to be computed directly in the Pytorch module.","title":"Available components :"},{"location":"pipelines/trainable/#utils","text":"","title":"Utils"},{"location":"pipelines/trainable/#training","text":"In addition to the spaCy train CLI, EDS-NLP offers a train function that can be called in Python directly with an existing spaCy pipeline. Experimental This training API is an experimental feature of edsnlp and could change at any time.","title":"Training"},{"location":"pipelines/trainable/#usage","text":"Let us define and train a full pipeline : from pathlib import Path import spacy from edsnlp.connectors.brat import BratConnector from edsnlp.utils.training import train , make_spacy_corpus_config tmp_path = Path ( \"/tmp/test-train\" ) nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"nested_ner\" ) # (1) # Train the model, with additional training configuration nlp = train ( nlp , output_path = tmp_path / \"model\" , config = dict ( ** make_spacy_corpus_config ( train_data = \"/path/to/the/training/set/brat/files\" , dev_data = \"/path/to/the/dev/set/brat/files\" , nlp = nlp , data_format = \"brat\" , ), training = dict ( max_steps = 4000 , ), ), ) # Finally, we can run the pipeline on a new document doc = nlp ( \"Arret du folfox si inefficace\" ) doc . spans [ \"drug\" ] # Out: [folfox] doc . spans [ \"criteria\" ] # Out: [si folfox inefficace] # And export new predictions as Brat annotations predicted_docs = BratConnector ( \"/path/to/the/new/files\" , run_pipe = True ) . brat2docs ( nlp ) BratConnector ( \"/path/to/predictions\" ) . docs2brat ( predicted_docs ) you can configure the component using the add_pipe(..., config=...) parameter","title":"Usage"},{"location":"pipelines/trainable/ner/","text":"Nested Named Entity Recognition The default spaCy Named Entity Recognizer (NER) pipeline only allows flat entity recognition, meaning that overlapping and nested entities cannot be extracted. The other spaCy component SpanCategorizer only supports assigning to a specific span group and both components are not well suited for extracting entities with ill-defined boundaries (this can occur if your training data contains difficult and long entities). We propose the new eds.ner component to extract almost any named entity: flat entities like spaCy's EntityRecognizer overlapping entities overlapping entities of different labels (much like spaCy's SpanCategorizer ) entities will ill-defined boundaries However, the model cannot currently extract entities that are nested inside larger entities of the same label. The pipeline assigns both doc.ents (in which overlapping entities are filtered out) and doc.spans . Architecture The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts $n_{labels}$ sequences of O, I, B, L, U tags. The architecture is displayed in the figure below. To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction. Nested NER architecture Usage Let us define the pipeline and train it: from pathlib import Path import spacy from edsnlp.connectors.brat import BratConnector from edsnlp.utils.training import train , make_spacy_corpus_config tmp_path = Path ( \"/tmp/test-nested-ner\" ) nlp = spacy . blank ( \"eds\" ) # \u2193 below is the nested ner pipeline \u2193 # you can configure it using the `add_pipe(..., config=...)` parameter nlp . add_pipe ( \"nested_ner\" ) # Train the model, with additional training configuration nlp = train ( nlp , output_path = tmp_path / \"model\" , config = dict ( ** make_spacy_corpus_config ( train_data = \"/path/to/the/training/set/brat/files\" , dev_data = \"/path/to/the/dev/set/brat/files\" , nlp = nlp , data_format = \"brat\" , ), training = dict ( max_steps = 4000 , ), ), ) # Finally, we can run the pipeline on a new document doc = nlp ( \"Arret du folfox si inefficace\" ) doc . spans [ \"drug\" ] # Out: [folfox] doc . spans [ \"criteria\" ] # Out: [si folfox inefficace] # And export new predictions as Brat annotations predicted_docs = BratConnector ( \"/path/to/the/new/files\" , run_pipe = True ) . brat2docs ( nlp ) BratConnector ( \"/path/to/predictions\" ) . docs2brat ( predicted_docs ) Configuration The pipeline component can be configured using the following parameters : Parameter Explanation Default ent_labels Labels to search in and assign to doc.ents . Expects a list. None (inits to all labels in doc.ents ) spans_labels Labels to search in and assign to doc.spans . Expects a dict of lists. None (inits to all span groups and their labels in doc.spans ) The default model eds.nested_ner_model.v1 can be configured using the following parameters : Parameter Explanation Default loss_mode How the CRF loss is computed joint \u2192 joint Loss accounts for CRF transitions \u2192 independent Loss does not account for CRF transitions (softmax loss) \u2192 marginal Tag scores are smoothly updated with CRF transitions, and softmax loss is applied Authors and citation The eds.nested_ner pipeline was developed by AP-HP's Data Science team. The deep learning model was adapted from Wajsb\u00fcrt 1 Perceval Wajsb\u00fcrt. Extraction and normalization of simple and structured entities in medical documents . Theses, Sorbonne Universit\u00e9, December 2021. URL: https://hal.archives-ouvertes.fr/tel-03624928 . \u21a9","title":"Nested Named Entity Recognition"},{"location":"pipelines/trainable/ner/#nested-named-entity-recognition","text":"The default spaCy Named Entity Recognizer (NER) pipeline only allows flat entity recognition, meaning that overlapping and nested entities cannot be extracted. The other spaCy component SpanCategorizer only supports assigning to a specific span group and both components are not well suited for extracting entities with ill-defined boundaries (this can occur if your training data contains difficult and long entities). We propose the new eds.ner component to extract almost any named entity: flat entities like spaCy's EntityRecognizer overlapping entities overlapping entities of different labels (much like spaCy's SpanCategorizer ) entities will ill-defined boundaries However, the model cannot currently extract entities that are nested inside larger entities of the same label. The pipeline assigns both doc.ents (in which overlapping entities are filtered out) and doc.spans .","title":"Nested Named Entity Recognition"},{"location":"pipelines/trainable/ner/#architecture","text":"The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts $n_{labels}$ sequences of O, I, B, L, U tags. The architecture is displayed in the figure below. To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction. Nested NER architecture","title":"Architecture"},{"location":"pipelines/trainable/ner/#usage","text":"Let us define the pipeline and train it: from pathlib import Path import spacy from edsnlp.connectors.brat import BratConnector from edsnlp.utils.training import train , make_spacy_corpus_config tmp_path = Path ( \"/tmp/test-nested-ner\" ) nlp = spacy . blank ( \"eds\" ) # \u2193 below is the nested ner pipeline \u2193 # you can configure it using the `add_pipe(..., config=...)` parameter nlp . add_pipe ( \"nested_ner\" ) # Train the model, with additional training configuration nlp = train ( nlp , output_path = tmp_path / \"model\" , config = dict ( ** make_spacy_corpus_config ( train_data = \"/path/to/the/training/set/brat/files\" , dev_data = \"/path/to/the/dev/set/brat/files\" , nlp = nlp , data_format = \"brat\" , ), training = dict ( max_steps = 4000 , ), ), ) # Finally, we can run the pipeline on a new document doc = nlp ( \"Arret du folfox si inefficace\" ) doc . spans [ \"drug\" ] # Out: [folfox] doc . spans [ \"criteria\" ] # Out: [si folfox inefficace] # And export new predictions as Brat annotations predicted_docs = BratConnector ( \"/path/to/the/new/files\" , run_pipe = True ) . brat2docs ( nlp ) BratConnector ( \"/path/to/predictions\" ) . docs2brat ( predicted_docs )","title":"Usage"},{"location":"pipelines/trainable/ner/#configuration","text":"The pipeline component can be configured using the following parameters : Parameter Explanation Default ent_labels Labels to search in and assign to doc.ents . Expects a list. None (inits to all labels in doc.ents ) spans_labels Labels to search in and assign to doc.spans . Expects a dict of lists. None (inits to all span groups and their labels in doc.spans ) The default model eds.nested_ner_model.v1 can be configured using the following parameters : Parameter Explanation Default loss_mode How the CRF loss is computed joint \u2192 joint Loss accounts for CRF transitions \u2192 independent Loss does not account for CRF transitions (softmax loss) \u2192 marginal Tag scores are smoothly updated with CRF transitions, and softmax loss is applied","title":"Configuration"},{"location":"pipelines/trainable/ner/#authors-and-citation","text":"The eds.nested_ner pipeline was developed by AP-HP's Data Science team. The deep learning model was adapted from Wajsb\u00fcrt 1 Perceval Wajsb\u00fcrt. Extraction and normalization of simple and structured entities in medical documents . Theses, Sorbonne Universit\u00e9, December 2021. URL: https://hal.archives-ouvertes.fr/tel-03624928 . \u21a9","title":"Authors and citation"},{"location":"reference/","text":"edsnlp EDS-NLP __version__ = '0.7.2' module-attribute BASE_DIR = Path ( __file__ ) . parent module-attribute","title":"`edsnlp`"},{"location":"reference/#edsnlp","text":"EDS-NLP","title":"edsnlp"},{"location":"reference/#edsnlp.__version__","text":"","title":"__version__"},{"location":"reference/#edsnlp.BASE_DIR","text":"","title":"BASE_DIR"},{"location":"reference/SUMMARY/","text":"edsnlp components conjugator connectors brat labeltool omop extensions language matchers regex simstring utils offset text models pytorch_wrapper stack_crf_ner torch crf pipelines base core context context factory contextual_matcher contextual_matcher factory models endlines endlines endlinesmodel factory functional matcher factory matcher normalizer accents accents factory patterns factory lowercase factory normalizer pollution factory patterns pollution quotes factory patterns quotes sentences factory terms terminology factory terminology factories misc consultation_dates consultation_dates factory patterns dates dates factory models patterns absolute atomic days delimiters directions modes months numbers time units years current duration false_positive relative measurements factory measurements patterns reason factory patterns reason sections factory patterns sections ner adicap adicap factory models patterns cim10 factory patterns covid factory patterns drugs factory patterns scores base_score charlson factory patterns elstonellis factory patterns emergency ccmu factory patterns gemsa factory patterns priority factory patterns factory sofa factory patterns sofa tnm factory models patterns tnm umls factory patterns qualifiers base factories family factory family patterns history factory history patterns hypothesis factory hypothesis patterns negation factory negation patterns reported_speech factory patterns reported_speech terminations trainable nested_ner processing distributed helpers parallel simple utils wrapper utils blocs colors deprecation examples filter inclusion lists merge_configs regex resources training","title":"SUMMARY"},{"location":"reference/components/","text":"edsnlp.components","title":"components"},{"location":"reference/components/#edsnlpcomponents","text":"","title":"edsnlp.components"},{"location":"reference/conjugator/","text":"edsnlp.conjugator conjugate_verb ( verb , conjugator ) Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas DataFrame . PARAMETER DESCRIPTION verb Verb to conjugate. TYPE: str conjugator mlconjug3 instance for conjugating. TYPE: mlconjug3.Conjugator RETURNS DESCRIPTION pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. Source code in edsnlp/conjugator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def conjugate_verb ( verb : str , conjugator : mlconjug3 . Conjugator , ) -> pd . DataFrame : \"\"\" Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas `DataFrame`. Parameters ---------- verb : str Verb to conjugate. conjugator : mlconjug3.Conjugator mlconjug3 instance for conjugating. Returns ------- pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. \"\"\" df = pd . DataFrame ( conjugator . conjugate ( verb ) . iterate (), columns = [ \"mode\" , \"tense\" , \"person\" , \"term\" ], ) df . term = df . term . fillna ( df . person ) df . loc [ df . person == df . term , \"person\" ] = None df . insert ( 0 , \"verb\" , verb ) return df conjugate ( verbs , language = 'fr' ) Conjugate a list of verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate TYPE: Union[str, List[str]] language Language to conjugate. Defaults to French ( fr ). TYPE: str DEFAULT: 'fr' RETURNS DESCRIPTION pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: verb , mode , tense , person , term Source code in edsnlp/conjugator.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def conjugate ( verbs : Union [ str , List [ str ]], language : str = \"fr\" , ) -> pd . DataFrame : \"\"\" Conjugate a list of verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate language: str Language to conjugate. Defaults to French (`fr`). Returns ------- pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: `verb`, `mode`, `tense`, `person`, `term` \"\"\" if isinstance ( verbs , str ): verbs = [ verbs ] conjugator = mlconjug3 . Conjugator ( language = language ) df = pd . concat ([ conjugate_verb ( verb , conjugator = conjugator ) for verb in verbs ]) df = df . reset_index ( drop = True ) return df get_conjugated_verbs ( verbs , matches , language = 'fr' ) Get a list of conjugated verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate. TYPE: Union[str, List[str]] matches List of dictionary describing the mode/tense/persons to keep. TYPE: Union[List[Dict[str, str]], Dict[str, str]] language [description], by default \"fr\" (French) TYPE: str, optional DEFAULT: 'fr' RETURNS DESCRIPTION List[str] List of terms to look for. Examples: >>> get_conjugated_verbs ( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] Source code in edsnlp/conjugator.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_conjugated_verbs ( verbs : Union [ str , List [ str ]], matches : Union [ List [ Dict [ str , str ]], Dict [ str , str ]], language : str = \"fr\" , ) -> List [ str ]: \"\"\" Get a list of conjugated verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate. matches : Union[List[Dict[str, str]], Dict[str, str]] List of dictionary describing the mode/tense/persons to keep. language : str, optional [description], by default \"fr\" (French) Returns ------- List[str] List of terms to look for. Examples -------- >>> get_conjugated_verbs( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] \"\"\" if isinstance ( matches , dict ): matches = [ matches ] terms = [] df = conjugate ( verbs = verbs , language = language , ) for match in matches : q = \" & \" . join ([ f ' { k } == \" { v } \"' for k , v in match . items ()]) terms . extend ( df . query ( q ) . term . unique ()) return list ( set ( terms ))","title":"conjugator"},{"location":"reference/conjugator/#edsnlpconjugator","text":"","title":"edsnlp.conjugator"},{"location":"reference/conjugator/#edsnlp.conjugator.conjugate_verb","text":"Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas DataFrame . PARAMETER DESCRIPTION verb Verb to conjugate. TYPE: str conjugator mlconjug3 instance for conjugating. TYPE: mlconjug3.Conjugator RETURNS DESCRIPTION pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. Source code in edsnlp/conjugator.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def conjugate_verb ( verb : str , conjugator : mlconjug3 . Conjugator , ) -> pd . DataFrame : \"\"\" Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas `DataFrame`. Parameters ---------- verb : str Verb to conjugate. conjugator : mlconjug3.Conjugator mlconjug3 instance for conjugating. Returns ------- pd.DataFrame Normalized dataframe containing all conjugated forms for the verb. \"\"\" df = pd . DataFrame ( conjugator . conjugate ( verb ) . iterate (), columns = [ \"mode\" , \"tense\" , \"person\" , \"term\" ], ) df . term = df . term . fillna ( df . person ) df . loc [ df . person == df . term , \"person\" ] = None df . insert ( 0 , \"verb\" , verb ) return df","title":"conjugate_verb()"},{"location":"reference/conjugator/#edsnlp.conjugator.conjugate","text":"Conjugate a list of verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate TYPE: Union[str, List[str]] language Language to conjugate. Defaults to French ( fr ). TYPE: str DEFAULT: 'fr' RETURNS DESCRIPTION pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: verb , mode , tense , person , term Source code in edsnlp/conjugator.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def conjugate ( verbs : Union [ str , List [ str ]], language : str = \"fr\" , ) -> pd . DataFrame : \"\"\" Conjugate a list of verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate language: str Language to conjugate. Defaults to French (`fr`). Returns ------- pd.DataFrame Dataframe containing the conjugations for the provided verbs. Columns: `verb`, `mode`, `tense`, `person`, `term` \"\"\" if isinstance ( verbs , str ): verbs = [ verbs ] conjugator = mlconjug3 . Conjugator ( language = language ) df = pd . concat ([ conjugate_verb ( verb , conjugator = conjugator ) for verb in verbs ]) df = df . reset_index ( drop = True ) return df","title":"conjugate()"},{"location":"reference/conjugator/#edsnlp.conjugator.get_conjugated_verbs","text":"Get a list of conjugated verbs. PARAMETER DESCRIPTION verbs List of verbs to conjugate. TYPE: Union[str, List[str]] matches List of dictionary describing the mode/tense/persons to keep. TYPE: Union[List[Dict[str, str]], Dict[str, str]] language [description], by default \"fr\" (French) TYPE: str, optional DEFAULT: 'fr' RETURNS DESCRIPTION List[str] List of terms to look for. Examples: >>> get_conjugated_verbs ( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] Source code in edsnlp/conjugator.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_conjugated_verbs ( verbs : Union [ str , List [ str ]], matches : Union [ List [ Dict [ str , str ]], Dict [ str , str ]], language : str = \"fr\" , ) -> List [ str ]: \"\"\" Get a list of conjugated verbs. Parameters ---------- verbs : Union[str, List[str]] List of verbs to conjugate. matches : Union[List[Dict[str, str]], Dict[str, str]] List of dictionary describing the mode/tense/persons to keep. language : str, optional [description], by default \"fr\" (French) Returns ------- List[str] List of terms to look for. Examples -------- >>> get_conjugated_verbs( \"aimer\", dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"), ) ['aimons'] \"\"\" if isinstance ( matches , dict ): matches = [ matches ] terms = [] df = conjugate ( verbs = verbs , language = language , ) for match in matches : q = \" & \" . join ([ f ' { k } == \" { v } \"' for k , v in match . items ()]) terms . extend ( df . query ( q ) . term . unique ()) return list ( set ( terms ))","title":"get_conjugated_verbs()"},{"location":"reference/extensions/","text":"edsnlp.extensions","title":"extensions"},{"location":"reference/extensions/#edsnlpextensions","text":"","title":"edsnlp.extensions"},{"location":"reference/language/","text":"edsnlp.language __all__ = [ 'EDSLanguage' ] module-attribute EDSDefaults Bases: FrenchDefaults Defaults for the EDSLanguage class Mostly identical to the FrenchDefaults, but without tokenization info Source code in edsnlp/language.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class EDSDefaults ( FrenchDefaults ): \"\"\" Defaults for the EDSLanguage class Mostly identical to the FrenchDefaults, but without tokenization info \"\"\" tokenizer_exceptions = {} infixes = [] lex_attr_getters = LEX_ATTRS syntax_iterators = SYNTAX_ITERATORS stop_words = STOP_WORDS config = FrenchDefaults . config . merge ( { \"nlp\" : { \"tokenizer\" : { \"@tokenizers\" : \"eds.tokenizer\" }}, } ) tokenizer_exceptions = {} class-attribute infixes = [] class-attribute lex_attr_getters = LEX_ATTRS class-attribute syntax_iterators = SYNTAX_ITERATORS class-attribute stop_words = STOP_WORDS class-attribute config = FrenchDefaults . config . merge ({ 'nlp' : { 'tokenizer' : { '@tokenizers' : 'eds.tokenizer' }}}) class-attribute EDSLanguage Bases: French French clinical language. It is shipped with the EDSTokenizer tokenizer that better handles tokenization for French clinical documents Source code in edsnlp/language.py 32 33 34 35 36 37 38 39 40 41 42 @spacy . registry . languages ( \"eds\" ) class EDSLanguage ( French ): \"\"\" French clinical language. It is shipped with the `EDSTokenizer` tokenizer that better handles tokenization for French clinical documents \"\"\" lang = \"eds\" Defaults = EDSDefaults default_config = Defaults lang = 'eds' class-attribute Defaults = EDSDefaults class-attribute default_config = Defaults class-attribute EDSTokenizer Bases: DummyTokenizer Source code in edsnlp/language.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class EDSTokenizer ( DummyTokenizer ): def __init__ ( self , vocab : Vocab ) -> None : \"\"\" Tokenizer class for French clinical documents. It better handles tokenization around: - numbers: \"ACR5\" -> [\"ACR\", \"5\"] instead of [\"ACR5\"] - newlines: \"\\n \\n \\n\" -> [\"\\n\", \"\\n\", \"\\n\"] instead of [\"\\n \\n \\n\"] and should be around 5-6 times faster than its standard French counterpart. Parameters ---------- vocab: Vocab The spacy vocabulary \"\"\" self . vocab = vocab punct = \"[:punct:]\" + \" \\\" '\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\" num_like = r \"\\d+(?:[.,]\\d+)?\" default = rf \"[^\\d { punct } '\\n[[:space:]]+(?:['\u02ca](?=[[:alpha:]]|$))?\" self . word_regex = regex . compile ( rf \"( { num_like } |[ { punct } ]|[\\n\\r\\t]|[^\\S\\r\\n\\t]+| { default } )([^\\S\\r\\n\\t])?\" ) def __call__ ( self , text : str ) -> Doc : \"\"\" Tokenizes the text using the EDSTokenizer Parameters ---------- text: str Returns ------- Doc \"\"\" last = 0 words = [] whitespaces = [] for match in self . word_regex . finditer ( text ): begin , end = match . start (), match . end () if last != begin : logger . warning ( \"Missed some characters during\" + f \" tokenization between { last } and { begin } : \" + text [ last - 10 : last ] + \"|\" + text [ last : begin ] + \"|\" + text [ begin : begin + 10 ], ) last = end words . append ( match . group ( 1 )) whitespaces . append ( bool ( match . group ( 2 ))) return Doc ( self . vocab , words = words , spaces = whitespaces ) vocab = vocab instance-attribute word_regex = regex . compile ( '( {num_like} |[ {punct} ]|[ \\\\ n \\\\ r \\\\ t]|[^ \\\\ S \\\\ r \\\\ n \\\\ t]+| {default} )([^ \\\\ S \\\\ r \\\\ n \\\\ t])?' ) instance-attribute __init__ ( vocab ) Tokenizer class for French clinical documents. It better handles tokenization around: - numbers: \"ACR5\" -> [\"ACR\", \"5\"] instead of [\"ACR5\"] - newlines: \" \" -> [\" \", \" \", \" \"] instead of [\" \"] and should be around 5-6 times faster than its standard French counterpart. Parameters ---------- vocab: Vocab The spacy vocabulary Source code in edsnlp/language.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , vocab : Vocab ) -> None : \"\"\" Tokenizer class for French clinical documents. It better handles tokenization around: - numbers: \"ACR5\" -> [\"ACR\", \"5\"] instead of [\"ACR5\"] - newlines: \"\\n \\n \\n\" -> [\"\\n\", \"\\n\", \"\\n\"] instead of [\"\\n \\n \\n\"] and should be around 5-6 times faster than its standard French counterpart. Parameters ---------- vocab: Vocab The spacy vocabulary \"\"\" self . vocab = vocab punct = \"[:punct:]\" + \" \\\" '\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\" num_like = r \"\\d+(?:[.,]\\d+)?\" default = rf \"[^\\d { punct } '\\n[[:space:]]+(?:['\u02ca](?=[[:alpha:]]|$))?\" self . word_regex = regex . compile ( rf \"( { num_like } |[ { punct } ]|[\\n\\r\\t]|[^\\S\\r\\n\\t]+| { default } )([^\\S\\r\\n\\t])?\" ) __call__ ( text ) Tokenizes the text using the EDSTokenizer PARAMETER DESCRIPTION text TYPE: str RETURNS DESCRIPTION Doc Source code in edsnlp/language.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __call__ ( self , text : str ) -> Doc : \"\"\" Tokenizes the text using the EDSTokenizer Parameters ---------- text: str Returns ------- Doc \"\"\" last = 0 words = [] whitespaces = [] for match in self . word_regex . finditer ( text ): begin , end = match . start (), match . end () if last != begin : logger . warning ( \"Missed some characters during\" + f \" tokenization between { last } and { begin } : \" + text [ last - 10 : last ] + \"|\" + text [ last : begin ] + \"|\" + text [ begin : begin + 10 ], ) last = end words . append ( match . group ( 1 )) whitespaces . append ( bool ( match . group ( 2 ))) return Doc ( self . vocab , words = words , spaces = whitespaces ) create_eds_tokenizer () Creates a factory that returns new EDSTokenizer instances RETURNS DESCRIPTION EDSTokenizer Source code in edsnlp/language.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @spacy . registry . tokenizers ( \"eds.tokenizer\" ) def create_eds_tokenizer (): \"\"\" Creates a factory that returns new EDSTokenizer instances Returns ------- EDSTokenizer \"\"\" def eds_tokenizer_factory ( nlp ): return EDSTokenizer ( nlp . vocab ) return eds_tokenizer_factory","title":"language"},{"location":"reference/language/#edsnlplanguage","text":"","title":"edsnlp.language"},{"location":"reference/language/#edsnlp.language.__all__","text":"","title":"__all__"},{"location":"reference/language/#edsnlp.language.EDSDefaults","text":"Bases: FrenchDefaults Defaults for the EDSLanguage class Mostly identical to the FrenchDefaults, but without tokenization info Source code in edsnlp/language.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class EDSDefaults ( FrenchDefaults ): \"\"\" Defaults for the EDSLanguage class Mostly identical to the FrenchDefaults, but without tokenization info \"\"\" tokenizer_exceptions = {} infixes = [] lex_attr_getters = LEX_ATTRS syntax_iterators = SYNTAX_ITERATORS stop_words = STOP_WORDS config = FrenchDefaults . config . merge ( { \"nlp\" : { \"tokenizer\" : { \"@tokenizers\" : \"eds.tokenizer\" }}, } )","title":"EDSDefaults"},{"location":"reference/language/#edsnlp.language.EDSDefaults.tokenizer_exceptions","text":"","title":"tokenizer_exceptions"},{"location":"reference/language/#edsnlp.language.EDSDefaults.infixes","text":"","title":"infixes"},{"location":"reference/language/#edsnlp.language.EDSDefaults.lex_attr_getters","text":"","title":"lex_attr_getters"},{"location":"reference/language/#edsnlp.language.EDSDefaults.syntax_iterators","text":"","title":"syntax_iterators"},{"location":"reference/language/#edsnlp.language.EDSDefaults.stop_words","text":"","title":"stop_words"},{"location":"reference/language/#edsnlp.language.EDSDefaults.config","text":"","title":"config"},{"location":"reference/language/#edsnlp.language.EDSLanguage","text":"Bases: French French clinical language. It is shipped with the EDSTokenizer tokenizer that better handles tokenization for French clinical documents Source code in edsnlp/language.py 32 33 34 35 36 37 38 39 40 41 42 @spacy . registry . languages ( \"eds\" ) class EDSLanguage ( French ): \"\"\" French clinical language. It is shipped with the `EDSTokenizer` tokenizer that better handles tokenization for French clinical documents \"\"\" lang = \"eds\" Defaults = EDSDefaults default_config = Defaults","title":"EDSLanguage"},{"location":"reference/language/#edsnlp.language.EDSLanguage.lang","text":"","title":"lang"},{"location":"reference/language/#edsnlp.language.EDSLanguage.Defaults","text":"","title":"Defaults"},{"location":"reference/language/#edsnlp.language.EDSLanguage.default_config","text":"","title":"default_config"},{"location":"reference/language/#edsnlp.language.EDSTokenizer","text":"Bases: DummyTokenizer Source code in edsnlp/language.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class EDSTokenizer ( DummyTokenizer ): def __init__ ( self , vocab : Vocab ) -> None : \"\"\" Tokenizer class for French clinical documents. It better handles tokenization around: - numbers: \"ACR5\" -> [\"ACR\", \"5\"] instead of [\"ACR5\"] - newlines: \"\\n \\n \\n\" -> [\"\\n\", \"\\n\", \"\\n\"] instead of [\"\\n \\n \\n\"] and should be around 5-6 times faster than its standard French counterpart. Parameters ---------- vocab: Vocab The spacy vocabulary \"\"\" self . vocab = vocab punct = \"[:punct:]\" + \" \\\" '\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\" num_like = r \"\\d+(?:[.,]\\d+)?\" default = rf \"[^\\d { punct } '\\n[[:space:]]+(?:['\u02ca](?=[[:alpha:]]|$))?\" self . word_regex = regex . compile ( rf \"( { num_like } |[ { punct } ]|[\\n\\r\\t]|[^\\S\\r\\n\\t]+| { default } )([^\\S\\r\\n\\t])?\" ) def __call__ ( self , text : str ) -> Doc : \"\"\" Tokenizes the text using the EDSTokenizer Parameters ---------- text: str Returns ------- Doc \"\"\" last = 0 words = [] whitespaces = [] for match in self . word_regex . finditer ( text ): begin , end = match . start (), match . end () if last != begin : logger . warning ( \"Missed some characters during\" + f \" tokenization between { last } and { begin } : \" + text [ last - 10 : last ] + \"|\" + text [ last : begin ] + \"|\" + text [ begin : begin + 10 ], ) last = end words . append ( match . group ( 1 )) whitespaces . append ( bool ( match . group ( 2 ))) return Doc ( self . vocab , words = words , spaces = whitespaces )","title":"EDSTokenizer"},{"location":"reference/language/#edsnlp.language.EDSTokenizer.vocab","text":"","title":"vocab"},{"location":"reference/language/#edsnlp.language.EDSTokenizer.word_regex","text":"","title":"word_regex"},{"location":"reference/language/#edsnlp.language.EDSTokenizer.__init__","text":"Tokenizer class for French clinical documents. It better handles tokenization around: - numbers: \"ACR5\" -> [\"ACR\", \"5\"] instead of [\"ACR5\"] - newlines: \" \" -> [\" \", \" \", \" \"] instead of [\" \"] and should be around 5-6 times faster than its standard French counterpart. Parameters ---------- vocab: Vocab The spacy vocabulary Source code in edsnlp/language.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , vocab : Vocab ) -> None : \"\"\" Tokenizer class for French clinical documents. It better handles tokenization around: - numbers: \"ACR5\" -> [\"ACR\", \"5\"] instead of [\"ACR5\"] - newlines: \"\\n \\n \\n\" -> [\"\\n\", \"\\n\", \"\\n\"] instead of [\"\\n \\n \\n\"] and should be around 5-6 times faster than its standard French counterpart. Parameters ---------- vocab: Vocab The spacy vocabulary \"\"\" self . vocab = vocab punct = \"[:punct:]\" + \" \\\" '\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\" num_like = r \"\\d+(?:[.,]\\d+)?\" default = rf \"[^\\d { punct } '\\n[[:space:]]+(?:['\u02ca](?=[[:alpha:]]|$))?\" self . word_regex = regex . compile ( rf \"( { num_like } |[ { punct } ]|[\\n\\r\\t]|[^\\S\\r\\n\\t]+| { default } )([^\\S\\r\\n\\t])?\" )","title":"__init__()"},{"location":"reference/language/#edsnlp.language.EDSTokenizer.__call__","text":"Tokenizes the text using the EDSTokenizer PARAMETER DESCRIPTION text TYPE: str RETURNS DESCRIPTION Doc Source code in edsnlp/language.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __call__ ( self , text : str ) -> Doc : \"\"\" Tokenizes the text using the EDSTokenizer Parameters ---------- text: str Returns ------- Doc \"\"\" last = 0 words = [] whitespaces = [] for match in self . word_regex . finditer ( text ): begin , end = match . start (), match . end () if last != begin : logger . warning ( \"Missed some characters during\" + f \" tokenization between { last } and { begin } : \" + text [ last - 10 : last ] + \"|\" + text [ last : begin ] + \"|\" + text [ begin : begin + 10 ], ) last = end words . append ( match . group ( 1 )) whitespaces . append ( bool ( match . group ( 2 ))) return Doc ( self . vocab , words = words , spaces = whitespaces )","title":"__call__()"},{"location":"reference/language/#edsnlp.language.create_eds_tokenizer","text":"Creates a factory that returns new EDSTokenizer instances RETURNS DESCRIPTION EDSTokenizer Source code in edsnlp/language.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @spacy . registry . tokenizers ( \"eds.tokenizer\" ) def create_eds_tokenizer (): \"\"\" Creates a factory that returns new EDSTokenizer instances Returns ------- EDSTokenizer \"\"\" def eds_tokenizer_factory ( nlp ): return EDSTokenizer ( nlp . vocab ) return eds_tokenizer_factory","title":"create_eds_tokenizer()"},{"location":"reference/connectors/","text":"edsnlp.connectors","title":"`edsnlp.connectors`"},{"location":"reference/connectors/#edsnlpconnectors","text":"","title":"edsnlp.connectors"},{"location":"reference/connectors/brat/","text":"edsnlp.connectors.brat REGEX_ENTITY = re . compile ( '^(T \\\\ d+) \\\\ t([^ \\\\ s]+)([^ \\\\ t]+) \\\\ t(.*)$' ) module-attribute REGEX_NOTE = re . compile ( '^(# \\\\ d+) \\\\ tAnnotatorNotes ([^ \\\\ t]+) \\\\ t(.*)$' ) module-attribute REGEX_RELATION = re . compile ( '^(R \\\\ d+) \\\\ t([^ \\\\ s]+) Arg1:([^ \\\\ s]+) Arg2:([^ \\\\ s]+)' ) module-attribute REGEX_ATTRIBUTE = re . compile ( '^([AM] \\\\ d+) \\\\ t(.+)$' ) module-attribute REGEX_EVENT = re . compile ( '^(E \\\\ d+) \\\\ t(.+)$' ) module-attribute REGEX_EVENT_PART = re . compile ( '([^ \\\\ s]+):([TE] \\\\ d+)' ) module-attribute BratParsingError Bases: ValueError Source code in edsnlp/connectors/brat.py 24 25 26 class BratParsingError ( ValueError ): def __init__ ( self , ann_file , line ): super () . __init__ ( f \"File { ann_file } , unrecognized Brat line { line } \" ) __init__ ( ann_file , line ) Source code in edsnlp/connectors/brat.py 25 26 def __init__ ( self , ann_file , line ): super () . __init__ ( f \"File { ann_file } , unrecognized Brat line { line } \" ) BratConnector Bases: object Two-way connector with BRAT. Supports entities only. PARAMETER DESCRIPTION directory Directory containing the BRAT files. TYPE: Union[str, Path] n_jobs Number of jobs for multiprocessing, by default 1 TYPE: int, optional attributes Mapping from BRAT attributes to spaCy Span extensions. Extensions / attributes that are not in the mapping are not imported or exported If left to None, the mapping is filled with all BRAT attributes. span_groups Additional span groups to look for entities in spaCy documents when exporting. Missing label (resp. span group) names are not imported (resp. exported) If left to None, the sequence is filled with all BRAT entity labels. Source code in edsnlp/connectors/brat.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 class BratConnector ( object ): \"\"\" Two-way connector with BRAT. Supports entities only. Parameters ---------- directory : Union[str, Path] Directory containing the BRAT files. n_jobs : int, optional Number of jobs for multiprocessing, by default 1 attributes: Optional[Union[Sequence[str], Mapping[str, str]]] Mapping from BRAT attributes to spaCy Span extensions. Extensions / attributes that are not in the mapping are not imported or exported If left to None, the mapping is filled with all BRAT attributes. span_groups: Optional[Sequence[str]] Additional span groups to look for entities in spaCy documents when exporting. Missing label (resp. span group) names are not imported (resp. exported) If left to None, the sequence is filled with all BRAT entity labels. \"\"\" def __init__ ( self , directory : Union [ str , Path ], n_jobs : int = 1 , attributes : Optional [ Union [ Sequence [ str ], Mapping [ str , str ]]] = None , span_groups : Optional [ Sequence [ str ]] = None , ): self . directory : Path = Path ( directory ) self . n_jobs = n_jobs if attributes is None : self . attr_map = None elif isinstance ( attributes , ( tuple , list )): self . attr_map = { k : k for k in attributes } elif isinstance ( attributes , dict ): self . attr_map = attributes else : raise TypeError ( \"`attributes` should be a list, tuple or mapping of strings\" ) self . span_groups = None if span_groups is None else tuple ( span_groups ) def full_path ( self , filename : str ) -> str : return os . path . join ( self . directory , filename ) def load_brat ( self ) -> List [ Dict ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: A spaCy pipeline. Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" filenames = list ( glob . iglob ( str ( self . directory / \"**\" / \"*.txt\" ), recursive = True ) ) assert filenames , f \"BRAT directory { self . directory } is empty!\" logger . info ( f \"The BRAT directory contains { len ( filenames ) } annotated documents.\" ) def load_and_rename ( filename ): res = load_from_brat ( filename ) res [ \"note_id\" ] = str ( Path ( filename ) . relative_to ( self . directory )) . rsplit ( \".\" , 1 )[ 0 ] return res iterator = tqdm ( filenames , ascii = True , ncols = 100 , desc = \"Annotation extraction\" ) with iterator : annotations = Parallel ( n_jobs = self . n_jobs )( delayed ( load_and_rename )( self . full_path ( filename )) for filename in filenames ) return annotations def brat2docs ( self , nlp : Language , run_pipe = False ) -> List [ Doc ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: Language A spaCy pipeline. run_pipe: bool Should the full spaCy pipeline be run on the documents, or just the tokenization (defaults to False ie only tokenization) Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" annotations = self . load_brat () texts = [ doc [ \"text\" ] for doc in annotations ] docs = [] if run_pipe : gold_docs = nlp . pipe ( texts , batch_size = 50 , n_process = self . n_jobs ) else : gold_docs = ( nlp . make_doc ( t ) for t in texts ) for doc , doc_annotations in tqdm ( zip ( gold_docs , annotations ), ascii = True , ncols = 100 , desc = \"spaCy conversion\" , total = len ( texts ), ): doc . _ . note_id = doc_annotations [ \"note_id\" ] spans = [] span_groups = defaultdict ( lambda : []) if self . attr_map is not None : for dst in self . attr_map . values (): if not Span . has_extension ( dst ): Span . set_extension ( dst , default = None ) encountered_attributes = set () for ent in doc_annotations [ \"entities\" ]: if self . attr_map is None : for a in ent [ \"attributes\" ]: if not Span . has_extension ( a [ \"label\" ]): Span . set_extension ( a [ \"label\" ], default = None ) encountered_attributes . add ( a [ \"label\" ]) for fragment in ent [ \"fragments\" ]: span = doc . char_span ( fragment [ \"begin\" ], fragment [ \"end\" ], label = ent [ \"label\" ], alignment_mode = \"expand\" , ) for a in ent [ \"attributes\" ]: if self . attr_map is None or a [ \"label\" ] in self . attr_map : new_name = ( a [ \"label\" ] if self . attr_map is None else self . attr_map [ a [ \"label\" ]] ) span . _ . set ( new_name , a [ \"value\" ] if a is not None else True ) spans . append ( span ) if self . span_groups is None or ent [ \"label\" ] in self . span_groups : span_groups [ ent [ \"label\" ]] . append ( span ) if self . attr_map is None : self . attr_map = { k : k for k in encountered_attributes } if self . span_groups is None : self . span_groups = sorted ( span_groups . keys ()) doc . ents = filter_spans ( spans ) for group_name , group in span_groups . items (): doc . spans [ group_name ] = group docs . append ( doc ) return docs def doc2brat ( self , doc : Doc ) -> None : \"\"\" Writes a spaCy document to file in the BRAT directory. Parameters ---------- doc: spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file. \"\"\" filename = str ( doc . _ . note_id ) if self . attr_map is None : rattr_map = {} else : rattr_map = { v : k for k , v in self . attr_map . items ()} annotations = { \"entities\" : [ { \"entity_id\" : i , \"fragments\" : [ { \"begin\" : ent . start_char , \"end\" : ent . end_char , } ], \"attributes\" : [ { \"label\" : rattr_map [ a ], \"value\" : getattr ( ent . _ , a )} for a in rattr_map if getattr ( ent . _ , a ) is not None ], \"label\" : ent . label_ , } for i , ent in enumerate ( sorted ( { * doc . ents , * ( span for name in doc . spans if self . span_groups is None or name in self . span_groups for span in doc . spans [ name ] ), } ) ) ], \"text\" : doc . text , } export_to_brat ( annotations , self . full_path ( f \" { filename } .txt\" ), overwrite_txt = False , overwrite_ann = True , ) def docs2brat ( self , docs : List [ Doc ]) -> None : \"\"\" Writes a list of spaCy documents to file. Parameters ---------- docs: List of spaCy documents. \"\"\" for doc in docs : self . doc2brat ( doc ) def get_brat ( self ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Reads texts and annotations, and returns two DataFrame objects. For backward compatibility Returns ------- texts: A DataFrame containing two fields, `note_id` and `note_text` annotations: A DataFrame containing the annotations. \"\"\" brat = self . load_brat () texts = pd . DataFrame ( [ { \"note_id\" : doc [ \"note_id\" ], \"note_text\" : doc [ \"text\" ], } for doc in brat ] ) annotations = pd . DataFrame ( [ { \"note_id\" : doc [ \"note_id\" ], \"index\" : i , \"begin\" : f [ \"begin\" ], \"end\" : f [ \"end\" ], \"label\" : e [ \"label\" ], \"lexical_variant\" : e [ \"text\" ], } for doc in brat for i , e in enumerate ( doc [ \"entities\" ]) for f in e [ \"fragments\" ] ] ) return texts , annotations directory : Path = Path ( directory ) instance-attribute n_jobs = n_jobs instance-attribute attr_map = None instance-attribute span_groups = None if span_groups is None else tuple ( span_groups ) instance-attribute __init__ ( directory , n_jobs = 1 , attributes = None , span_groups = None ) Source code in edsnlp/connectors/brat.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def __init__ ( self , directory : Union [ str , Path ], n_jobs : int = 1 , attributes : Optional [ Union [ Sequence [ str ], Mapping [ str , str ]]] = None , span_groups : Optional [ Sequence [ str ]] = None , ): self . directory : Path = Path ( directory ) self . n_jobs = n_jobs if attributes is None : self . attr_map = None elif isinstance ( attributes , ( tuple , list )): self . attr_map = { k : k for k in attributes } elif isinstance ( attributes , dict ): self . attr_map = attributes else : raise TypeError ( \"`attributes` should be a list, tuple or mapping of strings\" ) self . span_groups = None if span_groups is None else tuple ( span_groups ) full_path ( filename ) Source code in edsnlp/connectors/brat.py 309 310 def full_path ( self , filename : str ) -> str : return os . path . join ( self . directory , filename ) load_brat () Transforms a BRAT folder to a list of spaCy documents. PARAMETER DESCRIPTION nlp A spaCy pipeline. RETURNS DESCRIPTION docs List of spaCy documents, with annotations in the ents attribute. Source code in edsnlp/connectors/brat.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def load_brat ( self ) -> List [ Dict ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: A spaCy pipeline. Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" filenames = list ( glob . iglob ( str ( self . directory / \"**\" / \"*.txt\" ), recursive = True ) ) assert filenames , f \"BRAT directory { self . directory } is empty!\" logger . info ( f \"The BRAT directory contains { len ( filenames ) } annotated documents.\" ) def load_and_rename ( filename ): res = load_from_brat ( filename ) res [ \"note_id\" ] = str ( Path ( filename ) . relative_to ( self . directory )) . rsplit ( \".\" , 1 )[ 0 ] return res iterator = tqdm ( filenames , ascii = True , ncols = 100 , desc = \"Annotation extraction\" ) with iterator : annotations = Parallel ( n_jobs = self . n_jobs )( delayed ( load_and_rename )( self . full_path ( filename )) for filename in filenames ) return annotations brat2docs ( nlp , run_pipe = False ) Transforms a BRAT folder to a list of spaCy documents. PARAMETER DESCRIPTION nlp A spaCy pipeline. TYPE: Language run_pipe Should the full spaCy pipeline be run on the documents, or just the tokenization (defaults to False ie only tokenization) DEFAULT: False RETURNS DESCRIPTION docs List of spaCy documents, with annotations in the ents attribute. Source code in edsnlp/connectors/brat.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def brat2docs ( self , nlp : Language , run_pipe = False ) -> List [ Doc ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: Language A spaCy pipeline. run_pipe: bool Should the full spaCy pipeline be run on the documents, or just the tokenization (defaults to False ie only tokenization) Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" annotations = self . load_brat () texts = [ doc [ \"text\" ] for doc in annotations ] docs = [] if run_pipe : gold_docs = nlp . pipe ( texts , batch_size = 50 , n_process = self . n_jobs ) else : gold_docs = ( nlp . make_doc ( t ) for t in texts ) for doc , doc_annotations in tqdm ( zip ( gold_docs , annotations ), ascii = True , ncols = 100 , desc = \"spaCy conversion\" , total = len ( texts ), ): doc . _ . note_id = doc_annotations [ \"note_id\" ] spans = [] span_groups = defaultdict ( lambda : []) if self . attr_map is not None : for dst in self . attr_map . values (): if not Span . has_extension ( dst ): Span . set_extension ( dst , default = None ) encountered_attributes = set () for ent in doc_annotations [ \"entities\" ]: if self . attr_map is None : for a in ent [ \"attributes\" ]: if not Span . has_extension ( a [ \"label\" ]): Span . set_extension ( a [ \"label\" ], default = None ) encountered_attributes . add ( a [ \"label\" ]) for fragment in ent [ \"fragments\" ]: span = doc . char_span ( fragment [ \"begin\" ], fragment [ \"end\" ], label = ent [ \"label\" ], alignment_mode = \"expand\" , ) for a in ent [ \"attributes\" ]: if self . attr_map is None or a [ \"label\" ] in self . attr_map : new_name = ( a [ \"label\" ] if self . attr_map is None else self . attr_map [ a [ \"label\" ]] ) span . _ . set ( new_name , a [ \"value\" ] if a is not None else True ) spans . append ( span ) if self . span_groups is None or ent [ \"label\" ] in self . span_groups : span_groups [ ent [ \"label\" ]] . append ( span ) if self . attr_map is None : self . attr_map = { k : k for k in encountered_attributes } if self . span_groups is None : self . span_groups = sorted ( span_groups . keys ()) doc . ents = filter_spans ( spans ) for group_name , group in span_groups . items (): doc . spans [ group_name ] = group docs . append ( doc ) return docs doc2brat ( doc ) Writes a spaCy document to file in the BRAT directory. PARAMETER DESCRIPTION doc spaCy Doc object. The spans in ents will populate the note_id.ann file. TYPE: Doc Source code in edsnlp/connectors/brat.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 def doc2brat ( self , doc : Doc ) -> None : \"\"\" Writes a spaCy document to file in the BRAT directory. Parameters ---------- doc: spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file. \"\"\" filename = str ( doc . _ . note_id ) if self . attr_map is None : rattr_map = {} else : rattr_map = { v : k for k , v in self . attr_map . items ()} annotations = { \"entities\" : [ { \"entity_id\" : i , \"fragments\" : [ { \"begin\" : ent . start_char , \"end\" : ent . end_char , } ], \"attributes\" : [ { \"label\" : rattr_map [ a ], \"value\" : getattr ( ent . _ , a )} for a in rattr_map if getattr ( ent . _ , a ) is not None ], \"label\" : ent . label_ , } for i , ent in enumerate ( sorted ( { * doc . ents , * ( span for name in doc . spans if self . span_groups is None or name in self . span_groups for span in doc . spans [ name ] ), } ) ) ], \"text\" : doc . text , } export_to_brat ( annotations , self . full_path ( f \" { filename } .txt\" ), overwrite_txt = False , overwrite_ann = True , ) docs2brat ( docs ) Writes a list of spaCy documents to file. PARAMETER DESCRIPTION docs List of spaCy documents. TYPE: List [ Doc ] Source code in edsnlp/connectors/brat.py 496 497 498 499 500 501 502 503 504 505 506 def docs2brat ( self , docs : List [ Doc ]) -> None : \"\"\" Writes a list of spaCy documents to file. Parameters ---------- docs: List of spaCy documents. \"\"\" for doc in docs : self . doc2brat ( doc ) get_brat () Reads texts and annotations, and returns two DataFrame objects. For backward compatibility RETURNS DESCRIPTION texts A DataFrame containing two fields, note_id and note_text annotations A DataFrame containing the annotations. Source code in edsnlp/connectors/brat.py 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def get_brat ( self ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Reads texts and annotations, and returns two DataFrame objects. For backward compatibility Returns ------- texts: A DataFrame containing two fields, `note_id` and `note_text` annotations: A DataFrame containing the annotations. \"\"\" brat = self . load_brat () texts = pd . DataFrame ( [ { \"note_id\" : doc [ \"note_id\" ], \"note_text\" : doc [ \"text\" ], } for doc in brat ] ) annotations = pd . DataFrame ( [ { \"note_id\" : doc [ \"note_id\" ], \"index\" : i , \"begin\" : f [ \"begin\" ], \"end\" : f [ \"end\" ], \"label\" : e [ \"label\" ], \"lexical_variant\" : e [ \"text\" ], } for doc in brat for i , e in enumerate ( doc [ \"entities\" ]) for f in e [ \"fragments\" ] ] ) return texts , annotations load_from_brat ( path , merge_spaced_fragments = True ) Load a brat file Adapted from https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py PARAMETER DESCRIPTION path Path or glob path of the brat text file (.txt, not .ann) TYPE: str merge_spaced_fragments Merge fragments of a entity that was splitted by brat because it overlapped an end of line TYPE: bool DEFAULT: True RETURNS DESCRIPTION Iterator[Dict] Source code in edsnlp/connectors/brat.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def load_from_brat ( path : str , merge_spaced_fragments : bool = True ) -> Dict : \"\"\" Load a brat file Adapted from https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py Parameters ---------- path: str Path or glob path of the brat text file (.txt, not .ann) merge_spaced_fragments: bool Merge fragments of a entity that was splitted by brat because it overlapped an end of line Returns ------- Iterator[Dict] \"\"\" ann_filenames = [] for filename in glob . glob ( path . replace ( \".txt\" , \".a*\" ), recursive = True ): ann_filenames . append ( filename ) entities = {} relations = [] events = {} # doc_id = filename.replace('.txt', '').split(\"/\")[-1] with open ( path ) as f : text = f . read () note_id = path . split ( \"/\" )[ - 1 ] . rsplit ( \".\" , 1 )[ 0 ] if not len ( ann_filenames ): return { \"note_id\" : note_id , \"text\" : text , } for ann_file in ann_filenames : with open ( ann_file ) as f : for line_idx , line in enumerate ( f ): try : if line . startswith ( \"T\" ): match = REGEX_ENTITY . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) entity = match . group ( 2 ) span = match . group ( 3 ) mention_text = match . group ( 4 ) entities [ ann_id ] = { \"text\" : mention_text , \"entity_id\" : ann_id , \"fragments\" : [], \"attributes\" : [], \"comments\" : [], \"label\" : entity , } last_end = None fragment_i = 0 begins_ends = sorted ( [ ( int ( s . split ()[ 0 ]), int ( s . split ()[ 1 ])) for s in span . split ( \";\" ) ] ) for begin , end in begins_ends : # If merge_spaced_fragments, merge two fragments that are # only separated by a newline (brat automatically creates # multiple fragments for a entity that spans over more than # one line) if ( merge_spaced_fragments and last_end is not None and len ( text [ last_end : begin ] . strip ()) == 0 ): entities [ ann_id ][ \"fragments\" ][ - 1 ][ \"end\" ] = end last_end = end continue entities [ ann_id ][ \"fragments\" ] . append ( { \"begin\" : begin , \"end\" : end , } ) fragment_i += 1 last_end = end elif line . startswith ( \"A\" ) or line . startswith ( \"M\" ): match = REGEX_ATTRIBUTE . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) parts = match . group ( 2 ) . split ( \" \" ) if len ( parts ) >= 3 : entity , entity_id , value = parts elif len ( parts ) == 2 : entity , entity_id = parts value = None else : raise BratParsingError ( ann_file , line ) ( entities [ entity_id ] if entity_id . startswith ( \"T\" ) else events [ entity_id ] )[ \"attributes\" ] . append ( { \"attribute_id\" : ann_id , \"label\" : entity , \"value\" : value , } ) elif line . startswith ( \"R\" ): match = REGEX_RELATION . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) ann_name = match . group ( 2 ) arg1 = match . group ( 3 ) arg2 = match . group ( 4 ) relations . append ( { \"relation_id\" : ann_id , \"relation_label\" : ann_name , \"from_entity_id\" : arg1 , \"to_entity_id\" : arg2 , } ) elif line . startswith ( \"E\" ): match = REGEX_EVENT . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) arguments_txt = match . group ( 2 ) arguments = [] for argument in REGEX_EVENT_PART . finditer ( arguments_txt ): arguments . append ( { \"entity_id\" : argument . group ( 2 ), \"label\" : argument . group ( 1 ), } ) events [ ann_id ] = { \"event_id\" : ann_id , \"attributes\" : [], \"arguments\" : arguments , } elif line . startswith ( \"#\" ): match = REGEX_NOTE . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) entity_id = match . group ( 2 ) comment = match . group ( 3 ) entities [ entity_id ][ \"comments\" ] . append ( { \"comment_id\" : ann_id , \"comment\" : comment , } ) except Exception : raise Exception ( \"Could not parse line {} from {} : {} \" . format ( line_idx , filename . replace ( \".txt\" , \".ann\" ), repr ( line ) ) ) return { \"note_id\" : note_id , \"text\" : text , \"entities\" : list ( entities . values ()), \"relations\" : relations , \"events\" : list ( events . values ()), } export_to_brat ( doc , txt_filename , overwrite_txt = False , overwrite_ann = False ) Source code in edsnlp/connectors/brat.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def export_to_brat ( doc , txt_filename , overwrite_txt = False , overwrite_ann = False ): parent_dir = txt_filename . rsplit ( \"/\" , 1 )[ 0 ] if parent_dir and not os . path . exists ( parent_dir ): os . makedirs ( parent_dir , exist_ok = True ) if not os . path . exists ( txt_filename ) or overwrite_txt : with open ( txt_filename , \"w\" ) as f : f . write ( doc [ \"text\" ]) ann_filename = txt_filename . replace ( \".txt\" , \".ann\" ) attribute_idx = 1 entities_ids = defaultdict ( lambda : \"T\" + str ( len ( entities_ids ) + 1 )) if not os . path . exists ( ann_filename ) or overwrite_ann : with open ( ann_filename , \"w\" ) as f : if \"entities\" in doc : for entity in doc [ \"entities\" ]: idx = None spans = [] brat_entity_id = entities_ids [ entity [ \"entity_id\" ]] for fragment in sorted ( entity [ \"fragments\" ], key = lambda frag : frag [ \"begin\" ] ): idx = fragment [ \"begin\" ] entity_text = doc [ \"text\" ][ fragment [ \"begin\" ] : fragment [ \"end\" ]] for part in entity_text . split ( \" \\n \" ): begin = idx end = idx + len ( part ) idx = end + 1 if begin != end : spans . append (( begin , end )) print ( \" {} \\t {} {} \\t {} \" . format ( brat_entity_id , str ( entity [ \"label\" ]), \";\" . join ( \" \" . join ( map ( str , span )) for span in spans ), entity_text . replace ( \" \\n \" , \" \" ), ), file = f , ) if \"attributes\" in entity : for i , attribute in enumerate ( entity [ \"attributes\" ]): print ( \"A {} \\t {} {} {} \" . format ( attribute_idx , str ( attribute [ \"label\" ]), brat_entity_id , attribute [ \"value\" ], ), file = f , ) attribute_idx += 1","title":"brat"},{"location":"reference/connectors/brat/#edsnlpconnectorsbrat","text":"","title":"edsnlp.connectors.brat"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.REGEX_ENTITY","text":"","title":"REGEX_ENTITY"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.REGEX_NOTE","text":"","title":"REGEX_NOTE"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.REGEX_RELATION","text":"","title":"REGEX_RELATION"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.REGEX_ATTRIBUTE","text":"","title":"REGEX_ATTRIBUTE"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.REGEX_EVENT","text":"","title":"REGEX_EVENT"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.REGEX_EVENT_PART","text":"","title":"REGEX_EVENT_PART"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratParsingError","text":"Bases: ValueError Source code in edsnlp/connectors/brat.py 24 25 26 class BratParsingError ( ValueError ): def __init__ ( self , ann_file , line ): super () . __init__ ( f \"File { ann_file } , unrecognized Brat line { line } \" )","title":"BratParsingError"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratParsingError.__init__","text":"Source code in edsnlp/connectors/brat.py 25 26 def __init__ ( self , ann_file , line ): super () . __init__ ( f \"File { ann_file } , unrecognized Brat line { line } \" )","title":"__init__()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector","text":"Bases: object Two-way connector with BRAT. Supports entities only. PARAMETER DESCRIPTION directory Directory containing the BRAT files. TYPE: Union[str, Path] n_jobs Number of jobs for multiprocessing, by default 1 TYPE: int, optional attributes Mapping from BRAT attributes to spaCy Span extensions. Extensions / attributes that are not in the mapping are not imported or exported If left to None, the mapping is filled with all BRAT attributes. span_groups Additional span groups to look for entities in spaCy documents when exporting. Missing label (resp. span group) names are not imported (resp. exported) If left to None, the sequence is filled with all BRAT entity labels. Source code in edsnlp/connectors/brat.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 class BratConnector ( object ): \"\"\" Two-way connector with BRAT. Supports entities only. Parameters ---------- directory : Union[str, Path] Directory containing the BRAT files. n_jobs : int, optional Number of jobs for multiprocessing, by default 1 attributes: Optional[Union[Sequence[str], Mapping[str, str]]] Mapping from BRAT attributes to spaCy Span extensions. Extensions / attributes that are not in the mapping are not imported or exported If left to None, the mapping is filled with all BRAT attributes. span_groups: Optional[Sequence[str]] Additional span groups to look for entities in spaCy documents when exporting. Missing label (resp. span group) names are not imported (resp. exported) If left to None, the sequence is filled with all BRAT entity labels. \"\"\" def __init__ ( self , directory : Union [ str , Path ], n_jobs : int = 1 , attributes : Optional [ Union [ Sequence [ str ], Mapping [ str , str ]]] = None , span_groups : Optional [ Sequence [ str ]] = None , ): self . directory : Path = Path ( directory ) self . n_jobs = n_jobs if attributes is None : self . attr_map = None elif isinstance ( attributes , ( tuple , list )): self . attr_map = { k : k for k in attributes } elif isinstance ( attributes , dict ): self . attr_map = attributes else : raise TypeError ( \"`attributes` should be a list, tuple or mapping of strings\" ) self . span_groups = None if span_groups is None else tuple ( span_groups ) def full_path ( self , filename : str ) -> str : return os . path . join ( self . directory , filename ) def load_brat ( self ) -> List [ Dict ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: A spaCy pipeline. Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" filenames = list ( glob . iglob ( str ( self . directory / \"**\" / \"*.txt\" ), recursive = True ) ) assert filenames , f \"BRAT directory { self . directory } is empty!\" logger . info ( f \"The BRAT directory contains { len ( filenames ) } annotated documents.\" ) def load_and_rename ( filename ): res = load_from_brat ( filename ) res [ \"note_id\" ] = str ( Path ( filename ) . relative_to ( self . directory )) . rsplit ( \".\" , 1 )[ 0 ] return res iterator = tqdm ( filenames , ascii = True , ncols = 100 , desc = \"Annotation extraction\" ) with iterator : annotations = Parallel ( n_jobs = self . n_jobs )( delayed ( load_and_rename )( self . full_path ( filename )) for filename in filenames ) return annotations def brat2docs ( self , nlp : Language , run_pipe = False ) -> List [ Doc ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: Language A spaCy pipeline. run_pipe: bool Should the full spaCy pipeline be run on the documents, or just the tokenization (defaults to False ie only tokenization) Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" annotations = self . load_brat () texts = [ doc [ \"text\" ] for doc in annotations ] docs = [] if run_pipe : gold_docs = nlp . pipe ( texts , batch_size = 50 , n_process = self . n_jobs ) else : gold_docs = ( nlp . make_doc ( t ) for t in texts ) for doc , doc_annotations in tqdm ( zip ( gold_docs , annotations ), ascii = True , ncols = 100 , desc = \"spaCy conversion\" , total = len ( texts ), ): doc . _ . note_id = doc_annotations [ \"note_id\" ] spans = [] span_groups = defaultdict ( lambda : []) if self . attr_map is not None : for dst in self . attr_map . values (): if not Span . has_extension ( dst ): Span . set_extension ( dst , default = None ) encountered_attributes = set () for ent in doc_annotations [ \"entities\" ]: if self . attr_map is None : for a in ent [ \"attributes\" ]: if not Span . has_extension ( a [ \"label\" ]): Span . set_extension ( a [ \"label\" ], default = None ) encountered_attributes . add ( a [ \"label\" ]) for fragment in ent [ \"fragments\" ]: span = doc . char_span ( fragment [ \"begin\" ], fragment [ \"end\" ], label = ent [ \"label\" ], alignment_mode = \"expand\" , ) for a in ent [ \"attributes\" ]: if self . attr_map is None or a [ \"label\" ] in self . attr_map : new_name = ( a [ \"label\" ] if self . attr_map is None else self . attr_map [ a [ \"label\" ]] ) span . _ . set ( new_name , a [ \"value\" ] if a is not None else True ) spans . append ( span ) if self . span_groups is None or ent [ \"label\" ] in self . span_groups : span_groups [ ent [ \"label\" ]] . append ( span ) if self . attr_map is None : self . attr_map = { k : k for k in encountered_attributes } if self . span_groups is None : self . span_groups = sorted ( span_groups . keys ()) doc . ents = filter_spans ( spans ) for group_name , group in span_groups . items (): doc . spans [ group_name ] = group docs . append ( doc ) return docs def doc2brat ( self , doc : Doc ) -> None : \"\"\" Writes a spaCy document to file in the BRAT directory. Parameters ---------- doc: spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file. \"\"\" filename = str ( doc . _ . note_id ) if self . attr_map is None : rattr_map = {} else : rattr_map = { v : k for k , v in self . attr_map . items ()} annotations = { \"entities\" : [ { \"entity_id\" : i , \"fragments\" : [ { \"begin\" : ent . start_char , \"end\" : ent . end_char , } ], \"attributes\" : [ { \"label\" : rattr_map [ a ], \"value\" : getattr ( ent . _ , a )} for a in rattr_map if getattr ( ent . _ , a ) is not None ], \"label\" : ent . label_ , } for i , ent in enumerate ( sorted ( { * doc . ents , * ( span for name in doc . spans if self . span_groups is None or name in self . span_groups for span in doc . spans [ name ] ), } ) ) ], \"text\" : doc . text , } export_to_brat ( annotations , self . full_path ( f \" { filename } .txt\" ), overwrite_txt = False , overwrite_ann = True , ) def docs2brat ( self , docs : List [ Doc ]) -> None : \"\"\" Writes a list of spaCy documents to file. Parameters ---------- docs: List of spaCy documents. \"\"\" for doc in docs : self . doc2brat ( doc ) def get_brat ( self ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Reads texts and annotations, and returns two DataFrame objects. For backward compatibility Returns ------- texts: A DataFrame containing two fields, `note_id` and `note_text` annotations: A DataFrame containing the annotations. \"\"\" brat = self . load_brat () texts = pd . DataFrame ( [ { \"note_id\" : doc [ \"note_id\" ], \"note_text\" : doc [ \"text\" ], } for doc in brat ] ) annotations = pd . DataFrame ( [ { \"note_id\" : doc [ \"note_id\" ], \"index\" : i , \"begin\" : f [ \"begin\" ], \"end\" : f [ \"end\" ], \"label\" : e [ \"label\" ], \"lexical_variant\" : e [ \"text\" ], } for doc in brat for i , e in enumerate ( doc [ \"entities\" ]) for f in e [ \"fragments\" ] ] ) return texts , annotations","title":"BratConnector"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.directory","text":"","title":"directory"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.n_jobs","text":"","title":"n_jobs"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.attr_map","text":"","title":"attr_map"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.span_groups","text":"","title":"span_groups"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.__init__","text":"Source code in edsnlp/connectors/brat.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def __init__ ( self , directory : Union [ str , Path ], n_jobs : int = 1 , attributes : Optional [ Union [ Sequence [ str ], Mapping [ str , str ]]] = None , span_groups : Optional [ Sequence [ str ]] = None , ): self . directory : Path = Path ( directory ) self . n_jobs = n_jobs if attributes is None : self . attr_map = None elif isinstance ( attributes , ( tuple , list )): self . attr_map = { k : k for k in attributes } elif isinstance ( attributes , dict ): self . attr_map = attributes else : raise TypeError ( \"`attributes` should be a list, tuple or mapping of strings\" ) self . span_groups = None if span_groups is None else tuple ( span_groups )","title":"__init__()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.full_path","text":"Source code in edsnlp/connectors/brat.py 309 310 def full_path ( self , filename : str ) -> str : return os . path . join ( self . directory , filename )","title":"full_path()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.load_brat","text":"Transforms a BRAT folder to a list of spaCy documents. PARAMETER DESCRIPTION nlp A spaCy pipeline. RETURNS DESCRIPTION docs List of spaCy documents, with annotations in the ents attribute. Source code in edsnlp/connectors/brat.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def load_brat ( self ) -> List [ Dict ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: A spaCy pipeline. Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" filenames = list ( glob . iglob ( str ( self . directory / \"**\" / \"*.txt\" ), recursive = True ) ) assert filenames , f \"BRAT directory { self . directory } is empty!\" logger . info ( f \"The BRAT directory contains { len ( filenames ) } annotated documents.\" ) def load_and_rename ( filename ): res = load_from_brat ( filename ) res [ \"note_id\" ] = str ( Path ( filename ) . relative_to ( self . directory )) . rsplit ( \".\" , 1 )[ 0 ] return res iterator = tqdm ( filenames , ascii = True , ncols = 100 , desc = \"Annotation extraction\" ) with iterator : annotations = Parallel ( n_jobs = self . n_jobs )( delayed ( load_and_rename )( self . full_path ( filename )) for filename in filenames ) return annotations","title":"load_brat()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.brat2docs","text":"Transforms a BRAT folder to a list of spaCy documents. PARAMETER DESCRIPTION nlp A spaCy pipeline. TYPE: Language run_pipe Should the full spaCy pipeline be run on the documents, or just the tokenization (defaults to False ie only tokenization) DEFAULT: False RETURNS DESCRIPTION docs List of spaCy documents, with annotations in the ents attribute. Source code in edsnlp/connectors/brat.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def brat2docs ( self , nlp : Language , run_pipe = False ) -> List [ Doc ]: \"\"\" Transforms a BRAT folder to a list of spaCy documents. Parameters ---------- nlp: Language A spaCy pipeline. run_pipe: bool Should the full spaCy pipeline be run on the documents, or just the tokenization (defaults to False ie only tokenization) Returns ------- docs: List of spaCy documents, with annotations in the `ents` attribute. \"\"\" annotations = self . load_brat () texts = [ doc [ \"text\" ] for doc in annotations ] docs = [] if run_pipe : gold_docs = nlp . pipe ( texts , batch_size = 50 , n_process = self . n_jobs ) else : gold_docs = ( nlp . make_doc ( t ) for t in texts ) for doc , doc_annotations in tqdm ( zip ( gold_docs , annotations ), ascii = True , ncols = 100 , desc = \"spaCy conversion\" , total = len ( texts ), ): doc . _ . note_id = doc_annotations [ \"note_id\" ] spans = [] span_groups = defaultdict ( lambda : []) if self . attr_map is not None : for dst in self . attr_map . values (): if not Span . has_extension ( dst ): Span . set_extension ( dst , default = None ) encountered_attributes = set () for ent in doc_annotations [ \"entities\" ]: if self . attr_map is None : for a in ent [ \"attributes\" ]: if not Span . has_extension ( a [ \"label\" ]): Span . set_extension ( a [ \"label\" ], default = None ) encountered_attributes . add ( a [ \"label\" ]) for fragment in ent [ \"fragments\" ]: span = doc . char_span ( fragment [ \"begin\" ], fragment [ \"end\" ], label = ent [ \"label\" ], alignment_mode = \"expand\" , ) for a in ent [ \"attributes\" ]: if self . attr_map is None or a [ \"label\" ] in self . attr_map : new_name = ( a [ \"label\" ] if self . attr_map is None else self . attr_map [ a [ \"label\" ]] ) span . _ . set ( new_name , a [ \"value\" ] if a is not None else True ) spans . append ( span ) if self . span_groups is None or ent [ \"label\" ] in self . span_groups : span_groups [ ent [ \"label\" ]] . append ( span ) if self . attr_map is None : self . attr_map = { k : k for k in encountered_attributes } if self . span_groups is None : self . span_groups = sorted ( span_groups . keys ()) doc . ents = filter_spans ( spans ) for group_name , group in span_groups . items (): doc . spans [ group_name ] = group docs . append ( doc ) return docs","title":"brat2docs()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.doc2brat","text":"Writes a spaCy document to file in the BRAT directory. PARAMETER DESCRIPTION doc spaCy Doc object. The spans in ents will populate the note_id.ann file. TYPE: Doc Source code in edsnlp/connectors/brat.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 def doc2brat ( self , doc : Doc ) -> None : \"\"\" Writes a spaCy document to file in the BRAT directory. Parameters ---------- doc: spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file. \"\"\" filename = str ( doc . _ . note_id ) if self . attr_map is None : rattr_map = {} else : rattr_map = { v : k for k , v in self . attr_map . items ()} annotations = { \"entities\" : [ { \"entity_id\" : i , \"fragments\" : [ { \"begin\" : ent . start_char , \"end\" : ent . end_char , } ], \"attributes\" : [ { \"label\" : rattr_map [ a ], \"value\" : getattr ( ent . _ , a )} for a in rattr_map if getattr ( ent . _ , a ) is not None ], \"label\" : ent . label_ , } for i , ent in enumerate ( sorted ( { * doc . ents , * ( span for name in doc . spans if self . span_groups is None or name in self . span_groups for span in doc . spans [ name ] ), } ) ) ], \"text\" : doc . text , } export_to_brat ( annotations , self . full_path ( f \" { filename } .txt\" ), overwrite_txt = False , overwrite_ann = True , )","title":"doc2brat()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.docs2brat","text":"Writes a list of spaCy documents to file. PARAMETER DESCRIPTION docs List of spaCy documents. TYPE: List [ Doc ] Source code in edsnlp/connectors/brat.py 496 497 498 499 500 501 502 503 504 505 506 def docs2brat ( self , docs : List [ Doc ]) -> None : \"\"\" Writes a list of spaCy documents to file. Parameters ---------- docs: List of spaCy documents. \"\"\" for doc in docs : self . doc2brat ( doc )","title":"docs2brat()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.get_brat","text":"Reads texts and annotations, and returns two DataFrame objects. For backward compatibility RETURNS DESCRIPTION texts A DataFrame containing two fields, note_id and note_text annotations A DataFrame containing the annotations. Source code in edsnlp/connectors/brat.py 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def get_brat ( self ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Reads texts and annotations, and returns two DataFrame objects. For backward compatibility Returns ------- texts: A DataFrame containing two fields, `note_id` and `note_text` annotations: A DataFrame containing the annotations. \"\"\" brat = self . load_brat () texts = pd . DataFrame ( [ { \"note_id\" : doc [ \"note_id\" ], \"note_text\" : doc [ \"text\" ], } for doc in brat ] ) annotations = pd . DataFrame ( [ { \"note_id\" : doc [ \"note_id\" ], \"index\" : i , \"begin\" : f [ \"begin\" ], \"end\" : f [ \"end\" ], \"label\" : e [ \"label\" ], \"lexical_variant\" : e [ \"text\" ], } for doc in brat for i , e in enumerate ( doc [ \"entities\" ]) for f in e [ \"fragments\" ] ] ) return texts , annotations","title":"get_brat()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.load_from_brat","text":"Load a brat file Adapted from https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py PARAMETER DESCRIPTION path Path or glob path of the brat text file (.txt, not .ann) TYPE: str merge_spaced_fragments Merge fragments of a entity that was splitted by brat because it overlapped an end of line TYPE: bool DEFAULT: True RETURNS DESCRIPTION Iterator[Dict] Source code in edsnlp/connectors/brat.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def load_from_brat ( path : str , merge_spaced_fragments : bool = True ) -> Dict : \"\"\" Load a brat file Adapted from https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py Parameters ---------- path: str Path or glob path of the brat text file (.txt, not .ann) merge_spaced_fragments: bool Merge fragments of a entity that was splitted by brat because it overlapped an end of line Returns ------- Iterator[Dict] \"\"\" ann_filenames = [] for filename in glob . glob ( path . replace ( \".txt\" , \".a*\" ), recursive = True ): ann_filenames . append ( filename ) entities = {} relations = [] events = {} # doc_id = filename.replace('.txt', '').split(\"/\")[-1] with open ( path ) as f : text = f . read () note_id = path . split ( \"/\" )[ - 1 ] . rsplit ( \".\" , 1 )[ 0 ] if not len ( ann_filenames ): return { \"note_id\" : note_id , \"text\" : text , } for ann_file in ann_filenames : with open ( ann_file ) as f : for line_idx , line in enumerate ( f ): try : if line . startswith ( \"T\" ): match = REGEX_ENTITY . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) entity = match . group ( 2 ) span = match . group ( 3 ) mention_text = match . group ( 4 ) entities [ ann_id ] = { \"text\" : mention_text , \"entity_id\" : ann_id , \"fragments\" : [], \"attributes\" : [], \"comments\" : [], \"label\" : entity , } last_end = None fragment_i = 0 begins_ends = sorted ( [ ( int ( s . split ()[ 0 ]), int ( s . split ()[ 1 ])) for s in span . split ( \";\" ) ] ) for begin , end in begins_ends : # If merge_spaced_fragments, merge two fragments that are # only separated by a newline (brat automatically creates # multiple fragments for a entity that spans over more than # one line) if ( merge_spaced_fragments and last_end is not None and len ( text [ last_end : begin ] . strip ()) == 0 ): entities [ ann_id ][ \"fragments\" ][ - 1 ][ \"end\" ] = end last_end = end continue entities [ ann_id ][ \"fragments\" ] . append ( { \"begin\" : begin , \"end\" : end , } ) fragment_i += 1 last_end = end elif line . startswith ( \"A\" ) or line . startswith ( \"M\" ): match = REGEX_ATTRIBUTE . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) parts = match . group ( 2 ) . split ( \" \" ) if len ( parts ) >= 3 : entity , entity_id , value = parts elif len ( parts ) == 2 : entity , entity_id = parts value = None else : raise BratParsingError ( ann_file , line ) ( entities [ entity_id ] if entity_id . startswith ( \"T\" ) else events [ entity_id ] )[ \"attributes\" ] . append ( { \"attribute_id\" : ann_id , \"label\" : entity , \"value\" : value , } ) elif line . startswith ( \"R\" ): match = REGEX_RELATION . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) ann_name = match . group ( 2 ) arg1 = match . group ( 3 ) arg2 = match . group ( 4 ) relations . append ( { \"relation_id\" : ann_id , \"relation_label\" : ann_name , \"from_entity_id\" : arg1 , \"to_entity_id\" : arg2 , } ) elif line . startswith ( \"E\" ): match = REGEX_EVENT . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) arguments_txt = match . group ( 2 ) arguments = [] for argument in REGEX_EVENT_PART . finditer ( arguments_txt ): arguments . append ( { \"entity_id\" : argument . group ( 2 ), \"label\" : argument . group ( 1 ), } ) events [ ann_id ] = { \"event_id\" : ann_id , \"attributes\" : [], \"arguments\" : arguments , } elif line . startswith ( \"#\" ): match = REGEX_NOTE . match ( line ) if match is None : raise BratParsingError ( ann_file , line ) ann_id = match . group ( 1 ) entity_id = match . group ( 2 ) comment = match . group ( 3 ) entities [ entity_id ][ \"comments\" ] . append ( { \"comment_id\" : ann_id , \"comment\" : comment , } ) except Exception : raise Exception ( \"Could not parse line {} from {} : {} \" . format ( line_idx , filename . replace ( \".txt\" , \".ann\" ), repr ( line ) ) ) return { \"note_id\" : note_id , \"text\" : text , \"entities\" : list ( entities . values ()), \"relations\" : relations , \"events\" : list ( events . values ()), }","title":"load_from_brat()"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.export_to_brat","text":"Source code in edsnlp/connectors/brat.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def export_to_brat ( doc , txt_filename , overwrite_txt = False , overwrite_ann = False ): parent_dir = txt_filename . rsplit ( \"/\" , 1 )[ 0 ] if parent_dir and not os . path . exists ( parent_dir ): os . makedirs ( parent_dir , exist_ok = True ) if not os . path . exists ( txt_filename ) or overwrite_txt : with open ( txt_filename , \"w\" ) as f : f . write ( doc [ \"text\" ]) ann_filename = txt_filename . replace ( \".txt\" , \".ann\" ) attribute_idx = 1 entities_ids = defaultdict ( lambda : \"T\" + str ( len ( entities_ids ) + 1 )) if not os . path . exists ( ann_filename ) or overwrite_ann : with open ( ann_filename , \"w\" ) as f : if \"entities\" in doc : for entity in doc [ \"entities\" ]: idx = None spans = [] brat_entity_id = entities_ids [ entity [ \"entity_id\" ]] for fragment in sorted ( entity [ \"fragments\" ], key = lambda frag : frag [ \"begin\" ] ): idx = fragment [ \"begin\" ] entity_text = doc [ \"text\" ][ fragment [ \"begin\" ] : fragment [ \"end\" ]] for part in entity_text . split ( \" \\n \" ): begin = idx end = idx + len ( part ) idx = end + 1 if begin != end : spans . append (( begin , end )) print ( \" {} \\t {} {} \\t {} \" . format ( brat_entity_id , str ( entity [ \"label\" ]), \";\" . join ( \" \" . join ( map ( str , span )) for span in spans ), entity_text . replace ( \" \\n \" , \" \" ), ), file = f , ) if \"attributes\" in entity : for i , attribute in enumerate ( entity [ \"attributes\" ]): print ( \"A {} \\t {} {} {} \" . format ( attribute_idx , str ( attribute [ \"label\" ]), brat_entity_id , attribute [ \"value\" ], ), file = f , ) attribute_idx += 1","title":"export_to_brat()"},{"location":"reference/connectors/labeltool/","text":"edsnlp.connectors.labeltool docs2labeltool ( docs , extensions = None ) Returns a labeltool-ready dataframe from a list of annotated document. PARAMETER DESCRIPTION docs List of annotated spacy docs. TYPE: List [ Doc ] extensions List of extensions to use by labeltool. TYPE: Optional [ List [ str ]] DEFAULT: None RETURNS DESCRIPTION df DataFrame tailored for labeltool. Source code in edsnlp/connectors/labeltool.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def docs2labeltool ( docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> pd . DataFrame : \"\"\" Returns a labeltool-ready dataframe from a list of annotated document. Parameters ---------- docs: list of spaCy Doc List of annotated spacy docs. extensions: list of extensions List of extensions to use by labeltool. Returns ------- df: pd.DataFrame DataFrame tailored for labeltool. \"\"\" if extensions is None : extensions = [] entities = [] for i , doc in enumerate ( tqdm ( docs , ascii = True , ncols = 100 )): for ent in doc . ents : d = dict ( note_text = doc . text , offset_begin = ent . start_char , offset_end = ent . end_char , label_name = ent . label_ , label_value = ent . text , ) d [ \"note_id\" ] = doc . _ . note_id or i for ext in extensions : d [ ext ] = getattr ( ent . _ , ext ) entities . append ( d ) df = pd . DataFrame . from_records ( entities ) columns = [ \"note_id\" , \"note_text\" , \"offset_begin\" , \"offset_end\" , \"label_name\" , \"label_value\" , ] df = df [ columns + extensions ] return df","title":"labeltool"},{"location":"reference/connectors/labeltool/#edsnlpconnectorslabeltool","text":"","title":"edsnlp.connectors.labeltool"},{"location":"reference/connectors/labeltool/#edsnlp.connectors.labeltool.docs2labeltool","text":"Returns a labeltool-ready dataframe from a list of annotated document. PARAMETER DESCRIPTION docs List of annotated spacy docs. TYPE: List [ Doc ] extensions List of extensions to use by labeltool. TYPE: Optional [ List [ str ]] DEFAULT: None RETURNS DESCRIPTION df DataFrame tailored for labeltool. Source code in edsnlp/connectors/labeltool.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def docs2labeltool ( docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> pd . DataFrame : \"\"\" Returns a labeltool-ready dataframe from a list of annotated document. Parameters ---------- docs: list of spaCy Doc List of annotated spacy docs. extensions: list of extensions List of extensions to use by labeltool. Returns ------- df: pd.DataFrame DataFrame tailored for labeltool. \"\"\" if extensions is None : extensions = [] entities = [] for i , doc in enumerate ( tqdm ( docs , ascii = True , ncols = 100 )): for ent in doc . ents : d = dict ( note_text = doc . text , offset_begin = ent . start_char , offset_end = ent . end_char , label_name = ent . label_ , label_value = ent . text , ) d [ \"note_id\" ] = doc . _ . note_id or i for ext in extensions : d [ ext ] = getattr ( ent . _ , ext ) entities . append ( d ) df = pd . DataFrame . from_records ( entities ) columns = [ \"note_id\" , \"note_text\" , \"offset_begin\" , \"offset_end\" , \"label_name\" , \"label_value\" , ] df = df [ columns + extensions ] return df","title":"docs2labeltool()"},{"location":"reference/connectors/omop/","text":"edsnlp.connectors.omop OmopConnector Bases: object [summary] PARAMETER DESCRIPTION nlp spaCy language object. TYPE: Language start_char Name of the column containing the start character index of the entity, by default \"start_char\" TYPE: str, optional end_char Name of the column containing the end character index of the entity, by default \"end_char\" TYPE: str, optional Source code in edsnlp/connectors/omop.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 class OmopConnector ( object ): \"\"\" [summary] Parameters ---------- nlp : Language spaCy language object. start_char : str, optional Name of the column containing the start character index of the entity, by default \"start_char\" end_char : str, optional Name of the column containing the end character index of the entity, by default \"end_char\" \"\"\" def __init__ ( self , nlp : Language , start_char : str = \"start_char\" , end_char : str = \"end_char\" , ): self . start_char = start_char self . end_char = end_char self . nlp = nlp def preprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Preprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { self . start_char : \"start_char\" , self . end_char : \"end_char\" , } ) return note , note_nlp def postprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Postprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { \"start_char\" : self . start_char , \"end_char\" : self . end_char , } ) return note , note_nlp def omop2docs ( self , note : pd . DataFrame , note_nlp : pd . DataFrame , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms OMOP tables to a list of spaCy documents. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] List of spaCy documents. \"\"\" note , note_nlp = self . preprocess ( note , note_nlp ) return omop2docs ( note , note_nlp , self . nlp , extensions ) def docs2omop ( self , docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy documents to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of spaCy documents. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note , note_nlp = docs2omop ( docs , extensions = extensions ) note , note_nlp = self . postprocess ( note , note_nlp ) return note , note_nlp start_char = start_char instance-attribute end_char = end_char instance-attribute nlp = nlp instance-attribute __init__ ( nlp , start_char = 'start_char' , end_char = 'end_char' ) Source code in edsnlp/connectors/omop.py 201 202 203 204 205 206 207 208 209 210 211 def __init__ ( self , nlp : Language , start_char : str = \"start_char\" , end_char : str = \"end_char\" , ): self . start_char = start_char self . end_char = end_char self . nlp = nlp preprocess ( note , note_nlp ) Preprocess the input OMOP tables: modification of the column names. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def preprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Preprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { self . start_char : \"start_char\" , self . end_char : \"end_char\" , } ) return note , note_nlp postprocess ( note , note_nlp ) Postprocess the input OMOP tables: modification of the column names. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def postprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Postprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { \"start_char\" : self . start_char , \"end_char\" : self . end_char , } ) return note , note_nlp omop2docs ( note , note_nlp , extensions = None ) Transforms OMOP tables to a list of spaCy documents. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION List[Doc] List of spaCy documents. Source code in edsnlp/connectors/omop.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def omop2docs ( self , note : pd . DataFrame , note_nlp : pd . DataFrame , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms OMOP tables to a list of spaCy documents. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] List of spaCy documents. \"\"\" note , note_nlp = self . preprocess ( note , note_nlp ) return omop2docs ( note , note_nlp , self . nlp , extensions ) docs2omop ( docs , extensions = None ) Transforms a list of spaCy documents to a pair of OMOP tables. PARAMETER DESCRIPTION docs List of spaCy documents. TYPE: List[Doc] extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def docs2omop ( self , docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy documents to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of spaCy documents. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note , note_nlp = docs2omop ( docs , extensions = extensions ) note , note_nlp = self . postprocess ( note , note_nlp ) return note , note_nlp omop2docs ( note , note_nlp , nlp , extensions = None ) Transforms an OMOP-formatted pair of dataframes into a list of documents. PARAMETER DESCRIPTION note The OMOP note table. TYPE: pd.DataFrame note_nlp The OMOP note_nlp table TYPE: pd.DataFrame nlp spaCy language object. TYPE: Language extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION List[Doc] List of spaCy documents Source code in edsnlp/connectors/omop.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def omop2docs ( note : pd . DataFrame , note_nlp : pd . DataFrame , nlp : Language , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms an OMOP-formatted pair of dataframes into a list of documents. Parameters ---------- note : pd.DataFrame The OMOP `note` table. note_nlp : pd.DataFrame The OMOP `note_nlp` table nlp : Language spaCy language object. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] : List of spaCy documents \"\"\" note = note . copy () note_nlp = note_nlp . copy () extensions = extensions or [] def row2ent ( row ): d = dict ( start_char = row . start_char , end_char = row . end_char , label = row . get ( \"note_nlp_source_value\" ), extensions = { ext : row . get ( ext ) for ext in extensions }, ) return d # Create entities note_nlp [ \"ents\" ] = note_nlp . apply ( row2ent , axis = 1 ) note_nlp = note_nlp . groupby ( \"note_id\" , as_index = False )[ \"ents\" ] . agg ( list ) note = note . merge ( note_nlp , on = [ \"note_id\" ], how = \"left\" ) # Generate documents note [ \"doc\" ] = note . note_text . apply ( nlp ) # Process documents for _ , row in note . iterrows (): doc = row . doc doc . _ . note_id = row . note_id doc . _ . note_datetime = row . get ( \"note_datetime\" ) ents = [] if not isinstance ( row . ents , list ): continue for ent in row . ents : span = doc . char_span ( ent [ \"start_char\" ], ent [ \"end_char\" ], ent [ \"label\" ], alignment_mode = \"expand\" , ) for k , v in ent [ \"extensions\" ] . items (): setattr ( span . _ , k , v ) ents . append ( span ) if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [ span ] else : doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return list ( note . doc ) docs2omop ( docs , extensions = None ) Transforms a list of spaCy docs to a pair of OMOP tables. PARAMETER DESCRIPTION docs List of documents to transform. TYPE: List[Doc] extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION Tuple[pd.DataFrame, pd.DataFrame] Pair of OMOP tables ( note and note_nlp ) Source code in edsnlp/connectors/omop.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def docs2omop ( docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy docs to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of documents to transform. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- Tuple[pd.DataFrame, pd.DataFrame] Pair of OMOP tables (`note` and `note_nlp`) \"\"\" df = pd . DataFrame ( dict ( doc = docs )) df [ \"note_text\" ] = df . doc . apply ( lambda doc : doc . text ) df [ \"note_id\" ] = df . doc . apply ( lambda doc : doc . _ . note_id ) df [ \"note_datetime\" ] = df . doc . apply ( lambda doc : doc . _ . note_datetime ) if df . note_id . isna () . any (): df [ \"note_id\" ] = range ( len ( df )) df [ \"ents\" ] = df . doc . apply ( lambda doc : list ( doc . ents )) df [ \"ents\" ] += df . doc . apply ( lambda doc : list ( doc . spans [ \"discarded\" ])) note = df [[ \"note_id\" , \"note_text\" , \"note_datetime\" ]] df = df [[ \"note_id\" , \"ents\" ]] . explode ( \"ents\" ) extensions = extensions or [] def ent2dict ( ent : Span , ) -> Dict [ str , Any ]: d = dict ( start_char = ent . start_char , end_char = ent . end_char , note_nlp_source_value = ent . label_ , lexical_variant = ent . text , # normalized_variant=ent._.normalized.text, ) for ext in extensions : d [ ext ] = getattr ( ent . _ , ext ) return d df [ \"ents\" ] = df . ents . apply ( ent2dict ) columns = [ \"start_char\" , \"end_char\" , \"note_nlp_source_value\" , \"lexical_variant\" , # \"normalized_variant\", ] columns += extensions df [ columns ] = df . ents . apply ( pd . Series ) df [ \"term_modifiers\" ] = \"\" for i , ext in enumerate ( extensions ): if i > 0 : df . term_modifiers += \";\" df . term_modifiers += ext + \"=\" + df [ ext ] . astype ( str ) df [ \"note_nlp_id\" ] = range ( len ( df )) note_nlp = df [[ \"note_nlp_id\" , \"note_id\" ] + columns ] return note , note_nlp","title":"omop"},{"location":"reference/connectors/omop/#edsnlpconnectorsomop","text":"","title":"edsnlp.connectors.omop"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector","text":"Bases: object [summary] PARAMETER DESCRIPTION nlp spaCy language object. TYPE: Language start_char Name of the column containing the start character index of the entity, by default \"start_char\" TYPE: str, optional end_char Name of the column containing the end character index of the entity, by default \"end_char\" TYPE: str, optional Source code in edsnlp/connectors/omop.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 class OmopConnector ( object ): \"\"\" [summary] Parameters ---------- nlp : Language spaCy language object. start_char : str, optional Name of the column containing the start character index of the entity, by default \"start_char\" end_char : str, optional Name of the column containing the end character index of the entity, by default \"end_char\" \"\"\" def __init__ ( self , nlp : Language , start_char : str = \"start_char\" , end_char : str = \"end_char\" , ): self . start_char = start_char self . end_char = end_char self . nlp = nlp def preprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Preprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { self . start_char : \"start_char\" , self . end_char : \"end_char\" , } ) return note , note_nlp def postprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Postprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { \"start_char\" : self . start_char , \"end_char\" : self . end_char , } ) return note , note_nlp def omop2docs ( self , note : pd . DataFrame , note_nlp : pd . DataFrame , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms OMOP tables to a list of spaCy documents. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] List of spaCy documents. \"\"\" note , note_nlp = self . preprocess ( note , note_nlp ) return omop2docs ( note , note_nlp , self . nlp , extensions ) def docs2omop ( self , docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy documents to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of spaCy documents. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note , note_nlp = docs2omop ( docs , extensions = extensions ) note , note_nlp = self . postprocess ( note , note_nlp ) return note , note_nlp","title":"OmopConnector"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.start_char","text":"","title":"start_char"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.end_char","text":"","title":"end_char"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.nlp","text":"","title":"nlp"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.__init__","text":"Source code in edsnlp/connectors/omop.py 201 202 203 204 205 206 207 208 209 210 211 def __init__ ( self , nlp : Language , start_char : str = \"start_char\" , end_char : str = \"end_char\" , ): self . start_char = start_char self . end_char = end_char self . nlp = nlp","title":"__init__()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.preprocess","text":"Preprocess the input OMOP tables: modification of the column names. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def preprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Preprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { self . start_char : \"start_char\" , self . end_char : \"end_char\" , } ) return note , note_nlp","title":"preprocess()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.postprocess","text":"Postprocess the input OMOP tables: modification of the column names. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def postprocess ( self , note : pd . DataFrame , note_nlp : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Postprocess the input OMOP tables: modification of the column names. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note_nlp = note_nlp . rename ( columns = { \"start_char\" : self . start_char , \"end_char\" : self . end_char , } ) return note , note_nlp","title":"postprocess()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.omop2docs","text":"Transforms OMOP tables to a list of spaCy documents. PARAMETER DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION List[Doc] List of spaCy documents. Source code in edsnlp/connectors/omop.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def omop2docs ( self , note : pd . DataFrame , note_nlp : pd . DataFrame , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms OMOP tables to a list of spaCy documents. Parameters ---------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] List of spaCy documents. \"\"\" note , note_nlp = self . preprocess ( note , note_nlp ) return omop2docs ( note , note_nlp , self . nlp , extensions )","title":"omop2docs()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.docs2omop","text":"Transforms a list of spaCy documents to a pair of OMOP tables. PARAMETER DESCRIPTION docs List of spaCy documents. TYPE: List[Doc] extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION note OMOP note table. TYPE: pd.DataFrame note_nlp OMOP note_nlp table. TYPE: pd.DataFrame Source code in edsnlp/connectors/omop.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def docs2omop ( self , docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy documents to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of spaCy documents. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- note : pd.DataFrame OMOP `note` table. note_nlp : pd.DataFrame OMOP `note_nlp` table. \"\"\" note , note_nlp = docs2omop ( docs , extensions = extensions ) note , note_nlp = self . postprocess ( note , note_nlp ) return note , note_nlp","title":"docs2omop()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.omop2docs","text":"Transforms an OMOP-formatted pair of dataframes into a list of documents. PARAMETER DESCRIPTION note The OMOP note table. TYPE: pd.DataFrame note_nlp The OMOP note_nlp table TYPE: pd.DataFrame nlp spaCy language object. TYPE: Language extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION List[Doc] List of spaCy documents Source code in edsnlp/connectors/omop.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def omop2docs ( note : pd . DataFrame , note_nlp : pd . DataFrame , nlp : Language , extensions : Optional [ List [ str ]] = None , ) -> List [ Doc ]: \"\"\" Transforms an OMOP-formatted pair of dataframes into a list of documents. Parameters ---------- note : pd.DataFrame The OMOP `note` table. note_nlp : pd.DataFrame The OMOP `note_nlp` table nlp : Language spaCy language object. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- List[Doc] : List of spaCy documents \"\"\" note = note . copy () note_nlp = note_nlp . copy () extensions = extensions or [] def row2ent ( row ): d = dict ( start_char = row . start_char , end_char = row . end_char , label = row . get ( \"note_nlp_source_value\" ), extensions = { ext : row . get ( ext ) for ext in extensions }, ) return d # Create entities note_nlp [ \"ents\" ] = note_nlp . apply ( row2ent , axis = 1 ) note_nlp = note_nlp . groupby ( \"note_id\" , as_index = False )[ \"ents\" ] . agg ( list ) note = note . merge ( note_nlp , on = [ \"note_id\" ], how = \"left\" ) # Generate documents note [ \"doc\" ] = note . note_text . apply ( nlp ) # Process documents for _ , row in note . iterrows (): doc = row . doc doc . _ . note_id = row . note_id doc . _ . note_datetime = row . get ( \"note_datetime\" ) ents = [] if not isinstance ( row . ents , list ): continue for ent in row . ents : span = doc . char_span ( ent [ \"start_char\" ], ent [ \"end_char\" ], ent [ \"label\" ], alignment_mode = \"expand\" , ) for k , v in ent [ \"extensions\" ] . items (): setattr ( span . _ , k , v ) ents . append ( span ) if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [ span ] else : doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return list ( note . doc )","title":"omop2docs()"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.docs2omop","text":"Transforms a list of spaCy docs to a pair of OMOP tables. PARAMETER DESCRIPTION docs List of documents to transform. TYPE: List[Doc] extensions Extensions to keep, by default None TYPE: Optional[List[str]], optional DEFAULT: None RETURNS DESCRIPTION Tuple[pd.DataFrame, pd.DataFrame] Pair of OMOP tables ( note and note_nlp ) Source code in edsnlp/connectors/omop.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def docs2omop ( docs : List [ Doc ], extensions : Optional [ List [ str ]] = None , ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" Transforms a list of spaCy docs to a pair of OMOP tables. Parameters ---------- docs : List[Doc] List of documents to transform. extensions : Optional[List[str]], optional Extensions to keep, by default None Returns ------- Tuple[pd.DataFrame, pd.DataFrame] Pair of OMOP tables (`note` and `note_nlp`) \"\"\" df = pd . DataFrame ( dict ( doc = docs )) df [ \"note_text\" ] = df . doc . apply ( lambda doc : doc . text ) df [ \"note_id\" ] = df . doc . apply ( lambda doc : doc . _ . note_id ) df [ \"note_datetime\" ] = df . doc . apply ( lambda doc : doc . _ . note_datetime ) if df . note_id . isna () . any (): df [ \"note_id\" ] = range ( len ( df )) df [ \"ents\" ] = df . doc . apply ( lambda doc : list ( doc . ents )) df [ \"ents\" ] += df . doc . apply ( lambda doc : list ( doc . spans [ \"discarded\" ])) note = df [[ \"note_id\" , \"note_text\" , \"note_datetime\" ]] df = df [[ \"note_id\" , \"ents\" ]] . explode ( \"ents\" ) extensions = extensions or [] def ent2dict ( ent : Span , ) -> Dict [ str , Any ]: d = dict ( start_char = ent . start_char , end_char = ent . end_char , note_nlp_source_value = ent . label_ , lexical_variant = ent . text , # normalized_variant=ent._.normalized.text, ) for ext in extensions : d [ ext ] = getattr ( ent . _ , ext ) return d df [ \"ents\" ] = df . ents . apply ( ent2dict ) columns = [ \"start_char\" , \"end_char\" , \"note_nlp_source_value\" , \"lexical_variant\" , # \"normalized_variant\", ] columns += extensions df [ columns ] = df . ents . apply ( pd . Series ) df [ \"term_modifiers\" ] = \"\" for i , ext in enumerate ( extensions ): if i > 0 : df . term_modifiers += \";\" df . term_modifiers += ext + \"=\" + df [ ext ] . astype ( str ) df [ \"note_nlp_id\" ] = range ( len ( df )) note_nlp = df [[ \"note_nlp_id\" , \"note_id\" ] + columns ] return note , note_nlp","title":"docs2omop()"},{"location":"reference/matchers/","text":"edsnlp.matchers","title":"`edsnlp.matchers`"},{"location":"reference/matchers/#edsnlpmatchers","text":"","title":"edsnlp.matchers"},{"location":"reference/matchers/regex/","text":"edsnlp.matchers.regex RegexMatcher Bases: object Simple RegExp matcher. PARAMETER DESCRIPTION alignment_mode How spans should be aligned with tokens. Possible values are strict (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to expand . TYPE: str attr Default attribute to match on, by default \"TEXT\". Can be overiden in the add method. TYPE: str flags Additional flags provided to the re module. Can be overiden in the add method. TYPE: Union[re.RegexFlag, int] ignore_excluded Whether to skip exclusions TYPE: bool span_from_group If set to False , will create spans basede on the regex's full match. If set to True , will use the first matching capturing group as a span (and fall back to using the full match if no capturing group is matching) TYPE: bool Source code in edsnlp/matchers/regex.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 class RegexMatcher ( object ): \"\"\" Simple RegExp matcher. Parameters ---------- alignment_mode : str How spans should be aligned with tokens. Possible values are `strict` (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to `expand`. attr : str Default attribute to match on, by default \"TEXT\". Can be overiden in the `add` method. flags : Union[re.RegexFlag, int] Additional flags provided to the `re` module. Can be overiden in the `add` method. ignore_excluded : bool Whether to skip exclusions span_from_group : bool If set to `False`, will create spans basede on the regex's full match. If set to `True`, will use the first matching capturing group as a span (and fall back to using the full match if no capturing group is matching) \"\"\" def __init__ ( self , alignment_mode : str = \"expand\" , attr : str = \"TEXT\" , ignore_excluded : bool = False , flags : Union [ re . RegexFlag , int ] = 0 , # No additional flags span_from_group : bool = False , ): self . alignment_mode = alignment_mode self . regex = [] self . default_attr = attr self . flags = flags self . span_from_group = span_from_group self . ignore_excluded = ignore_excluded self . set_extensions () @classmethod def set_extensions ( cls ): if not Span . has_extension ( \"normalized_variant\" ): Span . set_extension ( \"normalized_variant\" , getter = get_normalized_variant ) def build_patterns ( self , regex : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- regex : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not regex : regex = dict () for key , patterns in regex . items (): if isinstance ( patterns , dict ): attr = patterns . get ( \"attr\" ) alignment_mode = patterns . get ( \"alignment_mode\" ) flags = patterns . get ( \"flags\" ) patterns = patterns . get ( \"regex\" ) else : attr = None alignment_mode = None flags = None if isinstance ( patterns , str ): patterns = [ patterns ] self . add ( key = key , patterns = patterns , attr = attr , alignment_mode = alignment_mode , flags = flags , ) def add ( self , key : str , patterns : List [ str ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , alignment_mode : Optional [ str ] = None , flags : Optional [ re . RegexFlag ] = None , ): \"\"\" Add a pattern to the registry. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Attribute to use for matching. By default uses the `default_attr` attribute ignore_excluded : bool, optional Whether to skip excluded tokens during matching. alignment_mode : str, optional Overwrite alignment mode. \"\"\" if attr is None : attr = self . default_attr if ignore_excluded is None : ignore_excluded = self . ignore_excluded if alignment_mode is None : alignment_mode = self . alignment_mode if flags is None : flags = self . flags patterns = [ compile_regex ( pattern , flags ) for pattern in patterns ] self . regex . append (( key , patterns , attr , ignore_excluded , alignment_mode )) def remove ( self , key : str , ): \"\"\" Remove a pattern for the registry. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError If the key is not present in the registered patterns. \"\"\" n = len ( self . regex ) self . regex = [( k , p , a , i , am ) for k , p , a , i , am in self . regex if k != key ] if len ( self . regex ) == n : raise ValueError ( f \"` { key } ` is not referenced in the matcher\" ) def __len__ ( self ): return len ( set ([ regex [ 0 ] for regex in self . regex ])) def match ( self , doclike : Union [ Doc , Span ], ) -> Tuple [ Span , re . Match ]: \"\"\" Iterates on the matches. Parameters ---------- doclike: spaCy Doc or Span object to match on. Yields ------- span: A match. \"\"\" for key , patterns , attr , ignore_excluded , alignment_mode in self . regex : text = get_text ( doclike , attr , ignore_excluded ) for pattern in patterns : for match in pattern . finditer ( text ): logger . trace ( f \"Matched a regex from { key } : { repr ( match . group ()) } \" ) start_char , end_char = span_from_match ( match = match , span_from_group = self . span_from_group , ) span = create_span ( doclike = doclike , start_char = start_char , end_char = end_char , key = key , attr = attr , alignment_mode = alignment_mode , ignore_excluded = ignore_excluded , ) if span is None : continue yield span , match def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , return_groupdict = False , ) -> Union [ Span , Tuple [ Span , Dict [ str , Any ]]]: \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Returns matches as spans. Yields ------ span: A match. groupdict: Additional information coming from the named patterns in the regular expression. \"\"\" for span , match in self . match ( doclike ): if not as_spans : offset = doclike [ 0 ] . i span = ( span . label , span . start - offset , span . end - offset ) if return_groupdict : yield span , match . groupdict () else : yield span alignment_mode = alignment_mode instance-attribute regex = [] instance-attribute default_attr = attr instance-attribute flags = flags instance-attribute span_from_group = span_from_group instance-attribute ignore_excluded = ignore_excluded instance-attribute __init__ ( alignment_mode = 'expand' , attr = 'TEXT' , ignore_excluded = False , flags = 0 , span_from_group = False ) Source code in edsnlp/matchers/regex.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def __init__ ( self , alignment_mode : str = \"expand\" , attr : str = \"TEXT\" , ignore_excluded : bool = False , flags : Union [ re . RegexFlag , int ] = 0 , # No additional flags span_from_group : bool = False , ): self . alignment_mode = alignment_mode self . regex = [] self . default_attr = attr self . flags = flags self . span_from_group = span_from_group self . ignore_excluded = ignore_excluded self . set_extensions () set_extensions () Source code in edsnlp/matchers/regex.py 214 215 216 217 @classmethod def set_extensions ( cls ): if not Span . has_extension ( \"normalized_variant\" ): Span . set_extension ( \"normalized_variant\" , getter = get_normalized_variant ) build_patterns ( regex ) Build patterns and adds them for matching. Helper function for pipelines using this matcher. PARAMETER DESCRIPTION regex Dictionary of label/terms, or label/dictionary of terms/attribute. TYPE: Patterns Source code in edsnlp/matchers/regex.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def build_patterns ( self , regex : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- regex : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not regex : regex = dict () for key , patterns in regex . items (): if isinstance ( patterns , dict ): attr = patterns . get ( \"attr\" ) alignment_mode = patterns . get ( \"alignment_mode\" ) flags = patterns . get ( \"flags\" ) patterns = patterns . get ( \"regex\" ) else : attr = None alignment_mode = None flags = None if isinstance ( patterns , str ): patterns = [ patterns ] self . add ( key = key , patterns = patterns , attr = attr , alignment_mode = alignment_mode , flags = flags , ) add ( key , patterns , attr = None , ignore_excluded = None , alignment_mode = None , flags = None ) Add a pattern to the registry. PARAMETER DESCRIPTION key Key of the new/updated pattern. TYPE: str patterns List of patterns to add. TYPE: List[str] attr Attribute to use for matching. By default uses the default_attr attribute TYPE: str, optional DEFAULT: None ignore_excluded Whether to skip excluded tokens during matching. TYPE: bool, optional DEFAULT: None alignment_mode Overwrite alignment mode. TYPE: str, optional DEFAULT: None Source code in edsnlp/matchers/regex.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def add ( self , key : str , patterns : List [ str ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , alignment_mode : Optional [ str ] = None , flags : Optional [ re . RegexFlag ] = None , ): \"\"\" Add a pattern to the registry. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Attribute to use for matching. By default uses the `default_attr` attribute ignore_excluded : bool, optional Whether to skip excluded tokens during matching. alignment_mode : str, optional Overwrite alignment mode. \"\"\" if attr is None : attr = self . default_attr if ignore_excluded is None : ignore_excluded = self . ignore_excluded if alignment_mode is None : alignment_mode = self . alignment_mode if flags is None : flags = self . flags patterns = [ compile_regex ( pattern , flags ) for pattern in patterns ] self . regex . append (( key , patterns , attr , ignore_excluded , alignment_mode )) remove ( key ) Remove a pattern for the registry. PARAMETER DESCRIPTION key key of the pattern to remove. TYPE: str RAISES DESCRIPTION ValueError If the key is not present in the registered patterns. Source code in edsnlp/matchers/regex.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def remove ( self , key : str , ): \"\"\" Remove a pattern for the registry. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError If the key is not present in the registered patterns. \"\"\" n = len ( self . regex ) self . regex = [( k , p , a , i , am ) for k , p , a , i , am in self . regex if k != key ] if len ( self . regex ) == n : raise ValueError ( f \"` { key } ` is not referenced in the matcher\" ) __len__ () Source code in edsnlp/matchers/regex.py 319 320 def __len__ ( self ): return len ( set ([ regex [ 0 ] for regex in self . regex ])) match ( doclike ) Iterates on the matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object to match on. TYPE: Union [ Doc , Span ] YIELDS DESCRIPTION span A match. Source code in edsnlp/matchers/regex.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def match ( self , doclike : Union [ Doc , Span ], ) -> Tuple [ Span , re . Match ]: \"\"\" Iterates on the matches. Parameters ---------- doclike: spaCy Doc or Span object to match on. Yields ------- span: A match. \"\"\" for key , patterns , attr , ignore_excluded , alignment_mode in self . regex : text = get_text ( doclike , attr , ignore_excluded ) for pattern in patterns : for match in pattern . finditer ( text ): logger . trace ( f \"Matched a regex from { key } : { repr ( match . group ()) } \" ) start_char , end_char = span_from_match ( match = match , span_from_group = self . span_from_group , ) span = create_span ( doclike = doclike , start_char = start_char , end_char = end_char , key = key , attr = attr , alignment_mode = alignment_mode , ignore_excluded = ignore_excluded , ) if span is None : continue yield span , match __call__ ( doclike , as_spans = False , return_groupdict = False ) Performs matching. Yields matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object. TYPE: Union [ Doc , Span ] as_spans Returns matches as spans. DEFAULT: False YIELDS DESCRIPTION span A match. groupdict Additional information coming from the named patterns in the regular expression. Source code in edsnlp/matchers/regex.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , return_groupdict = False , ) -> Union [ Span , Tuple [ Span , Dict [ str , Any ]]]: \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Returns matches as spans. Yields ------ span: A match. groupdict: Additional information coming from the named patterns in the regular expression. \"\"\" for span , match in self . match ( doclike ): if not as_spans : offset = doclike [ 0 ] . i span = ( span . label , span . start - offset , span . end - offset ) if return_groupdict : yield span , match . groupdict () else : yield span get_first_included ( doclike ) Source code in edsnlp/matchers/regex.py 13 14 15 16 17 18 @lru_cache ( 32 ) def get_first_included ( doclike : Union [ Doc , Span ]) -> Token : for token in doclike : if token . tag_ != \"EXCLUDED\" : return token raise IndexError ( \"The provided Span does not include any token\" ) get_normalized_variant ( doclike ) Source code in edsnlp/matchers/regex.py 21 22 23 24 25 26 def get_normalized_variant ( doclike ) -> str : tokens = [ t . text + t . whitespace_ for t in doclike if not t . _ . excluded ] variant = \"\" . join ( tokens ) variant = variant . rstrip ( \" \" ) variant = re . sub ( r \"\\s+\" , \" \" , variant ) return variant spans_generator ( match ) Iterates over every group, and then yields the full match PARAMETER DESCRIPTION match A match object TYPE: re.Match YIELDS DESCRIPTION Tuple[int, int] A tuple containing the start and end of the group or match Source code in edsnlp/matchers/regex.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def spans_generator ( match : re . Match ) -> Tuple [ int , int ]: \"\"\" Iterates over every group, and then yields the full match Parameters ---------- match : re.Match A match object Yields ------ Tuple[int, int] A tuple containing the start and end of the group or match \"\"\" for idx in range ( 1 , len ( match . groups ()) + 1 ): yield match . start ( idx ), match . end ( idx ) yield match . start ( 0 ), match . end ( 0 ) span_from_match ( match , span_from_group ) Return the span (as a (start, end) tuple) of the first matching group. If span_from_group=True , returns the full match instead. PARAMETER DESCRIPTION match The Match object TYPE: re.Match span_from_group Whether to work on groups or on the full match TYPE: bool RETURNS DESCRIPTION Tuple[int, int] A tuple containing the start and end of the group or match Source code in edsnlp/matchers/regex.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def span_from_match ( match : re . Match , span_from_group : bool , ) -> Tuple [ int , int ]: \"\"\" Return the span (as a (start, end) tuple) of the first matching group. If `span_from_group=True`, returns the full match instead. Parameters ---------- match : re.Match The Match object span_from_group : bool Whether to work on groups or on the full match Returns ------- Tuple[int, int] A tuple containing the start and end of the group or match \"\"\" if not span_from_group : start_char , end_char = match . start (), match . end () else : start_char , end_char = next ( filter ( lambda x : x [ 0 ] >= 0 , spans_generator ( match ))) return start_char , end_char create_span ( doclike , start_char , end_char , key , attr , alignment_mode , ignore_excluded ) spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this. PARAMETER DESCRIPTION doclike Doc or Span . TYPE: Union[Doc, Span] start_char Character index within the Doc-like object. TYPE: int end_char Character index of the end, within the Doc-like object. TYPE: int key The key used to match. TYPE: str alignment_mode The alignment mode. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool Returns span A span matched on the Doc-like object. Source code in edsnlp/matchers/regex.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def create_span ( doclike : Union [ Doc , Span ], start_char : int , end_char : int , key : str , attr : str , alignment_mode : str , ignore_excluded : bool , ) -> Span : \"\"\" spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this. Parameters ---------- doclike : Union[Doc, Span] `Doc` or `Span`. start_char : int Character index within the Doc-like object. end_char : int Character index of the end, within the Doc-like object. key : str The key used to match. alignment_mode : str The alignment mode. ignore_excluded : bool Whether to skip excluded tokens. Returns ------- span: A span matched on the Doc-like object. \"\"\" doc = doclike if isinstance ( doclike , Doc ) else doclike . doc # Handle the simple case immediately if attr in { \"TEXT\" , \"LOWER\" } and not ignore_excluded : off = doclike [ 0 ] . idx return doc . char_span ( start_char + off , end_char + off , label = key , alignment_mode = alignment_mode , ) # If doclike is a Span, we need to get the clean # index of the first included token if ignore_excluded : original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) first_included = get_first_included ( doclike ) i = bisect_left ( original , first_included . idx ) first = clean [ i ] else : first = doclike [ 0 ] . idx start_char = ( first + start_char + offset ( doc , attr = attr , ignore_excluded = ignore_excluded , index = first + start_char , ) ) end_char = ( first + end_char + offset ( doc , attr = attr , ignore_excluded = ignore_excluded , index = first + end_char , ) ) span = doc . char_span ( start_char , end_char , label = key , alignment_mode = alignment_mode , ) return span","title":"regex"},{"location":"reference/matchers/regex/#edsnlpmatchersregex","text":"","title":"edsnlp.matchers.regex"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher","text":"Bases: object Simple RegExp matcher. PARAMETER DESCRIPTION alignment_mode How spans should be aligned with tokens. Possible values are strict (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to expand . TYPE: str attr Default attribute to match on, by default \"TEXT\". Can be overiden in the add method. TYPE: str flags Additional flags provided to the re module. Can be overiden in the add method. TYPE: Union[re.RegexFlag, int] ignore_excluded Whether to skip exclusions TYPE: bool span_from_group If set to False , will create spans basede on the regex's full match. If set to True , will use the first matching capturing group as a span (and fall back to using the full match if no capturing group is matching) TYPE: bool Source code in edsnlp/matchers/regex.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 class RegexMatcher ( object ): \"\"\" Simple RegExp matcher. Parameters ---------- alignment_mode : str How spans should be aligned with tokens. Possible values are `strict` (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to `expand`. attr : str Default attribute to match on, by default \"TEXT\". Can be overiden in the `add` method. flags : Union[re.RegexFlag, int] Additional flags provided to the `re` module. Can be overiden in the `add` method. ignore_excluded : bool Whether to skip exclusions span_from_group : bool If set to `False`, will create spans basede on the regex's full match. If set to `True`, will use the first matching capturing group as a span (and fall back to using the full match if no capturing group is matching) \"\"\" def __init__ ( self , alignment_mode : str = \"expand\" , attr : str = \"TEXT\" , ignore_excluded : bool = False , flags : Union [ re . RegexFlag , int ] = 0 , # No additional flags span_from_group : bool = False , ): self . alignment_mode = alignment_mode self . regex = [] self . default_attr = attr self . flags = flags self . span_from_group = span_from_group self . ignore_excluded = ignore_excluded self . set_extensions () @classmethod def set_extensions ( cls ): if not Span . has_extension ( \"normalized_variant\" ): Span . set_extension ( \"normalized_variant\" , getter = get_normalized_variant ) def build_patterns ( self , regex : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- regex : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not regex : regex = dict () for key , patterns in regex . items (): if isinstance ( patterns , dict ): attr = patterns . get ( \"attr\" ) alignment_mode = patterns . get ( \"alignment_mode\" ) flags = patterns . get ( \"flags\" ) patterns = patterns . get ( \"regex\" ) else : attr = None alignment_mode = None flags = None if isinstance ( patterns , str ): patterns = [ patterns ] self . add ( key = key , patterns = patterns , attr = attr , alignment_mode = alignment_mode , flags = flags , ) def add ( self , key : str , patterns : List [ str ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , alignment_mode : Optional [ str ] = None , flags : Optional [ re . RegexFlag ] = None , ): \"\"\" Add a pattern to the registry. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Attribute to use for matching. By default uses the `default_attr` attribute ignore_excluded : bool, optional Whether to skip excluded tokens during matching. alignment_mode : str, optional Overwrite alignment mode. \"\"\" if attr is None : attr = self . default_attr if ignore_excluded is None : ignore_excluded = self . ignore_excluded if alignment_mode is None : alignment_mode = self . alignment_mode if flags is None : flags = self . flags patterns = [ compile_regex ( pattern , flags ) for pattern in patterns ] self . regex . append (( key , patterns , attr , ignore_excluded , alignment_mode )) def remove ( self , key : str , ): \"\"\" Remove a pattern for the registry. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError If the key is not present in the registered patterns. \"\"\" n = len ( self . regex ) self . regex = [( k , p , a , i , am ) for k , p , a , i , am in self . regex if k != key ] if len ( self . regex ) == n : raise ValueError ( f \"` { key } ` is not referenced in the matcher\" ) def __len__ ( self ): return len ( set ([ regex [ 0 ] for regex in self . regex ])) def match ( self , doclike : Union [ Doc , Span ], ) -> Tuple [ Span , re . Match ]: \"\"\" Iterates on the matches. Parameters ---------- doclike: spaCy Doc or Span object to match on. Yields ------- span: A match. \"\"\" for key , patterns , attr , ignore_excluded , alignment_mode in self . regex : text = get_text ( doclike , attr , ignore_excluded ) for pattern in patterns : for match in pattern . finditer ( text ): logger . trace ( f \"Matched a regex from { key } : { repr ( match . group ()) } \" ) start_char , end_char = span_from_match ( match = match , span_from_group = self . span_from_group , ) span = create_span ( doclike = doclike , start_char = start_char , end_char = end_char , key = key , attr = attr , alignment_mode = alignment_mode , ignore_excluded = ignore_excluded , ) if span is None : continue yield span , match def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , return_groupdict = False , ) -> Union [ Span , Tuple [ Span , Dict [ str , Any ]]]: \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Returns matches as spans. Yields ------ span: A match. groupdict: Additional information coming from the named patterns in the regular expression. \"\"\" for span , match in self . match ( doclike ): if not as_spans : offset = doclike [ 0 ] . i span = ( span . label , span . start - offset , span . end - offset ) if return_groupdict : yield span , match . groupdict () else : yield span","title":"RegexMatcher"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.alignment_mode","text":"","title":"alignment_mode"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.regex","text":"","title":"regex"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.default_attr","text":"","title":"default_attr"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.flags","text":"","title":"flags"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.span_from_group","text":"","title":"span_from_group"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.ignore_excluded","text":"","title":"ignore_excluded"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__init__","text":"Source code in edsnlp/matchers/regex.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def __init__ ( self , alignment_mode : str = \"expand\" , attr : str = \"TEXT\" , ignore_excluded : bool = False , flags : Union [ re . RegexFlag , int ] = 0 , # No additional flags span_from_group : bool = False , ): self . alignment_mode = alignment_mode self . regex = [] self . default_attr = attr self . flags = flags self . span_from_group = span_from_group self . ignore_excluded = ignore_excluded self . set_extensions ()","title":"__init__()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.set_extensions","text":"Source code in edsnlp/matchers/regex.py 214 215 216 217 @classmethod def set_extensions ( cls ): if not Span . has_extension ( \"normalized_variant\" ): Span . set_extension ( \"normalized_variant\" , getter = get_normalized_variant )","title":"set_extensions()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.build_patterns","text":"Build patterns and adds them for matching. Helper function for pipelines using this matcher. PARAMETER DESCRIPTION regex Dictionary of label/terms, or label/dictionary of terms/attribute. TYPE: Patterns Source code in edsnlp/matchers/regex.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 def build_patterns ( self , regex : Patterns ): \"\"\" Build patterns and adds them for matching. Helper function for pipelines using this matcher. Parameters ---------- regex : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" if not regex : regex = dict () for key , patterns in regex . items (): if isinstance ( patterns , dict ): attr = patterns . get ( \"attr\" ) alignment_mode = patterns . get ( \"alignment_mode\" ) flags = patterns . get ( \"flags\" ) patterns = patterns . get ( \"regex\" ) else : attr = None alignment_mode = None flags = None if isinstance ( patterns , str ): patterns = [ patterns ] self . add ( key = key , patterns = patterns , attr = attr , alignment_mode = alignment_mode , flags = flags , )","title":"build_patterns()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.add","text":"Add a pattern to the registry. PARAMETER DESCRIPTION key Key of the new/updated pattern. TYPE: str patterns List of patterns to add. TYPE: List[str] attr Attribute to use for matching. By default uses the default_attr attribute TYPE: str, optional DEFAULT: None ignore_excluded Whether to skip excluded tokens during matching. TYPE: bool, optional DEFAULT: None alignment_mode Overwrite alignment mode. TYPE: str, optional DEFAULT: None Source code in edsnlp/matchers/regex.py 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def add ( self , key : str , patterns : List [ str ], attr : Optional [ str ] = None , ignore_excluded : Optional [ bool ] = None , alignment_mode : Optional [ str ] = None , flags : Optional [ re . RegexFlag ] = None , ): \"\"\" Add a pattern to the registry. Parameters ---------- key : str Key of the new/updated pattern. patterns : List[str] List of patterns to add. attr : str, optional Attribute to use for matching. By default uses the `default_attr` attribute ignore_excluded : bool, optional Whether to skip excluded tokens during matching. alignment_mode : str, optional Overwrite alignment mode. \"\"\" if attr is None : attr = self . default_attr if ignore_excluded is None : ignore_excluded = self . ignore_excluded if alignment_mode is None : alignment_mode = self . alignment_mode if flags is None : flags = self . flags patterns = [ compile_regex ( pattern , flags ) for pattern in patterns ] self . regex . append (( key , patterns , attr , ignore_excluded , alignment_mode ))","title":"add()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.remove","text":"Remove a pattern for the registry. PARAMETER DESCRIPTION key key of the pattern to remove. TYPE: str RAISES DESCRIPTION ValueError If the key is not present in the registered patterns. Source code in edsnlp/matchers/regex.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def remove ( self , key : str , ): \"\"\" Remove a pattern for the registry. Parameters ---------- key : str key of the pattern to remove. Raises ------ ValueError If the key is not present in the registered patterns. \"\"\" n = len ( self . regex ) self . regex = [( k , p , a , i , am ) for k , p , a , i , am in self . regex if k != key ] if len ( self . regex ) == n : raise ValueError ( f \"` { key } ` is not referenced in the matcher\" )","title":"remove()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__len__","text":"Source code in edsnlp/matchers/regex.py 319 320 def __len__ ( self ): return len ( set ([ regex [ 0 ] for regex in self . regex ]))","title":"__len__()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match","text":"Iterates on the matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object to match on. TYPE: Union [ Doc , Span ] YIELDS DESCRIPTION span A match. Source code in edsnlp/matchers/regex.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def match ( self , doclike : Union [ Doc , Span ], ) -> Tuple [ Span , re . Match ]: \"\"\" Iterates on the matches. Parameters ---------- doclike: spaCy Doc or Span object to match on. Yields ------- span: A match. \"\"\" for key , patterns , attr , ignore_excluded , alignment_mode in self . regex : text = get_text ( doclike , attr , ignore_excluded ) for pattern in patterns : for match in pattern . finditer ( text ): logger . trace ( f \"Matched a regex from { key } : { repr ( match . group ()) } \" ) start_char , end_char = span_from_match ( match = match , span_from_group = self . span_from_group , ) span = create_span ( doclike = doclike , start_char = start_char , end_char = end_char , key = key , attr = attr , alignment_mode = alignment_mode , ignore_excluded = ignore_excluded , ) if span is None : continue yield span , match","title":"match()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__call__","text":"Performs matching. Yields matches. PARAMETER DESCRIPTION doclike spaCy Doc or Span object. TYPE: Union [ Doc , Span ] as_spans Returns matches as spans. DEFAULT: False YIELDS DESCRIPTION span A match. groupdict Additional information coming from the named patterns in the regular expression. Source code in edsnlp/matchers/regex.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def __call__ ( self , doclike : Union [ Doc , Span ], as_spans = False , return_groupdict = False , ) -> Union [ Span , Tuple [ Span , Dict [ str , Any ]]]: \"\"\" Performs matching. Yields matches. Parameters ---------- doclike: spaCy Doc or Span object. as_spans: Returns matches as spans. Yields ------ span: A match. groupdict: Additional information coming from the named patterns in the regular expression. \"\"\" for span , match in self . match ( doclike ): if not as_spans : offset = doclike [ 0 ] . i span = ( span . label , span . start - offset , span . end - offset ) if return_groupdict : yield span , match . groupdict () else : yield span","title":"__call__()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.get_first_included","text":"Source code in edsnlp/matchers/regex.py 13 14 15 16 17 18 @lru_cache ( 32 ) def get_first_included ( doclike : Union [ Doc , Span ]) -> Token : for token in doclike : if token . tag_ != \"EXCLUDED\" : return token raise IndexError ( \"The provided Span does not include any token\" )","title":"get_first_included()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.get_normalized_variant","text":"Source code in edsnlp/matchers/regex.py 21 22 23 24 25 26 def get_normalized_variant ( doclike ) -> str : tokens = [ t . text + t . whitespace_ for t in doclike if not t . _ . excluded ] variant = \"\" . join ( tokens ) variant = variant . rstrip ( \" \" ) variant = re . sub ( r \"\\s+\" , \" \" , variant ) return variant","title":"get_normalized_variant()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.spans_generator","text":"Iterates over every group, and then yields the full match PARAMETER DESCRIPTION match A match object TYPE: re.Match YIELDS DESCRIPTION Tuple[int, int] A tuple containing the start and end of the group or match Source code in edsnlp/matchers/regex.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def spans_generator ( match : re . Match ) -> Tuple [ int , int ]: \"\"\" Iterates over every group, and then yields the full match Parameters ---------- match : re.Match A match object Yields ------ Tuple[int, int] A tuple containing the start and end of the group or match \"\"\" for idx in range ( 1 , len ( match . groups ()) + 1 ): yield match . start ( idx ), match . end ( idx ) yield match . start ( 0 ), match . end ( 0 )","title":"spans_generator()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.span_from_match","text":"Return the span (as a (start, end) tuple) of the first matching group. If span_from_group=True , returns the full match instead. PARAMETER DESCRIPTION match The Match object TYPE: re.Match span_from_group Whether to work on groups or on the full match TYPE: bool RETURNS DESCRIPTION Tuple[int, int] A tuple containing the start and end of the group or match Source code in edsnlp/matchers/regex.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def span_from_match ( match : re . Match , span_from_group : bool , ) -> Tuple [ int , int ]: \"\"\" Return the span (as a (start, end) tuple) of the first matching group. If `span_from_group=True`, returns the full match instead. Parameters ---------- match : re.Match The Match object span_from_group : bool Whether to work on groups or on the full match Returns ------- Tuple[int, int] A tuple containing the start and end of the group or match \"\"\" if not span_from_group : start_char , end_char = match . start (), match . end () else : start_char , end_char = next ( filter ( lambda x : x [ 0 ] >= 0 , spans_generator ( match ))) return start_char , end_char","title":"span_from_match()"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.create_span","text":"spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this. PARAMETER DESCRIPTION doclike Doc or Span . TYPE: Union[Doc, Span] start_char Character index within the Doc-like object. TYPE: int end_char Character index of the end, within the Doc-like object. TYPE: int key The key used to match. TYPE: str alignment_mode The alignment mode. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool Returns span A span matched on the Doc-like object. Source code in edsnlp/matchers/regex.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def create_span ( doclike : Union [ Doc , Span ], start_char : int , end_char : int , key : str , attr : str , alignment_mode : str , ignore_excluded : bool , ) -> Span : \"\"\" spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this. Parameters ---------- doclike : Union[Doc, Span] `Doc` or `Span`. start_char : int Character index within the Doc-like object. end_char : int Character index of the end, within the Doc-like object. key : str The key used to match. alignment_mode : str The alignment mode. ignore_excluded : bool Whether to skip excluded tokens. Returns ------- span: A span matched on the Doc-like object. \"\"\" doc = doclike if isinstance ( doclike , Doc ) else doclike . doc # Handle the simple case immediately if attr in { \"TEXT\" , \"LOWER\" } and not ignore_excluded : off = doclike [ 0 ] . idx return doc . char_span ( start_char + off , end_char + off , label = key , alignment_mode = alignment_mode , ) # If doclike is a Span, we need to get the clean # index of the first included token if ignore_excluded : original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) first_included = get_first_included ( doclike ) i = bisect_left ( original , first_included . idx ) first = clean [ i ] else : first = doclike [ 0 ] . idx start_char = ( first + start_char + offset ( doc , attr = attr , ignore_excluded = ignore_excluded , index = first + start_char , ) ) end_char = ( first + end_char + offset ( doc , attr = attr , ignore_excluded = ignore_excluded , index = first + end_char , ) ) span = doc . char_span ( start_char , end_char , label = key , alignment_mode = alignment_mode , ) return span","title":"create_span()"},{"location":"reference/matchers/simstring/","text":"edsnlp.matchers.simstring SimstringWriter Source code in edsnlp/matchers/simstring.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class SimstringWriter : def __init__ ( self , path : Union [ str , Path ]): \"\"\" A context class to write a simstring database Parameters ---------- path: Union[str, Path] Path to database \"\"\" os . makedirs ( path , exist_ok = True ) self . path = path def __enter__ ( self ): path = os . path . join ( self . path , \"terms.simstring\" ) self . db = simstring . writer ( path , 3 , False , True ) return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . db . close () def insert ( self , term ): self . db . insert ( term ) path = path instance-attribute __init__ ( path ) A context class to write a simstring database PARAMETER DESCRIPTION path Path to database TYPE: Union [ str , Path ] Source code in edsnlp/matchers/simstring.py 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , path : Union [ str , Path ]): \"\"\" A context class to write a simstring database Parameters ---------- path: Union[str, Path] Path to database \"\"\" os . makedirs ( path , exist_ok = True ) self . path = path __enter__ () Source code in edsnlp/matchers/simstring.py 32 33 34 35 def __enter__ ( self ): path = os . path . join ( self . path , \"terms.simstring\" ) self . db = simstring . writer ( path , 3 , False , True ) return self __exit__ ( exc_type , exc_val , exc_tb ) Source code in edsnlp/matchers/simstring.py 37 38 def __exit__ ( self , exc_type , exc_val , exc_tb ): self . db . close () insert ( term ) Source code in edsnlp/matchers/simstring.py 40 41 def insert ( self , term ): self . db . insert ( term ) SimilarityMeasure Bases: str , Enum Source code in edsnlp/matchers/simstring.py 44 45 46 47 48 class SimilarityMeasure ( str , Enum ): jaccard = \"jaccard\" dice = \"dice\" overlap = \"overlap\" cosine = \"cosine\" jaccard = 'jaccard' class-attribute dice = 'dice' class-attribute overlap = 'overlap' class-attribute cosine = 'cosine' class-attribute SimstringMatcher Source code in edsnlp/matchers/simstring.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 class SimstringMatcher : def __init__ ( self , vocab : Vocab , path : Optional [ Union [ Path , str ]] = None , measure : SimilarityMeasure = SimilarityMeasure . dice , threshold : float = 0.75 , windows : int = 5 , ignore_excluded : bool = False , attr : str = \"NORM\" , ): \"\"\" PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS Parameters ---------- vocab : Vocab spaCy vocabulary to match on. path: Optional[Union[Path, str]] Path where we will store the precomputed patterns measure: SimilarityMeasure Name of the similarity measure. One of [jaccard, dice, overlap, cosine] windows: int Maximum number of words in a candidate span threshold: float Minimum similarity value to match a concept's synonym ignore_excluded : bool, optional Whether to exclude tokens that have a \"SPACE\" tag, by default False attr : str Default attribute to match on, by default \"TEXT\". Can be overridden in the `add` method. To match on a custom attribute, prepend the attribute name with `_`. \"\"\" assert measure in ( SimilarityMeasure . jaccard , SimilarityMeasure . dice , SimilarityMeasure . overlap , SimilarityMeasure . cosine , ) self . vocab = vocab self . windows = windows self . measure = measure self . threshold = threshold self . ignore_excluded = ignore_excluded self . attr = attr if path is None : path = tempfile . mkdtemp () self . path = Path ( path ) self . ss_reader = None self . syn2cuis = None def build_patterns ( self , nlp : Language , terms : Dict [ str , Iterable [ str ]]): \"\"\" Build patterns and adds them for matching. Parameters ---------- nlp : Language The instance of the spaCy language class. terms : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" self . ss_reader = None self . syn2cuis = None syn2cuis = defaultdict ( lambda : []) token_pipelines = [ name for name , pipe in nlp . pipeline if any ( \"token\" in assign and not assign == \"token.is_sent_start\" for assign in nlp . get_pipe_meta ( name ) . assigns ) ] with nlp . select_pipes ( enable = token_pipelines ): with SimstringWriter ( self . path ) as ss_db : for cui , synset in tqdm ( terms . items ()): for term in nlp . pipe ( synset ): norm_text = get_text ( term , self . attr , ignore_excluded = self . ignore_excluded ) term = \"##\" + norm_text + \"##\" ss_db . insert ( term ) syn2cuis [ term ] . append ( cui ) syn2cuis = { term : tuple ( sorted ( set ( cuis ))) for term , cuis in syn2cuis . items ()} with open ( self . path / \"cui-db.pkl\" , \"wb\" ) as f : pickle . dump ( syn2cuis , f ) def load ( self ): if self . ss_reader is None : self . ss_reader = simstring . reader ( os . path . join ( self . path , \"terms.simstring\" ) ) self . ss_reader . measure = getattr ( simstring , self . measure ) self . ss_reader . threshold = self . threshold with open ( os . path . join ( self . path , \"cui-db.pkl\" ), \"rb\" ) as f : self . syn2cuis = pickle . load ( f ) def __call__ ( self , doc , as_spans = False ): self . load () root = getattr ( doc , \"doc\" , doc ) if root . has_annotation ( \"IS_SENT_START\" ): sents = tuple ( doc . sents ) else : sents = ( doc ,) ents : List [ Tuple [ str , int , int , float ]] = [] for sent in sents : text , offsets = get_text_and_offsets ( doclike = sent , attr = self . attr , ignore_excluded = self . ignore_excluded , ) sent_start = getattr ( sent , \"start\" , 0 ) for size in range ( 1 , self . windows ): for i in range ( 0 , len ( offsets ) - size ): begin_char , _ , begin_i = offsets [ i ] _ , end_char , end_i = offsets [ i + size ] span_text = \"##\" + text [ begin_char : end_char ] + \"##\" matches = self . ss_reader . retrieve ( span_text ) for res in matches : sim = similarity ( span_text , res , measure = self . measure ) for cui in self . syn2cuis [ res ]: ents . append ( ( cui , begin_i + sent_start , end_i + sent_start , sim ) ) sorted_spans = sorted ( ents , key = simstring_sort_key , reverse = True ) results = [] seen_tokens = set () for span in sorted_spans : # Check for end - 1 here because boundaries are inclusive span_tokens = set ( range ( span [ 1 ], span [ 2 ])) if not ( span_tokens & seen_tokens ): results . append ( span ) seen_tokens . update ( span_tokens ) results = sorted ( results , key = lambda span : span [ 1 ]) if as_spans : spans = [ Span ( root , span_data [ 1 ], span_data [ 2 ], span_data [ 0 ]) for span_data in results ] return spans else : return [( self . vocab . strings [ span [ 0 ]], span [ 1 ], span [ 2 ]) for span in results ] vocab = vocab instance-attribute windows = windows instance-attribute measure = measure instance-attribute threshold = threshold instance-attribute ignore_excluded = ignore_excluded instance-attribute attr = attr instance-attribute path = Path ( path ) instance-attribute ss_reader = None instance-attribute syn2cuis = None instance-attribute __init__ ( vocab , path = None , measure = SimilarityMeasure . dice , threshold = 0.75 , windows = 5 , ignore_excluded = False , attr = 'NORM' ) PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS PARAMETER DESCRIPTION vocab spaCy vocabulary to match on. TYPE: Vocab path Path where we will store the precomputed patterns TYPE: Optional [ Union [ Path , str ]] DEFAULT: None measure Name of the similarity measure. One of [jaccard, dice, overlap, cosine] TYPE: SimilarityMeasure DEFAULT: SimilarityMeasure.dice windows Maximum number of words in a candidate span TYPE: int DEFAULT: 5 threshold Minimum similarity value to match a concept's synonym TYPE: float DEFAULT: 0.75 ignore_excluded Whether to exclude tokens that have a \"SPACE\" tag, by default False TYPE: bool, optional DEFAULT: False attr Default attribute to match on, by default \"TEXT\". Can be overridden in the add method. To match on a custom attribute, prepend the attribute name with _ . TYPE: str DEFAULT: 'NORM' Source code in edsnlp/matchers/simstring.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , vocab : Vocab , path : Optional [ Union [ Path , str ]] = None , measure : SimilarityMeasure = SimilarityMeasure . dice , threshold : float = 0.75 , windows : int = 5 , ignore_excluded : bool = False , attr : str = \"NORM\" , ): \"\"\" PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS Parameters ---------- vocab : Vocab spaCy vocabulary to match on. path: Optional[Union[Path, str]] Path where we will store the precomputed patterns measure: SimilarityMeasure Name of the similarity measure. One of [jaccard, dice, overlap, cosine] windows: int Maximum number of words in a candidate span threshold: float Minimum similarity value to match a concept's synonym ignore_excluded : bool, optional Whether to exclude tokens that have a \"SPACE\" tag, by default False attr : str Default attribute to match on, by default \"TEXT\". Can be overridden in the `add` method. To match on a custom attribute, prepend the attribute name with `_`. \"\"\" assert measure in ( SimilarityMeasure . jaccard , SimilarityMeasure . dice , SimilarityMeasure . overlap , SimilarityMeasure . cosine , ) self . vocab = vocab self . windows = windows self . measure = measure self . threshold = threshold self . ignore_excluded = ignore_excluded self . attr = attr if path is None : path = tempfile . mkdtemp () self . path = Path ( path ) self . ss_reader = None self . syn2cuis = None build_patterns ( nlp , terms ) Build patterns and adds them for matching. PARAMETER DESCRIPTION nlp The instance of the spaCy language class. TYPE: Language terms Dictionary of label/terms, or label/dictionary of terms/attribute. TYPE: Patterns Source code in edsnlp/matchers/simstring.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def build_patterns ( self , nlp : Language , terms : Dict [ str , Iterable [ str ]]): \"\"\" Build patterns and adds them for matching. Parameters ---------- nlp : Language The instance of the spaCy language class. terms : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" self . ss_reader = None self . syn2cuis = None syn2cuis = defaultdict ( lambda : []) token_pipelines = [ name for name , pipe in nlp . pipeline if any ( \"token\" in assign and not assign == \"token.is_sent_start\" for assign in nlp . get_pipe_meta ( name ) . assigns ) ] with nlp . select_pipes ( enable = token_pipelines ): with SimstringWriter ( self . path ) as ss_db : for cui , synset in tqdm ( terms . items ()): for term in nlp . pipe ( synset ): norm_text = get_text ( term , self . attr , ignore_excluded = self . ignore_excluded ) term = \"##\" + norm_text + \"##\" ss_db . insert ( term ) syn2cuis [ term ] . append ( cui ) syn2cuis = { term : tuple ( sorted ( set ( cuis ))) for term , cuis in syn2cuis . items ()} with open ( self . path / \"cui-db.pkl\" , \"wb\" ) as f : pickle . dump ( syn2cuis , f ) load () Source code in edsnlp/matchers/simstring.py 146 147 148 149 150 151 152 153 154 155 def load ( self ): if self . ss_reader is None : self . ss_reader = simstring . reader ( os . path . join ( self . path , \"terms.simstring\" ) ) self . ss_reader . measure = getattr ( simstring , self . measure ) self . ss_reader . threshold = self . threshold with open ( os . path . join ( self . path , \"cui-db.pkl\" ), \"rb\" ) as f : self . syn2cuis = pickle . load ( f ) __call__ ( doc , as_spans = False ) Source code in edsnlp/matchers/simstring.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def __call__ ( self , doc , as_spans = False ): self . load () root = getattr ( doc , \"doc\" , doc ) if root . has_annotation ( \"IS_SENT_START\" ): sents = tuple ( doc . sents ) else : sents = ( doc ,) ents : List [ Tuple [ str , int , int , float ]] = [] for sent in sents : text , offsets = get_text_and_offsets ( doclike = sent , attr = self . attr , ignore_excluded = self . ignore_excluded , ) sent_start = getattr ( sent , \"start\" , 0 ) for size in range ( 1 , self . windows ): for i in range ( 0 , len ( offsets ) - size ): begin_char , _ , begin_i = offsets [ i ] _ , end_char , end_i = offsets [ i + size ] span_text = \"##\" + text [ begin_char : end_char ] + \"##\" matches = self . ss_reader . retrieve ( span_text ) for res in matches : sim = similarity ( span_text , res , measure = self . measure ) for cui in self . syn2cuis [ res ]: ents . append ( ( cui , begin_i + sent_start , end_i + sent_start , sim ) ) sorted_spans = sorted ( ents , key = simstring_sort_key , reverse = True ) results = [] seen_tokens = set () for span in sorted_spans : # Check for end - 1 here because boundaries are inclusive span_tokens = set ( range ( span [ 1 ], span [ 2 ])) if not ( span_tokens & seen_tokens ): results . append ( span ) seen_tokens . update ( span_tokens ) results = sorted ( results , key = lambda span : span [ 1 ]) if as_spans : spans = [ Span ( root , span_data [ 1 ], span_data [ 2 ], span_data [ 0 ]) for span_data in results ] return spans else : return [( self . vocab . strings [ span [ 0 ]], span [ 1 ], span [ 2 ]) for span in results ] similarity ( x , y , measure = SimilarityMeasure . dice ) Source code in edsnlp/matchers/simstring.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def similarity ( x : str , y : str , measure : SimilarityMeasure = SimilarityMeasure . dice ): x_ngrams = { x [ i : i + 3 ] for i in range ( 0 , len ( x ) - 3 )} y_ngrams = { y [ i : i + 3 ] for i in range ( 0 , len ( y ) - 3 )} if measure == SimilarityMeasure . jaccard : return len ( x_ngrams & y_ngrams ) / ( len ( x_ngrams | y_ngrams )) if measure == SimilarityMeasure . dice : return 2 * len ( x_ngrams & y_ngrams ) / ( len ( x_ngrams ) + len ( y_ngrams )) if measure == SimilarityMeasure . cosine : return len ( x_ngrams & y_ngrams ) / sqrt ( len ( x_ngrams ) * len ( y_ngrams )) if measure == SimilarityMeasure . overlap : return len ( x_ngrams & y_ngrams ) raise ValueError ( \"Cannot compute similarity {} \" . format ( repr ( measure ))) simstring_sort_key ( span_data ) Source code in edsnlp/matchers/simstring.py 228 229 def simstring_sort_key ( span_data : Tuple [ str , int , int , float ]): return span_data [ 3 ], span_data [ 2 ] - span_data [ 1 ], - span_data [ 1 ] get_text_and_offsets ( doclike , attr = 'TEXT' , ignore_excluded = True ) Align different representations of a Doc or Span object. PARAMETER DESCRIPTION doclike spaCy Doc or Span object TYPE: Doc attr Attribute to use, by default \"TEXT\" TYPE: str, optional DEFAULT: 'TEXT' ignore_excluded Whether to remove excluded tokens, by default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION Tuple[str, List[Tuple[int, int, int]]] The new clean text and offset tuples for each word giving the begin char indice of the word in the new text, the end char indice of its preceding word and the indice of the word in the original document Source code in edsnlp/matchers/simstring.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 @lru_cache ( maxsize = 128 ) def get_text_and_offsets ( doclike : Union [ Span , Doc ], attr : str = \"TEXT\" , ignore_excluded : bool = True , ) -> Tuple [ str , List [ Tuple [ int , int , int ]]]: \"\"\" Align different representations of a `Doc` or `Span` object. Parameters ---------- doclike : Doc spaCy `Doc` or `Span` object attr : str, optional Attribute to use, by default `\"TEXT\"` ignore_excluded : bool, optional Whether to remove excluded tokens, by default True Returns ------- Tuple[str, List[Tuple[int, int, int]]] The new clean text and offset tuples for each word giving the begin char indice of the word in the new text, the end char indice of its preceding word and the indice of the word in the original document \"\"\" attr = attr . upper () attr = ATTRIBUTES . get ( attr , attr ) custom = attr . startswith ( \"_\" ) if custom : attr = attr [ 1 :] . lower () offsets = [] cursor = 0 text = [] last = cursor for i , token in enumerate ( doclike ): if not ignore_excluded or not token . _ . excluded : if custom : token_text = getattr ( token . _ , attr ) else : token_text = getattr ( token , attr ) # We add the cursor end = cursor + len ( token_text ) offsets . append (( cursor , last , i )) cursor = end last = end text . append ( token_text ) if token . whitespace_ : cursor += 1 text . append ( \" \" ) offsets . append (( cursor , last , len ( doclike ))) return \"\" . join ( text ), offsets","title":"simstring"},{"location":"reference/matchers/simstring/#edsnlpmatcherssimstring","text":"","title":"edsnlp.matchers.simstring"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter","text":"Source code in edsnlp/matchers/simstring.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class SimstringWriter : def __init__ ( self , path : Union [ str , Path ]): \"\"\" A context class to write a simstring database Parameters ---------- path: Union[str, Path] Path to database \"\"\" os . makedirs ( path , exist_ok = True ) self . path = path def __enter__ ( self ): path = os . path . join ( self . path , \"terms.simstring\" ) self . db = simstring . writer ( path , 3 , False , True ) return self def __exit__ ( self , exc_type , exc_val , exc_tb ): self . db . close () def insert ( self , term ): self . db . insert ( term )","title":"SimstringWriter"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter.path","text":"","title":"path"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter.__init__","text":"A context class to write a simstring database PARAMETER DESCRIPTION path Path to database TYPE: Union [ str , Path ] Source code in edsnlp/matchers/simstring.py 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , path : Union [ str , Path ]): \"\"\" A context class to write a simstring database Parameters ---------- path: Union[str, Path] Path to database \"\"\" os . makedirs ( path , exist_ok = True ) self . path = path","title":"__init__()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter.__enter__","text":"Source code in edsnlp/matchers/simstring.py 32 33 34 35 def __enter__ ( self ): path = os . path . join ( self . path , \"terms.simstring\" ) self . db = simstring . writer ( path , 3 , False , True ) return self","title":"__enter__()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter.__exit__","text":"Source code in edsnlp/matchers/simstring.py 37 38 def __exit__ ( self , exc_type , exc_val , exc_tb ): self . db . close ()","title":"__exit__()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter.insert","text":"Source code in edsnlp/matchers/simstring.py 40 41 def insert ( self , term ): self . db . insert ( term )","title":"insert()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimilarityMeasure","text":"Bases: str , Enum Source code in edsnlp/matchers/simstring.py 44 45 46 47 48 class SimilarityMeasure ( str , Enum ): jaccard = \"jaccard\" dice = \"dice\" overlap = \"overlap\" cosine = \"cosine\"","title":"SimilarityMeasure"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimilarityMeasure.jaccard","text":"","title":"jaccard"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimilarityMeasure.dice","text":"","title":"dice"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimilarityMeasure.overlap","text":"","title":"overlap"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimilarityMeasure.cosine","text":"","title":"cosine"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher","text":"Source code in edsnlp/matchers/simstring.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 class SimstringMatcher : def __init__ ( self , vocab : Vocab , path : Optional [ Union [ Path , str ]] = None , measure : SimilarityMeasure = SimilarityMeasure . dice , threshold : float = 0.75 , windows : int = 5 , ignore_excluded : bool = False , attr : str = \"NORM\" , ): \"\"\" PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS Parameters ---------- vocab : Vocab spaCy vocabulary to match on. path: Optional[Union[Path, str]] Path where we will store the precomputed patterns measure: SimilarityMeasure Name of the similarity measure. One of [jaccard, dice, overlap, cosine] windows: int Maximum number of words in a candidate span threshold: float Minimum similarity value to match a concept's synonym ignore_excluded : bool, optional Whether to exclude tokens that have a \"SPACE\" tag, by default False attr : str Default attribute to match on, by default \"TEXT\". Can be overridden in the `add` method. To match on a custom attribute, prepend the attribute name with `_`. \"\"\" assert measure in ( SimilarityMeasure . jaccard , SimilarityMeasure . dice , SimilarityMeasure . overlap , SimilarityMeasure . cosine , ) self . vocab = vocab self . windows = windows self . measure = measure self . threshold = threshold self . ignore_excluded = ignore_excluded self . attr = attr if path is None : path = tempfile . mkdtemp () self . path = Path ( path ) self . ss_reader = None self . syn2cuis = None def build_patterns ( self , nlp : Language , terms : Dict [ str , Iterable [ str ]]): \"\"\" Build patterns and adds them for matching. Parameters ---------- nlp : Language The instance of the spaCy language class. terms : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" self . ss_reader = None self . syn2cuis = None syn2cuis = defaultdict ( lambda : []) token_pipelines = [ name for name , pipe in nlp . pipeline if any ( \"token\" in assign and not assign == \"token.is_sent_start\" for assign in nlp . get_pipe_meta ( name ) . assigns ) ] with nlp . select_pipes ( enable = token_pipelines ): with SimstringWriter ( self . path ) as ss_db : for cui , synset in tqdm ( terms . items ()): for term in nlp . pipe ( synset ): norm_text = get_text ( term , self . attr , ignore_excluded = self . ignore_excluded ) term = \"##\" + norm_text + \"##\" ss_db . insert ( term ) syn2cuis [ term ] . append ( cui ) syn2cuis = { term : tuple ( sorted ( set ( cuis ))) for term , cuis in syn2cuis . items ()} with open ( self . path / \"cui-db.pkl\" , \"wb\" ) as f : pickle . dump ( syn2cuis , f ) def load ( self ): if self . ss_reader is None : self . ss_reader = simstring . reader ( os . path . join ( self . path , \"terms.simstring\" ) ) self . ss_reader . measure = getattr ( simstring , self . measure ) self . ss_reader . threshold = self . threshold with open ( os . path . join ( self . path , \"cui-db.pkl\" ), \"rb\" ) as f : self . syn2cuis = pickle . load ( f ) def __call__ ( self , doc , as_spans = False ): self . load () root = getattr ( doc , \"doc\" , doc ) if root . has_annotation ( \"IS_SENT_START\" ): sents = tuple ( doc . sents ) else : sents = ( doc ,) ents : List [ Tuple [ str , int , int , float ]] = [] for sent in sents : text , offsets = get_text_and_offsets ( doclike = sent , attr = self . attr , ignore_excluded = self . ignore_excluded , ) sent_start = getattr ( sent , \"start\" , 0 ) for size in range ( 1 , self . windows ): for i in range ( 0 , len ( offsets ) - size ): begin_char , _ , begin_i = offsets [ i ] _ , end_char , end_i = offsets [ i + size ] span_text = \"##\" + text [ begin_char : end_char ] + \"##\" matches = self . ss_reader . retrieve ( span_text ) for res in matches : sim = similarity ( span_text , res , measure = self . measure ) for cui in self . syn2cuis [ res ]: ents . append ( ( cui , begin_i + sent_start , end_i + sent_start , sim ) ) sorted_spans = sorted ( ents , key = simstring_sort_key , reverse = True ) results = [] seen_tokens = set () for span in sorted_spans : # Check for end - 1 here because boundaries are inclusive span_tokens = set ( range ( span [ 1 ], span [ 2 ])) if not ( span_tokens & seen_tokens ): results . append ( span ) seen_tokens . update ( span_tokens ) results = sorted ( results , key = lambda span : span [ 1 ]) if as_spans : spans = [ Span ( root , span_data [ 1 ], span_data [ 2 ], span_data [ 0 ]) for span_data in results ] return spans else : return [( self . vocab . strings [ span [ 0 ]], span [ 1 ], span [ 2 ]) for span in results ]","title":"SimstringMatcher"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.vocab","text":"","title":"vocab"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.windows","text":"","title":"windows"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.measure","text":"","title":"measure"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.threshold","text":"","title":"threshold"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.ignore_excluded","text":"","title":"ignore_excluded"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.attr","text":"","title":"attr"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.path","text":"","title":"path"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.ss_reader","text":"","title":"ss_reader"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.syn2cuis","text":"","title":"syn2cuis"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.__init__","text":"PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS PARAMETER DESCRIPTION vocab spaCy vocabulary to match on. TYPE: Vocab path Path where we will store the precomputed patterns TYPE: Optional [ Union [ Path , str ]] DEFAULT: None measure Name of the similarity measure. One of [jaccard, dice, overlap, cosine] TYPE: SimilarityMeasure DEFAULT: SimilarityMeasure.dice windows Maximum number of words in a candidate span TYPE: int DEFAULT: 5 threshold Minimum similarity value to match a concept's synonym TYPE: float DEFAULT: 0.75 ignore_excluded Whether to exclude tokens that have a \"SPACE\" tag, by default False TYPE: bool, optional DEFAULT: False attr Default attribute to match on, by default \"TEXT\". Can be overridden in the add method. To match on a custom attribute, prepend the attribute name with _ . TYPE: str DEFAULT: 'NORM' Source code in edsnlp/matchers/simstring.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , vocab : Vocab , path : Optional [ Union [ Path , str ]] = None , measure : SimilarityMeasure = SimilarityMeasure . dice , threshold : float = 0.75 , windows : int = 5 , ignore_excluded : bool = False , attr : str = \"NORM\" , ): \"\"\" PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS Parameters ---------- vocab : Vocab spaCy vocabulary to match on. path: Optional[Union[Path, str]] Path where we will store the precomputed patterns measure: SimilarityMeasure Name of the similarity measure. One of [jaccard, dice, overlap, cosine] windows: int Maximum number of words in a candidate span threshold: float Minimum similarity value to match a concept's synonym ignore_excluded : bool, optional Whether to exclude tokens that have a \"SPACE\" tag, by default False attr : str Default attribute to match on, by default \"TEXT\". Can be overridden in the `add` method. To match on a custom attribute, prepend the attribute name with `_`. \"\"\" assert measure in ( SimilarityMeasure . jaccard , SimilarityMeasure . dice , SimilarityMeasure . overlap , SimilarityMeasure . cosine , ) self . vocab = vocab self . windows = windows self . measure = measure self . threshold = threshold self . ignore_excluded = ignore_excluded self . attr = attr if path is None : path = tempfile . mkdtemp () self . path = Path ( path ) self . ss_reader = None self . syn2cuis = None","title":"__init__()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.build_patterns","text":"Build patterns and adds them for matching. PARAMETER DESCRIPTION nlp The instance of the spaCy language class. TYPE: Language terms Dictionary of label/terms, or label/dictionary of terms/attribute. TYPE: Patterns Source code in edsnlp/matchers/simstring.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def build_patterns ( self , nlp : Language , terms : Dict [ str , Iterable [ str ]]): \"\"\" Build patterns and adds them for matching. Parameters ---------- nlp : Language The instance of the spaCy language class. terms : Patterns Dictionary of label/terms, or label/dictionary of terms/attribute. \"\"\" self . ss_reader = None self . syn2cuis = None syn2cuis = defaultdict ( lambda : []) token_pipelines = [ name for name , pipe in nlp . pipeline if any ( \"token\" in assign and not assign == \"token.is_sent_start\" for assign in nlp . get_pipe_meta ( name ) . assigns ) ] with nlp . select_pipes ( enable = token_pipelines ): with SimstringWriter ( self . path ) as ss_db : for cui , synset in tqdm ( terms . items ()): for term in nlp . pipe ( synset ): norm_text = get_text ( term , self . attr , ignore_excluded = self . ignore_excluded ) term = \"##\" + norm_text + \"##\" ss_db . insert ( term ) syn2cuis [ term ] . append ( cui ) syn2cuis = { term : tuple ( sorted ( set ( cuis ))) for term , cuis in syn2cuis . items ()} with open ( self . path / \"cui-db.pkl\" , \"wb\" ) as f : pickle . dump ( syn2cuis , f )","title":"build_patterns()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.load","text":"Source code in edsnlp/matchers/simstring.py 146 147 148 149 150 151 152 153 154 155 def load ( self ): if self . ss_reader is None : self . ss_reader = simstring . reader ( os . path . join ( self . path , \"terms.simstring\" ) ) self . ss_reader . measure = getattr ( simstring , self . measure ) self . ss_reader . threshold = self . threshold with open ( os . path . join ( self . path , \"cui-db.pkl\" ), \"rb\" ) as f : self . syn2cuis = pickle . load ( f )","title":"load()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.__call__","text":"Source code in edsnlp/matchers/simstring.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def __call__ ( self , doc , as_spans = False ): self . load () root = getattr ( doc , \"doc\" , doc ) if root . has_annotation ( \"IS_SENT_START\" ): sents = tuple ( doc . sents ) else : sents = ( doc ,) ents : List [ Tuple [ str , int , int , float ]] = [] for sent in sents : text , offsets = get_text_and_offsets ( doclike = sent , attr = self . attr , ignore_excluded = self . ignore_excluded , ) sent_start = getattr ( sent , \"start\" , 0 ) for size in range ( 1 , self . windows ): for i in range ( 0 , len ( offsets ) - size ): begin_char , _ , begin_i = offsets [ i ] _ , end_char , end_i = offsets [ i + size ] span_text = \"##\" + text [ begin_char : end_char ] + \"##\" matches = self . ss_reader . retrieve ( span_text ) for res in matches : sim = similarity ( span_text , res , measure = self . measure ) for cui in self . syn2cuis [ res ]: ents . append ( ( cui , begin_i + sent_start , end_i + sent_start , sim ) ) sorted_spans = sorted ( ents , key = simstring_sort_key , reverse = True ) results = [] seen_tokens = set () for span in sorted_spans : # Check for end - 1 here because boundaries are inclusive span_tokens = set ( range ( span [ 1 ], span [ 2 ])) if not ( span_tokens & seen_tokens ): results . append ( span ) seen_tokens . update ( span_tokens ) results = sorted ( results , key = lambda span : span [ 1 ]) if as_spans : spans = [ Span ( root , span_data [ 1 ], span_data [ 2 ], span_data [ 0 ]) for span_data in results ] return spans else : return [( self . vocab . strings [ span [ 0 ]], span [ 1 ], span [ 2 ]) for span in results ]","title":"__call__()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.similarity","text":"Source code in edsnlp/matchers/simstring.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def similarity ( x : str , y : str , measure : SimilarityMeasure = SimilarityMeasure . dice ): x_ngrams = { x [ i : i + 3 ] for i in range ( 0 , len ( x ) - 3 )} y_ngrams = { y [ i : i + 3 ] for i in range ( 0 , len ( y ) - 3 )} if measure == SimilarityMeasure . jaccard : return len ( x_ngrams & y_ngrams ) / ( len ( x_ngrams | y_ngrams )) if measure == SimilarityMeasure . dice : return 2 * len ( x_ngrams & y_ngrams ) / ( len ( x_ngrams ) + len ( y_ngrams )) if measure == SimilarityMeasure . cosine : return len ( x_ngrams & y_ngrams ) / sqrt ( len ( x_ngrams ) * len ( y_ngrams )) if measure == SimilarityMeasure . overlap : return len ( x_ngrams & y_ngrams ) raise ValueError ( \"Cannot compute similarity {} \" . format ( repr ( measure )))","title":"similarity()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.simstring_sort_key","text":"Source code in edsnlp/matchers/simstring.py 228 229 def simstring_sort_key ( span_data : Tuple [ str , int , int , float ]): return span_data [ 3 ], span_data [ 2 ] - span_data [ 1 ], - span_data [ 1 ]","title":"simstring_sort_key()"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.get_text_and_offsets","text":"Align different representations of a Doc or Span object. PARAMETER DESCRIPTION doclike spaCy Doc or Span object TYPE: Doc attr Attribute to use, by default \"TEXT\" TYPE: str, optional DEFAULT: 'TEXT' ignore_excluded Whether to remove excluded tokens, by default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION Tuple[str, List[Tuple[int, int, int]]] The new clean text and offset tuples for each word giving the begin char indice of the word in the new text, the end char indice of its preceding word and the indice of the word in the original document Source code in edsnlp/matchers/simstring.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 @lru_cache ( maxsize = 128 ) def get_text_and_offsets ( doclike : Union [ Span , Doc ], attr : str = \"TEXT\" , ignore_excluded : bool = True , ) -> Tuple [ str , List [ Tuple [ int , int , int ]]]: \"\"\" Align different representations of a `Doc` or `Span` object. Parameters ---------- doclike : Doc spaCy `Doc` or `Span` object attr : str, optional Attribute to use, by default `\"TEXT\"` ignore_excluded : bool, optional Whether to remove excluded tokens, by default True Returns ------- Tuple[str, List[Tuple[int, int, int]]] The new clean text and offset tuples for each word giving the begin char indice of the word in the new text, the end char indice of its preceding word and the indice of the word in the original document \"\"\" attr = attr . upper () attr = ATTRIBUTES . get ( attr , attr ) custom = attr . startswith ( \"_\" ) if custom : attr = attr [ 1 :] . lower () offsets = [] cursor = 0 text = [] last = cursor for i , token in enumerate ( doclike ): if not ignore_excluded or not token . _ . excluded : if custom : token_text = getattr ( token . _ , attr ) else : token_text = getattr ( token , attr ) # We add the cursor end = cursor + len ( token_text ) offsets . append (( cursor , last , i )) cursor = end last = end text . append ( token_text ) if token . whitespace_ : cursor += 1 text . append ( \" \" ) offsets . append (( cursor , last , len ( doclike ))) return \"\" . join ( text ), offsets","title":"get_text_and_offsets()"},{"location":"reference/matchers/utils/","text":"edsnlp.matchers.utils ListOrStr = Union [ List [ str ], str ] module-attribute DictOrPattern = Union [ Dict [ str , ListOrStr ], ListOrStr ] module-attribute Patterns = Dict [ str , DictOrPattern ] module-attribute ATTRIBUTES = { 'LOWER' : 'lower_' , 'TEXT' : 'text' , 'NORM' : 'norm_' , 'SHAPE' : 'shape_' } module-attribute","title":"`edsnlp.matchers.utils`"},{"location":"reference/matchers/utils/#edsnlpmatchersutils","text":"","title":"edsnlp.matchers.utils"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.ListOrStr","text":"","title":"ListOrStr"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.DictOrPattern","text":"","title":"DictOrPattern"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.Patterns","text":"","title":"Patterns"},{"location":"reference/matchers/utils/#edsnlp.matchers.utils.ATTRIBUTES","text":"","title":"ATTRIBUTES"},{"location":"reference/matchers/utils/offset/","text":"edsnlp.matchers.utils.offset token_length ( token , custom , attr ) Source code in edsnlp/matchers/utils/offset.py 10 11 12 13 14 15 def token_length ( token : Token , custom : bool , attr : str ): if custom : text = getattr ( token . _ , attr ) else : text = getattr ( token , attr ) return len ( text ) alignment ( doc , attr = 'TEXT' , ignore_excluded = True ) Align different representations of a Doc or Span object. PARAMETER DESCRIPTION doc spaCy Doc or Span object TYPE: Doc attr Attribute to use, by default \"TEXT\" TYPE: str, optional DEFAULT: 'TEXT' ignore_excluded Whether to remove excluded tokens, by default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION Tuple[List[int], List[int]] An alignment tuple: original and clean lists. Source code in edsnlp/matchers/utils/offset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @lru_cache ( maxsize = 32 ) def alignment ( doc : Doc , attr : str = \"TEXT\" , ignore_excluded : bool = True , ) -> Tuple [ List [ int ], List [ int ]]: \"\"\" Align different representations of a `Doc` or `Span` object. Parameters ---------- doc : Doc spaCy `Doc` or `Span` object attr : str, optional Attribute to use, by default `\"TEXT\"` ignore_excluded : bool, optional Whether to remove excluded tokens, by default True Returns ------- Tuple[List[int], List[int]] An alignment tuple: original and clean lists. \"\"\" assert isinstance ( doc , Doc ) attr = attr . upper () attr = ATTRIBUTES . get ( attr , attr ) custom = attr . startswith ( \"_\" ) if custom : attr = attr [ 1 :] . lower () # Define the length function length = partial ( token_length , custom = custom , attr = attr ) original = [] clean = [] cursor = 0 for token in doc : if not ignore_excluded or token . tag_ != \"EXCLUDED\" : # The token is not excluded, we add its extremities to the list original . append ( token . idx ) # We add the cursor clean . append ( cursor ) cursor += length ( token ) if token . whitespace_ : cursor += 1 return original , clean offset ( doc , attr , ignore_excluded , index ) Compute offset between the original text and a given representation (defined by the couple attr , ignore_excluded ). The alignment itself is computed with alignment . PARAMETER DESCRIPTION doc The spaCy Doc object TYPE: Doc attr The attribute used by the RegexMatcher (eg NORM ) TYPE: str ignore_excluded Whether the RegexMatcher ignores excluded tokens. TYPE: bool index The index in the pre-processed text. TYPE: int RETURNS DESCRIPTION int The offset. To get the character index in the original document, just do: original = index + offset ( doc , attr , ignore_excluded , index ) Source code in edsnlp/matchers/utils/offset.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def offset ( doc : Doc , attr : str , ignore_excluded : bool , index : int , ) -> int : \"\"\" Compute offset between the original text and a given representation (defined by the couple `attr`, `ignore_excluded`). The alignment itself is computed with [`alignment`][edsnlp.matchers.utils.offset.alignment]. Parameters ---------- doc : Doc The spaCy `Doc` object attr : str The attribute used by the [`RegexMatcher`][edsnlp.matchers.regex.RegexMatcher] (eg `NORM`) ignore_excluded : bool Whether the RegexMatcher ignores excluded tokens. index : int The index in the pre-processed text. Returns ------- int The offset. To get the character index in the original document, just do: `#!python original = index + offset(doc, attr, ignore_excluded, index)` \"\"\" original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) # We use bisect to efficiently find the correct rightmost-lower index i = bisect_left ( clean , index ) i = min ( i , len ( original ) - 1 ) return original [ i ] - clean [ i ]","title":"offset"},{"location":"reference/matchers/utils/offset/#edsnlpmatchersutilsoffset","text":"","title":"edsnlp.matchers.utils.offset"},{"location":"reference/matchers/utils/offset/#edsnlp.matchers.utils.offset.token_length","text":"Source code in edsnlp/matchers/utils/offset.py 10 11 12 13 14 15 def token_length ( token : Token , custom : bool , attr : str ): if custom : text = getattr ( token . _ , attr ) else : text = getattr ( token , attr ) return len ( text )","title":"token_length()"},{"location":"reference/matchers/utils/offset/#edsnlp.matchers.utils.offset.alignment","text":"Align different representations of a Doc or Span object. PARAMETER DESCRIPTION doc spaCy Doc or Span object TYPE: Doc attr Attribute to use, by default \"TEXT\" TYPE: str, optional DEFAULT: 'TEXT' ignore_excluded Whether to remove excluded tokens, by default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION Tuple[List[int], List[int]] An alignment tuple: original and clean lists. Source code in edsnlp/matchers/utils/offset.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @lru_cache ( maxsize = 32 ) def alignment ( doc : Doc , attr : str = \"TEXT\" , ignore_excluded : bool = True , ) -> Tuple [ List [ int ], List [ int ]]: \"\"\" Align different representations of a `Doc` or `Span` object. Parameters ---------- doc : Doc spaCy `Doc` or `Span` object attr : str, optional Attribute to use, by default `\"TEXT\"` ignore_excluded : bool, optional Whether to remove excluded tokens, by default True Returns ------- Tuple[List[int], List[int]] An alignment tuple: original and clean lists. \"\"\" assert isinstance ( doc , Doc ) attr = attr . upper () attr = ATTRIBUTES . get ( attr , attr ) custom = attr . startswith ( \"_\" ) if custom : attr = attr [ 1 :] . lower () # Define the length function length = partial ( token_length , custom = custom , attr = attr ) original = [] clean = [] cursor = 0 for token in doc : if not ignore_excluded or token . tag_ != \"EXCLUDED\" : # The token is not excluded, we add its extremities to the list original . append ( token . idx ) # We add the cursor clean . append ( cursor ) cursor += length ( token ) if token . whitespace_ : cursor += 1 return original , clean","title":"alignment()"},{"location":"reference/matchers/utils/offset/#edsnlp.matchers.utils.offset.offset","text":"Compute offset between the original text and a given representation (defined by the couple attr , ignore_excluded ). The alignment itself is computed with alignment . PARAMETER DESCRIPTION doc The spaCy Doc object TYPE: Doc attr The attribute used by the RegexMatcher (eg NORM ) TYPE: str ignore_excluded Whether the RegexMatcher ignores excluded tokens. TYPE: bool index The index in the pre-processed text. TYPE: int RETURNS DESCRIPTION int The offset. To get the character index in the original document, just do: original = index + offset ( doc , attr , ignore_excluded , index ) Source code in edsnlp/matchers/utils/offset.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def offset ( doc : Doc , attr : str , ignore_excluded : bool , index : int , ) -> int : \"\"\" Compute offset between the original text and a given representation (defined by the couple `attr`, `ignore_excluded`). The alignment itself is computed with [`alignment`][edsnlp.matchers.utils.offset.alignment]. Parameters ---------- doc : Doc The spaCy `Doc` object attr : str The attribute used by the [`RegexMatcher`][edsnlp.matchers.regex.RegexMatcher] (eg `NORM`) ignore_excluded : bool Whether the RegexMatcher ignores excluded tokens. index : int The index in the pre-processed text. Returns ------- int The offset. To get the character index in the original document, just do: `#!python original = index + offset(doc, attr, ignore_excluded, index)` \"\"\" original , clean = alignment ( doc = doc , attr = attr , ignore_excluded = ignore_excluded , ) # We use bisect to efficiently find the correct rightmost-lower index i = bisect_left ( clean , index ) i = min ( i , len ( original ) - 1 ) return original [ i ] - clean [ i ]","title":"offset()"},{"location":"reference/matchers/utils/text/","text":"edsnlp.matchers.utils.text get_text ( doclike , attr , ignore_excluded ) Get text using a custom attribute, possibly ignoring excluded tokens. PARAMETER DESCRIPTION doclike Doc or Span to get text from. TYPE: Union[Doc, Span] attr Attribute to use. TYPE: str ignore_excluded Whether to skip excluded tokens, by default False TYPE: bool RETURNS DESCRIPTION str Extracted text. Source code in edsnlp/matchers/utils/text.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @lru_cache ( 32 ) def get_text ( doclike : Union [ Doc , Span ], attr : str , ignore_excluded : bool , ) -> str : \"\"\" Get text using a custom attribute, possibly ignoring excluded tokens. Parameters ---------- doclike : Union[Doc, Span] Doc or Span to get text from. attr : str Attribute to use. ignore_excluded : bool Whether to skip excluded tokens, by default False Returns ------- str Extracted text. \"\"\" attr = attr . upper () if not ignore_excluded : if attr == \"TEXT\" : return doclike . text elif attr == \"LOWER\" : return doclike . text . lower () else : tokens = doclike else : tokens = [ t for t in doclike if t . tag_ != \"EXCLUDED\" ] if not tokens : return \"\" attr = ATTRIBUTES . get ( attr , attr ) if attr . startswith ( \"_\" ): attr = attr [ 1 :] . lower () return \"\" . join ( [ getattr ( t . _ , attr ) + t . whitespace_ for t in tokens [: - 1 ]] ) + getattr ( tokens [ - 1 ], attr ) else : return \"\" . join ( [ getattr ( t , attr ) + t . whitespace_ for t in tokens [: - 1 ]] ) + getattr ( tokens [ - 1 ], attr )","title":"text"},{"location":"reference/matchers/utils/text/#edsnlpmatchersutilstext","text":"","title":"edsnlp.matchers.utils.text"},{"location":"reference/matchers/utils/text/#edsnlp.matchers.utils.text.get_text","text":"Get text using a custom attribute, possibly ignoring excluded tokens. PARAMETER DESCRIPTION doclike Doc or Span to get text from. TYPE: Union[Doc, Span] attr Attribute to use. TYPE: str ignore_excluded Whether to skip excluded tokens, by default False TYPE: bool RETURNS DESCRIPTION str Extracted text. Source code in edsnlp/matchers/utils/text.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @lru_cache ( 32 ) def get_text ( doclike : Union [ Doc , Span ], attr : str , ignore_excluded : bool , ) -> str : \"\"\" Get text using a custom attribute, possibly ignoring excluded tokens. Parameters ---------- doclike : Union[Doc, Span] Doc or Span to get text from. attr : str Attribute to use. ignore_excluded : bool Whether to skip excluded tokens, by default False Returns ------- str Extracted text. \"\"\" attr = attr . upper () if not ignore_excluded : if attr == \"TEXT\" : return doclike . text elif attr == \"LOWER\" : return doclike . text . lower () else : tokens = doclike else : tokens = [ t for t in doclike if t . tag_ != \"EXCLUDED\" ] if not tokens : return \"\" attr = ATTRIBUTES . get ( attr , attr ) if attr . startswith ( \"_\" ): attr = attr [ 1 :] . lower () return \"\" . join ( [ getattr ( t . _ , attr ) + t . whitespace_ for t in tokens [: - 1 ]] ) + getattr ( tokens [ - 1 ], attr ) else : return \"\" . join ( [ getattr ( t , attr ) + t . whitespace_ for t in tokens [: - 1 ]] ) + getattr ( tokens [ - 1 ], attr )","title":"get_text()"},{"location":"reference/models/","text":"edsnlp.models","title":"`edsnlp.models`"},{"location":"reference/models/#edsnlpmodels","text":"","title":"edsnlp.models"},{"location":"reference/models/pytorch_wrapper/","text":"edsnlp.models.pytorch_wrapper PredT = typing . TypeVar ( 'PredT' ) module-attribute PytorchWrapperModule Bases: torch . nn . Module Source code in edsnlp/models/pytorch_wrapper.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class PytorchWrapperModule ( torch . nn . Module ): def __init__ ( self , input_size : Optional [ int ] = None , n_labels : Optional [ int ] = None , ): \"\"\" Pytorch wrapping module for Spacy. Models that expect to be wrapped with [wrap_pytorch_model][edsnlp.models.pytorch_wrapper.wrap_pytorch_model] should inherit from this module. Parameters ---------- input_size: int Size of the input embeddings n_labels: int Number of labels predicted by the module \"\"\" super () . __init__ () self . cfg = { \"n_labels\" : n_labels , \"input_size\" : input_size } @property def n_labels ( self ): return self . cfg [ \"n_labels\" ] @property def input_size ( self ): return self . cfg [ \"input_size\" ] def load_state_dict ( self , state_dict : OrderedDict [ str , torch . Tensor ], strict : bool = True ): \"\"\" Loads the model inplace from a dumped `state_dict` object Parameters ---------- state_dict: OrderedDict[str, torch.Tensor] strict: bool \"\"\" self . cfg = state_dict . pop ( \"cfg\" ) self . initialize () super () . load_state_dict ( state_dict , strict ) def state_dict ( self , destination = None , prefix = \"\" , keep_vars = False ): \"\"\" Loads the model inplace from a dumped `state_dict` object Parameters ---------- destination: Any prefix: str keep_vars: bool Returns ------- dict \"\"\" state = super () . state_dict ( destination , prefix , keep_vars ) state [ \"cfg\" ] = self . cfg return state def set_n_labels ( self , n_labels ): \"\"\" Sets the number of labels. To instantiate the linear layer, we need to call the `initialize` method. Parameters ---------- n_labels: int Number of different labels predicted by this module \"\"\" self . cfg [ \"n_labels\" ] = n_labels def initialize ( self ): \"\"\" Once the number of labels n_labels are known, this method initializes the torch linear layer. \"\"\" raise NotImplementedError () def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , * , additional_outputs : typing . Dict [ str , Any ] = None , is_train : bool = False , is_predict : bool = False , ) -> Optional [ torch . FloatTensor ]: \"\"\" Apply the nested pytorch module to: - compute the loss - predict the outputs non exclusively. If outputs are predicted, they are assigned to the `additional_outputs` list. Parameters ---------- embeds: torch.FloatTensor Input embeddings mask: torch.BoolTensor Input embeddings mask additional_outputs: List Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This list will contain the predicted outputs is_train: bool=False Are we training the model (defaults to True) is_predict: bool=False Are we predicting the model (defaults to False) Returns ------- Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model \"\"\" raise NotImplementedError () cfg = { 'n_labels' : n_labels , 'input_size' : input_size } instance-attribute __init__ ( input_size = None , n_labels = None ) Pytorch wrapping module for Spacy. Models that expect to be wrapped with wrap_pytorch_model should inherit from this module. PARAMETER DESCRIPTION input_size Size of the input embeddings TYPE: Optional [ int ] DEFAULT: None n_labels Number of labels predicted by the module TYPE: Optional [ int ] DEFAULT: None Source code in edsnlp/models/pytorch_wrapper.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , input_size : Optional [ int ] = None , n_labels : Optional [ int ] = None , ): \"\"\" Pytorch wrapping module for Spacy. Models that expect to be wrapped with [wrap_pytorch_model][edsnlp.models.pytorch_wrapper.wrap_pytorch_model] should inherit from this module. Parameters ---------- input_size: int Size of the input embeddings n_labels: int Number of labels predicted by the module \"\"\" super () . __init__ () self . cfg = { \"n_labels\" : n_labels , \"input_size\" : input_size } n_labels () Source code in edsnlp/models/pytorch_wrapper.py 44 45 46 @property def n_labels ( self ): return self . cfg [ \"n_labels\" ] input_size () Source code in edsnlp/models/pytorch_wrapper.py 48 49 50 @property def input_size ( self ): return self . cfg [ \"input_size\" ] load_state_dict ( state_dict , strict = True ) Loads the model inplace from a dumped state_dict object PARAMETER DESCRIPTION state_dict TYPE: OrderedDict [ str , torch . Tensor ] strict TYPE: bool DEFAULT: True Source code in edsnlp/models/pytorch_wrapper.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def load_state_dict ( self , state_dict : OrderedDict [ str , torch . Tensor ], strict : bool = True ): \"\"\" Loads the model inplace from a dumped `state_dict` object Parameters ---------- state_dict: OrderedDict[str, torch.Tensor] strict: bool \"\"\" self . cfg = state_dict . pop ( \"cfg\" ) self . initialize () super () . load_state_dict ( state_dict , strict ) state_dict ( destination = None , prefix = '' , keep_vars = False ) Loads the model inplace from a dumped state_dict object PARAMETER DESCRIPTION destination DEFAULT: None prefix DEFAULT: '' keep_vars DEFAULT: False RETURNS DESCRIPTION dict Source code in edsnlp/models/pytorch_wrapper.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def state_dict ( self , destination = None , prefix = \"\" , keep_vars = False ): \"\"\" Loads the model inplace from a dumped `state_dict` object Parameters ---------- destination: Any prefix: str keep_vars: bool Returns ------- dict \"\"\" state = super () . state_dict ( destination , prefix , keep_vars ) state [ \"cfg\" ] = self . cfg return state set_n_labels ( n_labels ) Sets the number of labels. To instantiate the linear layer, we need to call the initialize method. PARAMETER DESCRIPTION n_labels Number of different labels predicted by this module Source code in edsnlp/models/pytorch_wrapper.py 85 86 87 88 89 90 91 92 93 94 95 def set_n_labels ( self , n_labels ): \"\"\" Sets the number of labels. To instantiate the linear layer, we need to call the `initialize` method. Parameters ---------- n_labels: int Number of different labels predicted by this module \"\"\" self . cfg [ \"n_labels\" ] = n_labels initialize () Once the number of labels n_labels are known, this method initializes the torch linear layer. Source code in edsnlp/models/pytorch_wrapper.py 97 98 99 100 101 102 def initialize ( self ): \"\"\" Once the number of labels n_labels are known, this method initializes the torch linear layer. \"\"\" raise NotImplementedError () forward ( embeds , mask , * , additional_outputs = None , is_train = False , is_predict = False ) Apply the nested pytorch module to: - compute the loss - predict the outputs non exclusively. If outputs are predicted, they are assigned to the additional_outputs list. PARAMETER DESCRIPTION embeds Input embeddings TYPE: torch . FloatTensor mask Input embeddings mask TYPE: torch . BoolTensor additional_outputs Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This list will contain the predicted outputs TYPE: typing . Dict [ str , Any ] DEFAULT: None is_train Are we training the model (defaults to True) TYPE: bool DEFAULT: False is_predict Are we predicting the model (defaults to False) TYPE: bool DEFAULT: False RETURNS DESCRIPTION Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model Source code in edsnlp/models/pytorch_wrapper.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , * , additional_outputs : typing . Dict [ str , Any ] = None , is_train : bool = False , is_predict : bool = False , ) -> Optional [ torch . FloatTensor ]: \"\"\" Apply the nested pytorch module to: - compute the loss - predict the outputs non exclusively. If outputs are predicted, they are assigned to the `additional_outputs` list. Parameters ---------- embeds: torch.FloatTensor Input embeddings mask: torch.BoolTensor Input embeddings mask additional_outputs: List Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This list will contain the predicted outputs is_train: bool=False Are we training the model (defaults to True) is_predict: bool=False Are we predicting the model (defaults to False) Returns ------- Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model \"\"\" raise NotImplementedError () custom_xp2torch ( model , X ) Source code in edsnlp/models/pytorch_wrapper.py 145 146 147 148 149 150 151 152 153 def custom_xp2torch ( model , X ): main = xp2torch ( X [ 0 ], requires_grad = True ) rest = convert_recursive ( is_xp_array , lambda x : xp2torch ( x ), X [ 1 :]) def reverse_conversion ( dXtorch ): dX = torch2xp ( dXtorch . args [ 0 ]) return dX return ( main , * rest ), reverse_conversion pytorch_forward ( model , X , is_train = False ) Run the stacked CRF pytorch model to train / run a nested NER model PARAMETER DESCRIPTION model TYPE: Model X TYPE: Tuple [ Iterable [ Doc ], PredT , bool ] is_train TYPE: bool DEFAULT: False RETURNS DESCRIPTION Tuple[Tuple[Floats1d, PredictionT], Callable[Floats1d, Any]] Source code in edsnlp/models/pytorch_wrapper.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def pytorch_forward ( model : Model , X : Tuple [ Iterable [ Doc ], PredT , bool ], is_train : bool = False , ) -> Tuple [ Tuple [ Floats1d , PredT ], Callable [[ Floats1d ], Any ]]: \"\"\" Run the stacked CRF pytorch model to train / run a nested NER model Parameters ---------- model: Model X: Tuple[Iterable[Doc], PredictionT, bool] is_train: bool Returns ------- Tuple[Tuple[Floats1d, PredictionT], Callable[Floats1d, Any]] \"\"\" [ docs , * rest_X , is_predict ] = X encoder : Model [ List [ Doc ], List [ Floats2d ]] = model . get_ref ( \"encoder\" ) embeds_list , bp_embeds = encoder ( docs , is_train = is_train ) embeds = model . ops . pad ( embeds_list ) # pad embeds ################################################## # Prepare the torch nested ner crf module inputs # ################################################## additional_outputs = {} # Convert input from numpy/cupy to torch ( torch_embeds , * torch_rest ), get_d_embeds = custom_xp2torch ( model , ( embeds , * rest_X ) ) # Prepare token mask from docs' lengths torch_mask = ( torch . arange ( embeds . shape [ 1 ], device = torch_embeds . device ) < torch . tensor ([ d . shape [ 0 ] for d in embeds_list ], device = torch_embeds . device )[ :, None ] ) ################# # Run the model # ################# loss_torch , torch_backprop = model . shims [ 0 ]( ArgsKwargs ( ( torch_embeds , torch_mask , * torch_rest ), { \"additional_outputs\" : additional_outputs , \"is_train\" : is_train , \"is_predict\" : is_predict , }, ), is_train , ) #################################### # Postprocess the module's outputs # #################################### loss = torch2xp ( loss_torch ) if loss_torch is not None else None additional_outputs = convert_recursive ( is_torch_array , torch2xp , additional_outputs ) def backprop ( d_loss : Floats1d ) -> Any : d_loss_torch = ArgsKwargs ( args = (( loss_torch ,),), kwargs = { \"grad_tensors\" : xp2torch ( d_loss )} ) d_embeds_torch = torch_backprop ( d_loss_torch ) d_embeds = get_d_embeds ( d_embeds_torch ) d_embeds_list = [ d_padded_row [: len ( d_item )] for d_item , d_padded_row in zip ( embeds_list , d_embeds ) ] d_docs = bp_embeds ( d_embeds_list ) return d_docs return ( loss , additional_outputs ), backprop instance_init ( model , X = None , Y = None ) Initializes the model by setting the input size of the model layers and the number of predicted labels PARAMETER DESCRIPTION model Nested NER thinc model TYPE: Model X list of documents on which we apply the encoder layer TYPE: List [ Doc ] DEFAULT: None Y Unused gold spans TYPE: Ints2d DEFAULT: None Source code in edsnlp/models/pytorch_wrapper.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def instance_init ( model : Model , X : List [ Doc ] = None , Y : Ints2d = None ) -> Model : \"\"\" Initializes the model by setting the input size of the model layers and the number of predicted labels Parameters ---------- model: Model Nested NER thinc model X: List[Doc] list of documents on which we apply the encoder layer Y: Ints2d Unused gold spans Returns ------- \"\"\" encoder = model . get_ref ( \"encoder\" ) if X is not None : encoder . initialize ( X ) pt_model = model . attrs [ \"pt_model\" ] pt_model . cfg [ \"input_size\" ] = encoder . get_dim ( \"nO\" ) pt_model . initialize () pt_model . to ( get_torch_default_device ()) model . set_dim ( \"nI\" , pt_model . input_size ) return model wrap_pytorch_model ( encoder , pt_model ) Chain and wraps a spaCy/Thinc encoder model (like a tok2vec) and a pytorch model. The loss should be computed directly in the Pytorch module and Categorical predictions are supported PARAMETER DESCRIPTION encoder The Thinc document token embedding layer TYPE: Model [ List [ Doc ], List [ Floats2d ]] pt_model The Pytorch model TYPE: PytorchWrapperModule RETURNS DESCRIPTION Tuple[Iterable[Doc], Optional[PredT], Optional[bool]], inputs (docs, gold, *rest, is_predict) Tuple[Floats1d, PredT], outputs (loss, *additional_outputs) Source code in edsnlp/models/pytorch_wrapper.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def wrap_pytorch_model ( encoder : Model [ List [ Doc ], List [ Floats2d ]], pt_model : PytorchWrapperModule , ) -> Model [ Tuple [ Iterable [ Doc ], Optional [ PredT ], Optional [ bool ]], Tuple [ Floats1d , PredT ], ]: \"\"\" Chain and wraps a spaCy/Thinc encoder model (like a tok2vec) and a pytorch model. The loss should be computed directly in the Pytorch module and Categorical predictions are supported Parameters ---------- encoder: Model[List[Doc], List[Floats2d]] The Thinc document token embedding layer pt_model: PytorchWrapperModule The Pytorch model Returns ------- Tuple[Iterable[Doc], Optional[PredT], Optional[bool]], # inputs (docs, gold, *rest, is_predict) Tuple[Floats1d, PredT], # outputs (loss, *additional_outputs) \"\"\" return Model ( \"pytorch\" , pytorch_forward , attrs = { \"set_n_labels\" : pt_model . set_n_labels , \"pt_model\" : pt_model , }, layers = [ encoder ], shims = [ PyTorchShim ( pt_model )], refs = { \"encoder\" : encoder }, dims = { \"nI\" : None , \"nO\" : None }, init = instance_init , )","title":"pytorch_wrapper"},{"location":"reference/models/pytorch_wrapper/#edsnlpmodelspytorch_wrapper","text":"","title":"edsnlp.models.pytorch_wrapper"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PredT","text":"","title":"PredT"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule","text":"Bases: torch . nn . Module Source code in edsnlp/models/pytorch_wrapper.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class PytorchWrapperModule ( torch . nn . Module ): def __init__ ( self , input_size : Optional [ int ] = None , n_labels : Optional [ int ] = None , ): \"\"\" Pytorch wrapping module for Spacy. Models that expect to be wrapped with [wrap_pytorch_model][edsnlp.models.pytorch_wrapper.wrap_pytorch_model] should inherit from this module. Parameters ---------- input_size: int Size of the input embeddings n_labels: int Number of labels predicted by the module \"\"\" super () . __init__ () self . cfg = { \"n_labels\" : n_labels , \"input_size\" : input_size } @property def n_labels ( self ): return self . cfg [ \"n_labels\" ] @property def input_size ( self ): return self . cfg [ \"input_size\" ] def load_state_dict ( self , state_dict : OrderedDict [ str , torch . Tensor ], strict : bool = True ): \"\"\" Loads the model inplace from a dumped `state_dict` object Parameters ---------- state_dict: OrderedDict[str, torch.Tensor] strict: bool \"\"\" self . cfg = state_dict . pop ( \"cfg\" ) self . initialize () super () . load_state_dict ( state_dict , strict ) def state_dict ( self , destination = None , prefix = \"\" , keep_vars = False ): \"\"\" Loads the model inplace from a dumped `state_dict` object Parameters ---------- destination: Any prefix: str keep_vars: bool Returns ------- dict \"\"\" state = super () . state_dict ( destination , prefix , keep_vars ) state [ \"cfg\" ] = self . cfg return state def set_n_labels ( self , n_labels ): \"\"\" Sets the number of labels. To instantiate the linear layer, we need to call the `initialize` method. Parameters ---------- n_labels: int Number of different labels predicted by this module \"\"\" self . cfg [ \"n_labels\" ] = n_labels def initialize ( self ): \"\"\" Once the number of labels n_labels are known, this method initializes the torch linear layer. \"\"\" raise NotImplementedError () def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , * , additional_outputs : typing . Dict [ str , Any ] = None , is_train : bool = False , is_predict : bool = False , ) -> Optional [ torch . FloatTensor ]: \"\"\" Apply the nested pytorch module to: - compute the loss - predict the outputs non exclusively. If outputs are predicted, they are assigned to the `additional_outputs` list. Parameters ---------- embeds: torch.FloatTensor Input embeddings mask: torch.BoolTensor Input embeddings mask additional_outputs: List Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This list will contain the predicted outputs is_train: bool=False Are we training the model (defaults to True) is_predict: bool=False Are we predicting the model (defaults to False) Returns ------- Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model \"\"\" raise NotImplementedError ()","title":"PytorchWrapperModule"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.cfg","text":"","title":"cfg"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.__init__","text":"Pytorch wrapping module for Spacy. Models that expect to be wrapped with wrap_pytorch_model should inherit from this module. PARAMETER DESCRIPTION input_size Size of the input embeddings TYPE: Optional [ int ] DEFAULT: None n_labels Number of labels predicted by the module TYPE: Optional [ int ] DEFAULT: None Source code in edsnlp/models/pytorch_wrapper.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , input_size : Optional [ int ] = None , n_labels : Optional [ int ] = None , ): \"\"\" Pytorch wrapping module for Spacy. Models that expect to be wrapped with [wrap_pytorch_model][edsnlp.models.pytorch_wrapper.wrap_pytorch_model] should inherit from this module. Parameters ---------- input_size: int Size of the input embeddings n_labels: int Number of labels predicted by the module \"\"\" super () . __init__ () self . cfg = { \"n_labels\" : n_labels , \"input_size\" : input_size }","title":"__init__()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.n_labels","text":"Source code in edsnlp/models/pytorch_wrapper.py 44 45 46 @property def n_labels ( self ): return self . cfg [ \"n_labels\" ]","title":"n_labels()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.input_size","text":"Source code in edsnlp/models/pytorch_wrapper.py 48 49 50 @property def input_size ( self ): return self . cfg [ \"input_size\" ]","title":"input_size()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.load_state_dict","text":"Loads the model inplace from a dumped state_dict object PARAMETER DESCRIPTION state_dict TYPE: OrderedDict [ str , torch . Tensor ] strict TYPE: bool DEFAULT: True Source code in edsnlp/models/pytorch_wrapper.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def load_state_dict ( self , state_dict : OrderedDict [ str , torch . Tensor ], strict : bool = True ): \"\"\" Loads the model inplace from a dumped `state_dict` object Parameters ---------- state_dict: OrderedDict[str, torch.Tensor] strict: bool \"\"\" self . cfg = state_dict . pop ( \"cfg\" ) self . initialize () super () . load_state_dict ( state_dict , strict )","title":"load_state_dict()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.state_dict","text":"Loads the model inplace from a dumped state_dict object PARAMETER DESCRIPTION destination DEFAULT: None prefix DEFAULT: '' keep_vars DEFAULT: False RETURNS DESCRIPTION dict Source code in edsnlp/models/pytorch_wrapper.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def state_dict ( self , destination = None , prefix = \"\" , keep_vars = False ): \"\"\" Loads the model inplace from a dumped `state_dict` object Parameters ---------- destination: Any prefix: str keep_vars: bool Returns ------- dict \"\"\" state = super () . state_dict ( destination , prefix , keep_vars ) state [ \"cfg\" ] = self . cfg return state","title":"state_dict()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.set_n_labels","text":"Sets the number of labels. To instantiate the linear layer, we need to call the initialize method. PARAMETER DESCRIPTION n_labels Number of different labels predicted by this module Source code in edsnlp/models/pytorch_wrapper.py 85 86 87 88 89 90 91 92 93 94 95 def set_n_labels ( self , n_labels ): \"\"\" Sets the number of labels. To instantiate the linear layer, we need to call the `initialize` method. Parameters ---------- n_labels: int Number of different labels predicted by this module \"\"\" self . cfg [ \"n_labels\" ] = n_labels","title":"set_n_labels()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.initialize","text":"Once the number of labels n_labels are known, this method initializes the torch linear layer. Source code in edsnlp/models/pytorch_wrapper.py 97 98 99 100 101 102 def initialize ( self ): \"\"\" Once the number of labels n_labels are known, this method initializes the torch linear layer. \"\"\" raise NotImplementedError ()","title":"initialize()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.forward","text":"Apply the nested pytorch module to: - compute the loss - predict the outputs non exclusively. If outputs are predicted, they are assigned to the additional_outputs list. PARAMETER DESCRIPTION embeds Input embeddings TYPE: torch . FloatTensor mask Input embeddings mask TYPE: torch . BoolTensor additional_outputs Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This list will contain the predicted outputs TYPE: typing . Dict [ str , Any ] DEFAULT: None is_train Are we training the model (defaults to True) TYPE: bool DEFAULT: False is_predict Are we predicting the model (defaults to False) TYPE: bool DEFAULT: False RETURNS DESCRIPTION Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model Source code in edsnlp/models/pytorch_wrapper.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , * , additional_outputs : typing . Dict [ str , Any ] = None , is_train : bool = False , is_predict : bool = False , ) -> Optional [ torch . FloatTensor ]: \"\"\" Apply the nested pytorch module to: - compute the loss - predict the outputs non exclusively. If outputs are predicted, they are assigned to the `additional_outputs` list. Parameters ---------- embeds: torch.FloatTensor Input embeddings mask: torch.BoolTensor Input embeddings mask additional_outputs: List Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This list will contain the predicted outputs is_train: bool=False Are we training the model (defaults to True) is_predict: bool=False Are we predicting the model (defaults to False) Returns ------- Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model \"\"\" raise NotImplementedError ()","title":"forward()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.custom_xp2torch","text":"Source code in edsnlp/models/pytorch_wrapper.py 145 146 147 148 149 150 151 152 153 def custom_xp2torch ( model , X ): main = xp2torch ( X [ 0 ], requires_grad = True ) rest = convert_recursive ( is_xp_array , lambda x : xp2torch ( x ), X [ 1 :]) def reverse_conversion ( dXtorch ): dX = torch2xp ( dXtorch . args [ 0 ]) return dX return ( main , * rest ), reverse_conversion","title":"custom_xp2torch()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.pytorch_forward","text":"Run the stacked CRF pytorch model to train / run a nested NER model PARAMETER DESCRIPTION model TYPE: Model X TYPE: Tuple [ Iterable [ Doc ], PredT , bool ] is_train TYPE: bool DEFAULT: False RETURNS DESCRIPTION Tuple[Tuple[Floats1d, PredictionT], Callable[Floats1d, Any]] Source code in edsnlp/models/pytorch_wrapper.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def pytorch_forward ( model : Model , X : Tuple [ Iterable [ Doc ], PredT , bool ], is_train : bool = False , ) -> Tuple [ Tuple [ Floats1d , PredT ], Callable [[ Floats1d ], Any ]]: \"\"\" Run the stacked CRF pytorch model to train / run a nested NER model Parameters ---------- model: Model X: Tuple[Iterable[Doc], PredictionT, bool] is_train: bool Returns ------- Tuple[Tuple[Floats1d, PredictionT], Callable[Floats1d, Any]] \"\"\" [ docs , * rest_X , is_predict ] = X encoder : Model [ List [ Doc ], List [ Floats2d ]] = model . get_ref ( \"encoder\" ) embeds_list , bp_embeds = encoder ( docs , is_train = is_train ) embeds = model . ops . pad ( embeds_list ) # pad embeds ################################################## # Prepare the torch nested ner crf module inputs # ################################################## additional_outputs = {} # Convert input from numpy/cupy to torch ( torch_embeds , * torch_rest ), get_d_embeds = custom_xp2torch ( model , ( embeds , * rest_X ) ) # Prepare token mask from docs' lengths torch_mask = ( torch . arange ( embeds . shape [ 1 ], device = torch_embeds . device ) < torch . tensor ([ d . shape [ 0 ] for d in embeds_list ], device = torch_embeds . device )[ :, None ] ) ################# # Run the model # ################# loss_torch , torch_backprop = model . shims [ 0 ]( ArgsKwargs ( ( torch_embeds , torch_mask , * torch_rest ), { \"additional_outputs\" : additional_outputs , \"is_train\" : is_train , \"is_predict\" : is_predict , }, ), is_train , ) #################################### # Postprocess the module's outputs # #################################### loss = torch2xp ( loss_torch ) if loss_torch is not None else None additional_outputs = convert_recursive ( is_torch_array , torch2xp , additional_outputs ) def backprop ( d_loss : Floats1d ) -> Any : d_loss_torch = ArgsKwargs ( args = (( loss_torch ,),), kwargs = { \"grad_tensors\" : xp2torch ( d_loss )} ) d_embeds_torch = torch_backprop ( d_loss_torch ) d_embeds = get_d_embeds ( d_embeds_torch ) d_embeds_list = [ d_padded_row [: len ( d_item )] for d_item , d_padded_row in zip ( embeds_list , d_embeds ) ] d_docs = bp_embeds ( d_embeds_list ) return d_docs return ( loss , additional_outputs ), backprop","title":"pytorch_forward()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.instance_init","text":"Initializes the model by setting the input size of the model layers and the number of predicted labels PARAMETER DESCRIPTION model Nested NER thinc model TYPE: Model X list of documents on which we apply the encoder layer TYPE: List [ Doc ] DEFAULT: None Y Unused gold spans TYPE: Ints2d DEFAULT: None Source code in edsnlp/models/pytorch_wrapper.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def instance_init ( model : Model , X : List [ Doc ] = None , Y : Ints2d = None ) -> Model : \"\"\" Initializes the model by setting the input size of the model layers and the number of predicted labels Parameters ---------- model: Model Nested NER thinc model X: List[Doc] list of documents on which we apply the encoder layer Y: Ints2d Unused gold spans Returns ------- \"\"\" encoder = model . get_ref ( \"encoder\" ) if X is not None : encoder . initialize ( X ) pt_model = model . attrs [ \"pt_model\" ] pt_model . cfg [ \"input_size\" ] = encoder . get_dim ( \"nO\" ) pt_model . initialize () pt_model . to ( get_torch_default_device ()) model . set_dim ( \"nI\" , pt_model . input_size ) return model","title":"instance_init()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.wrap_pytorch_model","text":"Chain and wraps a spaCy/Thinc encoder model (like a tok2vec) and a pytorch model. The loss should be computed directly in the Pytorch module and Categorical predictions are supported PARAMETER DESCRIPTION encoder The Thinc document token embedding layer TYPE: Model [ List [ Doc ], List [ Floats2d ]] pt_model The Pytorch model TYPE: PytorchWrapperModule RETURNS DESCRIPTION Tuple[Iterable[Doc], Optional[PredT], Optional[bool]],","title":"wrap_pytorch_model()"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.wrap_pytorch_model--inputs-docs-gold-rest-is_predict","text":"Tuple[Floats1d, PredT],","title":"inputs (docs, gold, *rest, is_predict)"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.wrap_pytorch_model--outputs-loss-additional_outputs","text":"Source code in edsnlp/models/pytorch_wrapper.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def wrap_pytorch_model ( encoder : Model [ List [ Doc ], List [ Floats2d ]], pt_model : PytorchWrapperModule , ) -> Model [ Tuple [ Iterable [ Doc ], Optional [ PredT ], Optional [ bool ]], Tuple [ Floats1d , PredT ], ]: \"\"\" Chain and wraps a spaCy/Thinc encoder model (like a tok2vec) and a pytorch model. The loss should be computed directly in the Pytorch module and Categorical predictions are supported Parameters ---------- encoder: Model[List[Doc], List[Floats2d]] The Thinc document token embedding layer pt_model: PytorchWrapperModule The Pytorch model Returns ------- Tuple[Iterable[Doc], Optional[PredT], Optional[bool]], # inputs (docs, gold, *rest, is_predict) Tuple[Floats1d, PredT], # outputs (loss, *additional_outputs) \"\"\" return Model ( \"pytorch\" , pytorch_forward , attrs = { \"set_n_labels\" : pt_model . set_n_labels , \"pt_model\" : pt_model , }, layers = [ encoder ], shims = [ PyTorchShim ( pt_model )], refs = { \"encoder\" : encoder }, dims = { \"nI\" : None , \"nO\" : None }, init = instance_init , )","title":"outputs (loss, *additional_outputs)"},{"location":"reference/models/stack_crf_ner/","text":"edsnlp.models.stack_crf_ner CRFMode Bases: str , Enum Source code in edsnlp/models/stack_crf_ner.py 15 16 17 18 class CRFMode ( str , Enum ): independent = \"independent\" joint = \"joint\" marginal = \"marginal\" independent = 'independent' class-attribute joint = 'joint' class-attribute marginal = 'marginal' class-attribute StackedCRFNERModule Bases: PytorchWrapperModule Source code in edsnlp/models/stack_crf_ner.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class StackedCRFNERModule ( PytorchWrapperModule ): def __init__ ( self , input_size : Optional [ int ] = None , n_labels : Optional [ int ] = None , mode : CRFMode = CRFMode . joint , ): \"\"\" Nested NER CRF module Parameters ---------- input_size: int Size of the input embeddings n_labels: int Number of labels predicted by the module mode: CRFMode Loss mode of the CRF \"\"\" super () . __init__ ( input_size , n_labels ) self . cfg [ \"mode\" ] = mode assert mode in ( CRFMode . independent , CRFMode . joint , CRFMode . marginal ) self . crf = MultiLabelBIOULDecoder ( 1 , learnable_transitions = False ) self . classifier = None def initialize ( self ): \"\"\" Once the number of labels n_labels are known, this method initializes the torch linear layer. \"\"\" num_tags = self . n_labels * self . crf . num_tags self . classifier = torch . nn . Linear ( self . input_size , num_tags ) def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , spans : Optional [ torch . LongTensor ] = None , additional_outputs : Dict [ str , Any ] = None , is_train : bool = False , is_predict : bool = False , ) -> Optional [ torch . FloatTensor ]: \"\"\" Apply the nested ner module to the document embeddings to: - compute the loss - predict the spans non exclusively. If spans are predicted, they are assigned to the `additional_outputs` dictionary. Parameters ---------- embeds: torch.FloatTensor Token embeddings to predict the tags from mask: torch.BoolTensor Mask of the sequences spans: Optional[torch.LongTensor] 2d tensor of n_spans * (doc_idx, label_idx, begin, end) additional_outputs: Dict[str, Any] Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This dict will contain the predicted 2d tensor of spans is_train: bool=False Are we training the model (defaults to True) is_predict: bool=False Are we predicting the model (defaults to False) Returns ------- Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model \"\"\" n_samples , n_tokens = embeds . shape [: 2 ] logits = self . classifier ( embeds ) crf_logits = flatten_dim ( logits . view ( n_samples , n_tokens , self . n_labels , self . crf . num_tags ) . permute ( 0 , 2 , 1 , 3 ), dim = 0 , ) crf_mask = repeat ( mask , self . n_labels , 0 ) loss = None if is_train : tags = self . crf . spans_to_tags ( spans , n_samples = n_samples , n_tokens = n_tokens , n_labels = self . n_labels ) crf_target = flatten_dim ( torch . nn . functional . one_hot ( tags , 5 ) . bool () if len ( tags . shape ) == 3 else tags , dim = 0 , ) if self . cfg [ \"mode\" ] == CRFMode . joint : loss = self . crf ( crf_logits , crf_mask , crf_target , ) elif self . cfg [ \"mode\" ] == CRFMode . independent : loss = ( - crf_logits . log_softmax ( - 1 ) . masked_fill ( ~ crf_target , IMPOSSIBLE ) . logsumexp ( - 1 )[ crf_mask ] . sum () ) elif self . cfg [ \"mode\" ] == CRFMode . marginal : crf_logits = self . crf . marginal ( crf_logits , crf_mask , ) loss = ( - crf_logits . log_softmax ( - 1 ) . masked_fill ( ~ crf_target , IMPOSSIBLE ) . logsumexp ( - 1 )[ crf_mask ] . sum () ) if ( loss > - IMPOSSIBLE ) . any (): logger . warning ( \"You likely have an impossible transition in your \" \"training data NER tags, skipping this batch.\" ) loss = torch . zeros ( 1 , dtype = torch . float , device = embeds . device ) loss = loss . sum () . unsqueeze ( 0 ) / 100.0 if is_predict : pred_tags = self . crf . decode ( crf_logits , crf_mask ) . reshape ( n_samples , self . n_labels , n_tokens ) pred_spans = self . crf . tags_to_spans ( pred_tags ) additional_outputs [ \"spans\" ] = pred_spans return loss crf = MultiLabelBIOULDecoder ( 1 , learnable_transitions = False ) instance-attribute classifier = None instance-attribute __init__ ( input_size = None , n_labels = None , mode = CRFMode . joint ) Nested NER CRF module PARAMETER DESCRIPTION input_size Size of the input embeddings TYPE: Optional [ int ] DEFAULT: None n_labels Number of labels predicted by the module TYPE: Optional [ int ] DEFAULT: None mode Loss mode of the CRF TYPE: CRFMode DEFAULT: CRFMode.joint Source code in edsnlp/models/stack_crf_ner.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , input_size : Optional [ int ] = None , n_labels : Optional [ int ] = None , mode : CRFMode = CRFMode . joint , ): \"\"\" Nested NER CRF module Parameters ---------- input_size: int Size of the input embeddings n_labels: int Number of labels predicted by the module mode: CRFMode Loss mode of the CRF \"\"\" super () . __init__ ( input_size , n_labels ) self . cfg [ \"mode\" ] = mode assert mode in ( CRFMode . independent , CRFMode . joint , CRFMode . marginal ) self . crf = MultiLabelBIOULDecoder ( 1 , learnable_transitions = False ) self . classifier = None initialize () Once the number of labels n_labels are known, this method initializes the torch linear layer. Source code in edsnlp/models/stack_crf_ner.py 67 68 69 70 71 72 73 def initialize ( self ): \"\"\" Once the number of labels n_labels are known, this method initializes the torch linear layer. \"\"\" num_tags = self . n_labels * self . crf . num_tags self . classifier = torch . nn . Linear ( self . input_size , num_tags ) forward ( embeds , mask , spans = None , additional_outputs = None , is_train = False , is_predict = False ) Apply the nested ner module to the document embeddings to: - compute the loss - predict the spans non exclusively. If spans are predicted, they are assigned to the additional_outputs dictionary. PARAMETER DESCRIPTION embeds Token embeddings to predict the tags from TYPE: torch . FloatTensor mask Mask of the sequences TYPE: torch . BoolTensor spans 2d tensor of n_spans * (doc_idx, label_idx, begin, end) TYPE: Optional [ torch . LongTensor ] DEFAULT: None additional_outputs Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This dict will contain the predicted 2d tensor of spans TYPE: Dict [ str , Any ] DEFAULT: None is_train Are we training the model (defaults to True) TYPE: bool DEFAULT: False is_predict Are we predicting the model (defaults to False) TYPE: bool DEFAULT: False RETURNS DESCRIPTION Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model Source code in edsnlp/models/stack_crf_ner.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , spans : Optional [ torch . LongTensor ] = None , additional_outputs : Dict [ str , Any ] = None , is_train : bool = False , is_predict : bool = False , ) -> Optional [ torch . FloatTensor ]: \"\"\" Apply the nested ner module to the document embeddings to: - compute the loss - predict the spans non exclusively. If spans are predicted, they are assigned to the `additional_outputs` dictionary. Parameters ---------- embeds: torch.FloatTensor Token embeddings to predict the tags from mask: torch.BoolTensor Mask of the sequences spans: Optional[torch.LongTensor] 2d tensor of n_spans * (doc_idx, label_idx, begin, end) additional_outputs: Dict[str, Any] Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This dict will contain the predicted 2d tensor of spans is_train: bool=False Are we training the model (defaults to True) is_predict: bool=False Are we predicting the model (defaults to False) Returns ------- Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model \"\"\" n_samples , n_tokens = embeds . shape [: 2 ] logits = self . classifier ( embeds ) crf_logits = flatten_dim ( logits . view ( n_samples , n_tokens , self . n_labels , self . crf . num_tags ) . permute ( 0 , 2 , 1 , 3 ), dim = 0 , ) crf_mask = repeat ( mask , self . n_labels , 0 ) loss = None if is_train : tags = self . crf . spans_to_tags ( spans , n_samples = n_samples , n_tokens = n_tokens , n_labels = self . n_labels ) crf_target = flatten_dim ( torch . nn . functional . one_hot ( tags , 5 ) . bool () if len ( tags . shape ) == 3 else tags , dim = 0 , ) if self . cfg [ \"mode\" ] == CRFMode . joint : loss = self . crf ( crf_logits , crf_mask , crf_target , ) elif self . cfg [ \"mode\" ] == CRFMode . independent : loss = ( - crf_logits . log_softmax ( - 1 ) . masked_fill ( ~ crf_target , IMPOSSIBLE ) . logsumexp ( - 1 )[ crf_mask ] . sum () ) elif self . cfg [ \"mode\" ] == CRFMode . marginal : crf_logits = self . crf . marginal ( crf_logits , crf_mask , ) loss = ( - crf_logits . log_softmax ( - 1 ) . masked_fill ( ~ crf_target , IMPOSSIBLE ) . logsumexp ( - 1 )[ crf_mask ] . sum () ) if ( loss > - IMPOSSIBLE ) . any (): logger . warning ( \"You likely have an impossible transition in your \" \"training data NER tags, skipping this batch.\" ) loss = torch . zeros ( 1 , dtype = torch . float , device = embeds . device ) loss = loss . sum () . unsqueeze ( 0 ) / 100.0 if is_predict : pred_tags = self . crf . decode ( crf_logits , crf_mask ) . reshape ( n_samples , self . n_labels , n_tokens ) pred_spans = self . crf . tags_to_spans ( pred_tags ) additional_outputs [ \"spans\" ] = pred_spans return loss repeat ( t , n , dim , interleave = True ) Source code in edsnlp/models/stack_crf_ner.py 21 22 23 24 25 26 27 28 29 30 31 32 def repeat ( t , n , dim , interleave = True ): repeat_dim = dim + ( 1 if interleave else 0 ) return ( t . unsqueeze ( repeat_dim ) . repeat_interleave ( n , repeat_dim ) . view ( tuple ( - 1 if ( i - dim + t . ndim ) % t . ndim == 0 else s for i , s in enumerate ( t . shape ) ) ) ) flatten_dim ( t , dim , ndim = 1 ) Source code in edsnlp/models/stack_crf_ner.py 35 36 def flatten_dim ( t , dim , ndim = 1 ): return t . reshape (( * t . shape [: dim ], - 1 , * t . shape [ dim + 1 + ndim :])) create_model ( tok2vec , mode , n_labels = None ) Source code in edsnlp/models/stack_crf_ner.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 @registry . layers ( \"eds.stack_crf_ner_model.v1\" ) def create_model ( tok2vec : Model [ List [ Doc ], List [ Floats2d ]], mode : CRFMode , n_labels : int = None , ) -> Model [ Tuple [ Iterable [ Doc ], Optional [ Ints2d ], Optional [ bool ]], Tuple [ Floats1d , Ints2d ], ]: return wrap_pytorch_model ( # noqa encoder = tok2vec , pt_model = StackedCRFNERModule ( input_size = None , # will be set later during initialization n_labels = n_labels , # will likely be set later during initialization mode = mode , ), )","title":"stack_crf_ner"},{"location":"reference/models/stack_crf_ner/#edsnlpmodelsstack_crf_ner","text":"","title":"edsnlp.models.stack_crf_ner"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.CRFMode","text":"Bases: str , Enum Source code in edsnlp/models/stack_crf_ner.py 15 16 17 18 class CRFMode ( str , Enum ): independent = \"independent\" joint = \"joint\" marginal = \"marginal\"","title":"CRFMode"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.CRFMode.independent","text":"","title":"independent"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.CRFMode.joint","text":"","title":"joint"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.CRFMode.marginal","text":"","title":"marginal"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule","text":"Bases: PytorchWrapperModule Source code in edsnlp/models/stack_crf_ner.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class StackedCRFNERModule ( PytorchWrapperModule ): def __init__ ( self , input_size : Optional [ int ] = None , n_labels : Optional [ int ] = None , mode : CRFMode = CRFMode . joint , ): \"\"\" Nested NER CRF module Parameters ---------- input_size: int Size of the input embeddings n_labels: int Number of labels predicted by the module mode: CRFMode Loss mode of the CRF \"\"\" super () . __init__ ( input_size , n_labels ) self . cfg [ \"mode\" ] = mode assert mode in ( CRFMode . independent , CRFMode . joint , CRFMode . marginal ) self . crf = MultiLabelBIOULDecoder ( 1 , learnable_transitions = False ) self . classifier = None def initialize ( self ): \"\"\" Once the number of labels n_labels are known, this method initializes the torch linear layer. \"\"\" num_tags = self . n_labels * self . crf . num_tags self . classifier = torch . nn . Linear ( self . input_size , num_tags ) def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , spans : Optional [ torch . LongTensor ] = None , additional_outputs : Dict [ str , Any ] = None , is_train : bool = False , is_predict : bool = False , ) -> Optional [ torch . FloatTensor ]: \"\"\" Apply the nested ner module to the document embeddings to: - compute the loss - predict the spans non exclusively. If spans are predicted, they are assigned to the `additional_outputs` dictionary. Parameters ---------- embeds: torch.FloatTensor Token embeddings to predict the tags from mask: torch.BoolTensor Mask of the sequences spans: Optional[torch.LongTensor] 2d tensor of n_spans * (doc_idx, label_idx, begin, end) additional_outputs: Dict[str, Any] Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This dict will contain the predicted 2d tensor of spans is_train: bool=False Are we training the model (defaults to True) is_predict: bool=False Are we predicting the model (defaults to False) Returns ------- Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model \"\"\" n_samples , n_tokens = embeds . shape [: 2 ] logits = self . classifier ( embeds ) crf_logits = flatten_dim ( logits . view ( n_samples , n_tokens , self . n_labels , self . crf . num_tags ) . permute ( 0 , 2 , 1 , 3 ), dim = 0 , ) crf_mask = repeat ( mask , self . n_labels , 0 ) loss = None if is_train : tags = self . crf . spans_to_tags ( spans , n_samples = n_samples , n_tokens = n_tokens , n_labels = self . n_labels ) crf_target = flatten_dim ( torch . nn . functional . one_hot ( tags , 5 ) . bool () if len ( tags . shape ) == 3 else tags , dim = 0 , ) if self . cfg [ \"mode\" ] == CRFMode . joint : loss = self . crf ( crf_logits , crf_mask , crf_target , ) elif self . cfg [ \"mode\" ] == CRFMode . independent : loss = ( - crf_logits . log_softmax ( - 1 ) . masked_fill ( ~ crf_target , IMPOSSIBLE ) . logsumexp ( - 1 )[ crf_mask ] . sum () ) elif self . cfg [ \"mode\" ] == CRFMode . marginal : crf_logits = self . crf . marginal ( crf_logits , crf_mask , ) loss = ( - crf_logits . log_softmax ( - 1 ) . masked_fill ( ~ crf_target , IMPOSSIBLE ) . logsumexp ( - 1 )[ crf_mask ] . sum () ) if ( loss > - IMPOSSIBLE ) . any (): logger . warning ( \"You likely have an impossible transition in your \" \"training data NER tags, skipping this batch.\" ) loss = torch . zeros ( 1 , dtype = torch . float , device = embeds . device ) loss = loss . sum () . unsqueeze ( 0 ) / 100.0 if is_predict : pred_tags = self . crf . decode ( crf_logits , crf_mask ) . reshape ( n_samples , self . n_labels , n_tokens ) pred_spans = self . crf . tags_to_spans ( pred_tags ) additional_outputs [ \"spans\" ] = pred_spans return loss","title":"StackedCRFNERModule"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule.crf","text":"","title":"crf"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule.classifier","text":"","title":"classifier"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule.__init__","text":"Nested NER CRF module PARAMETER DESCRIPTION input_size Size of the input embeddings TYPE: Optional [ int ] DEFAULT: None n_labels Number of labels predicted by the module TYPE: Optional [ int ] DEFAULT: None mode Loss mode of the CRF TYPE: CRFMode DEFAULT: CRFMode.joint Source code in edsnlp/models/stack_crf_ner.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , input_size : Optional [ int ] = None , n_labels : Optional [ int ] = None , mode : CRFMode = CRFMode . joint , ): \"\"\" Nested NER CRF module Parameters ---------- input_size: int Size of the input embeddings n_labels: int Number of labels predicted by the module mode: CRFMode Loss mode of the CRF \"\"\" super () . __init__ ( input_size , n_labels ) self . cfg [ \"mode\" ] = mode assert mode in ( CRFMode . independent , CRFMode . joint , CRFMode . marginal ) self . crf = MultiLabelBIOULDecoder ( 1 , learnable_transitions = False ) self . classifier = None","title":"__init__()"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule.initialize","text":"Once the number of labels n_labels are known, this method initializes the torch linear layer. Source code in edsnlp/models/stack_crf_ner.py 67 68 69 70 71 72 73 def initialize ( self ): \"\"\" Once the number of labels n_labels are known, this method initializes the torch linear layer. \"\"\" num_tags = self . n_labels * self . crf . num_tags self . classifier = torch . nn . Linear ( self . input_size , num_tags )","title":"initialize()"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule.forward","text":"Apply the nested ner module to the document embeddings to: - compute the loss - predict the spans non exclusively. If spans are predicted, they are assigned to the additional_outputs dictionary. PARAMETER DESCRIPTION embeds Token embeddings to predict the tags from TYPE: torch . FloatTensor mask Mask of the sequences TYPE: torch . BoolTensor spans 2d tensor of n_spans * (doc_idx, label_idx, begin, end) TYPE: Optional [ torch . LongTensor ] DEFAULT: None additional_outputs Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This dict will contain the predicted 2d tensor of spans TYPE: Dict [ str , Any ] DEFAULT: None is_train Are we training the model (defaults to True) TYPE: bool DEFAULT: False is_predict Are we predicting the model (defaults to False) TYPE: bool DEFAULT: False RETURNS DESCRIPTION Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model Source code in edsnlp/models/stack_crf_ner.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , spans : Optional [ torch . LongTensor ] = None , additional_outputs : Dict [ str , Any ] = None , is_train : bool = False , is_predict : bool = False , ) -> Optional [ torch . FloatTensor ]: \"\"\" Apply the nested ner module to the document embeddings to: - compute the loss - predict the spans non exclusively. If spans are predicted, they are assigned to the `additional_outputs` dictionary. Parameters ---------- embeds: torch.FloatTensor Token embeddings to predict the tags from mask: torch.BoolTensor Mask of the sequences spans: Optional[torch.LongTensor] 2d tensor of n_spans * (doc_idx, label_idx, begin, end) additional_outputs: Dict[str, Any] Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This dict will contain the predicted 2d tensor of spans is_train: bool=False Are we training the model (defaults to True) is_predict: bool=False Are we predicting the model (defaults to False) Returns ------- Optional[torch.FloatTensor] Optional 0d loss (shape = [1]) to train the model \"\"\" n_samples , n_tokens = embeds . shape [: 2 ] logits = self . classifier ( embeds ) crf_logits = flatten_dim ( logits . view ( n_samples , n_tokens , self . n_labels , self . crf . num_tags ) . permute ( 0 , 2 , 1 , 3 ), dim = 0 , ) crf_mask = repeat ( mask , self . n_labels , 0 ) loss = None if is_train : tags = self . crf . spans_to_tags ( spans , n_samples = n_samples , n_tokens = n_tokens , n_labels = self . n_labels ) crf_target = flatten_dim ( torch . nn . functional . one_hot ( tags , 5 ) . bool () if len ( tags . shape ) == 3 else tags , dim = 0 , ) if self . cfg [ \"mode\" ] == CRFMode . joint : loss = self . crf ( crf_logits , crf_mask , crf_target , ) elif self . cfg [ \"mode\" ] == CRFMode . independent : loss = ( - crf_logits . log_softmax ( - 1 ) . masked_fill ( ~ crf_target , IMPOSSIBLE ) . logsumexp ( - 1 )[ crf_mask ] . sum () ) elif self . cfg [ \"mode\" ] == CRFMode . marginal : crf_logits = self . crf . marginal ( crf_logits , crf_mask , ) loss = ( - crf_logits . log_softmax ( - 1 ) . masked_fill ( ~ crf_target , IMPOSSIBLE ) . logsumexp ( - 1 )[ crf_mask ] . sum () ) if ( loss > - IMPOSSIBLE ) . any (): logger . warning ( \"You likely have an impossible transition in your \" \"training data NER tags, skipping this batch.\" ) loss = torch . zeros ( 1 , dtype = torch . float , device = embeds . device ) loss = loss . sum () . unsqueeze ( 0 ) / 100.0 if is_predict : pred_tags = self . crf . decode ( crf_logits , crf_mask ) . reshape ( n_samples , self . n_labels , n_tokens ) pred_spans = self . crf . tags_to_spans ( pred_tags ) additional_outputs [ \"spans\" ] = pred_spans return loss","title":"forward()"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.repeat","text":"Source code in edsnlp/models/stack_crf_ner.py 21 22 23 24 25 26 27 28 29 30 31 32 def repeat ( t , n , dim , interleave = True ): repeat_dim = dim + ( 1 if interleave else 0 ) return ( t . unsqueeze ( repeat_dim ) . repeat_interleave ( n , repeat_dim ) . view ( tuple ( - 1 if ( i - dim + t . ndim ) % t . ndim == 0 else s for i , s in enumerate ( t . shape ) ) ) )","title":"repeat()"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.flatten_dim","text":"Source code in edsnlp/models/stack_crf_ner.py 35 36 def flatten_dim ( t , dim , ndim = 1 ): return t . reshape (( * t . shape [: dim ], - 1 , * t . shape [ dim + 1 + ndim :]))","title":"flatten_dim()"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.create_model","text":"Source code in edsnlp/models/stack_crf_ner.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 @registry . layers ( \"eds.stack_crf_ner_model.v1\" ) def create_model ( tok2vec : Model [ List [ Doc ], List [ Floats2d ]], mode : CRFMode , n_labels : int = None , ) -> Model [ Tuple [ Iterable [ Doc ], Optional [ Ints2d ], Optional [ bool ]], Tuple [ Floats1d , Ints2d ], ]: return wrap_pytorch_model ( # noqa encoder = tok2vec , pt_model = StackedCRFNERModule ( input_size = None , # will be set later during initialization n_labels = n_labels , # will likely be set later during initialization mode = mode , ), )","title":"create_model()"},{"location":"reference/models/torch/","text":"edsnlp.models.torch","title":"`edsnlp.models.torch`"},{"location":"reference/models/torch/#edsnlpmodelstorch","text":"","title":"edsnlp.models.torch"},{"location":"reference/models/torch/crf/","text":"edsnlp.models.torch.crf IMPOSSIBLE = - 10000000 module-attribute LinearChainCRF Bases: torch . nn . Module Source code in edsnlp/models/torch/crf.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class LinearChainCRF ( torch . nn . Module ): def __init__ ( self , forbidden_transitions , start_forbidden_transitions = None , end_forbidden_transitions = None , learnable_transitions = True , with_start_end_transitions = True , ): \"\"\" A linear chain CRF in Pytorch Parameters ---------- forbidden_transitions: torch.BoolTensor Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1 start_forbidden_transitions: Optional[torch.BoolTensor] Shape: n_tags Impossible transitions at the start of a sequence end_forbidden_transitions Optional[torch.BoolTensor] Shape: n_tags Impossible transitions at the end of a sequence learnable_transitions: bool Should we learn transition scores to complete the constraints ? with_start_end_transitions: Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores \"\"\" super () . __init__ () num_tags = forbidden_transitions . shape [ 0 ] self . register_buffer ( \"forbidden_transitions\" , forbidden_transitions . bool ()) if start_forbidden_transitions is not None : self . register_buffer ( \"start_forbidden_transitions\" , start_forbidden_transitions . bool () ) else : self . register_buffer ( \"start_forbidden_transitions\" , torch . zeros ( num_tags , dtype = torch . bool ) ) if end_forbidden_transitions is not None : self . register_buffer ( \"end_forbidden_transitions\" , end_forbidden_transitions . bool () ) else : self . register_buffer ( \"end_forbidden_transitions\" , torch . zeros ( num_tags , dtype = torch . bool ) ) if learnable_transitions : self . transitions = torch . nn . Parameter ( torch . zeros_like ( forbidden_transitions , dtype = torch . float ) ) else : self . register_buffer ( \"transitions\" , torch . zeros_like ( forbidden_transitions , dtype = torch . float ), ) if learnable_transitions and with_start_end_transitions : self . start_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float ) ) else : self . register_buffer ( \"start_transitions\" , torch . zeros ( num_tags , dtype = torch . float ) ) if learnable_transitions and with_start_end_transitions : self . end_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float ) ) else : self . register_buffer ( \"end_transitions\" , torch . zeros ( num_tags , dtype = torch . float ) ) def decode ( self , emissions , mask ): \"\"\" Decodes a sequence of tag scores using the Viterbi algorithm Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens Returns ------- torch.LongTensor Backtrack indices (= argmax), ie best tag sequence \"\"\" transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) n_samples , n_tokens = mask . shape emissions [ ... , 1 :][ ~ mask ] = IMPOSSIBLE emissions = emissions . transpose ( 0 , 1 ) # emissions: n_tokens * n_samples * n_tags out = [ emissions [ 0 ] + start_transitions ] backtrack = [] for k in range ( 1 , len ( emissions )): res , indices = max_reduce ( out [ - 1 ], transitions ) backtrack . append ( indices ) out . append ( res + emissions [ k ]) res , indices = max_reduce ( out [ - 1 ], end_transitions . unsqueeze ( - 1 )) path = torch . zeros ( n_samples , n_tokens , dtype = torch . long ) path [:, - 1 ] = indices . squeeze ( - 1 ) path_range = torch . arange ( n_samples , device = path . device ) if len ( backtrack ) > 1 : # Backward max path following for k , b in enumerate ( backtrack [:: - 1 ]): path [:, - k - 2 ] = b [ path_range , path [:, - k - 1 ]] return path def marginal ( self , emissions , mask ): \"\"\" Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF We could use the `propagate` method but this implementation is faster. Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens Returns ------- torch.FloatTensor Shape: ... * n_tokens * n_tags \"\"\" device = emissions . device transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) bi_transitions = torch . stack ([ transitions , transitions . t ()], dim = 0 ) # add start transitions (ie cannot start with ...) emissions [:, 0 ] = emissions [:, 0 ] + start_transitions # add end transitions (ie cannot end with ...): flip the emissions along the # token axis, and add the end transitions # emissions = masked_flip(emissions, mask, dim_x=1) emissions [ torch . arange ( mask . shape [ 0 ], device = device ), mask . long () . sum ( 1 ) - 1 ] = ( emissions [ torch . arange ( mask . shape [ 0 ], device = device ), mask . long () . sum ( 1 ) - 1 , ] + end_transitions ) # stack start -> end emissions (needs to flip the previously flipped emissions), # and end -> start emissions bi_emissions = torch . stack ( [ emissions , masked_flip ( emissions , mask , dim_x = 1 )], 1 ) bi_emissions = bi_emissions . transpose ( 0 , 2 ) out = [ bi_emissions [ 0 ]] for k in range ( 1 , len ( bi_emissions )): res = logsumexp_reduce ( out [ - 1 ], bi_transitions ) out . append ( res + bi_emissions [ k ]) out = torch . stack ( out , dim = 0 ) . transpose ( 0 , 2 ) forward = out [:, 0 ] backward = masked_flip ( out [:, 1 ], mask , dim_x = 1 ) backward_z = backward [:, 0 ] . logsumexp ( - 1 ) return forward + backward - emissions - backward_z [:, None , None ] def forward ( self , emissions , mask , target ): \"\"\" Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss. We could use the `propagate` method but this implementation is faster. Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens target: torch.BoolTensor Shape: ... * n_tokens * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training. Returns ------- torch.FloatTensor Shape: ... The loss \"\"\" transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) bi_emissions = torch . stack ( [ emissions . masked_fill ( ~ target , IMPOSSIBLE ), emissions ], 1 ) . transpose ( 0 , 2 ) # emissions: n_samples * n_tokens * n_tags # bi_emissions: n_tokens * 2 * n_samples * n_tags out = [ bi_emissions [ 0 ] + start_transitions ] for k in range ( 1 , len ( bi_emissions )): res = logsumexp_reduce ( out [ - 1 ], transitions ) out . append ( res + bi_emissions [ k ]) out = torch . stack ( out , dim = 0 ) . transpose ( 0 , 2 ) # n_samples * 2 * n_tokens * n_tags z = ( masked_flip ( out , mask . unsqueeze ( 1 ) . repeat ( 1 , 2 , 1 ), dim_x = 2 )[:, :, 0 ] + end_transitions ) supervised_z = z [:, 0 ] . logsumexp ( - 1 ) unsupervised_z = z [:, 1 ] . logsumexp ( - 1 ) return - ( supervised_z - unsupervised_z ) transitions = torch . nn . Parameter ( torch . zeros_like ( forbidden_transitions , dtype = torch . float )) instance-attribute start_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float )) instance-attribute end_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float )) instance-attribute __init__ ( forbidden_transitions , start_forbidden_transitions = None , end_forbidden_transitions = None , learnable_transitions = True , with_start_end_transitions = True ) A linear chain CRF in Pytorch PARAMETER DESCRIPTION forbidden_transitions Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1 start_forbidden_transitions Shape: n_tags Impossible transitions at the start of a sequence DEFAULT: None end_forbidden_transitions Shape: n_tags Impossible transitions at the end of a sequence DEFAULT: None learnable_transitions Should we learn transition scores to complete the constraints ? DEFAULT: True with_start_end_transitions Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores DEFAULT: True Source code in edsnlp/models/torch/crf.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , forbidden_transitions , start_forbidden_transitions = None , end_forbidden_transitions = None , learnable_transitions = True , with_start_end_transitions = True , ): \"\"\" A linear chain CRF in Pytorch Parameters ---------- forbidden_transitions: torch.BoolTensor Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1 start_forbidden_transitions: Optional[torch.BoolTensor] Shape: n_tags Impossible transitions at the start of a sequence end_forbidden_transitions Optional[torch.BoolTensor] Shape: n_tags Impossible transitions at the end of a sequence learnable_transitions: bool Should we learn transition scores to complete the constraints ? with_start_end_transitions: Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores \"\"\" super () . __init__ () num_tags = forbidden_transitions . shape [ 0 ] self . register_buffer ( \"forbidden_transitions\" , forbidden_transitions . bool ()) if start_forbidden_transitions is not None : self . register_buffer ( \"start_forbidden_transitions\" , start_forbidden_transitions . bool () ) else : self . register_buffer ( \"start_forbidden_transitions\" , torch . zeros ( num_tags , dtype = torch . bool ) ) if end_forbidden_transitions is not None : self . register_buffer ( \"end_forbidden_transitions\" , end_forbidden_transitions . bool () ) else : self . register_buffer ( \"end_forbidden_transitions\" , torch . zeros ( num_tags , dtype = torch . bool ) ) if learnable_transitions : self . transitions = torch . nn . Parameter ( torch . zeros_like ( forbidden_transitions , dtype = torch . float ) ) else : self . register_buffer ( \"transitions\" , torch . zeros_like ( forbidden_transitions , dtype = torch . float ), ) if learnable_transitions and with_start_end_transitions : self . start_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float ) ) else : self . register_buffer ( \"start_transitions\" , torch . zeros ( num_tags , dtype = torch . float ) ) if learnable_transitions and with_start_end_transitions : self . end_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float ) ) else : self . register_buffer ( \"end_transitions\" , torch . zeros ( num_tags , dtype = torch . float ) ) decode ( emissions , mask ) Decodes a sequence of tag scores using the Viterbi algorithm PARAMETER DESCRIPTION emissions Shape: ... * n_tokens * n_tags mask Shape: ... * n_tokens RETURNS DESCRIPTION torch.LongTensor Backtrack indices (= argmax), ie best tag sequence Source code in edsnlp/models/torch/crf.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def decode ( self , emissions , mask ): \"\"\" Decodes a sequence of tag scores using the Viterbi algorithm Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens Returns ------- torch.LongTensor Backtrack indices (= argmax), ie best tag sequence \"\"\" transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) n_samples , n_tokens = mask . shape emissions [ ... , 1 :][ ~ mask ] = IMPOSSIBLE emissions = emissions . transpose ( 0 , 1 ) # emissions: n_tokens * n_samples * n_tags out = [ emissions [ 0 ] + start_transitions ] backtrack = [] for k in range ( 1 , len ( emissions )): res , indices = max_reduce ( out [ - 1 ], transitions ) backtrack . append ( indices ) out . append ( res + emissions [ k ]) res , indices = max_reduce ( out [ - 1 ], end_transitions . unsqueeze ( - 1 )) path = torch . zeros ( n_samples , n_tokens , dtype = torch . long ) path [:, - 1 ] = indices . squeeze ( - 1 ) path_range = torch . arange ( n_samples , device = path . device ) if len ( backtrack ) > 1 : # Backward max path following for k , b in enumerate ( backtrack [:: - 1 ]): path [:, - k - 2 ] = b [ path_range , path [:, - k - 1 ]] return path marginal ( emissions , mask ) Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF We could use the propagate method but this implementation is faster. PARAMETER DESCRIPTION emissions Shape: ... * n_tokens * n_tags mask Shape: ... * n_tokens RETURNS DESCRIPTION torch.FloatTensor Shape: ... * n_tokens * n_tags Source code in edsnlp/models/torch/crf.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def marginal ( self , emissions , mask ): \"\"\" Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF We could use the `propagate` method but this implementation is faster. Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens Returns ------- torch.FloatTensor Shape: ... * n_tokens * n_tags \"\"\" device = emissions . device transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) bi_transitions = torch . stack ([ transitions , transitions . t ()], dim = 0 ) # add start transitions (ie cannot start with ...) emissions [:, 0 ] = emissions [:, 0 ] + start_transitions # add end transitions (ie cannot end with ...): flip the emissions along the # token axis, and add the end transitions # emissions = masked_flip(emissions, mask, dim_x=1) emissions [ torch . arange ( mask . shape [ 0 ], device = device ), mask . long () . sum ( 1 ) - 1 ] = ( emissions [ torch . arange ( mask . shape [ 0 ], device = device ), mask . long () . sum ( 1 ) - 1 , ] + end_transitions ) # stack start -> end emissions (needs to flip the previously flipped emissions), # and end -> start emissions bi_emissions = torch . stack ( [ emissions , masked_flip ( emissions , mask , dim_x = 1 )], 1 ) bi_emissions = bi_emissions . transpose ( 0 , 2 ) out = [ bi_emissions [ 0 ]] for k in range ( 1 , len ( bi_emissions )): res = logsumexp_reduce ( out [ - 1 ], bi_transitions ) out . append ( res + bi_emissions [ k ]) out = torch . stack ( out , dim = 0 ) . transpose ( 0 , 2 ) forward = out [:, 0 ] backward = masked_flip ( out [:, 1 ], mask , dim_x = 1 ) backward_z = backward [:, 0 ] . logsumexp ( - 1 ) return forward + backward - emissions - backward_z [:, None , None ] forward ( emissions , mask , target ) Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss. We could use the propagate method but this implementation is faster. PARAMETER DESCRIPTION emissions Shape: ... * n_tokens * n_tags mask Shape: ... * n_tokens target Shape: ... * n_tokens * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training. RETURNS DESCRIPTION torch.FloatTensor Shape: ... The loss Source code in edsnlp/models/torch/crf.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def forward ( self , emissions , mask , target ): \"\"\" Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss. We could use the `propagate` method but this implementation is faster. Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens target: torch.BoolTensor Shape: ... * n_tokens * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training. Returns ------- torch.FloatTensor Shape: ... The loss \"\"\" transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) bi_emissions = torch . stack ( [ emissions . masked_fill ( ~ target , IMPOSSIBLE ), emissions ], 1 ) . transpose ( 0 , 2 ) # emissions: n_samples * n_tokens * n_tags # bi_emissions: n_tokens * 2 * n_samples * n_tags out = [ bi_emissions [ 0 ] + start_transitions ] for k in range ( 1 , len ( bi_emissions )): res = logsumexp_reduce ( out [ - 1 ], transitions ) out . append ( res + bi_emissions [ k ]) out = torch . stack ( out , dim = 0 ) . transpose ( 0 , 2 ) # n_samples * 2 * n_tokens * n_tags z = ( masked_flip ( out , mask . unsqueeze ( 1 ) . repeat ( 1 , 2 , 1 ), dim_x = 2 )[:, :, 0 ] + end_transitions ) supervised_z = z [:, 0 ] . logsumexp ( - 1 ) unsupervised_z = z [:, 1 ] . logsumexp ( - 1 ) return - ( supervised_z - unsupervised_z ) MultiLabelBIOULDecoder Bases: LinearChainCRF Source code in edsnlp/models/torch/crf.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 class MultiLabelBIOULDecoder ( LinearChainCRF ): def __init__ ( self , num_labels , with_start_end_transitions = True , learnable_transitions = True , ): \"\"\" Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme Parameters ---------- num_labels: int with_start_end_transitions: bool learnable_transitions: bool \"\"\" O , I , B , L , U = 0 , 1 , 2 , 3 , 4 num_tags = 1 + num_labels * 4 self . num_tags = num_tags forbidden_transitions = torch . ones ( num_tags , num_tags , dtype = torch . bool ) forbidden_transitions [ O , O ] = 0 # O to O for i in range ( num_labels ): STRIDE = 4 * i for j in range ( num_labels ): STRIDE_J = j * 4 forbidden_transitions [ L + STRIDE , B + STRIDE_J ] = 0 # L-i to B-j forbidden_transitions [ L + STRIDE , U + STRIDE_J ] = 0 # L-i to U-j forbidden_transitions [ U + STRIDE , B + STRIDE_J ] = 0 # U-i to B-j forbidden_transitions [ U + STRIDE , U + STRIDE_J ] = 0 # U-i to U-j forbidden_transitions [ O , B + STRIDE ] = 0 # O to B-i forbidden_transitions [ B + STRIDE , I + STRIDE ] = 0 # B-i to I-i forbidden_transitions [ I + STRIDE , I + STRIDE ] = 0 # I-i to I-i forbidden_transitions [ I + STRIDE , L + STRIDE ] = 0 # I-i to L-i forbidden_transitions [ B + STRIDE , L + STRIDE ] = 0 # B-i to L-i forbidden_transitions [ L + STRIDE , O ] = 0 # L-i to O forbidden_transitions [ O , U + STRIDE ] = 0 # O to U-i forbidden_transitions [ U + STRIDE , O ] = 0 # U-i to O start_forbidden_transitions = torch . zeros ( num_tags , dtype = torch . bool ) if with_start_end_transitions : for i in range ( num_labels ): STRIDE = 4 * i start_forbidden_transitions [ I + STRIDE ] = 1 # forbidden to start by I-i start_forbidden_transitions [ L + STRIDE ] = 1 # forbidden to start by L-i end_forbidden_transitions = torch . zeros ( num_tags , dtype = torch . bool ) if with_start_end_transitions : for i in range ( num_labels ): STRIDE = 4 * i end_forbidden_transitions [ I + STRIDE ] = 1 # forbidden to end by I-i end_forbidden_transitions [ B + STRIDE ] = 1 # forbidden to end by B-i super () . __init__ ( forbidden_transitions , start_forbidden_transitions , end_forbidden_transitions , with_start_end_transitions = with_start_end_transitions , learnable_transitions = learnable_transitions , ) @staticmethod def spans_to_tags ( spans : torch . Tensor , n_samples : int , n_labels : int , n_tokens : int ): \"\"\" Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end) to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens Parameters ---------- spans: torch.Tensor n_samples: int n_labels: int n_tokens: int Returns ------- torch.Tensor \"\"\" device = spans . device cpu = torch . device ( \"cpu\" ) if not len ( spans ): return torch . zeros ( n_samples , n_labels , n_tokens , dtype = torch . long , device = device ) doc_indices , label_indices , begins , ends = spans . cpu () . unbind ( - 1 ) ends = ends - 1 pos = torch . arange ( n_tokens , device = cpu ) b_tags , l_tags , u_tags , i_tags = torch . zeros ( 4 , n_samples , n_labels , n_tokens , dtype = torch . bool , device = cpu ) . unbind ( 0 ) tags = torch . zeros ( n_samples , n_labels , n_tokens , dtype = torch . long , device = cpu ) where_u = begins == ends u_tags [ doc_indices [ where_u ], label_indices [ where_u ], begins [ where_u ]] = True b_tags [ doc_indices [ ~ where_u ], label_indices [ ~ where_u ], begins [ ~ where_u ]] = True l_tags [ doc_indices [ ~ where_u ], label_indices [ ~ where_u ], ends [ ~ where_u ]] = True i_tags . view ( - 1 , n_tokens ) . index_add_ ( 0 , doc_indices * n_labels + label_indices , ( begins . unsqueeze ( - 1 ) < pos ) & ( pos < ends . unsqueeze ( - 1 )), ) tags [ u_tags ] = 4 tags [ b_tags ] = 2 tags [ l_tags ] = 3 tags [ i_tags . bool ()] = 1 return tags . to ( device ) @staticmethod def tags_to_spans ( tags ): \"\"\" Convert a sequence of multiple label BIOUL tags to a sequence of spans Parameters ---------- tags: torch.LongTensor Shape: n_samples * n_labels * n_tokens mask: torch.BoolTensor Shape: n_samples * n_labels * n_tokens Returns ------- torch.LongTensor Shape: n_spans * 4 (doc_idx, label_idx, begin, end) \"\"\" return torch . cat ( [ torch . nonzero (( tags == 4 ) | ( tags == 2 )), torch . nonzero (( tags == 4 ) | ( tags == 3 ))[ ... , [ - 1 ]] + 1 , ], dim =- 1 , ) num_tags = num_tags instance-attribute __init__ ( num_labels , with_start_end_transitions = True , learnable_transitions = True ) Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme PARAMETER DESCRIPTION num_labels with_start_end_transitions DEFAULT: True learnable_transitions DEFAULT: True Source code in edsnlp/models/torch/crf.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def __init__ ( self , num_labels , with_start_end_transitions = True , learnable_transitions = True , ): \"\"\" Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme Parameters ---------- num_labels: int with_start_end_transitions: bool learnable_transitions: bool \"\"\" O , I , B , L , U = 0 , 1 , 2 , 3 , 4 num_tags = 1 + num_labels * 4 self . num_tags = num_tags forbidden_transitions = torch . ones ( num_tags , num_tags , dtype = torch . bool ) forbidden_transitions [ O , O ] = 0 # O to O for i in range ( num_labels ): STRIDE = 4 * i for j in range ( num_labels ): STRIDE_J = j * 4 forbidden_transitions [ L + STRIDE , B + STRIDE_J ] = 0 # L-i to B-j forbidden_transitions [ L + STRIDE , U + STRIDE_J ] = 0 # L-i to U-j forbidden_transitions [ U + STRIDE , B + STRIDE_J ] = 0 # U-i to B-j forbidden_transitions [ U + STRIDE , U + STRIDE_J ] = 0 # U-i to U-j forbidden_transitions [ O , B + STRIDE ] = 0 # O to B-i forbidden_transitions [ B + STRIDE , I + STRIDE ] = 0 # B-i to I-i forbidden_transitions [ I + STRIDE , I + STRIDE ] = 0 # I-i to I-i forbidden_transitions [ I + STRIDE , L + STRIDE ] = 0 # I-i to L-i forbidden_transitions [ B + STRIDE , L + STRIDE ] = 0 # B-i to L-i forbidden_transitions [ L + STRIDE , O ] = 0 # L-i to O forbidden_transitions [ O , U + STRIDE ] = 0 # O to U-i forbidden_transitions [ U + STRIDE , O ] = 0 # U-i to O start_forbidden_transitions = torch . zeros ( num_tags , dtype = torch . bool ) if with_start_end_transitions : for i in range ( num_labels ): STRIDE = 4 * i start_forbidden_transitions [ I + STRIDE ] = 1 # forbidden to start by I-i start_forbidden_transitions [ L + STRIDE ] = 1 # forbidden to start by L-i end_forbidden_transitions = torch . zeros ( num_tags , dtype = torch . bool ) if with_start_end_transitions : for i in range ( num_labels ): STRIDE = 4 * i end_forbidden_transitions [ I + STRIDE ] = 1 # forbidden to end by I-i end_forbidden_transitions [ B + STRIDE ] = 1 # forbidden to end by B-i super () . __init__ ( forbidden_transitions , start_forbidden_transitions , end_forbidden_transitions , with_start_end_transitions = with_start_end_transitions , learnable_transitions = learnable_transitions , ) spans_to_tags ( spans , n_samples , n_labels , n_tokens ) Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end) to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens PARAMETER DESCRIPTION spans TYPE: torch . Tensor n_samples TYPE: int n_labels TYPE: int n_tokens TYPE: int RETURNS DESCRIPTION torch.Tensor Source code in edsnlp/models/torch/crf.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 @staticmethod def spans_to_tags ( spans : torch . Tensor , n_samples : int , n_labels : int , n_tokens : int ): \"\"\" Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end) to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens Parameters ---------- spans: torch.Tensor n_samples: int n_labels: int n_tokens: int Returns ------- torch.Tensor \"\"\" device = spans . device cpu = torch . device ( \"cpu\" ) if not len ( spans ): return torch . zeros ( n_samples , n_labels , n_tokens , dtype = torch . long , device = device ) doc_indices , label_indices , begins , ends = spans . cpu () . unbind ( - 1 ) ends = ends - 1 pos = torch . arange ( n_tokens , device = cpu ) b_tags , l_tags , u_tags , i_tags = torch . zeros ( 4 , n_samples , n_labels , n_tokens , dtype = torch . bool , device = cpu ) . unbind ( 0 ) tags = torch . zeros ( n_samples , n_labels , n_tokens , dtype = torch . long , device = cpu ) where_u = begins == ends u_tags [ doc_indices [ where_u ], label_indices [ where_u ], begins [ where_u ]] = True b_tags [ doc_indices [ ~ where_u ], label_indices [ ~ where_u ], begins [ ~ where_u ]] = True l_tags [ doc_indices [ ~ where_u ], label_indices [ ~ where_u ], ends [ ~ where_u ]] = True i_tags . view ( - 1 , n_tokens ) . index_add_ ( 0 , doc_indices * n_labels + label_indices , ( begins . unsqueeze ( - 1 ) < pos ) & ( pos < ends . unsqueeze ( - 1 )), ) tags [ u_tags ] = 4 tags [ b_tags ] = 2 tags [ l_tags ] = 3 tags [ i_tags . bool ()] = 1 return tags . to ( device ) tags_to_spans ( tags ) Convert a sequence of multiple label BIOUL tags to a sequence of spans PARAMETER DESCRIPTION tags Shape: n_samples * n_labels * n_tokens mask Shape: n_samples * n_labels * n_tokens RETURNS DESCRIPTION torch.LongTensor Shape: n_spans * 4 (doc_idx, label_idx, begin, end) Source code in edsnlp/models/torch/crf.py 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 @staticmethod def tags_to_spans ( tags ): \"\"\" Convert a sequence of multiple label BIOUL tags to a sequence of spans Parameters ---------- tags: torch.LongTensor Shape: n_samples * n_labels * n_tokens mask: torch.BoolTensor Shape: n_samples * n_labels * n_tokens Returns ------- torch.LongTensor Shape: n_spans * 4 (doc_idx, label_idx, begin, end) \"\"\" return torch . cat ( [ torch . nonzero (( tags == 4 ) | ( tags == 2 )), torch . nonzero (( tags == 4 ) | ( tags == 3 ))[ ... , [ - 1 ]] + 1 , ], dim =- 1 , ) masked_flip ( x , mask , dim_x =- 2 ) Source code in edsnlp/models/torch/crf.py 6 7 8 9 def masked_flip ( x , mask , dim_x =- 2 ): flipped_x = torch . zeros_like ( x ) flipped_x [ mask ] = x . flip ( dim_x )[ mask . flip ( - 1 )] return flipped_x logsumexp_reduce ( log_A , log_B ) Source code in edsnlp/models/torch/crf.py 12 13 14 15 16 17 @torch . jit . script def logsumexp_reduce ( log_A , log_B ): # log_A: 2 * N * M # log_B: 2 * M * O # out: 2 * N * O return ( log_A . unsqueeze ( - 1 ) + log_B . unsqueeze ( - 3 )) . logsumexp ( - 2 ) max_reduce ( log_A , log_B ) Source code in edsnlp/models/torch/crf.py 20 21 22 23 24 25 @torch . jit . script def max_reduce ( log_A , log_B ): # log_A: 2 * N * M # log_B: 2 * M * O # out: 2 * N * O return ( log_A . unsqueeze ( - 1 ) + log_B . unsqueeze ( - 3 )) . max ( - 2 )","title":"crf"},{"location":"reference/models/torch/crf/#edsnlpmodelstorchcrf","text":"","title":"edsnlp.models.torch.crf"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.IMPOSSIBLE","text":"","title":"IMPOSSIBLE"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF","text":"Bases: torch . nn . Module Source code in edsnlp/models/torch/crf.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class LinearChainCRF ( torch . nn . Module ): def __init__ ( self , forbidden_transitions , start_forbidden_transitions = None , end_forbidden_transitions = None , learnable_transitions = True , with_start_end_transitions = True , ): \"\"\" A linear chain CRF in Pytorch Parameters ---------- forbidden_transitions: torch.BoolTensor Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1 start_forbidden_transitions: Optional[torch.BoolTensor] Shape: n_tags Impossible transitions at the start of a sequence end_forbidden_transitions Optional[torch.BoolTensor] Shape: n_tags Impossible transitions at the end of a sequence learnable_transitions: bool Should we learn transition scores to complete the constraints ? with_start_end_transitions: Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores \"\"\" super () . __init__ () num_tags = forbidden_transitions . shape [ 0 ] self . register_buffer ( \"forbidden_transitions\" , forbidden_transitions . bool ()) if start_forbidden_transitions is not None : self . register_buffer ( \"start_forbidden_transitions\" , start_forbidden_transitions . bool () ) else : self . register_buffer ( \"start_forbidden_transitions\" , torch . zeros ( num_tags , dtype = torch . bool ) ) if end_forbidden_transitions is not None : self . register_buffer ( \"end_forbidden_transitions\" , end_forbidden_transitions . bool () ) else : self . register_buffer ( \"end_forbidden_transitions\" , torch . zeros ( num_tags , dtype = torch . bool ) ) if learnable_transitions : self . transitions = torch . nn . Parameter ( torch . zeros_like ( forbidden_transitions , dtype = torch . float ) ) else : self . register_buffer ( \"transitions\" , torch . zeros_like ( forbidden_transitions , dtype = torch . float ), ) if learnable_transitions and with_start_end_transitions : self . start_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float ) ) else : self . register_buffer ( \"start_transitions\" , torch . zeros ( num_tags , dtype = torch . float ) ) if learnable_transitions and with_start_end_transitions : self . end_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float ) ) else : self . register_buffer ( \"end_transitions\" , torch . zeros ( num_tags , dtype = torch . float ) ) def decode ( self , emissions , mask ): \"\"\" Decodes a sequence of tag scores using the Viterbi algorithm Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens Returns ------- torch.LongTensor Backtrack indices (= argmax), ie best tag sequence \"\"\" transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) n_samples , n_tokens = mask . shape emissions [ ... , 1 :][ ~ mask ] = IMPOSSIBLE emissions = emissions . transpose ( 0 , 1 ) # emissions: n_tokens * n_samples * n_tags out = [ emissions [ 0 ] + start_transitions ] backtrack = [] for k in range ( 1 , len ( emissions )): res , indices = max_reduce ( out [ - 1 ], transitions ) backtrack . append ( indices ) out . append ( res + emissions [ k ]) res , indices = max_reduce ( out [ - 1 ], end_transitions . unsqueeze ( - 1 )) path = torch . zeros ( n_samples , n_tokens , dtype = torch . long ) path [:, - 1 ] = indices . squeeze ( - 1 ) path_range = torch . arange ( n_samples , device = path . device ) if len ( backtrack ) > 1 : # Backward max path following for k , b in enumerate ( backtrack [:: - 1 ]): path [:, - k - 2 ] = b [ path_range , path [:, - k - 1 ]] return path def marginal ( self , emissions , mask ): \"\"\" Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF We could use the `propagate` method but this implementation is faster. Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens Returns ------- torch.FloatTensor Shape: ... * n_tokens * n_tags \"\"\" device = emissions . device transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) bi_transitions = torch . stack ([ transitions , transitions . t ()], dim = 0 ) # add start transitions (ie cannot start with ...) emissions [:, 0 ] = emissions [:, 0 ] + start_transitions # add end transitions (ie cannot end with ...): flip the emissions along the # token axis, and add the end transitions # emissions = masked_flip(emissions, mask, dim_x=1) emissions [ torch . arange ( mask . shape [ 0 ], device = device ), mask . long () . sum ( 1 ) - 1 ] = ( emissions [ torch . arange ( mask . shape [ 0 ], device = device ), mask . long () . sum ( 1 ) - 1 , ] + end_transitions ) # stack start -> end emissions (needs to flip the previously flipped emissions), # and end -> start emissions bi_emissions = torch . stack ( [ emissions , masked_flip ( emissions , mask , dim_x = 1 )], 1 ) bi_emissions = bi_emissions . transpose ( 0 , 2 ) out = [ bi_emissions [ 0 ]] for k in range ( 1 , len ( bi_emissions )): res = logsumexp_reduce ( out [ - 1 ], bi_transitions ) out . append ( res + bi_emissions [ k ]) out = torch . stack ( out , dim = 0 ) . transpose ( 0 , 2 ) forward = out [:, 0 ] backward = masked_flip ( out [:, 1 ], mask , dim_x = 1 ) backward_z = backward [:, 0 ] . logsumexp ( - 1 ) return forward + backward - emissions - backward_z [:, None , None ] def forward ( self , emissions , mask , target ): \"\"\" Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss. We could use the `propagate` method but this implementation is faster. Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens target: torch.BoolTensor Shape: ... * n_tokens * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training. Returns ------- torch.FloatTensor Shape: ... The loss \"\"\" transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) bi_emissions = torch . stack ( [ emissions . masked_fill ( ~ target , IMPOSSIBLE ), emissions ], 1 ) . transpose ( 0 , 2 ) # emissions: n_samples * n_tokens * n_tags # bi_emissions: n_tokens * 2 * n_samples * n_tags out = [ bi_emissions [ 0 ] + start_transitions ] for k in range ( 1 , len ( bi_emissions )): res = logsumexp_reduce ( out [ - 1 ], transitions ) out . append ( res + bi_emissions [ k ]) out = torch . stack ( out , dim = 0 ) . transpose ( 0 , 2 ) # n_samples * 2 * n_tokens * n_tags z = ( masked_flip ( out , mask . unsqueeze ( 1 ) . repeat ( 1 , 2 , 1 ), dim_x = 2 )[:, :, 0 ] + end_transitions ) supervised_z = z [:, 0 ] . logsumexp ( - 1 ) unsupervised_z = z [:, 1 ] . logsumexp ( - 1 ) return - ( supervised_z - unsupervised_z )","title":"LinearChainCRF"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.transitions","text":"","title":"transitions"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.start_transitions","text":"","title":"start_transitions"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.end_transitions","text":"","title":"end_transitions"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.__init__","text":"A linear chain CRF in Pytorch PARAMETER DESCRIPTION forbidden_transitions Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1 start_forbidden_transitions Shape: n_tags Impossible transitions at the start of a sequence DEFAULT: None end_forbidden_transitions Shape: n_tags Impossible transitions at the end of a sequence DEFAULT: None learnable_transitions Should we learn transition scores to complete the constraints ? DEFAULT: True with_start_end_transitions Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores DEFAULT: True Source code in edsnlp/models/torch/crf.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , forbidden_transitions , start_forbidden_transitions = None , end_forbidden_transitions = None , learnable_transitions = True , with_start_end_transitions = True , ): \"\"\" A linear chain CRF in Pytorch Parameters ---------- forbidden_transitions: torch.BoolTensor Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1 start_forbidden_transitions: Optional[torch.BoolTensor] Shape: n_tags Impossible transitions at the start of a sequence end_forbidden_transitions Optional[torch.BoolTensor] Shape: n_tags Impossible transitions at the end of a sequence learnable_transitions: bool Should we learn transition scores to complete the constraints ? with_start_end_transitions: Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores \"\"\" super () . __init__ () num_tags = forbidden_transitions . shape [ 0 ] self . register_buffer ( \"forbidden_transitions\" , forbidden_transitions . bool ()) if start_forbidden_transitions is not None : self . register_buffer ( \"start_forbidden_transitions\" , start_forbidden_transitions . bool () ) else : self . register_buffer ( \"start_forbidden_transitions\" , torch . zeros ( num_tags , dtype = torch . bool ) ) if end_forbidden_transitions is not None : self . register_buffer ( \"end_forbidden_transitions\" , end_forbidden_transitions . bool () ) else : self . register_buffer ( \"end_forbidden_transitions\" , torch . zeros ( num_tags , dtype = torch . bool ) ) if learnable_transitions : self . transitions = torch . nn . Parameter ( torch . zeros_like ( forbidden_transitions , dtype = torch . float ) ) else : self . register_buffer ( \"transitions\" , torch . zeros_like ( forbidden_transitions , dtype = torch . float ), ) if learnable_transitions and with_start_end_transitions : self . start_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float ) ) else : self . register_buffer ( \"start_transitions\" , torch . zeros ( num_tags , dtype = torch . float ) ) if learnable_transitions and with_start_end_transitions : self . end_transitions = torch . nn . Parameter ( torch . zeros ( num_tags , dtype = torch . float ) ) else : self . register_buffer ( \"end_transitions\" , torch . zeros ( num_tags , dtype = torch . float ) )","title":"__init__()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.decode","text":"Decodes a sequence of tag scores using the Viterbi algorithm PARAMETER DESCRIPTION emissions Shape: ... * n_tokens * n_tags mask Shape: ... * n_tokens RETURNS DESCRIPTION torch.LongTensor Backtrack indices (= argmax), ie best tag sequence Source code in edsnlp/models/torch/crf.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def decode ( self , emissions , mask ): \"\"\" Decodes a sequence of tag scores using the Viterbi algorithm Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens Returns ------- torch.LongTensor Backtrack indices (= argmax), ie best tag sequence \"\"\" transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) n_samples , n_tokens = mask . shape emissions [ ... , 1 :][ ~ mask ] = IMPOSSIBLE emissions = emissions . transpose ( 0 , 1 ) # emissions: n_tokens * n_samples * n_tags out = [ emissions [ 0 ] + start_transitions ] backtrack = [] for k in range ( 1 , len ( emissions )): res , indices = max_reduce ( out [ - 1 ], transitions ) backtrack . append ( indices ) out . append ( res + emissions [ k ]) res , indices = max_reduce ( out [ - 1 ], end_transitions . unsqueeze ( - 1 )) path = torch . zeros ( n_samples , n_tokens , dtype = torch . long ) path [:, - 1 ] = indices . squeeze ( - 1 ) path_range = torch . arange ( n_samples , device = path . device ) if len ( backtrack ) > 1 : # Backward max path following for k , b in enumerate ( backtrack [:: - 1 ]): path [:, - k - 2 ] = b [ path_range , path [:, - k - 1 ]] return path","title":"decode()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.marginal","text":"Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF We could use the propagate method but this implementation is faster. PARAMETER DESCRIPTION emissions Shape: ... * n_tokens * n_tags mask Shape: ... * n_tokens RETURNS DESCRIPTION torch.FloatTensor Shape: ... * n_tokens * n_tags Source code in edsnlp/models/torch/crf.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def marginal ( self , emissions , mask ): \"\"\" Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF We could use the `propagate` method but this implementation is faster. Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens Returns ------- torch.FloatTensor Shape: ... * n_tokens * n_tags \"\"\" device = emissions . device transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) bi_transitions = torch . stack ([ transitions , transitions . t ()], dim = 0 ) # add start transitions (ie cannot start with ...) emissions [:, 0 ] = emissions [:, 0 ] + start_transitions # add end transitions (ie cannot end with ...): flip the emissions along the # token axis, and add the end transitions # emissions = masked_flip(emissions, mask, dim_x=1) emissions [ torch . arange ( mask . shape [ 0 ], device = device ), mask . long () . sum ( 1 ) - 1 ] = ( emissions [ torch . arange ( mask . shape [ 0 ], device = device ), mask . long () . sum ( 1 ) - 1 , ] + end_transitions ) # stack start -> end emissions (needs to flip the previously flipped emissions), # and end -> start emissions bi_emissions = torch . stack ( [ emissions , masked_flip ( emissions , mask , dim_x = 1 )], 1 ) bi_emissions = bi_emissions . transpose ( 0 , 2 ) out = [ bi_emissions [ 0 ]] for k in range ( 1 , len ( bi_emissions )): res = logsumexp_reduce ( out [ - 1 ], bi_transitions ) out . append ( res + bi_emissions [ k ]) out = torch . stack ( out , dim = 0 ) . transpose ( 0 , 2 ) forward = out [:, 0 ] backward = masked_flip ( out [:, 1 ], mask , dim_x = 1 ) backward_z = backward [:, 0 ] . logsumexp ( - 1 ) return forward + backward - emissions - backward_z [:, None , None ]","title":"marginal()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.forward","text":"Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss. We could use the propagate method but this implementation is faster. PARAMETER DESCRIPTION emissions Shape: ... * n_tokens * n_tags mask Shape: ... * n_tokens target Shape: ... * n_tokens * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training. RETURNS DESCRIPTION torch.FloatTensor Shape: ... The loss Source code in edsnlp/models/torch/crf.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def forward ( self , emissions , mask , target ): \"\"\" Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss. We could use the `propagate` method but this implementation is faster. Parameters ---------- emissions: torch.FloatTensor Shape: ... * n_tokens * n_tags mask: torch.BoolTensor Shape: ... * n_tokens target: torch.BoolTensor Shape: ... * n_tokens * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training. Returns ------- torch.FloatTensor Shape: ... The loss \"\"\" transitions = self . transitions . masked_fill ( self . forbidden_transitions , IMPOSSIBLE ) start_transitions = self . start_transitions . masked_fill ( self . start_forbidden_transitions , IMPOSSIBLE ) end_transitions = self . end_transitions . masked_fill ( self . end_forbidden_transitions , IMPOSSIBLE ) bi_emissions = torch . stack ( [ emissions . masked_fill ( ~ target , IMPOSSIBLE ), emissions ], 1 ) . transpose ( 0 , 2 ) # emissions: n_samples * n_tokens * n_tags # bi_emissions: n_tokens * 2 * n_samples * n_tags out = [ bi_emissions [ 0 ] + start_transitions ] for k in range ( 1 , len ( bi_emissions )): res = logsumexp_reduce ( out [ - 1 ], transitions ) out . append ( res + bi_emissions [ k ]) out = torch . stack ( out , dim = 0 ) . transpose ( 0 , 2 ) # n_samples * 2 * n_tokens * n_tags z = ( masked_flip ( out , mask . unsqueeze ( 1 ) . repeat ( 1 , 2 , 1 ), dim_x = 2 )[:, :, 0 ] + end_transitions ) supervised_z = z [:, 0 ] . logsumexp ( - 1 ) unsupervised_z = z [:, 1 ] . logsumexp ( - 1 ) return - ( supervised_z - unsupervised_z )","title":"forward()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder","text":"Bases: LinearChainCRF Source code in edsnlp/models/torch/crf.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 class MultiLabelBIOULDecoder ( LinearChainCRF ): def __init__ ( self , num_labels , with_start_end_transitions = True , learnable_transitions = True , ): \"\"\" Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme Parameters ---------- num_labels: int with_start_end_transitions: bool learnable_transitions: bool \"\"\" O , I , B , L , U = 0 , 1 , 2 , 3 , 4 num_tags = 1 + num_labels * 4 self . num_tags = num_tags forbidden_transitions = torch . ones ( num_tags , num_tags , dtype = torch . bool ) forbidden_transitions [ O , O ] = 0 # O to O for i in range ( num_labels ): STRIDE = 4 * i for j in range ( num_labels ): STRIDE_J = j * 4 forbidden_transitions [ L + STRIDE , B + STRIDE_J ] = 0 # L-i to B-j forbidden_transitions [ L + STRIDE , U + STRIDE_J ] = 0 # L-i to U-j forbidden_transitions [ U + STRIDE , B + STRIDE_J ] = 0 # U-i to B-j forbidden_transitions [ U + STRIDE , U + STRIDE_J ] = 0 # U-i to U-j forbidden_transitions [ O , B + STRIDE ] = 0 # O to B-i forbidden_transitions [ B + STRIDE , I + STRIDE ] = 0 # B-i to I-i forbidden_transitions [ I + STRIDE , I + STRIDE ] = 0 # I-i to I-i forbidden_transitions [ I + STRIDE , L + STRIDE ] = 0 # I-i to L-i forbidden_transitions [ B + STRIDE , L + STRIDE ] = 0 # B-i to L-i forbidden_transitions [ L + STRIDE , O ] = 0 # L-i to O forbidden_transitions [ O , U + STRIDE ] = 0 # O to U-i forbidden_transitions [ U + STRIDE , O ] = 0 # U-i to O start_forbidden_transitions = torch . zeros ( num_tags , dtype = torch . bool ) if with_start_end_transitions : for i in range ( num_labels ): STRIDE = 4 * i start_forbidden_transitions [ I + STRIDE ] = 1 # forbidden to start by I-i start_forbidden_transitions [ L + STRIDE ] = 1 # forbidden to start by L-i end_forbidden_transitions = torch . zeros ( num_tags , dtype = torch . bool ) if with_start_end_transitions : for i in range ( num_labels ): STRIDE = 4 * i end_forbidden_transitions [ I + STRIDE ] = 1 # forbidden to end by I-i end_forbidden_transitions [ B + STRIDE ] = 1 # forbidden to end by B-i super () . __init__ ( forbidden_transitions , start_forbidden_transitions , end_forbidden_transitions , with_start_end_transitions = with_start_end_transitions , learnable_transitions = learnable_transitions , ) @staticmethod def spans_to_tags ( spans : torch . Tensor , n_samples : int , n_labels : int , n_tokens : int ): \"\"\" Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end) to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens Parameters ---------- spans: torch.Tensor n_samples: int n_labels: int n_tokens: int Returns ------- torch.Tensor \"\"\" device = spans . device cpu = torch . device ( \"cpu\" ) if not len ( spans ): return torch . zeros ( n_samples , n_labels , n_tokens , dtype = torch . long , device = device ) doc_indices , label_indices , begins , ends = spans . cpu () . unbind ( - 1 ) ends = ends - 1 pos = torch . arange ( n_tokens , device = cpu ) b_tags , l_tags , u_tags , i_tags = torch . zeros ( 4 , n_samples , n_labels , n_tokens , dtype = torch . bool , device = cpu ) . unbind ( 0 ) tags = torch . zeros ( n_samples , n_labels , n_tokens , dtype = torch . long , device = cpu ) where_u = begins == ends u_tags [ doc_indices [ where_u ], label_indices [ where_u ], begins [ where_u ]] = True b_tags [ doc_indices [ ~ where_u ], label_indices [ ~ where_u ], begins [ ~ where_u ]] = True l_tags [ doc_indices [ ~ where_u ], label_indices [ ~ where_u ], ends [ ~ where_u ]] = True i_tags . view ( - 1 , n_tokens ) . index_add_ ( 0 , doc_indices * n_labels + label_indices , ( begins . unsqueeze ( - 1 ) < pos ) & ( pos < ends . unsqueeze ( - 1 )), ) tags [ u_tags ] = 4 tags [ b_tags ] = 2 tags [ l_tags ] = 3 tags [ i_tags . bool ()] = 1 return tags . to ( device ) @staticmethod def tags_to_spans ( tags ): \"\"\" Convert a sequence of multiple label BIOUL tags to a sequence of spans Parameters ---------- tags: torch.LongTensor Shape: n_samples * n_labels * n_tokens mask: torch.BoolTensor Shape: n_samples * n_labels * n_tokens Returns ------- torch.LongTensor Shape: n_spans * 4 (doc_idx, label_idx, begin, end) \"\"\" return torch . cat ( [ torch . nonzero (( tags == 4 ) | ( tags == 2 )), torch . nonzero (( tags == 4 ) | ( tags == 3 ))[ ... , [ - 1 ]] + 1 , ], dim =- 1 , )","title":"MultiLabelBIOULDecoder"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder.num_tags","text":"","title":"num_tags"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder.__init__","text":"Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme PARAMETER DESCRIPTION num_labels with_start_end_transitions DEFAULT: True learnable_transitions DEFAULT: True Source code in edsnlp/models/torch/crf.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def __init__ ( self , num_labels , with_start_end_transitions = True , learnable_transitions = True , ): \"\"\" Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme Parameters ---------- num_labels: int with_start_end_transitions: bool learnable_transitions: bool \"\"\" O , I , B , L , U = 0 , 1 , 2 , 3 , 4 num_tags = 1 + num_labels * 4 self . num_tags = num_tags forbidden_transitions = torch . ones ( num_tags , num_tags , dtype = torch . bool ) forbidden_transitions [ O , O ] = 0 # O to O for i in range ( num_labels ): STRIDE = 4 * i for j in range ( num_labels ): STRIDE_J = j * 4 forbidden_transitions [ L + STRIDE , B + STRIDE_J ] = 0 # L-i to B-j forbidden_transitions [ L + STRIDE , U + STRIDE_J ] = 0 # L-i to U-j forbidden_transitions [ U + STRIDE , B + STRIDE_J ] = 0 # U-i to B-j forbidden_transitions [ U + STRIDE , U + STRIDE_J ] = 0 # U-i to U-j forbidden_transitions [ O , B + STRIDE ] = 0 # O to B-i forbidden_transitions [ B + STRIDE , I + STRIDE ] = 0 # B-i to I-i forbidden_transitions [ I + STRIDE , I + STRIDE ] = 0 # I-i to I-i forbidden_transitions [ I + STRIDE , L + STRIDE ] = 0 # I-i to L-i forbidden_transitions [ B + STRIDE , L + STRIDE ] = 0 # B-i to L-i forbidden_transitions [ L + STRIDE , O ] = 0 # L-i to O forbidden_transitions [ O , U + STRIDE ] = 0 # O to U-i forbidden_transitions [ U + STRIDE , O ] = 0 # U-i to O start_forbidden_transitions = torch . zeros ( num_tags , dtype = torch . bool ) if with_start_end_transitions : for i in range ( num_labels ): STRIDE = 4 * i start_forbidden_transitions [ I + STRIDE ] = 1 # forbidden to start by I-i start_forbidden_transitions [ L + STRIDE ] = 1 # forbidden to start by L-i end_forbidden_transitions = torch . zeros ( num_tags , dtype = torch . bool ) if with_start_end_transitions : for i in range ( num_labels ): STRIDE = 4 * i end_forbidden_transitions [ I + STRIDE ] = 1 # forbidden to end by I-i end_forbidden_transitions [ B + STRIDE ] = 1 # forbidden to end by B-i super () . __init__ ( forbidden_transitions , start_forbidden_transitions , end_forbidden_transitions , with_start_end_transitions = with_start_end_transitions , learnable_transitions = learnable_transitions , )","title":"__init__()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder.spans_to_tags","text":"Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end) to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens PARAMETER DESCRIPTION spans TYPE: torch . Tensor n_samples TYPE: int n_labels TYPE: int n_tokens TYPE: int RETURNS DESCRIPTION torch.Tensor Source code in edsnlp/models/torch/crf.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 @staticmethod def spans_to_tags ( spans : torch . Tensor , n_samples : int , n_labels : int , n_tokens : int ): \"\"\" Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end) to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens Parameters ---------- spans: torch.Tensor n_samples: int n_labels: int n_tokens: int Returns ------- torch.Tensor \"\"\" device = spans . device cpu = torch . device ( \"cpu\" ) if not len ( spans ): return torch . zeros ( n_samples , n_labels , n_tokens , dtype = torch . long , device = device ) doc_indices , label_indices , begins , ends = spans . cpu () . unbind ( - 1 ) ends = ends - 1 pos = torch . arange ( n_tokens , device = cpu ) b_tags , l_tags , u_tags , i_tags = torch . zeros ( 4 , n_samples , n_labels , n_tokens , dtype = torch . bool , device = cpu ) . unbind ( 0 ) tags = torch . zeros ( n_samples , n_labels , n_tokens , dtype = torch . long , device = cpu ) where_u = begins == ends u_tags [ doc_indices [ where_u ], label_indices [ where_u ], begins [ where_u ]] = True b_tags [ doc_indices [ ~ where_u ], label_indices [ ~ where_u ], begins [ ~ where_u ]] = True l_tags [ doc_indices [ ~ where_u ], label_indices [ ~ where_u ], ends [ ~ where_u ]] = True i_tags . view ( - 1 , n_tokens ) . index_add_ ( 0 , doc_indices * n_labels + label_indices , ( begins . unsqueeze ( - 1 ) < pos ) & ( pos < ends . unsqueeze ( - 1 )), ) tags [ u_tags ] = 4 tags [ b_tags ] = 2 tags [ l_tags ] = 3 tags [ i_tags . bool ()] = 1 return tags . to ( device )","title":"spans_to_tags()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder.tags_to_spans","text":"Convert a sequence of multiple label BIOUL tags to a sequence of spans PARAMETER DESCRIPTION tags Shape: n_samples * n_labels * n_tokens mask Shape: n_samples * n_labels * n_tokens RETURNS DESCRIPTION torch.LongTensor Shape: n_spans * 4 (doc_idx, label_idx, begin, end) Source code in edsnlp/models/torch/crf.py 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 @staticmethod def tags_to_spans ( tags ): \"\"\" Convert a sequence of multiple label BIOUL tags to a sequence of spans Parameters ---------- tags: torch.LongTensor Shape: n_samples * n_labels * n_tokens mask: torch.BoolTensor Shape: n_samples * n_labels * n_tokens Returns ------- torch.LongTensor Shape: n_spans * 4 (doc_idx, label_idx, begin, end) \"\"\" return torch . cat ( [ torch . nonzero (( tags == 4 ) | ( tags == 2 )), torch . nonzero (( tags == 4 ) | ( tags == 3 ))[ ... , [ - 1 ]] + 1 , ], dim =- 1 , )","title":"tags_to_spans()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.masked_flip","text":"Source code in edsnlp/models/torch/crf.py 6 7 8 9 def masked_flip ( x , mask , dim_x =- 2 ): flipped_x = torch . zeros_like ( x ) flipped_x [ mask ] = x . flip ( dim_x )[ mask . flip ( - 1 )] return flipped_x","title":"masked_flip()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.logsumexp_reduce","text":"Source code in edsnlp/models/torch/crf.py 12 13 14 15 16 17 @torch . jit . script def logsumexp_reduce ( log_A , log_B ): # log_A: 2 * N * M # log_B: 2 * M * O # out: 2 * N * O return ( log_A . unsqueeze ( - 1 ) + log_B . unsqueeze ( - 3 )) . logsumexp ( - 2 )","title":"logsumexp_reduce()"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.max_reduce","text":"Source code in edsnlp/models/torch/crf.py 20 21 22 23 24 25 @torch . jit . script def max_reduce ( log_A , log_B ): # log_A: 2 * N * M # log_B: 2 * M * O # out: 2 * N * O return ( log_A . unsqueeze ( - 1 ) + log_B . unsqueeze ( - 3 )) . max ( - 2 )","title":"max_reduce()"},{"location":"reference/pipelines/","text":"edsnlp.pipelines","title":"`edsnlp.pipelines`"},{"location":"reference/pipelines/#edsnlppipelines","text":"","title":"edsnlp.pipelines"},{"location":"reference/pipelines/base/","text":"edsnlp.pipelines.base BaseComponent Bases: object The BaseComponent adds a set_extensions method, called at the creation of the object. It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset. Source code in edsnlp/pipelines/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseComponent ( object ): \"\"\" The `BaseComponent` adds a `set_extensions` method, called at the creation of the object. It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset. \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : \"\"\" Set `Doc`, `Span` and `Token` extensions. \"\"\" pass def _boundaries ( self , doc : Doc , terminations : Optional [ List [ Span ]] = None ) -> List [ Tuple [ int , int ]]: \"\"\" Create sub sentences based sentences and terminations found in text. Parameters ---------- doc: spaCy Doc object terminations: List of tuples with (match_id, start, end) Returns ------- boundaries: List of tuples with (start, end) of spans \"\"\" if terminations is None : terminations = [] sent_starts = [ sent . start for sent in doc . sents ] termination_starts = [ t . start for t in terminations ] starts = sent_starts + termination_starts + [ len ( doc )] # Remove duplicates starts = list ( set ( starts )) # Sort starts starts . sort () boundaries = [( start , end ) for start , end in zip ( starts [: - 1 ], starts [ 1 :])] return boundaries __init__ ( * args , ** kwargs ) Source code in edsnlp/pipelines/base.py 17 18 19 20 def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_extensions () set_extensions () Set Doc , Span and Token extensions. Source code in edsnlp/pipelines/base.py 22 23 24 25 26 27 @classmethod def set_extensions ( cls ) -> None : \"\"\" Set `Doc`, `Span` and `Token` extensions. \"\"\" pass","title":"base"},{"location":"reference/pipelines/base/#edsnlppipelinesbase","text":"","title":"edsnlp.pipelines.base"},{"location":"reference/pipelines/base/#edsnlp.pipelines.base.BaseComponent","text":"Bases: object The BaseComponent adds a set_extensions method, called at the creation of the object. It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset. Source code in edsnlp/pipelines/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseComponent ( object ): \"\"\" The `BaseComponent` adds a `set_extensions` method, called at the creation of the object. It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset. \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : \"\"\" Set `Doc`, `Span` and `Token` extensions. \"\"\" pass def _boundaries ( self , doc : Doc , terminations : Optional [ List [ Span ]] = None ) -> List [ Tuple [ int , int ]]: \"\"\" Create sub sentences based sentences and terminations found in text. Parameters ---------- doc: spaCy Doc object terminations: List of tuples with (match_id, start, end) Returns ------- boundaries: List of tuples with (start, end) of spans \"\"\" if terminations is None : terminations = [] sent_starts = [ sent . start for sent in doc . sents ] termination_starts = [ t . start for t in terminations ] starts = sent_starts + termination_starts + [ len ( doc )] # Remove duplicates starts = list ( set ( starts )) # Sort starts starts . sort () boundaries = [( start , end ) for start , end in zip ( starts [: - 1 ], starts [ 1 :])] return boundaries","title":"BaseComponent"},{"location":"reference/pipelines/base/#edsnlp.pipelines.base.BaseComponent.__init__","text":"Source code in edsnlp/pipelines/base.py 17 18 19 20 def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/base/#edsnlp.pipelines.base.BaseComponent.set_extensions","text":"Set Doc , Span and Token extensions. Source code in edsnlp/pipelines/base.py 22 23 24 25 26 27 @classmethod def set_extensions ( cls ) -> None : \"\"\" Set `Doc`, `Span` and `Token` extensions. \"\"\" pass","title":"set_extensions()"},{"location":"reference/pipelines/factories/","text":"edsnlp.pipelines.factories","title":"factories"},{"location":"reference/pipelines/factories/#edsnlppipelinesfactories","text":"","title":"edsnlp.pipelines.factories"},{"location":"reference/pipelines/terminations/","text":"edsnlp.pipelines.terminations termination : List [ str ] = [ 'et' , 'bien que' , 'm\u00eame si' , 'mais' , 'or' , 'alors que' , 'sauf' , 'cependant' , 'pourtant' , 'cause de' , 'source de' , 'hormis' , 'car' , 'parce que' , 'pourtant' , 'puisque' , 'ni' , 'en raison de' , 'qui' , 'que' , 'ainsi que' , 'avec' , 'toutefois' , 'en dehors' , 'dans le cadre' , 'du fait' , '.' , ',' , ';' , '...' , '\u2026' , '(' , ')' , '\"' ] module-attribute","title":"terminations"},{"location":"reference/pipelines/terminations/#edsnlppipelinesterminations","text":"","title":"edsnlp.pipelines.terminations"},{"location":"reference/pipelines/terminations/#edsnlp.pipelines.terminations.termination","text":"","title":"termination"},{"location":"reference/pipelines/core/","text":"edsnlp.pipelines.core","title":"`edsnlp.pipelines.core`"},{"location":"reference/pipelines/core/#edsnlppipelinescore","text":"","title":"edsnlp.pipelines.core"},{"location":"reference/pipelines/core/context/","text":"edsnlp.pipelines.core.context","title":"`edsnlp.pipelines.core.context`"},{"location":"reference/pipelines/core/context/#edsnlppipelinescorecontext","text":"","title":"edsnlp.pipelines.core.context"},{"location":"reference/pipelines/core/context/context/","text":"edsnlp.pipelines.core.context.context ContextAdder Bases: BaseComponent Provides a generic context adder component. PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language context The list of extensions to add to the Doc TYPE: List[str] Source code in edsnlp/pipelines/core/context/context.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class ContextAdder ( BaseComponent ): \"\"\" Provides a generic context adder component. Parameters ---------- nlp : Language The spaCy object. context : List[str] The list of extensions to add to the `Doc` \"\"\" def __init__ ( self , nlp : Language , context : List [ str ], ): self . nlp = nlp self . context = context self . set_extensions () def set_extensions ( self ): for col in self . context : if not Doc . has_extension ( col ): Doc . set_extension ( col , default = None ) def __call__ ( self , doc : Doc ) -> Doc : return doc nlp = nlp instance-attribute context = context instance-attribute __init__ ( nlp , context ) Source code in edsnlp/pipelines/core/context/context.py 21 22 23 24 25 26 27 28 29 def __init__ ( self , nlp : Language , context : List [ str ], ): self . nlp = nlp self . context = context self . set_extensions () set_extensions () Source code in edsnlp/pipelines/core/context/context.py 31 32 33 34 def set_extensions ( self ): for col in self . context : if not Doc . has_extension ( col ): Doc . set_extension ( col , default = None ) __call__ ( doc ) Source code in edsnlp/pipelines/core/context/context.py 36 37 def __call__ ( self , doc : Doc ) -> Doc : return doc","title":"context"},{"location":"reference/pipelines/core/context/context/#edsnlppipelinescorecontextcontext","text":"","title":"edsnlp.pipelines.core.context.context"},{"location":"reference/pipelines/core/context/context/#edsnlp.pipelines.core.context.context.ContextAdder","text":"Bases: BaseComponent Provides a generic context adder component. PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language context The list of extensions to add to the Doc TYPE: List[str] Source code in edsnlp/pipelines/core/context/context.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class ContextAdder ( BaseComponent ): \"\"\" Provides a generic context adder component. Parameters ---------- nlp : Language The spaCy object. context : List[str] The list of extensions to add to the `Doc` \"\"\" def __init__ ( self , nlp : Language , context : List [ str ], ): self . nlp = nlp self . context = context self . set_extensions () def set_extensions ( self ): for col in self . context : if not Doc . has_extension ( col ): Doc . set_extension ( col , default = None ) def __call__ ( self , doc : Doc ) -> Doc : return doc","title":"ContextAdder"},{"location":"reference/pipelines/core/context/context/#edsnlp.pipelines.core.context.context.ContextAdder.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/core/context/context/#edsnlp.pipelines.core.context.context.ContextAdder.context","text":"","title":"context"},{"location":"reference/pipelines/core/context/context/#edsnlp.pipelines.core.context.context.ContextAdder.__init__","text":"Source code in edsnlp/pipelines/core/context/context.py 21 22 23 24 25 26 27 28 29 def __init__ ( self , nlp : Language , context : List [ str ], ): self . nlp = nlp self . context = context self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/core/context/context/#edsnlp.pipelines.core.context.context.ContextAdder.set_extensions","text":"Source code in edsnlp/pipelines/core/context/context.py 31 32 33 34 def set_extensions ( self ): for col in self . context : if not Doc . has_extension ( col ): Doc . set_extension ( col , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/core/context/context/#edsnlp.pipelines.core.context.context.ContextAdder.__call__","text":"Source code in edsnlp/pipelines/core/context/context.py 36 37 def __call__ ( self , doc : Doc ) -> Doc : return doc","title":"__call__()"},{"location":"reference/pipelines/core/context/factory/","text":"edsnlp.pipelines.core.context.factory DEFAULT_CONFIG = dict ( context = [ 'note_id' ]) module-attribute create_component ( nlp , name , context ) Source code in edsnlp/pipelines/core/context/factory.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Language . factory ( \"eds.context\" , default_config = DEFAULT_CONFIG , ) def create_component ( nlp : Language , name : str , context : List [ str ], ): return ContextAdder ( nlp , context = context , )","title":"factory"},{"location":"reference/pipelines/core/context/factory/#edsnlppipelinescorecontextfactory","text":"","title":"edsnlp.pipelines.core.context.factory"},{"location":"reference/pipelines/core/context/factory/#edsnlp.pipelines.core.context.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/context/factory/#edsnlp.pipelines.core.context.factory.create_component","text":"Source code in edsnlp/pipelines/core/context/factory.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Language . factory ( \"eds.context\" , default_config = DEFAULT_CONFIG , ) def create_component ( nlp : Language , name : str , context : List [ str ], ): return ContextAdder ( nlp , context = context , )","title":"create_component()"},{"location":"reference/pipelines/core/contextual_matcher/","text":"edsnlp.pipelines.core.contextual_matcher","title":"`edsnlp.pipelines.core.contextual_matcher`"},{"location":"reference/pipelines/core/contextual_matcher/#edsnlppipelinescorecontextual_matcher","text":"","title":"edsnlp.pipelines.core.contextual_matcher"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/","text":"edsnlp.pipelines.core.contextual_matcher.contextual_matcher ContextualMatcher Bases: BaseComponent Allows additional matching in the surrounding context of the main match group, for qualification/filtering. PARAMETER DESCRIPTION nlp spaCy Language object. TYPE: Language name The name of the pipe TYPE: str patterns The configuration dictionary attr Attribute to match on, eg TEXT , NORM , etc. TYPE: str ignore_excluded Whether to skip excluded tokens during matching. TYPE: bool alignment_mode Overwrite alignment mode. TYPE: str Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 class ContextualMatcher ( BaseComponent ): \"\"\" Allows additional matching in the surrounding context of the main match group, for qualification/filtering. Parameters ---------- nlp : Language spaCy `Language` object. name : str The name of the pipe patterns: Union[Dict[str, Any], List[Dict[str, Any]]] The configuration dictionary attr : str Attribute to match on, eg `TEXT`, `NORM`, etc. ignore_excluded : bool Whether to skip excluded tokens during matching. alignment_mode : str Overwrite alignment mode. \"\"\" def __init__ ( self , nlp : Language , name : str , patterns : Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]], assign_as_span : bool , alignment_mode : str , attr : str , regex_flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , include_assigned : bool , ): self . name = name self . nlp = nlp self . attr = attr self . assign_as_span = assign_as_span self . ignore_excluded = ignore_excluded self . alignment_mode = alignment_mode self . regex_flags = regex_flags self . include_assigned = include_assigned # Configuration parsing patterns = models . FullConfig . parse_obj ( patterns ) . __root__ self . patterns = { pattern . source : pattern for pattern in patterns } # Matchers for the anchors self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ) self . regex_matcher = RegexMatcher ( attr = attr , flags = regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = { source : { \"patterns\" : p . terms , } for source , p in self . patterns . items () }, ) self . regex_matcher . build_patterns ( regex = { source : { \"regex\" : p . regex , \"attr\" : p . regex_attr , \"flags\" : p . regex_flags , } for source , p in self . patterns . items () } ) self . exclude_matchers = defaultdict ( list ) # Will contain all the exclusion matchers self . assign_matchers = defaultdict ( list ) # Will contain all the assign matchers # Will contain the reduce mode (for each source and assign matcher) self . reduce_mode = {} # Will contain the name of the assign matcher from which # entity will be replaced (for each source) self . replace_key = {} for source , p in self . patterns . items (): p = p . dict () for exclude in p [ \"exclude\" ]: exclude_matcher = RegexMatcher ( attr = p [ \"regex_attr\" ] or self . attr , flags = p [ \"regex_flags\" ] or self . regex_flags , ignore_excluded = ignore_excluded , alignment_mode = \"expand\" , ) exclude_matcher . build_patterns ( regex = { \"exclude\" : exclude [ \"regex\" ]}) self . exclude_matchers [ source ] . append ( dict ( matcher = exclude_matcher , window = exclude [ \"window\" ], ) ) replace_key = None for assign in p [ \"assign\" ]: assign_matcher = RegexMatcher ( attr = p [ \"regex_attr\" ] or self . attr , flags = p [ \"regex_flags\" ] or self . regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode , span_from_group = True , ) assign_matcher . build_patterns ( regex = { assign [ \"name\" ]: assign [ \"regex\" ]}, ) self . assign_matchers [ source ] . append ( dict ( name = assign [ \"name\" ], matcher = assign_matcher , window = assign [ \"window\" ], replace_entity = assign [ \"replace_entity\" ], reduce_mode = assign [ \"reduce_mode\" ], ) ) if assign [ \"replace_entity\" ]: # We know that there is only one assign name # with `replace_entity==True` # from PyDantic validation replace_key = assign [ \"name\" ] self . replace_key [ source ] = replace_key self . reduce_mode [ source ] = { d [ \"name\" ]: d [ \"reduce_mode\" ] for d in self . assign_matchers [ source ] } self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"assigned\" ): Span . set_extension ( \"assigned\" , default = dict ()) if not Span . has_extension ( \"source\" ): Span . set_extension ( \"source\" , default = None ) def filter_one ( self , span : Span ) -> Span : \"\"\" Filter extracted entity based on the \"exclusion filter\" mentionned in the configuration Parameters ---------- span : Span Span to filter Returns ------- Optional[Span] None if the span was filtered, the span else \"\"\" source = span . label_ to_keep = True for matcher in self . exclude_matchers [ source ]: window = matcher [ \"window\" ] snippet = get_window ( doclike = span , window = window , ) if ( next ( matcher [ \"matcher\" ]( snippet , as_spans = True ), None , ) is not None ): to_keep = False logger . trace ( f \"Entity { span } was filtered out\" ) break if to_keep : return span def assign_one ( self , span : Span ) -> Span : \"\"\" Get additional information in the context of each entity. This function will populate two custom attributes: - `ent._.source` - `ent._.assigned`, a dictionary with all retrieved information Parameters ---------- span : Span Span to enrich Returns ------- Span Span with additional information \"\"\" if span is None : yield from [] return source = span . label_ assigned_dict = models . AssignDict ( reduce_mode = self . reduce_mode [ source ]) replace_key = None for matcher in self . assign_matchers [ source ]: attr = self . patterns [ source ] . regex_attr or matcher [ \"matcher\" ] . default_attr window = matcher [ \"window\" ] replace_entity = matcher [ \"replace_entity\" ] # Boolean snippet = get_window ( doclike = span , window = window , ) # Getting the matches assigned_list = list ( matcher [ \"matcher\" ] . match ( snippet )) assigned_list = [ ( span , span ) if not match . groups () else ( span , create_span ( doclike = snippet , start_char = match . start ( 0 ), end_char = match . end ( 0 ), key = matcher [ \"matcher\" ] . regex [ 0 ][ 0 ], attr = matcher [ \"matcher\" ] . regex [ 0 ][ 2 ], alignment_mode = matcher [ \"matcher\" ] . regex [ 0 ][ 4 ], ignore_excluded = matcher [ \"matcher\" ] . regex [ 0 ][ 3 ], ), ) for ( span , match ) in assigned_list ] # assigned_list now contains tuples with # - the first element being the span extracted from the group # - the second element being the full match if not assigned_list : # No match was found continue for assigned in assigned_list : if assigned is None : continue if replace_entity : replace_key = assigned [ 1 ] . label_ # Using he overrid `__setitem__` method from AssignDict here: assigned_dict [ assigned [ 1 ] . label_ ] = { \"span\" : assigned [ 1 ], # Full span \"value_span\" : assigned [ 0 ], # Span of the group \"value_text\" : get_text ( assigned [ 0 ], attr = attr , ignore_excluded = self . ignore_excluded , ), # Text of the group } logger . trace ( f \"Assign key { matcher [ 'name' ] } matched on entity { span } \" ) if replace_key is None and self . replace_key [ source ] is not None : # There should have been a replacement, but none was found # So we discard the entity yield from [] return # Entity replacement if replace_key is not None : replacables = assigned_dict [ replace_key ][ \"span\" ] kept_ents = ( replacables if isinstance ( replacables , list ) else [ replacables ] ) . copy () if self . include_assigned : # We look for the closest closest = min ( kept_ents , key = lambda e : abs ( e . start - span . start ), ) kept_ents . remove ( closest ) expandables = flatten ( [ a [ \"span\" ] for k , a in assigned_dict . items () if k != replace_key ] ) + [ span , closest ] closest = Span ( span . doc , min ( expandables , key = attrgetter ( \"start\" )) . start , max ( expandables , key = attrgetter ( \"end\" )) . end , span . label_ , ) kept_ents . append ( closest ) kept_ents . sort ( key = attrgetter ( \"start\" )) for replaced in kept_ents : # Propagating attributes from the anchor replaced . _ . source = source replaced . label_ = self . name else : # Entity expansion expandables = flatten ([ a [ \"span\" ] for a in assigned_dict . values ()]) if self . include_assigned and expandables : span = Span ( span . doc , min ( expandables + [ span ], key = attrgetter ( \"start\" )) . start , max ( expandables + [ span ], key = attrgetter ( \"end\" )) . end , span . label_ , ) span . _ . source = source span . label_ = self . name kept_ents = [ span ] key = \"value_span\" if self . assign_as_span else \"value_text\" for idx , e in enumerate ( kept_ents ): e . _ . assigned = { k : v [ key ][ idx ] if (( k == replace_key ) and self . reduce_mode [ source ][ k ] is None ) else v [ key ] for k , v in assigned_dict . items () } yield from kept_ents def process_one ( self , span ): filtered = self . filter_one ( span ) yield from self . assign_one ( filtered ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Process the document, looking for named entities. Parameters ---------- doc : Doc spaCy Doc object Returns ------- List[Span] List of detected spans. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = ( * matches , * regex_matches ) for span in spans : yield from self . process_one ( span ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = list ( self . process ( doc )) doc . spans [ self . name ] = ents ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc name = name instance-attribute nlp = nlp instance-attribute attr = attr instance-attribute assign_as_span = assign_as_span instance-attribute ignore_excluded = ignore_excluded instance-attribute alignment_mode = alignment_mode instance-attribute regex_flags = regex_flags instance-attribute include_assigned = include_assigned instance-attribute patterns = { pattern . source : pattern for pattern in patterns } instance-attribute phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded ) instance-attribute regex_matcher = RegexMatcher ( attr = attr , flags = regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode ) instance-attribute exclude_matchers = defaultdict ( list ) instance-attribute assign_matchers = defaultdict ( list ) instance-attribute reduce_mode = {} instance-attribute replace_key = {} instance-attribute __init__ ( nlp , name , patterns , assign_as_span , alignment_mode , attr , regex_flags , ignore_excluded , include_assigned ) Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def __init__ ( self , nlp : Language , name : str , patterns : Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]], assign_as_span : bool , alignment_mode : str , attr : str , regex_flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , include_assigned : bool , ): self . name = name self . nlp = nlp self . attr = attr self . assign_as_span = assign_as_span self . ignore_excluded = ignore_excluded self . alignment_mode = alignment_mode self . regex_flags = regex_flags self . include_assigned = include_assigned # Configuration parsing patterns = models . FullConfig . parse_obj ( patterns ) . __root__ self . patterns = { pattern . source : pattern for pattern in patterns } # Matchers for the anchors self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ) self . regex_matcher = RegexMatcher ( attr = attr , flags = regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = { source : { \"patterns\" : p . terms , } for source , p in self . patterns . items () }, ) self . regex_matcher . build_patterns ( regex = { source : { \"regex\" : p . regex , \"attr\" : p . regex_attr , \"flags\" : p . regex_flags , } for source , p in self . patterns . items () } ) self . exclude_matchers = defaultdict ( list ) # Will contain all the exclusion matchers self . assign_matchers = defaultdict ( list ) # Will contain all the assign matchers # Will contain the reduce mode (for each source and assign matcher) self . reduce_mode = {} # Will contain the name of the assign matcher from which # entity will be replaced (for each source) self . replace_key = {} for source , p in self . patterns . items (): p = p . dict () for exclude in p [ \"exclude\" ]: exclude_matcher = RegexMatcher ( attr = p [ \"regex_attr\" ] or self . attr , flags = p [ \"regex_flags\" ] or self . regex_flags , ignore_excluded = ignore_excluded , alignment_mode = \"expand\" , ) exclude_matcher . build_patterns ( regex = { \"exclude\" : exclude [ \"regex\" ]}) self . exclude_matchers [ source ] . append ( dict ( matcher = exclude_matcher , window = exclude [ \"window\" ], ) ) replace_key = None for assign in p [ \"assign\" ]: assign_matcher = RegexMatcher ( attr = p [ \"regex_attr\" ] or self . attr , flags = p [ \"regex_flags\" ] or self . regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode , span_from_group = True , ) assign_matcher . build_patterns ( regex = { assign [ \"name\" ]: assign [ \"regex\" ]}, ) self . assign_matchers [ source ] . append ( dict ( name = assign [ \"name\" ], matcher = assign_matcher , window = assign [ \"window\" ], replace_entity = assign [ \"replace_entity\" ], reduce_mode = assign [ \"reduce_mode\" ], ) ) if assign [ \"replace_entity\" ]: # We know that there is only one assign name # with `replace_entity==True` # from PyDantic validation replace_key = assign [ \"name\" ] self . replace_key [ source ] = replace_key self . reduce_mode [ source ] = { d [ \"name\" ]: d [ \"reduce_mode\" ] for d in self . assign_matchers [ source ] } self . set_extensions () set_extensions () Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 187 188 189 190 191 192 @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"assigned\" ): Span . set_extension ( \"assigned\" , default = dict ()) if not Span . has_extension ( \"source\" ): Span . set_extension ( \"source\" , default = None ) filter_one ( span ) Filter extracted entity based on the \"exclusion filter\" mentionned in the configuration PARAMETER DESCRIPTION span Span to filter TYPE: Span RETURNS DESCRIPTION Optional[Span] None if the span was filtered, the span else Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def filter_one ( self , span : Span ) -> Span : \"\"\" Filter extracted entity based on the \"exclusion filter\" mentionned in the configuration Parameters ---------- span : Span Span to filter Returns ------- Optional[Span] None if the span was filtered, the span else \"\"\" source = span . label_ to_keep = True for matcher in self . exclude_matchers [ source ]: window = matcher [ \"window\" ] snippet = get_window ( doclike = span , window = window , ) if ( next ( matcher [ \"matcher\" ]( snippet , as_spans = True ), None , ) is not None ): to_keep = False logger . trace ( f \"Entity { span } was filtered out\" ) break if to_keep : return span assign_one ( span ) Get additional information in the context of each entity. This function will populate two custom attributes: ent._.source ent._.assigned , a dictionary with all retrieved information PARAMETER DESCRIPTION span Span to enrich TYPE: Span RETURNS DESCRIPTION Span Span with additional information Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 def assign_one ( self , span : Span ) -> Span : \"\"\" Get additional information in the context of each entity. This function will populate two custom attributes: - `ent._.source` - `ent._.assigned`, a dictionary with all retrieved information Parameters ---------- span : Span Span to enrich Returns ------- Span Span with additional information \"\"\" if span is None : yield from [] return source = span . label_ assigned_dict = models . AssignDict ( reduce_mode = self . reduce_mode [ source ]) replace_key = None for matcher in self . assign_matchers [ source ]: attr = self . patterns [ source ] . regex_attr or matcher [ \"matcher\" ] . default_attr window = matcher [ \"window\" ] replace_entity = matcher [ \"replace_entity\" ] # Boolean snippet = get_window ( doclike = span , window = window , ) # Getting the matches assigned_list = list ( matcher [ \"matcher\" ] . match ( snippet )) assigned_list = [ ( span , span ) if not match . groups () else ( span , create_span ( doclike = snippet , start_char = match . start ( 0 ), end_char = match . end ( 0 ), key = matcher [ \"matcher\" ] . regex [ 0 ][ 0 ], attr = matcher [ \"matcher\" ] . regex [ 0 ][ 2 ], alignment_mode = matcher [ \"matcher\" ] . regex [ 0 ][ 4 ], ignore_excluded = matcher [ \"matcher\" ] . regex [ 0 ][ 3 ], ), ) for ( span , match ) in assigned_list ] # assigned_list now contains tuples with # - the first element being the span extracted from the group # - the second element being the full match if not assigned_list : # No match was found continue for assigned in assigned_list : if assigned is None : continue if replace_entity : replace_key = assigned [ 1 ] . label_ # Using he overrid `__setitem__` method from AssignDict here: assigned_dict [ assigned [ 1 ] . label_ ] = { \"span\" : assigned [ 1 ], # Full span \"value_span\" : assigned [ 0 ], # Span of the group \"value_text\" : get_text ( assigned [ 0 ], attr = attr , ignore_excluded = self . ignore_excluded , ), # Text of the group } logger . trace ( f \"Assign key { matcher [ 'name' ] } matched on entity { span } \" ) if replace_key is None and self . replace_key [ source ] is not None : # There should have been a replacement, but none was found # So we discard the entity yield from [] return # Entity replacement if replace_key is not None : replacables = assigned_dict [ replace_key ][ \"span\" ] kept_ents = ( replacables if isinstance ( replacables , list ) else [ replacables ] ) . copy () if self . include_assigned : # We look for the closest closest = min ( kept_ents , key = lambda e : abs ( e . start - span . start ), ) kept_ents . remove ( closest ) expandables = flatten ( [ a [ \"span\" ] for k , a in assigned_dict . items () if k != replace_key ] ) + [ span , closest ] closest = Span ( span . doc , min ( expandables , key = attrgetter ( \"start\" )) . start , max ( expandables , key = attrgetter ( \"end\" )) . end , span . label_ , ) kept_ents . append ( closest ) kept_ents . sort ( key = attrgetter ( \"start\" )) for replaced in kept_ents : # Propagating attributes from the anchor replaced . _ . source = source replaced . label_ = self . name else : # Entity expansion expandables = flatten ([ a [ \"span\" ] for a in assigned_dict . values ()]) if self . include_assigned and expandables : span = Span ( span . doc , min ( expandables + [ span ], key = attrgetter ( \"start\" )) . start , max ( expandables + [ span ], key = attrgetter ( \"end\" )) . end , span . label_ , ) span . _ . source = source span . label_ = self . name kept_ents = [ span ] key = \"value_span\" if self . assign_as_span else \"value_text\" for idx , e in enumerate ( kept_ents ): e . _ . assigned = { k : v [ key ][ idx ] if (( k == replace_key ) and self . reduce_mode [ source ][ k ] is None ) else v [ key ] for k , v in assigned_dict . items () } yield from kept_ents process_one ( span ) Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 386 387 388 def process_one ( self , span ): filtered = self . filter_one ( span ) yield from self . assign_one ( filtered ) process ( doc ) Process the document, looking for named entities. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION List[Span] List of detected spans. Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Process the document, looking for named entities. Parameters ---------- doc : Doc spaCy Doc object Returns ------- List[Span] List of detected spans. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = ( * matches , * regex_matches ) for span in spans : yield from self . process_one ( span ) __call__ ( doc ) Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = list ( self . process ( doc )) doc . spans [ self . name ] = ents ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc get_window ( doclike , window ) Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 21 22 23 24 25 26 27 28 29 30 31 @lru_cache ( 64 ) def get_window ( doclike : Union [ Doc , Span ], window : Tuple [ int , int ], ): return doclike . doc [ max ( doclike . start + window [ 0 ], doclike . sent . start ) : min ( doclike . end + window [ 1 ], doclike . sent . end ) ]","title":"contextual_matcher"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlppipelinescorecontextual_matchercontextual_matcher","text":"","title":"edsnlp.pipelines.core.contextual_matcher.contextual_matcher"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher","text":"Bases: BaseComponent Allows additional matching in the surrounding context of the main match group, for qualification/filtering. PARAMETER DESCRIPTION nlp spaCy Language object. TYPE: Language name The name of the pipe TYPE: str patterns The configuration dictionary attr Attribute to match on, eg TEXT , NORM , etc. TYPE: str ignore_excluded Whether to skip excluded tokens during matching. TYPE: bool alignment_mode Overwrite alignment mode. TYPE: str Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 class ContextualMatcher ( BaseComponent ): \"\"\" Allows additional matching in the surrounding context of the main match group, for qualification/filtering. Parameters ---------- nlp : Language spaCy `Language` object. name : str The name of the pipe patterns: Union[Dict[str, Any], List[Dict[str, Any]]] The configuration dictionary attr : str Attribute to match on, eg `TEXT`, `NORM`, etc. ignore_excluded : bool Whether to skip excluded tokens during matching. alignment_mode : str Overwrite alignment mode. \"\"\" def __init__ ( self , nlp : Language , name : str , patterns : Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]], assign_as_span : bool , alignment_mode : str , attr : str , regex_flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , include_assigned : bool , ): self . name = name self . nlp = nlp self . attr = attr self . assign_as_span = assign_as_span self . ignore_excluded = ignore_excluded self . alignment_mode = alignment_mode self . regex_flags = regex_flags self . include_assigned = include_assigned # Configuration parsing patterns = models . FullConfig . parse_obj ( patterns ) . __root__ self . patterns = { pattern . source : pattern for pattern in patterns } # Matchers for the anchors self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ) self . regex_matcher = RegexMatcher ( attr = attr , flags = regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = { source : { \"patterns\" : p . terms , } for source , p in self . patterns . items () }, ) self . regex_matcher . build_patterns ( regex = { source : { \"regex\" : p . regex , \"attr\" : p . regex_attr , \"flags\" : p . regex_flags , } for source , p in self . patterns . items () } ) self . exclude_matchers = defaultdict ( list ) # Will contain all the exclusion matchers self . assign_matchers = defaultdict ( list ) # Will contain all the assign matchers # Will contain the reduce mode (for each source and assign matcher) self . reduce_mode = {} # Will contain the name of the assign matcher from which # entity will be replaced (for each source) self . replace_key = {} for source , p in self . patterns . items (): p = p . dict () for exclude in p [ \"exclude\" ]: exclude_matcher = RegexMatcher ( attr = p [ \"regex_attr\" ] or self . attr , flags = p [ \"regex_flags\" ] or self . regex_flags , ignore_excluded = ignore_excluded , alignment_mode = \"expand\" , ) exclude_matcher . build_patterns ( regex = { \"exclude\" : exclude [ \"regex\" ]}) self . exclude_matchers [ source ] . append ( dict ( matcher = exclude_matcher , window = exclude [ \"window\" ], ) ) replace_key = None for assign in p [ \"assign\" ]: assign_matcher = RegexMatcher ( attr = p [ \"regex_attr\" ] or self . attr , flags = p [ \"regex_flags\" ] or self . regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode , span_from_group = True , ) assign_matcher . build_patterns ( regex = { assign [ \"name\" ]: assign [ \"regex\" ]}, ) self . assign_matchers [ source ] . append ( dict ( name = assign [ \"name\" ], matcher = assign_matcher , window = assign [ \"window\" ], replace_entity = assign [ \"replace_entity\" ], reduce_mode = assign [ \"reduce_mode\" ], ) ) if assign [ \"replace_entity\" ]: # We know that there is only one assign name # with `replace_entity==True` # from PyDantic validation replace_key = assign [ \"name\" ] self . replace_key [ source ] = replace_key self . reduce_mode [ source ] = { d [ \"name\" ]: d [ \"reduce_mode\" ] for d in self . assign_matchers [ source ] } self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"assigned\" ): Span . set_extension ( \"assigned\" , default = dict ()) if not Span . has_extension ( \"source\" ): Span . set_extension ( \"source\" , default = None ) def filter_one ( self , span : Span ) -> Span : \"\"\" Filter extracted entity based on the \"exclusion filter\" mentionned in the configuration Parameters ---------- span : Span Span to filter Returns ------- Optional[Span] None if the span was filtered, the span else \"\"\" source = span . label_ to_keep = True for matcher in self . exclude_matchers [ source ]: window = matcher [ \"window\" ] snippet = get_window ( doclike = span , window = window , ) if ( next ( matcher [ \"matcher\" ]( snippet , as_spans = True ), None , ) is not None ): to_keep = False logger . trace ( f \"Entity { span } was filtered out\" ) break if to_keep : return span def assign_one ( self , span : Span ) -> Span : \"\"\" Get additional information in the context of each entity. This function will populate two custom attributes: - `ent._.source` - `ent._.assigned`, a dictionary with all retrieved information Parameters ---------- span : Span Span to enrich Returns ------- Span Span with additional information \"\"\" if span is None : yield from [] return source = span . label_ assigned_dict = models . AssignDict ( reduce_mode = self . reduce_mode [ source ]) replace_key = None for matcher in self . assign_matchers [ source ]: attr = self . patterns [ source ] . regex_attr or matcher [ \"matcher\" ] . default_attr window = matcher [ \"window\" ] replace_entity = matcher [ \"replace_entity\" ] # Boolean snippet = get_window ( doclike = span , window = window , ) # Getting the matches assigned_list = list ( matcher [ \"matcher\" ] . match ( snippet )) assigned_list = [ ( span , span ) if not match . groups () else ( span , create_span ( doclike = snippet , start_char = match . start ( 0 ), end_char = match . end ( 0 ), key = matcher [ \"matcher\" ] . regex [ 0 ][ 0 ], attr = matcher [ \"matcher\" ] . regex [ 0 ][ 2 ], alignment_mode = matcher [ \"matcher\" ] . regex [ 0 ][ 4 ], ignore_excluded = matcher [ \"matcher\" ] . regex [ 0 ][ 3 ], ), ) for ( span , match ) in assigned_list ] # assigned_list now contains tuples with # - the first element being the span extracted from the group # - the second element being the full match if not assigned_list : # No match was found continue for assigned in assigned_list : if assigned is None : continue if replace_entity : replace_key = assigned [ 1 ] . label_ # Using he overrid `__setitem__` method from AssignDict here: assigned_dict [ assigned [ 1 ] . label_ ] = { \"span\" : assigned [ 1 ], # Full span \"value_span\" : assigned [ 0 ], # Span of the group \"value_text\" : get_text ( assigned [ 0 ], attr = attr , ignore_excluded = self . ignore_excluded , ), # Text of the group } logger . trace ( f \"Assign key { matcher [ 'name' ] } matched on entity { span } \" ) if replace_key is None and self . replace_key [ source ] is not None : # There should have been a replacement, but none was found # So we discard the entity yield from [] return # Entity replacement if replace_key is not None : replacables = assigned_dict [ replace_key ][ \"span\" ] kept_ents = ( replacables if isinstance ( replacables , list ) else [ replacables ] ) . copy () if self . include_assigned : # We look for the closest closest = min ( kept_ents , key = lambda e : abs ( e . start - span . start ), ) kept_ents . remove ( closest ) expandables = flatten ( [ a [ \"span\" ] for k , a in assigned_dict . items () if k != replace_key ] ) + [ span , closest ] closest = Span ( span . doc , min ( expandables , key = attrgetter ( \"start\" )) . start , max ( expandables , key = attrgetter ( \"end\" )) . end , span . label_ , ) kept_ents . append ( closest ) kept_ents . sort ( key = attrgetter ( \"start\" )) for replaced in kept_ents : # Propagating attributes from the anchor replaced . _ . source = source replaced . label_ = self . name else : # Entity expansion expandables = flatten ([ a [ \"span\" ] for a in assigned_dict . values ()]) if self . include_assigned and expandables : span = Span ( span . doc , min ( expandables + [ span ], key = attrgetter ( \"start\" )) . start , max ( expandables + [ span ], key = attrgetter ( \"end\" )) . end , span . label_ , ) span . _ . source = source span . label_ = self . name kept_ents = [ span ] key = \"value_span\" if self . assign_as_span else \"value_text\" for idx , e in enumerate ( kept_ents ): e . _ . assigned = { k : v [ key ][ idx ] if (( k == replace_key ) and self . reduce_mode [ source ][ k ] is None ) else v [ key ] for k , v in assigned_dict . items () } yield from kept_ents def process_one ( self , span ): filtered = self . filter_one ( span ) yield from self . assign_one ( filtered ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Process the document, looking for named entities. Parameters ---------- doc : Doc spaCy Doc object Returns ------- List[Span] List of detected spans. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = ( * matches , * regex_matches ) for span in spans : yield from self . process_one ( span ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = list ( self . process ( doc )) doc . spans [ self . name ] = ents ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"ContextualMatcher"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.name","text":"","title":"name"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.attr","text":"","title":"attr"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.assign_as_span","text":"","title":"assign_as_span"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.ignore_excluded","text":"","title":"ignore_excluded"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.alignment_mode","text":"","title":"alignment_mode"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.regex_flags","text":"","title":"regex_flags"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.include_assigned","text":"","title":"include_assigned"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.patterns","text":"","title":"patterns"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.phrase_matcher","text":"","title":"phrase_matcher"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.regex_matcher","text":"","title":"regex_matcher"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.exclude_matchers","text":"","title":"exclude_matchers"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.assign_matchers","text":"","title":"assign_matchers"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.reduce_mode","text":"","title":"reduce_mode"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.replace_key","text":"","title":"replace_key"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.__init__","text":"Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def __init__ ( self , nlp : Language , name : str , patterns : Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]], assign_as_span : bool , alignment_mode : str , attr : str , regex_flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , include_assigned : bool , ): self . name = name self . nlp = nlp self . attr = attr self . assign_as_span = assign_as_span self . ignore_excluded = ignore_excluded self . alignment_mode = alignment_mode self . regex_flags = regex_flags self . include_assigned = include_assigned # Configuration parsing patterns = models . FullConfig . parse_obj ( patterns ) . __root__ self . patterns = { pattern . source : pattern for pattern in patterns } # Matchers for the anchors self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ) self . regex_matcher = RegexMatcher ( attr = attr , flags = regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = { source : { \"patterns\" : p . terms , } for source , p in self . patterns . items () }, ) self . regex_matcher . build_patterns ( regex = { source : { \"regex\" : p . regex , \"attr\" : p . regex_attr , \"flags\" : p . regex_flags , } for source , p in self . patterns . items () } ) self . exclude_matchers = defaultdict ( list ) # Will contain all the exclusion matchers self . assign_matchers = defaultdict ( list ) # Will contain all the assign matchers # Will contain the reduce mode (for each source and assign matcher) self . reduce_mode = {} # Will contain the name of the assign matcher from which # entity will be replaced (for each source) self . replace_key = {} for source , p in self . patterns . items (): p = p . dict () for exclude in p [ \"exclude\" ]: exclude_matcher = RegexMatcher ( attr = p [ \"regex_attr\" ] or self . attr , flags = p [ \"regex_flags\" ] or self . regex_flags , ignore_excluded = ignore_excluded , alignment_mode = \"expand\" , ) exclude_matcher . build_patterns ( regex = { \"exclude\" : exclude [ \"regex\" ]}) self . exclude_matchers [ source ] . append ( dict ( matcher = exclude_matcher , window = exclude [ \"window\" ], ) ) replace_key = None for assign in p [ \"assign\" ]: assign_matcher = RegexMatcher ( attr = p [ \"regex_attr\" ] or self . attr , flags = p [ \"regex_flags\" ] or self . regex_flags , ignore_excluded = ignore_excluded , alignment_mode = alignment_mode , span_from_group = True , ) assign_matcher . build_patterns ( regex = { assign [ \"name\" ]: assign [ \"regex\" ]}, ) self . assign_matchers [ source ] . append ( dict ( name = assign [ \"name\" ], matcher = assign_matcher , window = assign [ \"window\" ], replace_entity = assign [ \"replace_entity\" ], reduce_mode = assign [ \"reduce_mode\" ], ) ) if assign [ \"replace_entity\" ]: # We know that there is only one assign name # with `replace_entity==True` # from PyDantic validation replace_key = assign [ \"name\" ] self . replace_key [ source ] = replace_key self . reduce_mode [ source ] = { d [ \"name\" ]: d [ \"reduce_mode\" ] for d in self . assign_matchers [ source ] } self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.set_extensions","text":"Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 187 188 189 190 191 192 @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"assigned\" ): Span . set_extension ( \"assigned\" , default = dict ()) if not Span . has_extension ( \"source\" ): Span . set_extension ( \"source\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.filter_one","text":"Filter extracted entity based on the \"exclusion filter\" mentionned in the configuration PARAMETER DESCRIPTION span Span to filter TYPE: Span RETURNS DESCRIPTION Optional[Span] None if the span was filtered, the span else Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def filter_one ( self , span : Span ) -> Span : \"\"\" Filter extracted entity based on the \"exclusion filter\" mentionned in the configuration Parameters ---------- span : Span Span to filter Returns ------- Optional[Span] None if the span was filtered, the span else \"\"\" source = span . label_ to_keep = True for matcher in self . exclude_matchers [ source ]: window = matcher [ \"window\" ] snippet = get_window ( doclike = span , window = window , ) if ( next ( matcher [ \"matcher\" ]( snippet , as_spans = True ), None , ) is not None ): to_keep = False logger . trace ( f \"Entity { span } was filtered out\" ) break if to_keep : return span","title":"filter_one()"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.assign_one","text":"Get additional information in the context of each entity. This function will populate two custom attributes: ent._.source ent._.assigned , a dictionary with all retrieved information PARAMETER DESCRIPTION span Span to enrich TYPE: Span RETURNS DESCRIPTION Span Span with additional information Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 def assign_one ( self , span : Span ) -> Span : \"\"\" Get additional information in the context of each entity. This function will populate two custom attributes: - `ent._.source` - `ent._.assigned`, a dictionary with all retrieved information Parameters ---------- span : Span Span to enrich Returns ------- Span Span with additional information \"\"\" if span is None : yield from [] return source = span . label_ assigned_dict = models . AssignDict ( reduce_mode = self . reduce_mode [ source ]) replace_key = None for matcher in self . assign_matchers [ source ]: attr = self . patterns [ source ] . regex_attr or matcher [ \"matcher\" ] . default_attr window = matcher [ \"window\" ] replace_entity = matcher [ \"replace_entity\" ] # Boolean snippet = get_window ( doclike = span , window = window , ) # Getting the matches assigned_list = list ( matcher [ \"matcher\" ] . match ( snippet )) assigned_list = [ ( span , span ) if not match . groups () else ( span , create_span ( doclike = snippet , start_char = match . start ( 0 ), end_char = match . end ( 0 ), key = matcher [ \"matcher\" ] . regex [ 0 ][ 0 ], attr = matcher [ \"matcher\" ] . regex [ 0 ][ 2 ], alignment_mode = matcher [ \"matcher\" ] . regex [ 0 ][ 4 ], ignore_excluded = matcher [ \"matcher\" ] . regex [ 0 ][ 3 ], ), ) for ( span , match ) in assigned_list ] # assigned_list now contains tuples with # - the first element being the span extracted from the group # - the second element being the full match if not assigned_list : # No match was found continue for assigned in assigned_list : if assigned is None : continue if replace_entity : replace_key = assigned [ 1 ] . label_ # Using he overrid `__setitem__` method from AssignDict here: assigned_dict [ assigned [ 1 ] . label_ ] = { \"span\" : assigned [ 1 ], # Full span \"value_span\" : assigned [ 0 ], # Span of the group \"value_text\" : get_text ( assigned [ 0 ], attr = attr , ignore_excluded = self . ignore_excluded , ), # Text of the group } logger . trace ( f \"Assign key { matcher [ 'name' ] } matched on entity { span } \" ) if replace_key is None and self . replace_key [ source ] is not None : # There should have been a replacement, but none was found # So we discard the entity yield from [] return # Entity replacement if replace_key is not None : replacables = assigned_dict [ replace_key ][ \"span\" ] kept_ents = ( replacables if isinstance ( replacables , list ) else [ replacables ] ) . copy () if self . include_assigned : # We look for the closest closest = min ( kept_ents , key = lambda e : abs ( e . start - span . start ), ) kept_ents . remove ( closest ) expandables = flatten ( [ a [ \"span\" ] for k , a in assigned_dict . items () if k != replace_key ] ) + [ span , closest ] closest = Span ( span . doc , min ( expandables , key = attrgetter ( \"start\" )) . start , max ( expandables , key = attrgetter ( \"end\" )) . end , span . label_ , ) kept_ents . append ( closest ) kept_ents . sort ( key = attrgetter ( \"start\" )) for replaced in kept_ents : # Propagating attributes from the anchor replaced . _ . source = source replaced . label_ = self . name else : # Entity expansion expandables = flatten ([ a [ \"span\" ] for a in assigned_dict . values ()]) if self . include_assigned and expandables : span = Span ( span . doc , min ( expandables + [ span ], key = attrgetter ( \"start\" )) . start , max ( expandables + [ span ], key = attrgetter ( \"end\" )) . end , span . label_ , ) span . _ . source = source span . label_ = self . name kept_ents = [ span ] key = \"value_span\" if self . assign_as_span else \"value_text\" for idx , e in enumerate ( kept_ents ): e . _ . assigned = { k : v [ key ][ idx ] if (( k == replace_key ) and self . reduce_mode [ source ][ k ] is None ) else v [ key ] for k , v in assigned_dict . items () } yield from kept_ents","title":"assign_one()"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.process_one","text":"Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 386 387 388 def process_one ( self , span ): filtered = self . filter_one ( span ) yield from self . assign_one ( filtered )","title":"process_one()"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.process","text":"Process the document, looking for named entities. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION List[Span] List of detected spans. Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Process the document, looking for named entities. Parameters ---------- doc : Doc spaCy Doc object Returns ------- List[Span] List of detected spans. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = ( * matches , * regex_matches ) for span in spans : yield from self . process_one ( span )","title":"process()"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.__call__","text":"Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = list ( self . process ( doc )) doc . spans [ self . name ] = ents ents , discarded = filter_spans ( list ( doc . ents ) + ents , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.get_window","text":"Source code in edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py 21 22 23 24 25 26 27 28 29 30 31 @lru_cache ( 64 ) def get_window ( doclike : Union [ Doc , Span ], window : Tuple [ int , int ], ): return doclike . doc [ max ( doclike . start + window [ 0 ], doclike . sent . start ) : min ( doclike . end + window [ 1 ], doclike . sent . end ) ]","title":"get_window()"},{"location":"reference/pipelines/core/contextual_matcher/factory/","text":"edsnlp.pipelines.core.contextual_matcher.factory DEFAULT_CONFIG = dict ( attr = 'NORM' , ignore_excluded = False , regex_flags = 0 , alignment_mode = 'expand' , assign_as_span = False , include_assigned = False ) module-attribute create_component ( nlp , name , patterns , assign_as_span , alignment_mode , attr , ignore_excluded , regex_flags , include_assigned ) Source code in edsnlp/pipelines/core/contextual_matcher/factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @deprecated_factory ( \"contextual-matcher\" , \"eds.contextual-matcher\" , default_config = DEFAULT_CONFIG ) @Language . factory ( \"eds.contextual-matcher\" , default_config = DEFAULT_CONFIG ) def create_component ( nlp : Language , name : str , patterns : Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]], assign_as_span : bool , alignment_mode : str , attr : str , ignore_excluded : bool , regex_flags : Union [ re . RegexFlag , int ], include_assigned : bool , ): return ContextualMatcher ( nlp , name , patterns , assign_as_span , alignment_mode , attr = attr , ignore_excluded = ignore_excluded , regex_flags = regex_flags , include_assigned = include_assigned , )","title":"factory"},{"location":"reference/pipelines/core/contextual_matcher/factory/#edsnlppipelinescorecontextual_matcherfactory","text":"","title":"edsnlp.pipelines.core.contextual_matcher.factory"},{"location":"reference/pipelines/core/contextual_matcher/factory/#edsnlp.pipelines.core.contextual_matcher.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/contextual_matcher/factory/#edsnlp.pipelines.core.contextual_matcher.factory.create_component","text":"Source code in edsnlp/pipelines/core/contextual_matcher/factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @deprecated_factory ( \"contextual-matcher\" , \"eds.contextual-matcher\" , default_config = DEFAULT_CONFIG ) @Language . factory ( \"eds.contextual-matcher\" , default_config = DEFAULT_CONFIG ) def create_component ( nlp : Language , name : str , patterns : Union [ Dict [ str , Any ], List [ Dict [ str , Any ]]], assign_as_span : bool , alignment_mode : str , attr : str , ignore_excluded : bool , regex_flags : Union [ re . RegexFlag , int ], include_assigned : bool , ): return ContextualMatcher ( nlp , name , patterns , assign_as_span , alignment_mode , attr = attr , ignore_excluded = ignore_excluded , regex_flags = regex_flags , include_assigned = include_assigned , )","title":"create_component()"},{"location":"reference/pipelines/core/contextual_matcher/models/","text":"edsnlp.pipelines.core.contextual_matcher.models Flags = Union [ re . RegexFlag , int ] module-attribute Window = Union [ Tuple [ int , int ], List [ int ], int ] module-attribute AssignDict Bases: dict Custom dictionary that overrides the setitem method depending on the reduce_mode Source code in edsnlp/pipelines/core/contextual_matcher/models.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class AssignDict ( dict ): \"\"\" Custom dictionary that overrides the __setitem__ method depending on the reduce_mode \"\"\" def __init__ ( self , reduce_mode : dict ): super () . __init__ () self . reduce_mode = reduce_mode self . _setitem_ = self . __setitem_options__ () def __missing__ ( self , key ): return ( { \"span\" : [], \"value_span\" : [], \"value_text\" : [], } if self . reduce_mode [ key ] is None else {} ) def __setitem__ ( self , key , value ): self . _setitem_ [ self . reduce_mode [ key ]]( key , value ) def __setitem_options__ ( self ): def keep_list ( key , value ): old_values = self . __getitem__ ( key ) value [ \"span\" ] = old_values [ \"span\" ] + [ value [ \"span\" ]] value [ \"value_span\" ] = old_values [ \"value_span\" ] + [ value [ \"value_span\" ]] value [ \"value_text\" ] = old_values [ \"value_text\" ] + [ value [ \"value_text\" ]] dict . __setitem__ ( self , key , value ) def keep_first ( key , value ): old_values = self . __getitem__ ( key ) if ( old_values . get ( \"span\" ) is None or value [ \"span\" ] . start <= old_values [ \"span\" ] . start ): dict . __setitem__ ( self , key , value ) def keep_last ( key , value ): old_values = self . __getitem__ ( key ) if ( old_values . get ( \"span\" ) is None or value [ \"span\" ] . start >= old_values [ \"span\" ] . start ): dict . __setitem__ ( self , key , value ) return { None : keep_list , \"keep_first\" : keep_first , \"keep_last\" : keep_last , } reduce_mode = reduce_mode instance-attribute __init__ ( reduce_mode ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 38 39 40 41 def __init__ ( self , reduce_mode : dict ): super () . __init__ () self . reduce_mode = reduce_mode self . _setitem_ = self . __setitem_options__ () __missing__ ( key ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 43 44 45 46 47 48 49 50 51 52 def __missing__ ( self , key ): return ( { \"span\" : [], \"value_span\" : [], \"value_text\" : [], } if self . reduce_mode [ key ] is None else {} ) __setitem__ ( key , value ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 54 55 def __setitem__ ( self , key , value ): self . _setitem_ [ self . reduce_mode [ key ]]( key , value ) __setitem_options__ () Source code in edsnlp/pipelines/core/contextual_matcher/models.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __setitem_options__ ( self ): def keep_list ( key , value ): old_values = self . __getitem__ ( key ) value [ \"span\" ] = old_values [ \"span\" ] + [ value [ \"span\" ]] value [ \"value_span\" ] = old_values [ \"value_span\" ] + [ value [ \"value_span\" ]] value [ \"value_text\" ] = old_values [ \"value_text\" ] + [ value [ \"value_text\" ]] dict . __setitem__ ( self , key , value ) def keep_first ( key , value ): old_values = self . __getitem__ ( key ) if ( old_values . get ( \"span\" ) is None or value [ \"span\" ] . start <= old_values [ \"span\" ] . start ): dict . __setitem__ ( self , key , value ) def keep_last ( key , value ): old_values = self . __getitem__ ( key ) if ( old_values . get ( \"span\" ) is None or value [ \"span\" ] . start >= old_values [ \"span\" ] . start ): dict . __setitem__ ( self , key , value ) return { None : keep_list , \"keep_first\" : keep_first , \"keep_last\" : keep_last , } SingleExcludeModel Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 89 90 91 92 93 94 95 96 97 98 99 100 class SingleExcludeModel ( BaseModel ): regex : ListOrStr = [] window : Window regex_flags : Optional [ Flags ] = None @validator ( \"regex\" ) def exclude_regex_validation ( cls , v ): if type ( v ) == str : v = [ v ] return v _normalize_window = validator ( \"window\" , allow_reuse = True )( normalize_window ) regex : ListOrStr = [] class-attribute window : Window = None class-attribute regex_flags : Optional [ Flags ] = None class-attribute exclude_regex_validation ( v ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 94 95 96 97 98 @validator ( \"regex\" ) def exclude_regex_validation ( cls , v ): if type ( v ) == str : v = [ v ] return v ExcludeModel Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 103 104 105 106 107 108 109 110 111 112 113 114 class ExcludeModel ( BaseModel , extra = Extra . forbid ): __root__ : Union [ List [ SingleExcludeModel ], SingleExcludeModel , ] @validator ( \"__root__\" , pre = True ) def item_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v __root__ : Union [ List [ SingleExcludeModel ], SingleExcludeModel ] = None class-attribute item_to_list ( v ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 110 111 112 113 114 @validator ( \"__root__\" , pre = True ) def item_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v SingleAssignModel Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class SingleAssignModel ( BaseModel ): name : str regex : str window : Window regex_flags : Optional [ Flags ] = None replace_entity : bool = False reduce_mode : Optional [ str ] = None @validator ( \"regex\" ) def check_single_regex_group ( cls , pat ): compiled_pat = re . compile ( pat ) n_groups = compiled_pat . groups assert n_groups == 1 , ( \"The pattern {pat} should have only one capturing group, not {n_groups} \" ) . format ( pat = pat , n_groups = n_groups , ) return pat _normalize_window = validator ( \"window\" , allow_reuse = True )( normalize_window ) name : str = None class-attribute regex : str = None class-attribute window : Window = None class-attribute regex_flags : Optional [ Flags ] = None class-attribute replace_entity : bool = False class-attribute reduce_mode : Optional [ str ] = None class-attribute check_single_regex_group ( pat ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 125 126 127 128 129 130 131 132 133 134 135 136 @validator ( \"regex\" ) def check_single_regex_group ( cls , pat ): compiled_pat = re . compile ( pat ) n_groups = compiled_pat . groups assert n_groups == 1 , ( \"The pattern {pat} should have only one capturing group, not {n_groups} \" ) . format ( pat = pat , n_groups = n_groups , ) return pat AssignModel Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class AssignModel ( BaseModel , extra = Extra . forbid ): __root__ : Union [ List [ SingleAssignModel ], SingleAssignModel , ] @validator ( \"__root__\" , pre = True ) def item_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v @validator ( \"__root__\" ) def name_uniqueness ( cls , v ): names = [ item . name for item in v ] assert len ( names ) == len ( set ( names )), \"Each `name` field should be unique\" return v @validator ( \"__root__\" ) def replace_uniqueness ( cls , v ): replace = [ item for item in v if item . replace_entity ] assert ( len ( replace ) <= 1 ), \"Only 1 assign element can be set with `replace_entity=True`\" return v __root__ : Union [ List [ SingleAssignModel ], SingleAssignModel ] = None class-attribute item_to_list ( v ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 148 149 150 151 152 @validator ( \"__root__\" , pre = True ) def item_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v name_uniqueness ( v ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 154 155 156 157 158 @validator ( \"__root__\" ) def name_uniqueness ( cls , v ): names = [ item . name for item in v ] assert len ( names ) == len ( set ( names )), \"Each `name` field should be unique\" return v replace_uniqueness ( v ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 160 161 162 163 164 165 166 @validator ( \"__root__\" ) def replace_uniqueness ( cls , v ): replace = [ item for item in v if item . replace_entity ] assert ( len ( replace ) <= 1 ), \"Only 1 assign element can be set with `replace_entity=True`\" return v SingleConfig Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 169 170 171 172 173 174 175 176 177 class SingleConfig ( BaseModel , extra = Extra . forbid ): source : str terms : ListOrStr = [] regex : ListOrStr = [] regex_attr : Optional [ str ] = None regex_flags : Union [ re . RegexFlag , int ] = None exclude : Optional [ ExcludeModel ] = [] assign : Optional [ AssignModel ] = [] source : str = None class-attribute terms : ListOrStr = [] class-attribute regex : ListOrStr = [] class-attribute regex_attr : Optional [ str ] = None class-attribute regex_flags : Union [ re . RegexFlag , int ] = None class-attribute exclude : Optional [ ExcludeModel ] = [] class-attribute assign : Optional [ AssignModel ] = [] class-attribute FullConfig Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class FullConfig ( BaseModel , extra = Extra . forbid ): __root__ : Union [ List [ SingleConfig ], SingleConfig , ] @validator ( \"__root__\" , pre = True ) def pattern_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v @validator ( \"__root__\" , pre = True ) def source_uniqueness ( cls , v ): sources = [ item [ \"source\" ] for item in v ] assert len ( sources ) == len ( set ( sources )), \"Each `source` field should be unique\" return v __root__ : Union [ List [ SingleConfig ], SingleConfig ] = None class-attribute pattern_to_list ( v ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 187 188 189 190 191 @validator ( \"__root__\" , pre = True ) def pattern_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v source_uniqueness ( v ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 193 194 195 196 197 @validator ( \"__root__\" , pre = True ) def source_uniqueness ( cls , v ): sources = [ item [ \"source\" ] for item in v ] assert len ( sources ) == len ( set ( sources )), \"Each `source` field should be unique\" return v normalize_window ( cls , v ) Source code in edsnlp/pipelines/core/contextual_matcher/models.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def normalize_window ( cls , v ): if isinstance ( v , list ): assert ( len ( v ) == 2 ), \"`window` should be a tuple/list of two integer, or a single integer\" v = tuple ( v ) if isinstance ( v , int ): assert v != 0 , \"The provided `window` should not be 0\" if v < 0 : return ( v , 0 ) if v > 0 : return ( 0 , v ) assert v [ 0 ] < v [ 1 ], \"The provided `window` should contain at least 1 token\" return v","title":"models"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlppipelinescorecontextual_matchermodels","text":"","title":"edsnlp.pipelines.core.contextual_matcher.models"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.Flags","text":"","title":"Flags"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.Window","text":"","title":"Window"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignDict","text":"Bases: dict Custom dictionary that overrides the setitem method depending on the reduce_mode Source code in edsnlp/pipelines/core/contextual_matcher/models.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class AssignDict ( dict ): \"\"\" Custom dictionary that overrides the __setitem__ method depending on the reduce_mode \"\"\" def __init__ ( self , reduce_mode : dict ): super () . __init__ () self . reduce_mode = reduce_mode self . _setitem_ = self . __setitem_options__ () def __missing__ ( self , key ): return ( { \"span\" : [], \"value_span\" : [], \"value_text\" : [], } if self . reduce_mode [ key ] is None else {} ) def __setitem__ ( self , key , value ): self . _setitem_ [ self . reduce_mode [ key ]]( key , value ) def __setitem_options__ ( self ): def keep_list ( key , value ): old_values = self . __getitem__ ( key ) value [ \"span\" ] = old_values [ \"span\" ] + [ value [ \"span\" ]] value [ \"value_span\" ] = old_values [ \"value_span\" ] + [ value [ \"value_span\" ]] value [ \"value_text\" ] = old_values [ \"value_text\" ] + [ value [ \"value_text\" ]] dict . __setitem__ ( self , key , value ) def keep_first ( key , value ): old_values = self . __getitem__ ( key ) if ( old_values . get ( \"span\" ) is None or value [ \"span\" ] . start <= old_values [ \"span\" ] . start ): dict . __setitem__ ( self , key , value ) def keep_last ( key , value ): old_values = self . __getitem__ ( key ) if ( old_values . get ( \"span\" ) is None or value [ \"span\" ] . start >= old_values [ \"span\" ] . start ): dict . __setitem__ ( self , key , value ) return { None : keep_list , \"keep_first\" : keep_first , \"keep_last\" : keep_last , }","title":"AssignDict"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignDict.reduce_mode","text":"","title":"reduce_mode"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignDict.__init__","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 38 39 40 41 def __init__ ( self , reduce_mode : dict ): super () . __init__ () self . reduce_mode = reduce_mode self . _setitem_ = self . __setitem_options__ ()","title":"__init__()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignDict.__missing__","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 43 44 45 46 47 48 49 50 51 52 def __missing__ ( self , key ): return ( { \"span\" : [], \"value_span\" : [], \"value_text\" : [], } if self . reduce_mode [ key ] is None else {} )","title":"__missing__()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignDict.__setitem__","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 54 55 def __setitem__ ( self , key , value ): self . _setitem_ [ self . reduce_mode [ key ]]( key , value )","title":"__setitem__()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignDict.__setitem_options__","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __setitem_options__ ( self ): def keep_list ( key , value ): old_values = self . __getitem__ ( key ) value [ \"span\" ] = old_values [ \"span\" ] + [ value [ \"span\" ]] value [ \"value_span\" ] = old_values [ \"value_span\" ] + [ value [ \"value_span\" ]] value [ \"value_text\" ] = old_values [ \"value_text\" ] + [ value [ \"value_text\" ]] dict . __setitem__ ( self , key , value ) def keep_first ( key , value ): old_values = self . __getitem__ ( key ) if ( old_values . get ( \"span\" ) is None or value [ \"span\" ] . start <= old_values [ \"span\" ] . start ): dict . __setitem__ ( self , key , value ) def keep_last ( key , value ): old_values = self . __getitem__ ( key ) if ( old_values . get ( \"span\" ) is None or value [ \"span\" ] . start >= old_values [ \"span\" ] . start ): dict . __setitem__ ( self , key , value ) return { None : keep_list , \"keep_first\" : keep_first , \"keep_last\" : keep_last , }","title":"__setitem_options__()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleExcludeModel","text":"Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 89 90 91 92 93 94 95 96 97 98 99 100 class SingleExcludeModel ( BaseModel ): regex : ListOrStr = [] window : Window regex_flags : Optional [ Flags ] = None @validator ( \"regex\" ) def exclude_regex_validation ( cls , v ): if type ( v ) == str : v = [ v ] return v _normalize_window = validator ( \"window\" , allow_reuse = True )( normalize_window )","title":"SingleExcludeModel"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleExcludeModel.regex","text":"","title":"regex"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleExcludeModel.window","text":"","title":"window"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleExcludeModel.regex_flags","text":"","title":"regex_flags"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleExcludeModel.exclude_regex_validation","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 94 95 96 97 98 @validator ( \"regex\" ) def exclude_regex_validation ( cls , v ): if type ( v ) == str : v = [ v ] return v","title":"exclude_regex_validation()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.ExcludeModel","text":"Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 103 104 105 106 107 108 109 110 111 112 113 114 class ExcludeModel ( BaseModel , extra = Extra . forbid ): __root__ : Union [ List [ SingleExcludeModel ], SingleExcludeModel , ] @validator ( \"__root__\" , pre = True ) def item_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v","title":"ExcludeModel"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.ExcludeModel.__root__","text":"","title":"__root__"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.ExcludeModel.item_to_list","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 110 111 112 113 114 @validator ( \"__root__\" , pre = True ) def item_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v","title":"item_to_list()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleAssignModel","text":"Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class SingleAssignModel ( BaseModel ): name : str regex : str window : Window regex_flags : Optional [ Flags ] = None replace_entity : bool = False reduce_mode : Optional [ str ] = None @validator ( \"regex\" ) def check_single_regex_group ( cls , pat ): compiled_pat = re . compile ( pat ) n_groups = compiled_pat . groups assert n_groups == 1 , ( \"The pattern {pat} should have only one capturing group, not {n_groups} \" ) . format ( pat = pat , n_groups = n_groups , ) return pat _normalize_window = validator ( \"window\" , allow_reuse = True )( normalize_window )","title":"SingleAssignModel"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleAssignModel.name","text":"","title":"name"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleAssignModel.regex","text":"","title":"regex"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleAssignModel.window","text":"","title":"window"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleAssignModel.regex_flags","text":"","title":"regex_flags"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleAssignModel.replace_entity","text":"","title":"replace_entity"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleAssignModel.reduce_mode","text":"","title":"reduce_mode"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleAssignModel.check_single_regex_group","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 125 126 127 128 129 130 131 132 133 134 135 136 @validator ( \"regex\" ) def check_single_regex_group ( cls , pat ): compiled_pat = re . compile ( pat ) n_groups = compiled_pat . groups assert n_groups == 1 , ( \"The pattern {pat} should have only one capturing group, not {n_groups} \" ) . format ( pat = pat , n_groups = n_groups , ) return pat","title":"check_single_regex_group()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignModel","text":"Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class AssignModel ( BaseModel , extra = Extra . forbid ): __root__ : Union [ List [ SingleAssignModel ], SingleAssignModel , ] @validator ( \"__root__\" , pre = True ) def item_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v @validator ( \"__root__\" ) def name_uniqueness ( cls , v ): names = [ item . name for item in v ] assert len ( names ) == len ( set ( names )), \"Each `name` field should be unique\" return v @validator ( \"__root__\" ) def replace_uniqueness ( cls , v ): replace = [ item for item in v if item . replace_entity ] assert ( len ( replace ) <= 1 ), \"Only 1 assign element can be set with `replace_entity=True`\" return v","title":"AssignModel"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignModel.__root__","text":"","title":"__root__"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignModel.item_to_list","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 148 149 150 151 152 @validator ( \"__root__\" , pre = True ) def item_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v","title":"item_to_list()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignModel.name_uniqueness","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 154 155 156 157 158 @validator ( \"__root__\" ) def name_uniqueness ( cls , v ): names = [ item . name for item in v ] assert len ( names ) == len ( set ( names )), \"Each `name` field should be unique\" return v","title":"name_uniqueness()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignModel.replace_uniqueness","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 160 161 162 163 164 165 166 @validator ( \"__root__\" ) def replace_uniqueness ( cls , v ): replace = [ item for item in v if item . replace_entity ] assert ( len ( replace ) <= 1 ), \"Only 1 assign element can be set with `replace_entity=True`\" return v","title":"replace_uniqueness()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleConfig","text":"Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 169 170 171 172 173 174 175 176 177 class SingleConfig ( BaseModel , extra = Extra . forbid ): source : str terms : ListOrStr = [] regex : ListOrStr = [] regex_attr : Optional [ str ] = None regex_flags : Union [ re . RegexFlag , int ] = None exclude : Optional [ ExcludeModel ] = [] assign : Optional [ AssignModel ] = []","title":"SingleConfig"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleConfig.source","text":"","title":"source"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleConfig.terms","text":"","title":"terms"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleConfig.regex","text":"","title":"regex"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleConfig.regex_attr","text":"","title":"regex_attr"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleConfig.regex_flags","text":"","title":"regex_flags"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleConfig.exclude","text":"","title":"exclude"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.SingleConfig.assign","text":"","title":"assign"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.FullConfig","text":"Bases: BaseModel Source code in edsnlp/pipelines/core/contextual_matcher/models.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 class FullConfig ( BaseModel , extra = Extra . forbid ): __root__ : Union [ List [ SingleConfig ], SingleConfig , ] @validator ( \"__root__\" , pre = True ) def pattern_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v @validator ( \"__root__\" , pre = True ) def source_uniqueness ( cls , v ): sources = [ item [ \"source\" ] for item in v ] assert len ( sources ) == len ( set ( sources )), \"Each `source` field should be unique\" return v","title":"FullConfig"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.FullConfig.__root__","text":"","title":"__root__"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.FullConfig.pattern_to_list","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 187 188 189 190 191 @validator ( \"__root__\" , pre = True ) def pattern_to_list ( cls , v ): if not isinstance ( v , list ): return [ v ] return v","title":"pattern_to_list()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.FullConfig.source_uniqueness","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 193 194 195 196 197 @validator ( \"__root__\" , pre = True ) def source_uniqueness ( cls , v ): sources = [ item [ \"source\" ] for item in v ] assert len ( sources ) == len ( set ( sources )), \"Each `source` field should be unique\" return v","title":"source_uniqueness()"},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.normalize_window","text":"Source code in edsnlp/pipelines/core/contextual_matcher/models.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def normalize_window ( cls , v ): if isinstance ( v , list ): assert ( len ( v ) == 2 ), \"`window` should be a tuple/list of two integer, or a single integer\" v = tuple ( v ) if isinstance ( v , int ): assert v != 0 , \"The provided `window` should not be 0\" if v < 0 : return ( v , 0 ) if v > 0 : return ( 0 , v ) assert v [ 0 ] < v [ 1 ], \"The provided `window` should contain at least 1 token\" return v","title":"normalize_window()"},{"location":"reference/pipelines/core/endlines/","text":"edsnlp.pipelines.core.endlines","title":"`edsnlp.pipelines.core.endlines`"},{"location":"reference/pipelines/core/endlines/#edsnlppipelinescoreendlines","text":"","title":"edsnlp.pipelines.core.endlines"},{"location":"reference/pipelines/core/endlines/endlines/","text":"edsnlp.pipelines.core.endlines.endlines EndLines Bases: GenericMatcher spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF). The pipeline will add the extension end_line to spans and tokens. The end_line attribute is a boolean or None , set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. If no classification has been done over that token, it will remain None . PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language end_lines_model : Optional[Union[str, EndLinesModel]], by default None path to trained model. If None, it will use a default model Source code in edsnlp/pipelines/core/endlines/endlines.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 class EndLines ( GenericMatcher ): \"\"\" spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF). The pipeline will add the extension `end_line` to spans and tokens. The `end_line` attribute is a boolean or `None`, set to `True` if the pipeline predicts that the new line is an end line character. Otherwise, it is set to `False` if the new line is classified as a space. If no classification has been done over that token, it will remain `None`. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. end_lines_model : Optional[Union[str, EndLinesModel]], by default None path to trained model. If None, it will use a default model \"\"\" def __init__ ( self , nlp : Language , end_lines_model : Optional [ Union [ str , EndLinesModel ]], ** kwargs , ): super () . __init__ ( nlp , terms = None , attr = \"TEXT\" , regex = dict ( new_line = r \"\\n+\" , ), ignore_excluded = False , ** kwargs , ) self . _read_model ( end_lines_model ) def _read_model ( self , end_lines_model : Optional [ Union [ str , EndLinesModel ]]): \"\"\" Parameters ---------- end_lines_model : Optional[Union[str, EndLinesModel]] Raises ------ TypeError \"\"\" if end_lines_model is None : path = build_path ( __file__ , \"base_model.pkl\" ) with open ( path , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == str : with open ( end_lines_model , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == EndLinesModel : self . model = end_lines_model else : raise TypeError ( \"type(`end_lines_model`) should be one of {None, str, EndLinesModel}\" ) @classmethod def _spacy_compute_a3a4 ( cls , token : Token ) -> str : \"\"\"Function to compute A3 and A4 Parameters ---------- token : Token Returns ------- str \"\"\" if token . is_upper : return \"UPPER\" elif token . shape_ . startswith ( \"Xx\" ): return \"S_UPPER\" elif token . shape_ . startswith ( \"x\" ): return \"LOWER\" elif ( token . is_digit ) & ( ( token . doc [ max ( token . i - 1 , 0 )] . is_punct ) | ( token . doc [ min ( token . i + 1 , len ( token . doc ) - 1 )] . is_punct ) ): return \"ENUMERATION\" elif token . is_digit : return \"DIGIT\" elif ( token . is_punct ) & ( token . text in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"STRONG_PUNCT\" elif ( token . is_punct ) & ( token . text not in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"SOFT_PUNCT\" else : return \"OTHER\" @classmethod def _compute_length ( cls , doc : Doc , start : int , end : int ) -> int : \"\"\"Compute length without spaces Parameters ---------- doc : Doc start : int end : int Returns ------- int \"\"\" length = 0 for t in doc [ start : end ]: length += len ( t . text ) return length def _get_df ( self , doc : Doc , new_lines : List [ Span ]) -> pd . DataFrame : \"\"\"Get a pandas DataFrame to call the classifier Parameters ---------- doc : Doc new_lines : List[Span] Returns ------- pd.DataFrame \"\"\" data = [] for i , span in enumerate ( new_lines ): start = span . start end = span . end max_index = len ( doc ) - 1 a1_token = doc [ max ( start - 1 , 0 )] a2_token = doc [ min ( start + 1 , max_index )] a1 = a1_token . orth a2 = a2_token . orth a3 = self . _spacy_compute_a3a4 ( a1_token ) a4 = self . _spacy_compute_a3a4 ( a2_token ) blank_line = \" \\n\\n \" in span . text if i > 0 : start_previous = new_lines [ i - 1 ] . start + 1 else : start_previous = 0 length = self . _compute_length ( doc , start = start_previous , end = start ) # It's ok cause i count the total length from the previous up to this one data_dict = dict ( span_start = start , span_end = end , A1 = a1 , A2 = a2 , A3 = a3 , A4 = a4 , BLANK_LINE = blank_line , length = length , ) data . append ( data_dict ) df = pd . DataFrame ( data ) mu = df [ \"length\" ] . mean () sigma = df [ \"length\" ] . std () if np . isnan ( sigma ): sigma = 1 cv = sigma / mu df [ \"B1\" ] = ( df [ \"length\" ] - mu ) / sigma df [ \"B2\" ] = cv return df def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Predict for each new line if it's an end of line or a space. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, with each new line annotated \"\"\" matches = self . process ( doc ) new_lines = get_spans ( matches , \"new_line\" ) if len ( new_lines ) > 0 : df = self . _get_df ( doc = doc , new_lines = new_lines ) df = self . model . predict ( df ) for span , prediction in zip ( new_lines , df . PREDICTED_END_LINE ): for t in span : t . tag_ = \"ENDLINE\" if prediction else \"EXCLUDED\" if prediction : t . _ . excluded = True return doc __init__ ( nlp , end_lines_model , ** kwargs ) Source code in edsnlp/pipelines/core/endlines/endlines.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , nlp : Language , end_lines_model : Optional [ Union [ str , EndLinesModel ]], ** kwargs , ): super () . __init__ ( nlp , terms = None , attr = \"TEXT\" , regex = dict ( new_line = r \"\\n+\" , ), ignore_excluded = False , ** kwargs , ) self . _read_model ( end_lines_model ) __call__ ( doc ) Predict for each new line if it's an end of line or a space. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/core/endlines/endlines.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Predict for each new line if it's an end of line or a space. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, with each new line annotated \"\"\" matches = self . process ( doc ) new_lines = get_spans ( matches , \"new_line\" ) if len ( new_lines ) > 0 : df = self . _get_df ( doc = doc , new_lines = new_lines ) df = self . model . predict ( df ) for span , prediction in zip ( new_lines , df . PREDICTED_END_LINE ): for t in span : t . tag_ = \"ENDLINE\" if prediction else \"EXCLUDED\" if prediction : t . _ . excluded = True return doc","title":"endlines"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlppipelinescoreendlinesendlines","text":"","title":"edsnlp.pipelines.core.endlines.endlines"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines","text":"Bases: GenericMatcher spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF). The pipeline will add the extension end_line to spans and tokens. The end_line attribute is a boolean or None , set to True if the pipeline predicts that the new line is an end line character. Otherwise, it is set to False if the new line is classified as a space. If no classification has been done over that token, it will remain None . PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language end_lines_model : Optional[Union[str, EndLinesModel]], by default None path to trained model. If None, it will use a default model Source code in edsnlp/pipelines/core/endlines/endlines.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 class EndLines ( GenericMatcher ): \"\"\" spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF). The pipeline will add the extension `end_line` to spans and tokens. The `end_line` attribute is a boolean or `None`, set to `True` if the pipeline predicts that the new line is an end line character. Otherwise, it is set to `False` if the new line is classified as a space. If no classification has been done over that token, it will remain `None`. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. end_lines_model : Optional[Union[str, EndLinesModel]], by default None path to trained model. If None, it will use a default model \"\"\" def __init__ ( self , nlp : Language , end_lines_model : Optional [ Union [ str , EndLinesModel ]], ** kwargs , ): super () . __init__ ( nlp , terms = None , attr = \"TEXT\" , regex = dict ( new_line = r \"\\n+\" , ), ignore_excluded = False , ** kwargs , ) self . _read_model ( end_lines_model ) def _read_model ( self , end_lines_model : Optional [ Union [ str , EndLinesModel ]]): \"\"\" Parameters ---------- end_lines_model : Optional[Union[str, EndLinesModel]] Raises ------ TypeError \"\"\" if end_lines_model is None : path = build_path ( __file__ , \"base_model.pkl\" ) with open ( path , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == str : with open ( end_lines_model , \"rb\" ) as inp : self . model = pickle . load ( inp ) elif type ( end_lines_model ) == EndLinesModel : self . model = end_lines_model else : raise TypeError ( \"type(`end_lines_model`) should be one of {None, str, EndLinesModel}\" ) @classmethod def _spacy_compute_a3a4 ( cls , token : Token ) -> str : \"\"\"Function to compute A3 and A4 Parameters ---------- token : Token Returns ------- str \"\"\" if token . is_upper : return \"UPPER\" elif token . shape_ . startswith ( \"Xx\" ): return \"S_UPPER\" elif token . shape_ . startswith ( \"x\" ): return \"LOWER\" elif ( token . is_digit ) & ( ( token . doc [ max ( token . i - 1 , 0 )] . is_punct ) | ( token . doc [ min ( token . i + 1 , len ( token . doc ) - 1 )] . is_punct ) ): return \"ENUMERATION\" elif token . is_digit : return \"DIGIT\" elif ( token . is_punct ) & ( token . text in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"STRONG_PUNCT\" elif ( token . is_punct ) & ( token . text not in [ \".\" , \";\" , \"..\" , \"...\" ]): return \"SOFT_PUNCT\" else : return \"OTHER\" @classmethod def _compute_length ( cls , doc : Doc , start : int , end : int ) -> int : \"\"\"Compute length without spaces Parameters ---------- doc : Doc start : int end : int Returns ------- int \"\"\" length = 0 for t in doc [ start : end ]: length += len ( t . text ) return length def _get_df ( self , doc : Doc , new_lines : List [ Span ]) -> pd . DataFrame : \"\"\"Get a pandas DataFrame to call the classifier Parameters ---------- doc : Doc new_lines : List[Span] Returns ------- pd.DataFrame \"\"\" data = [] for i , span in enumerate ( new_lines ): start = span . start end = span . end max_index = len ( doc ) - 1 a1_token = doc [ max ( start - 1 , 0 )] a2_token = doc [ min ( start + 1 , max_index )] a1 = a1_token . orth a2 = a2_token . orth a3 = self . _spacy_compute_a3a4 ( a1_token ) a4 = self . _spacy_compute_a3a4 ( a2_token ) blank_line = \" \\n\\n \" in span . text if i > 0 : start_previous = new_lines [ i - 1 ] . start + 1 else : start_previous = 0 length = self . _compute_length ( doc , start = start_previous , end = start ) # It's ok cause i count the total length from the previous up to this one data_dict = dict ( span_start = start , span_end = end , A1 = a1 , A2 = a2 , A3 = a3 , A4 = a4 , BLANK_LINE = blank_line , length = length , ) data . append ( data_dict ) df = pd . DataFrame ( data ) mu = df [ \"length\" ] . mean () sigma = df [ \"length\" ] . std () if np . isnan ( sigma ): sigma = 1 cv = sigma / mu df [ \"B1\" ] = ( df [ \"length\" ] - mu ) / sigma df [ \"B2\" ] = cv return df def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Predict for each new line if it's an end of line or a space. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, with each new line annotated \"\"\" matches = self . process ( doc ) new_lines = get_spans ( matches , \"new_line\" ) if len ( new_lines ) > 0 : df = self . _get_df ( doc = doc , new_lines = new_lines ) df = self . model . predict ( df ) for span , prediction in zip ( new_lines , df . PREDICTED_END_LINE ): for t in span : t . tag_ = \"ENDLINE\" if prediction else \"EXCLUDED\" if prediction : t . _ . excluded = True return doc","title":"EndLines"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines.__init__","text":"Source code in edsnlp/pipelines/core/endlines/endlines.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , nlp : Language , end_lines_model : Optional [ Union [ str , EndLinesModel ]], ** kwargs , ): super () . __init__ ( nlp , terms = None , attr = \"TEXT\" , regex = dict ( new_line = r \"\\n+\" , ), ignore_excluded = False , ** kwargs , ) self . _read_model ( end_lines_model )","title":"__init__()"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines.__call__","text":"Predict for each new line if it's an end of line or a space. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/core/endlines/endlines.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Predict for each new line if it's an end of line or a space. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, with each new line annotated \"\"\" matches = self . process ( doc ) new_lines = get_spans ( matches , \"new_line\" ) if len ( new_lines ) > 0 : df = self . _get_df ( doc = doc , new_lines = new_lines ) df = self . model . predict ( df ) for span , prediction in zip ( new_lines , df . PREDICTED_END_LINE ): for t in span : t . tag_ = \"ENDLINE\" if prediction else \"EXCLUDED\" if prediction : t . _ . excluded = True return doc","title":"__call__()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/","text":"edsnlp.pipelines.core.endlines.endlinesmodel EndLinesModel Model to classify if an end line is a real one or it should be a space. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 class EndLinesModel : \"\"\"Model to classify if an end line is a real one or it should be a space. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. \"\"\" def __init__ ( self , nlp : Language ): self . nlp = nlp def _preprocess_data ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\" Parameters ---------- corpus : Iterable[Doc] Corpus of documents Returns ------- pd.DataFrame Preprocessed data \"\"\" # Extract the vocabulary string_store = self . nlp . vocab . strings # Iterate in the corpus and construct a dataframe train_data_list = [] for i , doc in enumerate ( corpus ): train_data_list . append ( self . _get_attributes ( doc , i )) df = pd . concat ( train_data_list ) df . reset_index ( inplace = True , drop = False ) df . rename ( columns = { \"ORTH\" : \"A1\" , \"index\" : \"original_token_index\" }, inplace = True ) # Retrieve string representation of token_id and shape df [ \"TEXT\" ] = df . A1 . apply ( self . _get_string , string_store = string_store ) df [ \"SHAPE_\" ] = df . SHAPE . apply ( self . _get_string , string_store = string_store ) # Convert new lines as an attribute instead of a row df = self . _convert_line_to_attribute ( df , expr = \" \\n \" , col = \"END_LINE\" ) df = self . _convert_line_to_attribute ( df , expr = \" \\n\\n \" , col = \"BLANK_LINE\" ) df = df . loc [ ~ ( df . END_LINE | df . BLANK_LINE )] df = df . drop ( columns = \"END_LINE\" ) df = df . drop ( columns = \"BLANK_LINE\" ) df . rename ( columns = { \"TEMP_END_LINE\" : \"END_LINE\" , \"TEMP_BLANK_LINE\" : \"BLANK_LINE\" }, inplace = True , ) # Construct A2 by shifting df = self . _shift_col ( df , \"A1\" , \"A2\" , direction = \"backward\" ) # Compute A3 and A4 df = self . _compute_a3 ( df ) df = self . _shift_col ( df , \"A3\" , \"A4\" , direction = \"backward\" ) # SPACE is the class to predict. Set 1 if not an END_LINE df [ \"SPACE\" ] = np . logical_not ( df [ \"END_LINE\" ]) . astype ( \"int\" ) df [[ \"END_LINE\" , \"BLANK_LINE\" ]] = df [[ \"END_LINE\" , \"BLANK_LINE\" ]] . fillna ( True , inplace = False ) # Assign a sentence id to each token df = df . groupby ( \"DOC_ID\" ) . apply ( self . _retrieve_lines ) df [ \"SENTENCE_ID\" ] = df [ \"SENTENCE_ID\" ] . astype ( \"int\" ) # Compute B1 and B2 df = self . _compute_B ( df ) # Drop Tokens without info (last token of doc) df . dropna ( subset = [ \"A1\" , \"A2\" , \"A3\" , \"A4\" ], inplace = True ) # Export the vocabularies to be able to use the model with another corpus voc_a3a4 = self . _create_vocabulary ( df . A3_ . cat . categories ) voc_B2 = self . _create_vocabulary ( df . cv_bin . cat . categories ) voc_B1 = self . _create_vocabulary ( df . l_norm_bin . cat . categories ) vocabulary = { \"A3A4\" : voc_a3a4 , \"B1\" : voc_B1 , \"B2\" : voc_B2 } self . vocabulary = vocabulary return df def fit_and_predict ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\"Fit the model and predict for the training data Parameters ---------- corpus : Iterable[Doc] An iterable of Documents Returns ------- pd.DataFrame one line by end_line prediction \"\"\" # Preprocess data to have a pd DF df = self . _preprocess_data ( corpus ) # Train and predict M1 self . _fit_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , df . SPACE ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] # Force Blank lines to 0 df . loc [ df . BLANK_LINE , \"M1\" ] = 0 # Train and predict M2 df_endlines = df . loc [ df . END_LINE ] self . _fit_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 , label = df_endlines . M1 ) outputs_M2 = self . _predict_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 ) df . loc [ df . END_LINE , \"M2\" ] = outputs_M2 [ \"predictions\" ] df . loc [ df . END_LINE , \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df = df . loc [ df . END_LINE ] df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M2\" , \"M1M2\" ]] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df def predict ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Use the model for inference The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Parameters ---------- df : pd.DataFrame The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Returns ------- pd.DataFrame The result is added to the column `PREDICTED_END_LINE` \"\"\" df = self . _convert_raw_data_to_codes ( df ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . _A3 , df . _A4 ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] outputs_M2 = self . _predict_M2 ( B1 = df . _B1 , B2 = df . _B2 ) df [ \"M2\" ] = outputs_M2 [ \"predictions\" ] df [ \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M1M2\" , ], ] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df def save ( self , path = \"base_model.pkl\" ): \"\"\"Save a pickle of the model. It could be read by the pipeline later. Parameters ---------- path : str, optional path to file .pkl, by default `base_model.pkl` \"\"\" with open ( path , \"wb\" ) as outp : del self . nlp pickle . dump ( self , outp , pickle . HIGHEST_PROTOCOL ) def _convert_A ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame col : str column to translate Returns ------- pd.DataFrame \"\"\" cat_type_A = CategoricalDtype ( categories = self . vocabulary [ \"A3A4\" ] . keys (), ordered = True ) new_col = \"_\" + col df [ new_col ] = df [ col ] . astype ( cat_type_A ) df [ new_col ] = df [ new_col ] . cat . codes # Ensure that not known values are coded as OTHER df . loc [ ~ df [ col ] . isin ( self . vocabulary [ \"A3A4\" ] . keys ()), new_col ] = self . vocabulary [ \"A3A4\" ][ \"OTHER\" ] return df def _convert_B ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame [description] col : str column to translate Returns ------- pd.DataFrame [description] \"\"\" # Translate B1 index_B = pd . IntervalIndex ( list ( self . vocabulary [ col ] . keys ())) new_col = \"_\" + col df [ new_col ] = pd . cut ( df [ col ], index_B ) df [ new_col ] = df [ new_col ] . cat . codes df . loc [ df [ col ] >= index_B . right . max (), new_col ] = max ( self . vocabulary [ col ] . values () ) df . loc [ df [ col ] <= index_B . left . min (), new_col ] = min ( self . vocabulary [ col ] . values () ) return df def _convert_raw_data_to_codes ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Function to translate data as extracted from spacy to the model codes. `A1` and `A2` are not translated cause are supposed to be already in good encoding. Parameters ---------- df : pd.DataFrame It should have columns `['A3','A4','B1','B2']` Returns ------- pd.DataFrame \"\"\" df = self . _convert_A ( df , \"A3\" ) df = self . _convert_A ( df , \"A4\" ) df = self . _convert_B ( df , \"B1\" ) df = self . _convert_B ( df , \"B2\" ) return df def _convert_line_to_attribute ( self , df : pd . DataFrame , expr : str , col : str ) -> pd . DataFrame : \"\"\" Function to convert a line into an attribute (column) of the previous row. Particularly we use it to identify \"\\\\n\" and \"\\\\n\\\\n\" that are considered tokens, express this information as an attribute of the previous token. Parameters ---------- df : pd.DataFrame expr : str pattern to search in the text. Ex.: \"\\\\n\" col : str name of the new column Returns ------- pd.DataFrame \"\"\" idx = df . TEXT . str . contains ( expr ) df . loc [ idx , col ] = True df [ col ] = df [ col ] . fillna ( False ) df = self . _shift_col ( df , col , \"TEMP_\" + col , direction = \"backward\" ) return df def _compute_a3 ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" A3 (A4 respectively): typographic form of left word (or right) : - All in capital letter - It starts with a capital letter - Starts by lowercase - It's a number - Strong punctuation - Soft punctuation - A number followed or preced by a punctuation (it's the case of enumerations) Parameters ---------- df: pd.DataFrame Returns ------- df: pd.DataFrame with the columns `A3` and `A3_` \"\"\" df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_+1\" , direction = \"backward\" , fill = False ) df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_-1\" , direction = \"forward\" , fill = False ) CONDITION1 = df . IS_UPPER CONDITION2 = df . SHAPE_ . str . startswith ( \"Xx\" , na = False ) CONDITION3 = df . SHAPE_ . str . startswith ( \"x\" , na = False ) CONDITION4 = df . IS_DIGIT STRONG_PUNCT = [ \".\" , \";\" , \"..\" , \"...\" ] CONDITION5 = ( df . IS_PUNCT ) & ( df . TEXT . isin ( STRONG_PUNCT )) CONDITION6 = ( df . IS_PUNCT ) & ( ~ df . TEXT . isin ( STRONG_PUNCT )) CONDITION7 = ( df . IS_DIGIT ) & ( df [ \"IS_PUNCT_+1\" ] | df [ \"IS_PUNCT_-1\" ]) # discuss df [ \"A3_\" ] = None df . loc [ CONDITION1 , \"A3_\" ] = \"UPPER\" df . loc [ CONDITION2 , \"A3_\" ] = \"S_UPPER\" df . loc [ CONDITION3 , \"A3_\" ] = \"LOWER\" df . loc [ CONDITION4 , \"A3_\" ] = \"DIGIT\" df . loc [ CONDITION5 , \"A3_\" ] = \"STRONG_PUNCT\" df . loc [ CONDITION6 , \"A3_\" ] = \"SOFT_PUNCT\" df . loc [ CONDITION7 , \"A3_\" ] = \"ENUMERATION\" df = df . drop ( columns = [ \"IS_PUNCT_+1\" , \"IS_PUNCT_-1\" ]) df [ \"A3_\" ] = df [ \"A3_\" ] . astype ( \"category\" ) df [ \"A3_\" ] = df [ \"A3_\" ] . cat . add_categories ( \"OTHER\" ) df [ \"A3_\" ] . fillna ( \"OTHER\" , inplace = True ) df [ \"A3\" ] = df [ \"A3_\" ] . cat . codes return df def _fit_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series , label : pd . Series , ): \"\"\"Function to train M1 classifier (Naive Bayes) Parameters ---------- A1 : pd.Series [description] A2 : pd.Series [description] A3 : pd.Series [description] A4 : pd.Series [description] label : pd.Series [description] \"\"\" # Encode classes to OneHotEncoder representation encoder_A1_A2 = self . _fit_encoder_2S ( A1 , A2 ) self . encoder_A1_A2 = encoder_A1_A2 encoder_A3_A4 = self . _fit_encoder_2S ( A3 , A4 ) self . encoder_A3_A4 = encoder_A3_A4 # M1 m1 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) m1 . fit ( X , label ) self . m1 = m1 def _fit_M2 ( self , B1 : pd . Series , B2 : pd . Series , label : pd . Series ): \"\"\"Function to train M2 classifier (Naive Bayes) Parameters ---------- B1 : pd.Series B2 : pd.Series label : pd.Series \"\"\" # Encode classes to OneHotEncoder representation encoder_B1 = self . _fit_encoder_1S ( B1 ) self . encoder_B1 = encoder_B1 encoder_B2 = self . _fit_encoder_1S ( B2 ) self . encoder_B2 = encoder_B2 # Multinomial Naive Bayes m2 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M2 ( B1 , B2 ) m2 . fit ( X , label ) self . m2 = m2 def _get_X_for_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- np.ndarray \"\"\" A1_enc = self . _encode_series ( self . encoder_A1_A2 , A1 ) A2_enc = self . _encode_series ( self . encoder_A1_A2 , A2 ) A3_enc = self . _encode_series ( self . encoder_A3_A4 , A3 ) A4_enc = self . _encode_series ( self . encoder_A3_A4 , A4 ) X = hstack ([ A1_enc , A2_enc , A3_enc , A4_enc ]) return X def _get_X_for_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- np.ndarray \"\"\" B1_enc = self . _encode_series ( self . encoder_B1 , B1 ) B2_enc = self . _encode_series ( self . encoder_B2 , B2 ) X = hstack ([ B1_enc , B2_enc ]) return X def _predict_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M1 for prediction Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) predictions = self . m1 . predict ( X ) predictions_proba = self . m1 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs def _predict_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M2 for prediction Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M2 ( B1 , B2 ) predictions = self . m2 . predict ( X ) predictions_proba = self . m2 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs def _fit_encoder_2S ( self , S1 : pd . Series , S2 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 2 Series. It concatenates the series and after it fits. Parameters ---------- S1 : pd.Series S2 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) _S2 = _convert_series_to_array ( S2 ) S = np . concatenate ([ _S1 , _S2 ]) encoder = self . _fit_one_hot_encoder ( S ) return encoder def _fit_encoder_1S ( self , S1 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 1 Series. Parameters ---------- S1 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) encoder = self . _fit_one_hot_encoder ( _S1 ) return encoder def _encode_series ( self , encoder : OneHotEncoder , S : pd . Series ) -> np . ndarray : \"\"\"Use the one hot encoder to transform a series. Parameters ---------- encoder : OneHotEncoder S : pd.Series a series to encode (transform) Returns ------- np.ndarray \"\"\" _S = _convert_series_to_array ( S ) S_enc = encoder . transform ( _S ) return S_enc @classmethod def _retrieve_lines ( cls , dfg : DataFrameGroupBy ) -> DataFrameGroupBy : \"\"\"Function to give a sentence_id to each token. Parameters ---------- dfg : DataFrameGroupBy Returns ------- DataFrameGroupBy Same DataFrameGroupBy with the column `SENTENCE_ID` \"\"\" sentences_ids = np . arange ( dfg . END_LINE . sum ()) dfg . loc [ dfg . END_LINE , \"SENTENCE_ID\" ] = sentences_ids dfg [ \"SENTENCE_ID\" ] = dfg [ \"SENTENCE_ID\" ] . fillna ( method = \"bfill\" ) return dfg @classmethod def _create_vocabulary ( cls , x : iterable ) -> dict : \"\"\"Function to create a vocabulary for attributes in the training set. Parameters ---------- x : iterable Returns ------- dict \"\"\" v = {} for i , key in enumerate ( x ): v [ key ] = i return v @classmethod def _compute_B ( cls , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Function to compute B1 and B2 Parameters ---------- df : pd.DataFrame Returns ------- pd.DataFrame \"\"\" data = df . groupby ([ \"DOC_ID\" , \"SENTENCE_ID\" ]) . agg ( l = ( \"LENGTH\" , \"sum\" )) df_t = df . loc [ df . END_LINE , [ \"DOC_ID\" , \"SENTENCE_ID\" ]] . merge ( data , left_on = [ \"DOC_ID\" , \"SENTENCE_ID\" ], right_index = True , how = \"left\" ) stats_doc = df_t . groupby ( \"DOC_ID\" ) . agg ( mu = ( \"l\" , \"mean\" ), sigma = ( \"l\" , \"std\" )) stats_doc [ \"sigma\" ] . replace ( 0.0 , 1.0 , inplace = True ) # Replace the 0 std by unit std, otherwise it breaks the code. stats_doc [ \"cv\" ] = stats_doc [ \"sigma\" ] / stats_doc [ \"mu\" ] df_t = df_t . drop ( columns = [ \"DOC_ID\" , \"SENTENCE_ID\" ]) df2 = df . merge ( df_t , left_index = True , right_index = True , how = \"left\" ) df2 = df2 . merge ( stats_doc , on = [ \"DOC_ID\" ], how = \"left\" ) df2 [ \"l_norm\" ] = ( df2 [ \"l\" ] - df2 [ \"mu\" ]) / df2 [ \"sigma\" ] df2 [ \"cv_bin\" ] = pd . cut ( df2 [ \"cv\" ], bins = 10 ) df2 [ \"B2\" ] = df2 [ \"cv_bin\" ] . cat . codes df2 [ \"l_norm_bin\" ] = pd . cut ( df2 [ \"l_norm\" ], bins = 10 ) df2 [ \"B1\" ] = df2 [ \"l_norm_bin\" ] . cat . codes return df2 @classmethod def _shift_col ( cls , df : pd . DataFrame , col : str , new_col : str , direction = \"backward\" , fill = None ) -> pd . DataFrame : \"\"\"Shifts a column one position into backward / forward direction. Parameters ---------- df : pd.DataFrame col : str column to shift new_col : str column name to save the results direction : str, optional one of {\"backward\", \"forward\"}, by default \"backward\" fill : [type], optional , by default None Returns ------- pd.DataFrame same df with `new_col` added. \"\"\" df [ new_col ] = fill if direction == \"backward\" : df . loc [ df . index [: - 1 ], new_col ] = df [ col ] . values [ 1 :] different_doc_id = df [ \"DOC_ID\" ] . values [: - 1 ] != df [ \"DOC_ID\" ] . values [ 1 :] different_doc_id = np . append ( different_doc_id , True ) if direction == \"forward\" : df . loc [ df . index [ 1 :], new_col ] = df [ col ] . values [: - 1 ] different_doc_id = df [ \"DOC_ID\" ] . values [ 1 :] != df [ \"DOC_ID\" ] . values [: - 1 ] different_doc_id = np . append ( True , different_doc_id ) df . loc [ different_doc_id , new_col ] = fill return df @classmethod def _get_attributes ( cls , doc : Doc , i = 0 ): \"\"\"Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format. Parameters ---------- doc : Doc spacy Doc i : int, optional document id, by default 0 Returns ------- pd.DataFrame Returns a dataframe with one line per token. It has the following columns : `[ \"ORTH\", \"LOWER\", \"SHAPE\", \"IS_DIGIT\", \"IS_SPACE\", \"IS_UPPER\", \"IS_PUNCT\", \"LENGTH\", ]` \"\"\" attributes = [ \"ORTH\" , \"LOWER\" , \"SHAPE\" , \"IS_DIGIT\" , \"IS_SPACE\" , \"IS_UPPER\" , \"IS_PUNCT\" , \"LENGTH\" , ] attributes_array = doc . to_array ( attributes ) attributes_df = pd . DataFrame ( attributes_array , columns = attributes ) attributes_df [ \"DOC_ID\" ] = i boolean_attr = [] for a in attributes : if a [: 3 ] == \"IS_\" : boolean_attr . append ( a ) attributes_df [ boolean_attr ] = attributes_df [ boolean_attr ] . astype ( \"boolean\" ) return attributes_df @classmethod def _get_string ( cls , _id : int , string_store : StringStore ) -> str : \"\"\"Returns the string corresponding to the token_id Parameters ---------- _id : int token id string_store : StringStore spaCy Language String Store Returns ------- str string representation of the token. \"\"\" return string_store [ _id ] @classmethod def _fit_one_hot_encoder ( cls , X : np . ndarray ) -> OneHotEncoder : \"\"\"Fit a one hot encoder. Parameters ---------- X : np.ndarray of shape (n,1) Returns ------- OneHotEncoder \"\"\" encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) encoder . fit ( X ) return encoder nlp = nlp instance-attribute __init__ ( nlp ) Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 28 29 def __init__ ( self , nlp : Language ): self . nlp = nlp fit_and_predict ( corpus ) Fit the model and predict for the training data PARAMETER DESCRIPTION corpus An iterable of Documents TYPE: Iterable[Doc] RETURNS DESCRIPTION pd.DataFrame one line by end_line prediction Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def fit_and_predict ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\"Fit the model and predict for the training data Parameters ---------- corpus : Iterable[Doc] An iterable of Documents Returns ------- pd.DataFrame one line by end_line prediction \"\"\" # Preprocess data to have a pd DF df = self . _preprocess_data ( corpus ) # Train and predict M1 self . _fit_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , df . SPACE ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] # Force Blank lines to 0 df . loc [ df . BLANK_LINE , \"M1\" ] = 0 # Train and predict M2 df_endlines = df . loc [ df . END_LINE ] self . _fit_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 , label = df_endlines . M1 ) outputs_M2 = self . _predict_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 ) df . loc [ df . END_LINE , \"M2\" ] = outputs_M2 [ \"predictions\" ] df . loc [ df . END_LINE , \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df = df . loc [ df . END_LINE ] df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M2\" , \"M1M2\" ]] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df predict ( df ) Use the model for inference The df should have the following columns: [\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"] PARAMETER DESCRIPTION df The df should have the following columns: [\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"] TYPE: pd.DataFrame RETURNS DESCRIPTION pd.DataFrame The result is added to the column PREDICTED_END_LINE Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def predict ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Use the model for inference The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Parameters ---------- df : pd.DataFrame The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Returns ------- pd.DataFrame The result is added to the column `PREDICTED_END_LINE` \"\"\" df = self . _convert_raw_data_to_codes ( df ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . _A3 , df . _A4 ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] outputs_M2 = self . _predict_M2 ( B1 = df . _B1 , B2 = df . _B2 ) df [ \"M2\" ] = outputs_M2 [ \"predictions\" ] df [ \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M1M2\" , ], ] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df save ( path = 'base_model.pkl' ) Save a pickle of the model. It could be read by the pipeline later. PARAMETER DESCRIPTION path path to file .pkl, by default base_model.pkl TYPE: str, optional DEFAULT: 'base_model.pkl' Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 213 214 215 216 217 218 219 220 221 222 223 def save ( self , path = \"base_model.pkl\" ): \"\"\"Save a pickle of the model. It could be read by the pipeline later. Parameters ---------- path : str, optional path to file .pkl, by default `base_model.pkl` \"\"\" with open ( path , \"wb\" ) as outp : del self . nlp pickle . dump ( self , outp , pickle . HIGHEST_PROTOCOL )","title":"endlinesmodel"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlppipelinescoreendlinesendlinesmodel","text":"","title":"edsnlp.pipelines.core.endlines.endlinesmodel"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel","text":"Model to classify if an end line is a real one or it should be a space. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 class EndLinesModel : \"\"\"Model to classify if an end line is a real one or it should be a space. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. \"\"\" def __init__ ( self , nlp : Language ): self . nlp = nlp def _preprocess_data ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\" Parameters ---------- corpus : Iterable[Doc] Corpus of documents Returns ------- pd.DataFrame Preprocessed data \"\"\" # Extract the vocabulary string_store = self . nlp . vocab . strings # Iterate in the corpus and construct a dataframe train_data_list = [] for i , doc in enumerate ( corpus ): train_data_list . append ( self . _get_attributes ( doc , i )) df = pd . concat ( train_data_list ) df . reset_index ( inplace = True , drop = False ) df . rename ( columns = { \"ORTH\" : \"A1\" , \"index\" : \"original_token_index\" }, inplace = True ) # Retrieve string representation of token_id and shape df [ \"TEXT\" ] = df . A1 . apply ( self . _get_string , string_store = string_store ) df [ \"SHAPE_\" ] = df . SHAPE . apply ( self . _get_string , string_store = string_store ) # Convert new lines as an attribute instead of a row df = self . _convert_line_to_attribute ( df , expr = \" \\n \" , col = \"END_LINE\" ) df = self . _convert_line_to_attribute ( df , expr = \" \\n\\n \" , col = \"BLANK_LINE\" ) df = df . loc [ ~ ( df . END_LINE | df . BLANK_LINE )] df = df . drop ( columns = \"END_LINE\" ) df = df . drop ( columns = \"BLANK_LINE\" ) df . rename ( columns = { \"TEMP_END_LINE\" : \"END_LINE\" , \"TEMP_BLANK_LINE\" : \"BLANK_LINE\" }, inplace = True , ) # Construct A2 by shifting df = self . _shift_col ( df , \"A1\" , \"A2\" , direction = \"backward\" ) # Compute A3 and A4 df = self . _compute_a3 ( df ) df = self . _shift_col ( df , \"A3\" , \"A4\" , direction = \"backward\" ) # SPACE is the class to predict. Set 1 if not an END_LINE df [ \"SPACE\" ] = np . logical_not ( df [ \"END_LINE\" ]) . astype ( \"int\" ) df [[ \"END_LINE\" , \"BLANK_LINE\" ]] = df [[ \"END_LINE\" , \"BLANK_LINE\" ]] . fillna ( True , inplace = False ) # Assign a sentence id to each token df = df . groupby ( \"DOC_ID\" ) . apply ( self . _retrieve_lines ) df [ \"SENTENCE_ID\" ] = df [ \"SENTENCE_ID\" ] . astype ( \"int\" ) # Compute B1 and B2 df = self . _compute_B ( df ) # Drop Tokens without info (last token of doc) df . dropna ( subset = [ \"A1\" , \"A2\" , \"A3\" , \"A4\" ], inplace = True ) # Export the vocabularies to be able to use the model with another corpus voc_a3a4 = self . _create_vocabulary ( df . A3_ . cat . categories ) voc_B2 = self . _create_vocabulary ( df . cv_bin . cat . categories ) voc_B1 = self . _create_vocabulary ( df . l_norm_bin . cat . categories ) vocabulary = { \"A3A4\" : voc_a3a4 , \"B1\" : voc_B1 , \"B2\" : voc_B2 } self . vocabulary = vocabulary return df def fit_and_predict ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\"Fit the model and predict for the training data Parameters ---------- corpus : Iterable[Doc] An iterable of Documents Returns ------- pd.DataFrame one line by end_line prediction \"\"\" # Preprocess data to have a pd DF df = self . _preprocess_data ( corpus ) # Train and predict M1 self . _fit_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , df . SPACE ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] # Force Blank lines to 0 df . loc [ df . BLANK_LINE , \"M1\" ] = 0 # Train and predict M2 df_endlines = df . loc [ df . END_LINE ] self . _fit_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 , label = df_endlines . M1 ) outputs_M2 = self . _predict_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 ) df . loc [ df . END_LINE , \"M2\" ] = outputs_M2 [ \"predictions\" ] df . loc [ df . END_LINE , \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df = df . loc [ df . END_LINE ] df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M2\" , \"M1M2\" ]] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df def predict ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Use the model for inference The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Parameters ---------- df : pd.DataFrame The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Returns ------- pd.DataFrame The result is added to the column `PREDICTED_END_LINE` \"\"\" df = self . _convert_raw_data_to_codes ( df ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . _A3 , df . _A4 ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] outputs_M2 = self . _predict_M2 ( B1 = df . _B1 , B2 = df . _B2 ) df [ \"M2\" ] = outputs_M2 [ \"predictions\" ] df [ \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M1M2\" , ], ] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df def save ( self , path = \"base_model.pkl\" ): \"\"\"Save a pickle of the model. It could be read by the pipeline later. Parameters ---------- path : str, optional path to file .pkl, by default `base_model.pkl` \"\"\" with open ( path , \"wb\" ) as outp : del self . nlp pickle . dump ( self , outp , pickle . HIGHEST_PROTOCOL ) def _convert_A ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame col : str column to translate Returns ------- pd.DataFrame \"\"\" cat_type_A = CategoricalDtype ( categories = self . vocabulary [ \"A3A4\" ] . keys (), ordered = True ) new_col = \"_\" + col df [ new_col ] = df [ col ] . astype ( cat_type_A ) df [ new_col ] = df [ new_col ] . cat . codes # Ensure that not known values are coded as OTHER df . loc [ ~ df [ col ] . isin ( self . vocabulary [ \"A3A4\" ] . keys ()), new_col ] = self . vocabulary [ \"A3A4\" ][ \"OTHER\" ] return df def _convert_B ( self , df : pd . DataFrame , col : str ) -> pd . DataFrame : \"\"\" Parameters ---------- df : pd.DataFrame [description] col : str column to translate Returns ------- pd.DataFrame [description] \"\"\" # Translate B1 index_B = pd . IntervalIndex ( list ( self . vocabulary [ col ] . keys ())) new_col = \"_\" + col df [ new_col ] = pd . cut ( df [ col ], index_B ) df [ new_col ] = df [ new_col ] . cat . codes df . loc [ df [ col ] >= index_B . right . max (), new_col ] = max ( self . vocabulary [ col ] . values () ) df . loc [ df [ col ] <= index_B . left . min (), new_col ] = min ( self . vocabulary [ col ] . values () ) return df def _convert_raw_data_to_codes ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Function to translate data as extracted from spacy to the model codes. `A1` and `A2` are not translated cause are supposed to be already in good encoding. Parameters ---------- df : pd.DataFrame It should have columns `['A3','A4','B1','B2']` Returns ------- pd.DataFrame \"\"\" df = self . _convert_A ( df , \"A3\" ) df = self . _convert_A ( df , \"A4\" ) df = self . _convert_B ( df , \"B1\" ) df = self . _convert_B ( df , \"B2\" ) return df def _convert_line_to_attribute ( self , df : pd . DataFrame , expr : str , col : str ) -> pd . DataFrame : \"\"\" Function to convert a line into an attribute (column) of the previous row. Particularly we use it to identify \"\\\\n\" and \"\\\\n\\\\n\" that are considered tokens, express this information as an attribute of the previous token. Parameters ---------- df : pd.DataFrame expr : str pattern to search in the text. Ex.: \"\\\\n\" col : str name of the new column Returns ------- pd.DataFrame \"\"\" idx = df . TEXT . str . contains ( expr ) df . loc [ idx , col ] = True df [ col ] = df [ col ] . fillna ( False ) df = self . _shift_col ( df , col , \"TEMP_\" + col , direction = \"backward\" ) return df def _compute_a3 ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\" A3 (A4 respectively): typographic form of left word (or right) : - All in capital letter - It starts with a capital letter - Starts by lowercase - It's a number - Strong punctuation - Soft punctuation - A number followed or preced by a punctuation (it's the case of enumerations) Parameters ---------- df: pd.DataFrame Returns ------- df: pd.DataFrame with the columns `A3` and `A3_` \"\"\" df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_+1\" , direction = \"backward\" , fill = False ) df = self . _shift_col ( df , \"IS_PUNCT\" , \"IS_PUNCT_-1\" , direction = \"forward\" , fill = False ) CONDITION1 = df . IS_UPPER CONDITION2 = df . SHAPE_ . str . startswith ( \"Xx\" , na = False ) CONDITION3 = df . SHAPE_ . str . startswith ( \"x\" , na = False ) CONDITION4 = df . IS_DIGIT STRONG_PUNCT = [ \".\" , \";\" , \"..\" , \"...\" ] CONDITION5 = ( df . IS_PUNCT ) & ( df . TEXT . isin ( STRONG_PUNCT )) CONDITION6 = ( df . IS_PUNCT ) & ( ~ df . TEXT . isin ( STRONG_PUNCT )) CONDITION7 = ( df . IS_DIGIT ) & ( df [ \"IS_PUNCT_+1\" ] | df [ \"IS_PUNCT_-1\" ]) # discuss df [ \"A3_\" ] = None df . loc [ CONDITION1 , \"A3_\" ] = \"UPPER\" df . loc [ CONDITION2 , \"A3_\" ] = \"S_UPPER\" df . loc [ CONDITION3 , \"A3_\" ] = \"LOWER\" df . loc [ CONDITION4 , \"A3_\" ] = \"DIGIT\" df . loc [ CONDITION5 , \"A3_\" ] = \"STRONG_PUNCT\" df . loc [ CONDITION6 , \"A3_\" ] = \"SOFT_PUNCT\" df . loc [ CONDITION7 , \"A3_\" ] = \"ENUMERATION\" df = df . drop ( columns = [ \"IS_PUNCT_+1\" , \"IS_PUNCT_-1\" ]) df [ \"A3_\" ] = df [ \"A3_\" ] . astype ( \"category\" ) df [ \"A3_\" ] = df [ \"A3_\" ] . cat . add_categories ( \"OTHER\" ) df [ \"A3_\" ] . fillna ( \"OTHER\" , inplace = True ) df [ \"A3\" ] = df [ \"A3_\" ] . cat . codes return df def _fit_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series , label : pd . Series , ): \"\"\"Function to train M1 classifier (Naive Bayes) Parameters ---------- A1 : pd.Series [description] A2 : pd.Series [description] A3 : pd.Series [description] A4 : pd.Series [description] label : pd.Series [description] \"\"\" # Encode classes to OneHotEncoder representation encoder_A1_A2 = self . _fit_encoder_2S ( A1 , A2 ) self . encoder_A1_A2 = encoder_A1_A2 encoder_A3_A4 = self . _fit_encoder_2S ( A3 , A4 ) self . encoder_A3_A4 = encoder_A3_A4 # M1 m1 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) m1 . fit ( X , label ) self . m1 = m1 def _fit_M2 ( self , B1 : pd . Series , B2 : pd . Series , label : pd . Series ): \"\"\"Function to train M2 classifier (Naive Bayes) Parameters ---------- B1 : pd.Series B2 : pd.Series label : pd.Series \"\"\" # Encode classes to OneHotEncoder representation encoder_B1 = self . _fit_encoder_1S ( B1 ) self . encoder_B1 = encoder_B1 encoder_B2 = self . _fit_encoder_1S ( B2 ) self . encoder_B2 = encoder_B2 # Multinomial Naive Bayes m2 = MultinomialNB ( alpha = 1 ) X = self . _get_X_for_M2 ( B1 , B2 ) m2 . fit ( X , label ) self . m2 = m2 def _get_X_for_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- np.ndarray \"\"\" A1_enc = self . _encode_series ( self . encoder_A1_A2 , A1 ) A2_enc = self . _encode_series ( self . encoder_A1_A2 , A2 ) A3_enc = self . _encode_series ( self . encoder_A3_A4 , A3 ) A4_enc = self . _encode_series ( self . encoder_A3_A4 , A4 ) X = hstack ([ A1_enc , A2_enc , A3_enc , A4_enc ]) return X def _get_X_for_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> np . ndarray : \"\"\"Get X matrix for classifier Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- np.ndarray \"\"\" B1_enc = self . _encode_series ( self . encoder_B1 , B1 ) B2_enc = self . _encode_series ( self . encoder_B2 , B2 ) X = hstack ([ B1_enc , B2_enc ]) return X def _predict_M1 ( self , A1 : pd . Series , A2 : pd . Series , A3 : pd . Series , A4 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M1 for prediction Parameters ---------- A1 : pd.Series A2 : pd.Series A3 : pd.Series A4 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M1 ( A1 , A2 , A3 , A4 ) predictions = self . m1 . predict ( X ) predictions_proba = self . m1 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs def _predict_M2 ( self , B1 : pd . Series , B2 : pd . Series ) -> Dict [ str , Any ]: \"\"\"Use M2 for prediction Parameters ---------- B1 : pd.Series B2 : pd.Series Returns ------- Dict[str, Any] \"\"\" X = self . _get_X_for_M2 ( B1 , B2 ) predictions = self . m2 . predict ( X ) predictions_proba = self . m2 . predict_proba ( X )[:, 1 ] outputs = { \"predictions\" : predictions , \"predictions_proba\" : predictions_proba } return outputs def _fit_encoder_2S ( self , S1 : pd . Series , S2 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 2 Series. It concatenates the series and after it fits. Parameters ---------- S1 : pd.Series S2 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) _S2 = _convert_series_to_array ( S2 ) S = np . concatenate ([ _S1 , _S2 ]) encoder = self . _fit_one_hot_encoder ( S ) return encoder def _fit_encoder_1S ( self , S1 : pd . Series ) -> OneHotEncoder : \"\"\"Fit a one hot encoder with 1 Series. Parameters ---------- S1 : pd.Series Returns ------- OneHotEncoder \"\"\" _S1 = _convert_series_to_array ( S1 ) encoder = self . _fit_one_hot_encoder ( _S1 ) return encoder def _encode_series ( self , encoder : OneHotEncoder , S : pd . Series ) -> np . ndarray : \"\"\"Use the one hot encoder to transform a series. Parameters ---------- encoder : OneHotEncoder S : pd.Series a series to encode (transform) Returns ------- np.ndarray \"\"\" _S = _convert_series_to_array ( S ) S_enc = encoder . transform ( _S ) return S_enc @classmethod def _retrieve_lines ( cls , dfg : DataFrameGroupBy ) -> DataFrameGroupBy : \"\"\"Function to give a sentence_id to each token. Parameters ---------- dfg : DataFrameGroupBy Returns ------- DataFrameGroupBy Same DataFrameGroupBy with the column `SENTENCE_ID` \"\"\" sentences_ids = np . arange ( dfg . END_LINE . sum ()) dfg . loc [ dfg . END_LINE , \"SENTENCE_ID\" ] = sentences_ids dfg [ \"SENTENCE_ID\" ] = dfg [ \"SENTENCE_ID\" ] . fillna ( method = \"bfill\" ) return dfg @classmethod def _create_vocabulary ( cls , x : iterable ) -> dict : \"\"\"Function to create a vocabulary for attributes in the training set. Parameters ---------- x : iterable Returns ------- dict \"\"\" v = {} for i , key in enumerate ( x ): v [ key ] = i return v @classmethod def _compute_B ( cls , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Function to compute B1 and B2 Parameters ---------- df : pd.DataFrame Returns ------- pd.DataFrame \"\"\" data = df . groupby ([ \"DOC_ID\" , \"SENTENCE_ID\" ]) . agg ( l = ( \"LENGTH\" , \"sum\" )) df_t = df . loc [ df . END_LINE , [ \"DOC_ID\" , \"SENTENCE_ID\" ]] . merge ( data , left_on = [ \"DOC_ID\" , \"SENTENCE_ID\" ], right_index = True , how = \"left\" ) stats_doc = df_t . groupby ( \"DOC_ID\" ) . agg ( mu = ( \"l\" , \"mean\" ), sigma = ( \"l\" , \"std\" )) stats_doc [ \"sigma\" ] . replace ( 0.0 , 1.0 , inplace = True ) # Replace the 0 std by unit std, otherwise it breaks the code. stats_doc [ \"cv\" ] = stats_doc [ \"sigma\" ] / stats_doc [ \"mu\" ] df_t = df_t . drop ( columns = [ \"DOC_ID\" , \"SENTENCE_ID\" ]) df2 = df . merge ( df_t , left_index = True , right_index = True , how = \"left\" ) df2 = df2 . merge ( stats_doc , on = [ \"DOC_ID\" ], how = \"left\" ) df2 [ \"l_norm\" ] = ( df2 [ \"l\" ] - df2 [ \"mu\" ]) / df2 [ \"sigma\" ] df2 [ \"cv_bin\" ] = pd . cut ( df2 [ \"cv\" ], bins = 10 ) df2 [ \"B2\" ] = df2 [ \"cv_bin\" ] . cat . codes df2 [ \"l_norm_bin\" ] = pd . cut ( df2 [ \"l_norm\" ], bins = 10 ) df2 [ \"B1\" ] = df2 [ \"l_norm_bin\" ] . cat . codes return df2 @classmethod def _shift_col ( cls , df : pd . DataFrame , col : str , new_col : str , direction = \"backward\" , fill = None ) -> pd . DataFrame : \"\"\"Shifts a column one position into backward / forward direction. Parameters ---------- df : pd.DataFrame col : str column to shift new_col : str column name to save the results direction : str, optional one of {\"backward\", \"forward\"}, by default \"backward\" fill : [type], optional , by default None Returns ------- pd.DataFrame same df with `new_col` added. \"\"\" df [ new_col ] = fill if direction == \"backward\" : df . loc [ df . index [: - 1 ], new_col ] = df [ col ] . values [ 1 :] different_doc_id = df [ \"DOC_ID\" ] . values [: - 1 ] != df [ \"DOC_ID\" ] . values [ 1 :] different_doc_id = np . append ( different_doc_id , True ) if direction == \"forward\" : df . loc [ df . index [ 1 :], new_col ] = df [ col ] . values [: - 1 ] different_doc_id = df [ \"DOC_ID\" ] . values [ 1 :] != df [ \"DOC_ID\" ] . values [: - 1 ] different_doc_id = np . append ( True , different_doc_id ) df . loc [ different_doc_id , new_col ] = fill return df @classmethod def _get_attributes ( cls , doc : Doc , i = 0 ): \"\"\"Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format. Parameters ---------- doc : Doc spacy Doc i : int, optional document id, by default 0 Returns ------- pd.DataFrame Returns a dataframe with one line per token. It has the following columns : `[ \"ORTH\", \"LOWER\", \"SHAPE\", \"IS_DIGIT\", \"IS_SPACE\", \"IS_UPPER\", \"IS_PUNCT\", \"LENGTH\", ]` \"\"\" attributes = [ \"ORTH\" , \"LOWER\" , \"SHAPE\" , \"IS_DIGIT\" , \"IS_SPACE\" , \"IS_UPPER\" , \"IS_PUNCT\" , \"LENGTH\" , ] attributes_array = doc . to_array ( attributes ) attributes_df = pd . DataFrame ( attributes_array , columns = attributes ) attributes_df [ \"DOC_ID\" ] = i boolean_attr = [] for a in attributes : if a [: 3 ] == \"IS_\" : boolean_attr . append ( a ) attributes_df [ boolean_attr ] = attributes_df [ boolean_attr ] . astype ( \"boolean\" ) return attributes_df @classmethod def _get_string ( cls , _id : int , string_store : StringStore ) -> str : \"\"\"Returns the string corresponding to the token_id Parameters ---------- _id : int token id string_store : StringStore spaCy Language String Store Returns ------- str string representation of the token. \"\"\" return string_store [ _id ] @classmethod def _fit_one_hot_encoder ( cls , X : np . ndarray ) -> OneHotEncoder : \"\"\"Fit a one hot encoder. Parameters ---------- X : np.ndarray of shape (n,1) Returns ------- OneHotEncoder \"\"\" encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) encoder . fit ( X ) return encoder","title":"EndLinesModel"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.__init__","text":"Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 28 29 def __init__ ( self , nlp : Language ): self . nlp = nlp","title":"__init__()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.fit_and_predict","text":"Fit the model and predict for the training data PARAMETER DESCRIPTION corpus An iterable of Documents TYPE: Iterable[Doc] RETURNS DESCRIPTION pd.DataFrame one line by end_line prediction Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def fit_and_predict ( self , corpus : Iterable [ Doc ]) -> pd . DataFrame : \"\"\"Fit the model and predict for the training data Parameters ---------- corpus : Iterable[Doc] An iterable of Documents Returns ------- pd.DataFrame one line by end_line prediction \"\"\" # Preprocess data to have a pd DF df = self . _preprocess_data ( corpus ) # Train and predict M1 self . _fit_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , df . SPACE ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . A3 , df . A4 , ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] # Force Blank lines to 0 df . loc [ df . BLANK_LINE , \"M1\" ] = 0 # Train and predict M2 df_endlines = df . loc [ df . END_LINE ] self . _fit_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 , label = df_endlines . M1 ) outputs_M2 = self . _predict_M2 ( B1 = df_endlines . B1 , B2 = df_endlines . B2 ) df . loc [ df . END_LINE , \"M2\" ] = outputs_M2 [ \"predictions\" ] df . loc [ df . END_LINE , \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df = df . loc [ df . END_LINE ] df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M2\" , \"M1M2\" ]] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df","title":"fit_and_predict()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.predict","text":"Use the model for inference The df should have the following columns: [\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"] PARAMETER DESCRIPTION df The df should have the following columns: [\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"] TYPE: pd.DataFrame RETURNS DESCRIPTION pd.DataFrame The result is added to the column PREDICTED_END_LINE Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def predict ( self , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Use the model for inference The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Parameters ---------- df : pd.DataFrame The df should have the following columns: `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]` Returns ------- pd.DataFrame The result is added to the column `PREDICTED_END_LINE` \"\"\" df = self . _convert_raw_data_to_codes ( df ) outputs_M1 = self . _predict_M1 ( df . A1 , df . A2 , df . _A3 , df . _A4 ) df [ \"M1\" ] = outputs_M1 [ \"predictions\" ] df [ \"M1_proba\" ] = outputs_M1 [ \"predictions_proba\" ] outputs_M2 = self . _predict_M2 ( B1 = df . _B1 , B2 = df . _B2 ) df [ \"M2\" ] = outputs_M2 [ \"predictions\" ] df [ \"M2_proba\" ] = outputs_M2 [ \"predictions_proba\" ] df [ \"M2\" ] = df [ \"M2\" ] . astype ( pd . Int64Dtype () ) # cast to pd.Int64Dtype cause there are None values # M1M2 df [ \"M1M2_lr\" ] = ( df [ \"M2_proba\" ] / ( 1 - df [ \"M2_proba\" ])) * ( df [ \"M1_proba\" ] / ( 1 - df [ \"M1_proba\" ]) ) df [ \"M1M2\" ] = ( df [ \"M1M2_lr\" ] > 1 ) . astype ( \"int\" ) # Force Blank lines to 0 df . loc [ df . BLANK_LINE , [ \"M1M2\" , ], ] = 0 # Make binary col df [ \"PREDICTED_END_LINE\" ] = np . logical_not ( df [ \"M1M2\" ] . astype ( bool )) return df","title":"predict()"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.save","text":"Save a pickle of the model. It could be read by the pipeline later. PARAMETER DESCRIPTION path path to file .pkl, by default base_model.pkl TYPE: str, optional DEFAULT: 'base_model.pkl' Source code in edsnlp/pipelines/core/endlines/endlinesmodel.py 213 214 215 216 217 218 219 220 221 222 223 def save ( self , path = \"base_model.pkl\" ): \"\"\"Save a pickle of the model. It could be read by the pipeline later. Parameters ---------- path : str, optional path to file .pkl, by default `base_model.pkl` \"\"\" with open ( path , \"wb\" ) as outp : del self . nlp pickle . dump ( self , outp , pickle . HIGHEST_PROTOCOL )","title":"save()"},{"location":"reference/pipelines/core/endlines/factory/","text":"edsnlp.pipelines.core.endlines.factory create_component ( nlp , name , model_path ) Source code in edsnlp/pipelines/core/endlines/factory.py 10 11 12 13 14 15 16 17 @deprecated_factory ( \"endlines\" , \"eds.endlines\" ) @Language . factory ( \"eds.endlines\" ) def create_component ( nlp : Language , name : str , model_path : Optional [ str ], ): return EndLines ( nlp , end_lines_model = model_path )","title":"factory"},{"location":"reference/pipelines/core/endlines/factory/#edsnlppipelinescoreendlinesfactory","text":"","title":"edsnlp.pipelines.core.endlines.factory"},{"location":"reference/pipelines/core/endlines/factory/#edsnlp.pipelines.core.endlines.factory.create_component","text":"Source code in edsnlp/pipelines/core/endlines/factory.py 10 11 12 13 14 15 16 17 @deprecated_factory ( \"endlines\" , \"eds.endlines\" ) @Language . factory ( \"eds.endlines\" ) def create_component ( nlp : Language , name : str , model_path : Optional [ str ], ): return EndLines ( nlp , end_lines_model = model_path )","title":"create_component()"},{"location":"reference/pipelines/core/endlines/functional/","text":"edsnlp.pipelines.core.endlines.functional get_dir_path ( file ) Source code in edsnlp/pipelines/core/endlines/functional.py 7 8 9 def get_dir_path ( file ): path_file = os . path . dirname ( os . path . realpath ( file )) return path_file build_path ( file , relative_path ) Function to build an absolut path. PARAMETER DESCRIPTION file relative_path relative path from the main file to the desired output RETURNS DESCRIPTION path Source code in edsnlp/pipelines/core/endlines/functional.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def build_path ( file , relative_path ): \"\"\" Function to build an absolut path. Parameters ---------- file: main file from where we are calling. It could be __file__ relative_path: str, relative path from the main file to the desired output Returns ------- path: absolute path \"\"\" dir_path = get_dir_path ( file ) path = os . path . abspath ( os . path . join ( dir_path , relative_path )) return path","title":"functional"},{"location":"reference/pipelines/core/endlines/functional/#edsnlppipelinescoreendlinesfunctional","text":"","title":"edsnlp.pipelines.core.endlines.functional"},{"location":"reference/pipelines/core/endlines/functional/#edsnlp.pipelines.core.endlines.functional.get_dir_path","text":"Source code in edsnlp/pipelines/core/endlines/functional.py 7 8 9 def get_dir_path ( file ): path_file = os . path . dirname ( os . path . realpath ( file )) return path_file","title":"get_dir_path()"},{"location":"reference/pipelines/core/endlines/functional/#edsnlp.pipelines.core.endlines.functional.build_path","text":"Function to build an absolut path. PARAMETER DESCRIPTION file relative_path relative path from the main file to the desired output RETURNS DESCRIPTION path Source code in edsnlp/pipelines/core/endlines/functional.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def build_path ( file , relative_path ): \"\"\" Function to build an absolut path. Parameters ---------- file: main file from where we are calling. It could be __file__ relative_path: str, relative path from the main file to the desired output Returns ------- path: absolute path \"\"\" dir_path = get_dir_path ( file ) path = os . path . abspath ( os . path . join ( dir_path , relative_path )) return path","title":"build_path()"},{"location":"reference/pipelines/core/matcher/","text":"edsnlp.pipelines.core.matcher","title":"`edsnlp.pipelines.core.matcher`"},{"location":"reference/pipelines/core/matcher/#edsnlppipelinescorematcher","text":"","title":"edsnlp.pipelines.core.matcher"},{"location":"reference/pipelines/core/matcher/factory/","text":"edsnlp.pipelines.core.matcher.factory DEFAULT_CONFIG = dict ( terms = None , regex = None , attr = 'TEXT' , ignore_excluded = False , term_matcher = GenericTermMatcher . exact , term_matcher_config = {}) module-attribute create_component ( nlp , name , terms , attr , regex , ignore_excluded , term_matcher , term_matcher_config ) Source code in edsnlp/pipelines/core/matcher/factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"matcher\" , \"eds.matcher\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.matcher\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ] ) def create_component ( nlp : Language , name : str , terms : Optional [ Dict [ str , Union [ str , List [ str ]]]], attr : Union [ str , Dict [ str , str ]], regex : Optional [ Dict [ str , Union [ str , List [ str ]]]], ignore_excluded : bool , term_matcher : GenericTermMatcher , term_matcher_config : Dict [ str , Any ], ): assert not ( terms is None and regex is None ) if terms is None : terms = dict () if regex is None : regex = dict () return GenericMatcher ( nlp , terms = terms , attr = attr , regex = regex , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"factory"},{"location":"reference/pipelines/core/matcher/factory/#edsnlppipelinescorematcherfactory","text":"","title":"edsnlp.pipelines.core.matcher.factory"},{"location":"reference/pipelines/core/matcher/factory/#edsnlp.pipelines.core.matcher.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/matcher/factory/#edsnlp.pipelines.core.matcher.factory.create_component","text":"Source code in edsnlp/pipelines/core/matcher/factory.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"matcher\" , \"eds.matcher\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.matcher\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ] ) def create_component ( nlp : Language , name : str , terms : Optional [ Dict [ str , Union [ str , List [ str ]]]], attr : Union [ str , Dict [ str , str ]], regex : Optional [ Dict [ str , Union [ str , List [ str ]]]], ignore_excluded : bool , term_matcher : GenericTermMatcher , term_matcher_config : Dict [ str , Any ], ): assert not ( terms is None and regex is None ) if terms is None : terms = dict () if regex is None : regex = dict () return GenericMatcher ( nlp , terms = terms , attr = attr , regex = regex , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"create_component()"},{"location":"reference/pipelines/core/matcher/matcher/","text":"edsnlp.pipelines.core.matcher.matcher GenericTermMatcher Bases: str , Enum Source code in edsnlp/pipelines/core/matcher/matcher.py 15 16 17 class GenericTermMatcher ( str , Enum ): exact = \"exact\" simstring = \"simstring\" exact = 'exact' class-attribute simstring = 'simstring' class-attribute GenericMatcher Bases: BaseComponent Provides a generic matcher component. PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language terms A dictionary of terms. TYPE: Optional[Patterns] regex A dictionary of regular expressions. TYPE: Optional[Patterns] attr The default attribute to use for matching. Can be overridden using the terms and regex configurations. TYPE: str ignore_excluded Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). TYPE: bool term_matcher The matcher to use for matching phrases ? One of (exact, simstring) term_matcher_config Parameters of the matcher class Source code in edsnlp/pipelines/core/matcher/matcher.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class GenericMatcher ( BaseComponent ): \"\"\" Provides a generic matcher component. Parameters ---------- nlp : Language The spaCy object. terms : Optional[Patterns] A dictionary of terms. regex : Optional[Patterns] A dictionary of regular expressions. attr : str The default attribute to use for matching. Can be overridden using the `terms` and `regex` configurations. ignore_excluded : bool Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). term_matcher: GenericTermMatcher The matcher to use for matching phrases ? One of (exact, simstring) term_matcher_config: Dict[str,Any] Parameters of the matcher class \"\"\" def __init__ ( self , nlp : Language , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , term_matcher : GenericTermMatcher = GenericTermMatcher . exact , term_matcher_config : Dict [ str , Any ] = None , ): self . nlp = nlp self . attr = attr if term_matcher == GenericTermMatcher . exact : self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) elif term_matcher == GenericTermMatcher . simstring : self . phrase_matcher = SimstringMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) else : raise ValueError ( f \"Algorithm { repr ( term_matcher ) } does not belong to\" f \" known matcher [exact, simstring].\" ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc nlp = nlp instance-attribute attr = attr instance-attribute phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , None = term_matcher_config or {}) instance-attribute regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded ) instance-attribute __init__ ( nlp , terms , regex , attr , ignore_excluded , term_matcher = GenericTermMatcher . exact , term_matcher_config = None ) Source code in edsnlp/pipelines/core/matcher/matcher.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , nlp : Language , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , term_matcher : GenericTermMatcher = GenericTermMatcher . exact , term_matcher_config : Dict [ str , Any ] = None , ): self . nlp = nlp self . attr = attr if term_matcher == GenericTermMatcher . exact : self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) elif term_matcher == GenericTermMatcher . simstring : self . phrase_matcher = SimstringMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) else : raise ValueError ( f \"Algorithm { repr ( term_matcher ) } does not belong to\" f \" known matcher [exact, simstring].\" ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () process ( doc ) Find matching spans in doc. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION spans List of Spans returned by the matchers. Source code in edsnlp/pipelines/core/matcher/matcher.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans __call__ ( doc ) Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/matcher/matcher.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"matcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlppipelinescorematchermatcher","text":"","title":"edsnlp.pipelines.core.matcher.matcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericTermMatcher","text":"Bases: str , Enum Source code in edsnlp/pipelines/core/matcher/matcher.py 15 16 17 class GenericTermMatcher ( str , Enum ): exact = \"exact\" simstring = \"simstring\"","title":"GenericTermMatcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericTermMatcher.exact","text":"","title":"exact"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericTermMatcher.simstring","text":"","title":"simstring"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher","text":"Bases: BaseComponent Provides a generic matcher component. PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language terms A dictionary of terms. TYPE: Optional[Patterns] regex A dictionary of regular expressions. TYPE: Optional[Patterns] attr The default attribute to use for matching. Can be overridden using the terms and regex configurations. TYPE: str ignore_excluded Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). TYPE: bool term_matcher The matcher to use for matching phrases ? One of (exact, simstring) term_matcher_config Parameters of the matcher class Source code in edsnlp/pipelines/core/matcher/matcher.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class GenericMatcher ( BaseComponent ): \"\"\" Provides a generic matcher component. Parameters ---------- nlp : Language The spaCy object. terms : Optional[Patterns] A dictionary of terms. regex : Optional[Patterns] A dictionary of regular expressions. attr : str The default attribute to use for matching. Can be overridden using the `terms` and `regex` configurations. ignore_excluded : bool Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). term_matcher: GenericTermMatcher The matcher to use for matching phrases ? One of (exact, simstring) term_matcher_config: Dict[str,Any] Parameters of the matcher class \"\"\" def __init__ ( self , nlp : Language , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , term_matcher : GenericTermMatcher = GenericTermMatcher . exact , term_matcher_config : Dict [ str , Any ] = None , ): self . nlp = nlp self . attr = attr if term_matcher == GenericTermMatcher . exact : self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) elif term_matcher == GenericTermMatcher . simstring : self . phrase_matcher = SimstringMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) else : raise ValueError ( f \"Algorithm { repr ( term_matcher ) } does not belong to\" f \" known matcher [exact, simstring].\" ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"GenericMatcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.attr","text":"","title":"attr"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.phrase_matcher","text":"","title":"phrase_matcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.regex_matcher","text":"","title":"regex_matcher"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.__init__","text":"Source code in edsnlp/pipelines/core/matcher/matcher.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , nlp : Language , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , term_matcher : GenericTermMatcher = GenericTermMatcher . exact , term_matcher_config : Dict [ str , Any ] = None , ): self . nlp = nlp self . attr = attr if term_matcher == GenericTermMatcher . exact : self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) elif term_matcher == GenericTermMatcher . simstring : self . phrase_matcher = SimstringMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) else : raise ValueError ( f \"Algorithm { repr ( term_matcher ) } does not belong to\" f \" known matcher [exact, simstring].\" ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.process","text":"Find matching spans in doc. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION spans List of Spans returned by the matchers. Source code in edsnlp/pipelines/core/matcher/matcher.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = list ( matches ) + list ( regex_matches ) return spans","title":"process()"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.__call__","text":"Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/matcher/matcher.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) for span in matches : if span . label_ not in doc . spans : doc . spans [ span . label_ ] = [] doc . spans [ span . label_ ] . append ( span ) ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/","text":"edsnlp.pipelines.core.normalizer excluded_or_space_getter ( t ) Source code in edsnlp/pipelines/core/normalizer/__init__.py 14 15 def excluded_or_space_getter ( t ): return t . is_space or t . tag_ == \"EXCLUDED\"","title":"`edsnlp.pipelines.core.normalizer`"},{"location":"reference/pipelines/core/normalizer/#edsnlppipelinescorenormalizer","text":"","title":"edsnlp.pipelines.core.normalizer"},{"location":"reference/pipelines/core/normalizer/#edsnlp.pipelines.core.normalizer.excluded_or_space_getter","text":"Source code in edsnlp/pipelines/core/normalizer/__init__.py 14 15 def excluded_or_space_getter ( t ): return t . is_space or t . tag_ == \"EXCLUDED\"","title":"excluded_or_space_getter()"},{"location":"reference/pipelines/core/normalizer/factory/","text":"edsnlp.pipelines.core.normalizer.factory DEFAULT_CONFIG = dict ( accents = True , lowercase = True , quotes = True , pollution = True ) module-attribute create_component ( nlp , name , accents , lowercase , quotes , pollution ) Source code in edsnlp/pipelines/core/normalizer/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @deprecated_factory ( \"normalizer\" , \"eds.normalizer\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" , \"token.tag\" ], ) @Language . factory ( \"eds.normalizer\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" , \"token.tag\" ] ) def create_component ( nlp : Language , name : str , accents : Union [ bool , Dict [ str , Any ]], lowercase : Union [ bool , Dict [ str , Any ]], quotes : Union [ bool , Dict [ str , Any ]], pollution : Union [ bool , Dict [ str , Any ]], ): if accents : config = dict ( ** accents_config ) if isinstance ( accents , dict ): config . update ( accents ) accents = registry . get ( \"factories\" , \"eds.accents\" )( nlp , \"eds.accents\" , ** config ) if quotes : config = dict ( ** quotes_config ) if isinstance ( quotes , dict ): config . update ( quotes ) quotes = registry . get ( \"factories\" , \"eds.quotes\" )( nlp , \"eds.quotes\" , ** config ) if pollution : config = dict ( ** pollution_config [ \"pollution\" ]) if isinstance ( pollution , dict ): config . update ( pollution ) pollution = registry . get ( \"factories\" , \"eds.pollution\" )( nlp , \"eds.pollution\" , pollution = config ) normalizer = Normalizer ( lowercase = lowercase , accents = accents or None , quotes = quotes or None , pollution = pollution or None , ) return normalizer","title":"factory"},{"location":"reference/pipelines/core/normalizer/factory/#edsnlppipelinescorenormalizerfactory","text":"","title":"edsnlp.pipelines.core.normalizer.factory"},{"location":"reference/pipelines/core/normalizer/factory/#edsnlp.pipelines.core.normalizer.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/normalizer/factory/#edsnlp.pipelines.core.normalizer.factory.create_component","text":"Source code in edsnlp/pipelines/core/normalizer/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @deprecated_factory ( \"normalizer\" , \"eds.normalizer\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" , \"token.tag\" ], ) @Language . factory ( \"eds.normalizer\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" , \"token.tag\" ] ) def create_component ( nlp : Language , name : str , accents : Union [ bool , Dict [ str , Any ]], lowercase : Union [ bool , Dict [ str , Any ]], quotes : Union [ bool , Dict [ str , Any ]], pollution : Union [ bool , Dict [ str , Any ]], ): if accents : config = dict ( ** accents_config ) if isinstance ( accents , dict ): config . update ( accents ) accents = registry . get ( \"factories\" , \"eds.accents\" )( nlp , \"eds.accents\" , ** config ) if quotes : config = dict ( ** quotes_config ) if isinstance ( quotes , dict ): config . update ( quotes ) quotes = registry . get ( \"factories\" , \"eds.quotes\" )( nlp , \"eds.quotes\" , ** config ) if pollution : config = dict ( ** pollution_config [ \"pollution\" ]) if isinstance ( pollution , dict ): config . update ( pollution ) pollution = registry . get ( \"factories\" , \"eds.pollution\" )( nlp , \"eds.pollution\" , pollution = config ) normalizer = Normalizer ( lowercase = lowercase , accents = accents or None , quotes = quotes or None , pollution = pollution or None , ) return normalizer","title":"create_component()"},{"location":"reference/pipelines/core/normalizer/normalizer/","text":"edsnlp.pipelines.core.normalizer.normalizer Normalizer Bases: object Normalisation pipeline. Modifies the NORM attribute, acting on four dimensions : lowercase : using the default NORM accents : deterministic and fixed-length normalisation of accents. quotes : deterministic and fixed-length normalisation of quotation marks. pollution : removal of pollutions. PARAMETER DESCRIPTION lowercase Whether to remove case. TYPE: bool accents Optional Accents object. TYPE: Optional[Accents] quotes Optional Quotes object. TYPE: Optional[Quotes] pollution Optional Pollution object. TYPE: Optional[Pollution] Source code in edsnlp/pipelines/core/normalizer/normalizer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class Normalizer ( object ): \"\"\" Normalisation pipeline. Modifies the `NORM` attribute, acting on four dimensions : - `lowercase`: using the default `NORM` - `accents`: deterministic and fixed-length normalisation of accents. - `quotes`: deterministic and fixed-length normalisation of quotation marks. - `pollution`: removal of pollutions. Parameters ---------- lowercase : bool Whether to remove case. accents : Optional[Accents] Optional `Accents` object. quotes : Optional[Quotes] Optional `Quotes` object. pollution : Optional[Pollution] Optional `Pollution` object. \"\"\" def __init__ ( self , lowercase : bool , accents : Optional [ Accents ], quotes : Optional [ Quotes ], pollution : Optional [ Pollution ], ): self . lowercase = lowercase self . accents = accents self . quotes = quotes self . pollution = pollution def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Apply the normalisation pipeline, one component at a time. Parameters ---------- doc : Doc spaCy `Doc` object Returns ------- Doc Doc object with `NORM` attribute modified \"\"\" if not self . lowercase : remove_lowercase ( doc ) if self . accents is not None : self . accents ( doc ) if self . quotes is not None : self . quotes ( doc ) if self . pollution is not None : self . pollution ( doc ) return doc lowercase = lowercase instance-attribute accents = accents instance-attribute quotes = quotes instance-attribute pollution = pollution instance-attribute __init__ ( lowercase , accents , quotes , pollution ) Source code in edsnlp/pipelines/core/normalizer/normalizer.py 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , lowercase : bool , accents : Optional [ Accents ], quotes : Optional [ Quotes ], pollution : Optional [ Pollution ], ): self . lowercase = lowercase self . accents = accents self . quotes = quotes self . pollution = pollution __call__ ( doc ) Apply the normalisation pipeline, one component at a time. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION Doc Doc object with NORM attribute modified Source code in edsnlp/pipelines/core/normalizer/normalizer.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Apply the normalisation pipeline, one component at a time. Parameters ---------- doc : Doc spaCy `Doc` object Returns ------- Doc Doc object with `NORM` attribute modified \"\"\" if not self . lowercase : remove_lowercase ( doc ) if self . accents is not None : self . accents ( doc ) if self . quotes is not None : self . quotes ( doc ) if self . pollution is not None : self . pollution ( doc ) return doc","title":"normalizer"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlppipelinescorenormalizernormalizer","text":"","title":"edsnlp.pipelines.core.normalizer.normalizer"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer","text":"Bases: object Normalisation pipeline. Modifies the NORM attribute, acting on four dimensions : lowercase : using the default NORM accents : deterministic and fixed-length normalisation of accents. quotes : deterministic and fixed-length normalisation of quotation marks. pollution : removal of pollutions. PARAMETER DESCRIPTION lowercase Whether to remove case. TYPE: bool accents Optional Accents object. TYPE: Optional[Accents] quotes Optional Quotes object. TYPE: Optional[Quotes] pollution Optional Pollution object. TYPE: Optional[Pollution] Source code in edsnlp/pipelines/core/normalizer/normalizer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class Normalizer ( object ): \"\"\" Normalisation pipeline. Modifies the `NORM` attribute, acting on four dimensions : - `lowercase`: using the default `NORM` - `accents`: deterministic and fixed-length normalisation of accents. - `quotes`: deterministic and fixed-length normalisation of quotation marks. - `pollution`: removal of pollutions. Parameters ---------- lowercase : bool Whether to remove case. accents : Optional[Accents] Optional `Accents` object. quotes : Optional[Quotes] Optional `Quotes` object. pollution : Optional[Pollution] Optional `Pollution` object. \"\"\" def __init__ ( self , lowercase : bool , accents : Optional [ Accents ], quotes : Optional [ Quotes ], pollution : Optional [ Pollution ], ): self . lowercase = lowercase self . accents = accents self . quotes = quotes self . pollution = pollution def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Apply the normalisation pipeline, one component at a time. Parameters ---------- doc : Doc spaCy `Doc` object Returns ------- Doc Doc object with `NORM` attribute modified \"\"\" if not self . lowercase : remove_lowercase ( doc ) if self . accents is not None : self . accents ( doc ) if self . quotes is not None : self . quotes ( doc ) if self . pollution is not None : self . pollution ( doc ) return doc","title":"Normalizer"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer.lowercase","text":"","title":"lowercase"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer.accents","text":"","title":"accents"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer.quotes","text":"","title":"quotes"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer.pollution","text":"","title":"pollution"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer.__init__","text":"Source code in edsnlp/pipelines/core/normalizer/normalizer.py 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , lowercase : bool , accents : Optional [ Accents ], quotes : Optional [ Quotes ], pollution : Optional [ Pollution ], ): self . lowercase = lowercase self . accents = accents self . quotes = quotes self . pollution = pollution","title":"__init__()"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer.__call__","text":"Apply the normalisation pipeline, one component at a time. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION Doc Doc object with NORM attribute modified Source code in edsnlp/pipelines/core/normalizer/normalizer.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Apply the normalisation pipeline, one component at a time. Parameters ---------- doc : Doc spaCy `Doc` object Returns ------- Doc Doc object with `NORM` attribute modified \"\"\" if not self . lowercase : remove_lowercase ( doc ) if self . accents is not None : self . accents ( doc ) if self . quotes is not None : self . quotes ( doc ) if self . pollution is not None : self . pollution ( doc ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/accents/","text":"edsnlp.pipelines.core.normalizer.accents","title":"`edsnlp.pipelines.core.normalizer.accents`"},{"location":"reference/pipelines/core/normalizer/accents/#edsnlppipelinescorenormalizeraccents","text":"","title":"edsnlp.pipelines.core.normalizer.accents"},{"location":"reference/pipelines/core/normalizer/accents/accents/","text":"edsnlp.pipelines.core.normalizer.accents.accents Accents Bases: object Normalises accents, using a same-length strategy. PARAMETER DESCRIPTION accents List of accentuated characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class Accents ( object ): \"\"\" Normalises accents, using a same-length strategy. Parameters ---------- accents : List[Tuple[str, str]] List of accentuated characters and their transcription. \"\"\" def __init__ ( self , accents : Optional [ List [ Tuple [ str , str ]]]) -> None : if accents is None : accents = patterns . accents self . translation_table = str . maketrans ( \"\" . join ( accent_group for accent_group , _ in accents ), \"\" . join ( rep * len ( accent_group ) for accent_group , rep in accents ), ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = token . norm_ . translate ( self . translation_table ) return doc translation_table = str . maketrans ( '' . join ( accent_group for ( accent_group , _ ) in accents ), '' . join ( rep * len ( accent_group ) for ( accent_group , rep ) in accents )) instance-attribute __init__ ( accents ) Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 18 19 20 21 22 23 24 25 def __init__ ( self , accents : Optional [ List [ Tuple [ str , str ]]]) -> None : if accents is None : accents = patterns . accents self . translation_table = str . maketrans ( \"\" . join ( accent_group for accent_group , _ in accents ), \"\" . join ( rep * len ( accent_group ) for accent_group , rep in accents ), ) __call__ ( doc ) Remove accents from spacy NORM attribute. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with accents removed in Token.norm_ . Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = token . norm_ . translate ( self . translation_table ) return doc","title":"accents"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlppipelinescorenormalizeraccentsaccents","text":"","title":"edsnlp.pipelines.core.normalizer.accents.accents"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents","text":"Bases: object Normalises accents, using a same-length strategy. PARAMETER DESCRIPTION accents List of accentuated characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class Accents ( object ): \"\"\" Normalises accents, using a same-length strategy. Parameters ---------- accents : List[Tuple[str, str]] List of accentuated characters and their transcription. \"\"\" def __init__ ( self , accents : Optional [ List [ Tuple [ str , str ]]]) -> None : if accents is None : accents = patterns . accents self . translation_table = str . maketrans ( \"\" . join ( accent_group for accent_group , _ in accents ), \"\" . join ( rep * len ( accent_group ) for accent_group , rep in accents ), ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = token . norm_ . translate ( self . translation_table ) return doc","title":"Accents"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents.translation_table","text":"","title":"translation_table"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents.__init__","text":"Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 18 19 20 21 22 23 24 25 def __init__ ( self , accents : Optional [ List [ Tuple [ str , str ]]]) -> None : if accents is None : accents = patterns . accents self . translation_table = str . maketrans ( \"\" . join ( accent_group for accent_group , _ in accents ), \"\" . join ( rep * len ( accent_group ) for accent_group , rep in accents ), )","title":"__init__()"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents.__call__","text":"Remove accents from spacy NORM attribute. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with accents removed in Token.norm_ . Source code in edsnlp/pipelines/core/normalizer/accents/accents.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Remove accents from spacy `NORM` attribute. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with accents removed in `Token.norm_`. \"\"\" for token in doc : token . norm_ = token . norm_ . translate ( self . translation_table ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/accents/factory/","text":"edsnlp.pipelines.core.normalizer.accents.factory DEFAULT_CONFIG = dict ( accents = None ) module-attribute create_component ( nlp , name , accents ) Source code in edsnlp/pipelines/core/normalizer/accents/factory.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @deprecated_factory ( \"accents\" , \"eds.accents\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" ] ) @Language . factory ( \"eds.accents\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" ], ) def create_component ( nlp : Language , name : str , accents : Optional [ List [ Tuple [ str , str ]]], ): return Accents ( accents = accents , )","title":"factory"},{"location":"reference/pipelines/core/normalizer/accents/factory/#edsnlppipelinescorenormalizeraccentsfactory","text":"","title":"edsnlp.pipelines.core.normalizer.accents.factory"},{"location":"reference/pipelines/core/normalizer/accents/factory/#edsnlp.pipelines.core.normalizer.accents.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/normalizer/accents/factory/#edsnlp.pipelines.core.normalizer.accents.factory.create_component","text":"Source code in edsnlp/pipelines/core/normalizer/accents/factory.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @deprecated_factory ( \"accents\" , \"eds.accents\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" ] ) @Language . factory ( \"eds.accents\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" ], ) def create_component ( nlp : Language , name : str , accents : Optional [ List [ Tuple [ str , str ]]], ): return Accents ( accents = accents , )","title":"create_component()"},{"location":"reference/pipelines/core/normalizer/accents/patterns/","text":"edsnlp.pipelines.core.normalizer.accents.patterns accents : List [ Tuple [ str , str ]] = [( '\u00e7' , 'c' ), ( '\u00e0\u00e1\u00e2\u00e4' , 'a' ), ( '\u00e8\u00e9\u00ea\u00eb' , 'e' ), ( '\u00ec\u00ed\u00ee\u00ef' , 'i' ), ( '\u00f2\u00f3\u00f4\u00f6' , 'o' ), ( '\u00f9\u00fa\u00fb\u00fc' , 'u' )] module-attribute","title":"patterns"},{"location":"reference/pipelines/core/normalizer/accents/patterns/#edsnlppipelinescorenormalizeraccentspatterns","text":"","title":"edsnlp.pipelines.core.normalizer.accents.patterns"},{"location":"reference/pipelines/core/normalizer/accents/patterns/#edsnlp.pipelines.core.normalizer.accents.patterns.accents","text":"","title":"accents"},{"location":"reference/pipelines/core/normalizer/lowercase/","text":"edsnlp.pipelines.core.normalizer.lowercase","title":"`edsnlp.pipelines.core.normalizer.lowercase`"},{"location":"reference/pipelines/core/normalizer/lowercase/#edsnlppipelinescorenormalizerlowercase","text":"","title":"edsnlp.pipelines.core.normalizer.lowercase"},{"location":"reference/pipelines/core/normalizer/lowercase/factory/","text":"edsnlp.pipelines.core.normalizer.lowercase.factory remove_lowercase ( doc ) Add case on the NORM custom attribute. Should always be applied first. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with case put back in NORM . Source code in edsnlp/pipelines/core/normalizer/lowercase/factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Language . component ( \"remove-lowercase\" , assigns = [ \"token.norm\" ]) @Language . component ( \"eds.remove-lowercase\" , assigns = [ \"token.norm\" ]) def remove_lowercase ( doc : Doc ): \"\"\" Add case on the `NORM` custom attribute. Should always be applied first. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with case put back in `NORM`. \"\"\" for token in doc : token . norm_ = token . text return doc","title":"factory"},{"location":"reference/pipelines/core/normalizer/lowercase/factory/#edsnlppipelinescorenormalizerlowercasefactory","text":"","title":"edsnlp.pipelines.core.normalizer.lowercase.factory"},{"location":"reference/pipelines/core/normalizer/lowercase/factory/#edsnlp.pipelines.core.normalizer.lowercase.factory.remove_lowercase","text":"Add case on the NORM custom attribute. Should always be applied first. PARAMETER DESCRIPTION doc The spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION Doc The document, with case put back in NORM . Source code in edsnlp/pipelines/core/normalizer/lowercase/factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Language . component ( \"remove-lowercase\" , assigns = [ \"token.norm\" ]) @Language . component ( \"eds.remove-lowercase\" , assigns = [ \"token.norm\" ]) def remove_lowercase ( doc : Doc ): \"\"\" Add case on the `NORM` custom attribute. Should always be applied first. Parameters ---------- doc : Doc The spaCy `Doc` object. Returns ------- Doc The document, with case put back in `NORM`. \"\"\" for token in doc : token . norm_ = token . text return doc","title":"remove_lowercase()"},{"location":"reference/pipelines/core/normalizer/pollution/","text":"edsnlp.pipelines.core.normalizer.pollution","title":"`edsnlp.pipelines.core.normalizer.pollution`"},{"location":"reference/pipelines/core/normalizer/pollution/#edsnlppipelinescorenormalizerpollution","text":"","title":"edsnlp.pipelines.core.normalizer.pollution"},{"location":"reference/pipelines/core/normalizer/pollution/factory/","text":"edsnlp.pipelines.core.normalizer.pollution.factory DEFAULT_CONFIG = dict ( pollution = dict ( information = True , bars = True , biology = False , doctors = True , web = True , coding = False , footer = True )) module-attribute create_component ( nlp , name , pollution ) Source code in edsnlp/pipelines/core/normalizer/pollution/factory.py 22 23 24 25 26 27 28 29 30 31 32 33 34 @deprecated_factory ( \"pollution\" , \"eds.pollution\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.tag\" ] ) @Language . factory ( \"eds.pollution\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.tag\" ]) def create_component ( nlp : Language , name : str , pollution : Optional [ Dict [ str , Union [ bool , str , List [ str ]]]], ): return Pollution ( nlp , pollution = pollution , )","title":"factory"},{"location":"reference/pipelines/core/normalizer/pollution/factory/#edsnlppipelinescorenormalizerpollutionfactory","text":"","title":"edsnlp.pipelines.core.normalizer.pollution.factory"},{"location":"reference/pipelines/core/normalizer/pollution/factory/#edsnlp.pipelines.core.normalizer.pollution.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/normalizer/pollution/factory/#edsnlp.pipelines.core.normalizer.pollution.factory.create_component","text":"Source code in edsnlp/pipelines/core/normalizer/pollution/factory.py 22 23 24 25 26 27 28 29 30 31 32 33 34 @deprecated_factory ( \"pollution\" , \"eds.pollution\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.tag\" ] ) @Language . factory ( \"eds.pollution\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.tag\" ]) def create_component ( nlp : Language , name : str , pollution : Optional [ Dict [ str , Union [ bool , str , List [ str ]]]], ): return Pollution ( nlp , pollution = pollution , )","title":"create_component()"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/","text":"edsnlp.pipelines.core.normalizer.pollution.patterns information = \"(?s)(=====+ \\\\ s*)?(L \\\\ s*e \\\\ s*s \\\\ sdonn\u00e9es \\\\ s*administratives, \\\\ s*sociales \\\\ s*|I?nfo \\\\ s*rmation \\\\ s*aux? \\\\ s*patients?|L[\u2019']AP-HP \\\\ s*collecte \\\\ s*vos \\\\ s*donn\u00e9es \\\\ s*administratives|L[\u2019']Assistance \\\\ s*Publique \\\\ s*- \\\\ s*H\u00f4pitaux \\\\ s*de \\\\ s*Paris \\\\ s* \\\\ (?AP-HP \\\\ )? \\\\ s*a \\\\ s*cr\u00e9\u00e9 \\\\ s*une \\\\ s*base \\\\ s*de \\\\ s*donn\u00e9es).{,2000}https?: \\\\ / \\\\ /recherche \\\\ .aphp \\\\ .fr \\\\ /eds \\\\ /droit-opposition[ \\\\ s \\\\ .]*\" module-attribute bars = '(?i)([nbw]|_|-|=){5,}' module-attribute biology = '( \\\\ b.*[|\u00a6].* \\\\ n)+' module-attribute doctors = '(?mi)(^((dr)|(pr))( \\\\ .| \\\\ s|of).*)+' module-attribute web = '(www \\\\ . \\\\ S*)|( \\\\ S*@ \\\\ S*)' module-attribute coding = '.*?[a-zA-Z] \\\\ d{2,4}.*?( \\\\ n|[a-zA-Z] \\\\ d{2,4})' module-attribute date = ' \\\\ b \\\\ d \\\\ d/ \\\\ d \\\\ d/ \\\\ d \\\\ d \\\\ d \\\\ d \\\\ b' module-attribute ipp = '80 \\\\ d {8} ' module-attribute page = '((^ \\\\ d \\\\ / \\\\ d \\\\ s?)|(^ \\\\ d \\\\ d? \\\\ / \\\\ d \\\\ d \\\\ ?))' module-attribute footer = '(?i)( {page} .* \\\\ n?pat.*(ipp)?.* \\\\ n?(courrier valid.*)?)' module-attribute pollution = dict ( information = information , bars = bars , biology = biology , doctors = doctors , web = web , coding = coding , footer = footer ) module-attribute","title":"patterns"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlppipelinescorenormalizerpollutionpatterns","text":"","title":"edsnlp.pipelines.core.normalizer.pollution.patterns"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.information","text":"","title":"information"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.bars","text":"","title":"bars"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.biology","text":"","title":"biology"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.doctors","text":"","title":"doctors"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.web","text":"","title":"web"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.coding","text":"","title":"coding"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.date","text":"","title":"date"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.ipp","text":"","title":"ipp"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.page","text":"","title":"page"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.footer","text":"","title":"footer"},{"location":"reference/pipelines/core/normalizer/pollution/patterns/#edsnlp.pipelines.core.normalizer.pollution.patterns.pollution","text":"","title":"pollution"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/","text":"edsnlp.pipelines.core.normalizer.pollution.pollution Pollution Bases: BaseComponent Tags pollution tokens. Populates a number of spaCy extensions : Token._.pollution : indicates whether the token is a pollution Doc._.clean : lists non-pollution tokens Doc._.clean_ : original text with pollutions removed. Doc._.char_clean_span : method to create a Span using character indices extracted using the cleaned text. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language pollution Dictionary containing regular expressions of pollution. TYPE: Dict[str, Union[str, List[str]]] Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class Pollution ( BaseComponent ): \"\"\" Tags pollution tokens. Populates a number of spaCy extensions : - `Token._.pollution` : indicates whether the token is a pollution - `Doc._.clean` : lists non-pollution tokens - `Doc._.clean_` : original text with pollutions removed. - `Doc._.char_clean_span` : method to create a Span using character indices extracted using the cleaned text. Parameters ---------- nlp : Language Language pipeline object pollution : Dict[str, Union[str, List[str]]] Dictionary containing regular expressions of pollution. \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , pollution : Optional [ Dict [ str , Union [ bool , str , List [ str ]]]], ): self . nlp = nlp self . nlp . vocab . strings . add ( \"EXCLUDED\" ) if pollution is None : pollution = { k : True for k in patterns . pollution . keys ()} self . pollution = dict () for k , v in pollution . items (): if v is True : self . pollution [ k ] = [ patterns . pollution [ k ]] elif isinstance ( v , str ): self . pollution [ k ] = [ v ] elif isinstance ( v , list ): self . pollution [ k ] = v self . regex_matcher = RegexMatcher ( flags = re . MULTILINE ) self . build_patterns () def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" excluded_hash = doc . vocab . strings [ \"EXCLUDED\" ] pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True token . tag = excluded_hash doc . spans [ \"pollutions\" ] = pollutions return doc nlp = nlp instance-attribute pollution = dict () instance-attribute regex_matcher = RegexMatcher ( flags = re . MULTILINE ) instance-attribute __init__ ( nlp , pollution ) Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , nlp : Language , pollution : Optional [ Dict [ str , Union [ bool , str , List [ str ]]]], ): self . nlp = nlp self . nlp . vocab . strings . add ( \"EXCLUDED\" ) if pollution is None : pollution = { k : True for k in patterns . pollution . keys ()} self . pollution = dict () for k , v in pollution . items (): if v is True : self . pollution [ k ] = [ patterns . pollution [ k ]] elif isinstance ( v , str ): self . pollution [ k ] = [ v ] elif isinstance ( v , list ): self . pollution [ k ] = v self . regex_matcher = RegexMatcher ( flags = re . MULTILINE ) self . build_patterns () build_patterns () Builds the patterns for phrase matching. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 59 60 61 62 63 64 65 66 def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) process ( doc ) Find pollutions in doc and clean candidate negations to remove pseudo negations PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION pollution list of pollution spans Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions __call__ ( doc ) Tags pollutions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for pollutions. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" excluded_hash = doc . vocab . strings [ \"EXCLUDED\" ] pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True token . tag = excluded_hash doc . spans [ \"pollutions\" ] = pollutions return doc","title":"pollution"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlppipelinescorenormalizerpollutionpollution","text":"","title":"edsnlp.pipelines.core.normalizer.pollution.pollution"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution","text":"Bases: BaseComponent Tags pollution tokens. Populates a number of spaCy extensions : Token._.pollution : indicates whether the token is a pollution Doc._.clean : lists non-pollution tokens Doc._.clean_ : original text with pollutions removed. Doc._.char_clean_span : method to create a Span using character indices extracted using the cleaned text. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language pollution Dictionary containing regular expressions of pollution. TYPE: Dict[str, Union[str, List[str]]] Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class Pollution ( BaseComponent ): \"\"\" Tags pollution tokens. Populates a number of spaCy extensions : - `Token._.pollution` : indicates whether the token is a pollution - `Doc._.clean` : lists non-pollution tokens - `Doc._.clean_` : original text with pollutions removed. - `Doc._.char_clean_span` : method to create a Span using character indices extracted using the cleaned text. Parameters ---------- nlp : Language Language pipeline object pollution : Dict[str, Union[str, List[str]]] Dictionary containing regular expressions of pollution. \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , pollution : Optional [ Dict [ str , Union [ bool , str , List [ str ]]]], ): self . nlp = nlp self . nlp . vocab . strings . add ( \"EXCLUDED\" ) if pollution is None : pollution = { k : True for k in patterns . pollution . keys ()} self . pollution = dict () for k , v in pollution . items (): if v is True : self . pollution [ k ] = [ patterns . pollution [ k ]] elif isinstance ( v , str ): self . pollution [ k ] = [ v ] elif isinstance ( v , list ): self . pollution [ k ] = v self . regex_matcher = RegexMatcher ( flags = re . MULTILINE ) self . build_patterns () def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" excluded_hash = doc . vocab . strings [ \"EXCLUDED\" ] pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True token . tag = excluded_hash doc . spans [ \"pollutions\" ] = pollutions return doc","title":"Pollution"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.pollution","text":"","title":"pollution"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.regex_matcher","text":"","title":"regex_matcher"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.__init__","text":"Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , nlp : Language , pollution : Optional [ Dict [ str , Union [ bool , str , List [ str ]]]], ): self . nlp = nlp self . nlp . vocab . strings . add ( \"EXCLUDED\" ) if pollution is None : pollution = { k : True for k in patterns . pollution . keys ()} self . pollution = dict () for k , v in pollution . items (): if v is True : self . pollution [ k ] = [ patterns . pollution [ k ]] elif isinstance ( v , str ): self . pollution [ k ] = [ v ] elif isinstance ( v , list ): self . pollution [ k ] = v self . regex_matcher = RegexMatcher ( flags = re . MULTILINE ) self . build_patterns ()","title":"__init__()"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.build_patterns","text":"Builds the patterns for phrase matching. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 59 60 61 62 63 64 65 66 def build_patterns ( self ) -> None : \"\"\" Builds the patterns for phrase matching. \"\"\" # efficiently build spaCy matcher patterns for k , v in self . pollution . items (): self . regex_matcher . add ( k , v )","title":"build_patterns()"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.process","text":"Find pollutions in doc and clean candidate negations to remove pseudo negations PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION pollution list of pollution spans Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find pollutions in doc and clean candidate negations to remove pseudo negations Parameters ---------- doc: spaCy Doc object Returns ------- pollution: list of pollution spans \"\"\" pollutions = self . regex_matcher ( doc , as_spans = True ) pollutions = filter_spans ( pollutions ) return pollutions","title":"process()"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.__call__","text":"Tags pollutions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for pollutions. Source code in edsnlp/pipelines/core/normalizer/pollution/pollution.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags pollutions. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for pollutions. \"\"\" excluded_hash = doc . vocab . strings [ \"EXCLUDED\" ] pollutions = self . process ( doc ) for pollution in pollutions : for token in pollution : token . _ . excluded = True token . tag = excluded_hash doc . spans [ \"pollutions\" ] = pollutions return doc","title":"__call__()"},{"location":"reference/pipelines/core/normalizer/quotes/","text":"edsnlp.pipelines.core.normalizer.quotes","title":"`edsnlp.pipelines.core.normalizer.quotes`"},{"location":"reference/pipelines/core/normalizer/quotes/#edsnlppipelinescorenormalizerquotes","text":"","title":"edsnlp.pipelines.core.normalizer.quotes"},{"location":"reference/pipelines/core/normalizer/quotes/factory/","text":"edsnlp.pipelines.core.normalizer.quotes.factory DEFAULT_CONFIG = dict ( quotes = None ) module-attribute create_component ( nlp , name , quotes ) Source code in edsnlp/pipelines/core/normalizer/quotes/factory.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @deprecated_factory ( \"quotes\" , \"eds.quotes\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" ] ) @Language . factory ( \"eds.quotes\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" ], ) def create_component ( nlp : Language , name : str , quotes : Optional [ List [ Tuple [ str , str ]]], ): return Quotes ( quotes = quotes , )","title":"factory"},{"location":"reference/pipelines/core/normalizer/quotes/factory/#edsnlppipelinescorenormalizerquotesfactory","text":"","title":"edsnlp.pipelines.core.normalizer.quotes.factory"},{"location":"reference/pipelines/core/normalizer/quotes/factory/#edsnlp.pipelines.core.normalizer.quotes.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/normalizer/quotes/factory/#edsnlp.pipelines.core.normalizer.quotes.factory.create_component","text":"Source code in edsnlp/pipelines/core/normalizer/quotes/factory.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @deprecated_factory ( \"quotes\" , \"eds.quotes\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" ] ) @Language . factory ( \"eds.quotes\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.norm\" ], ) def create_component ( nlp : Language , name : str , quotes : Optional [ List [ Tuple [ str , str ]]], ): return Quotes ( quotes = quotes , )","title":"create_component()"},{"location":"reference/pipelines/core/normalizer/quotes/patterns/","text":"edsnlp.pipelines.core.normalizer.quotes.patterns quotes : List [ str ] = [ '\uff02' , '\u3003' , '\u05f2' , '\u1cd3' , '\u2033' , '\u05f4' , '\u2036' , '\u02f6' , '\u02ba' , '\u201c' , '\u201d' , '\u02dd' , '\u201f' ] module-attribute apostrophes : List [ str ] = [ '\uff40' , '\u0384' , '\uff07' , '\u02c8' , '\u02ca' , '\u144a' , '\u02cb' , '\ua78c' , '\u16cc' , '\ud81b\udf52' , '\ud81b\udf51' , '\u2018' , '\u2019' , '\u05d9' , '\u055a' , '\u201b' , '\u055d' , '`' , '\u1fef' , '\u2032' , '\u05f3' , '\u00b4' , '\u0374' , '\u02f4' , '\u07f4' , '\u2035' , '\u07f5' , '\u02b9' , '\u02bb' , '\u02bc' , '\u1ffd' , '\u1fbd' , '\u02bd' , '\u1ffe' , '\u02be' , '\u1fbf' ] module-attribute quotes_and_apostrophes : List [ Tuple [ str , str ]] = [( '' . join ( quotes ), '\"' ), ( '' . join ( apostrophes ), \"'\" )] module-attribute","title":"patterns"},{"location":"reference/pipelines/core/normalizer/quotes/patterns/#edsnlppipelinescorenormalizerquotespatterns","text":"","title":"edsnlp.pipelines.core.normalizer.quotes.patterns"},{"location":"reference/pipelines/core/normalizer/quotes/patterns/#edsnlp.pipelines.core.normalizer.quotes.patterns.quotes","text":"","title":"quotes"},{"location":"reference/pipelines/core/normalizer/quotes/patterns/#edsnlp.pipelines.core.normalizer.quotes.patterns.apostrophes","text":"","title":"apostrophes"},{"location":"reference/pipelines/core/normalizer/quotes/patterns/#edsnlp.pipelines.core.normalizer.quotes.patterns.quotes_and_apostrophes","text":"","title":"quotes_and_apostrophes"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/","text":"edsnlp.pipelines.core.normalizer.quotes.quotes Quotes Bases: object We normalise quotes, following this source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html> _. PARAMETER DESCRIPTION quotes List of quotation characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class Quotes ( object ): \"\"\" We normalise quotes, following this `source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html>`_. Parameters ---------- quotes : List[Tuple[str, str]] List of quotation characters and their transcription. \"\"\" def __init__ ( self , quotes : Optional [ List [ Tuple [ str , str ]]]) -> None : if quotes is None : quotes = quotes_and_apostrophes self . translation_table = str . maketrans ( \"\" . join ( quote_group for quote_group , _ in quotes ), \"\" . join ( rep * len ( quote_group ) for quote_group , rep in quotes ), ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = token . norm_ . translate ( self . translation_table ) return doc translation_table = str . maketrans ( '' . join ( quote_group for ( quote_group , _ ) in quotes ), '' . join ( rep * len ( quote_group ) for ( quote_group , rep ) in quotes )) instance-attribute __init__ ( quotes ) Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 19 20 21 22 23 24 25 26 def __init__ ( self , quotes : Optional [ List [ Tuple [ str , str ]]]) -> None : if quotes is None : quotes = quotes_and_apostrophes self . translation_table = str . maketrans ( \"\" . join ( quote_group for quote_group , _ in quotes ), \"\" . join ( rep * len ( quote_group ) for quote_group , rep in quotes ), ) __call__ ( doc ) Normalises quotes. PARAMETER DESCRIPTION doc Document to process. TYPE: Doc RETURNS DESCRIPTION Doc Same document, with quotes normalised. Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = token . norm_ . translate ( self . translation_table ) return doc","title":"quotes"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlppipelinescorenormalizerquotesquotes","text":"","title":"edsnlp.pipelines.core.normalizer.quotes.quotes"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes","text":"Bases: object We normalise quotes, following this source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html> _. PARAMETER DESCRIPTION quotes List of quotation characters and their transcription. TYPE: List[Tuple[str, str]] Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class Quotes ( object ): \"\"\" We normalise quotes, following this `source <https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html>`_. Parameters ---------- quotes : List[Tuple[str, str]] List of quotation characters and their transcription. \"\"\" def __init__ ( self , quotes : Optional [ List [ Tuple [ str , str ]]]) -> None : if quotes is None : quotes = quotes_and_apostrophes self . translation_table = str . maketrans ( \"\" . join ( quote_group for quote_group , _ in quotes ), \"\" . join ( rep * len ( quote_group ) for quote_group , rep in quotes ), ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = token . norm_ . translate ( self . translation_table ) return doc","title":"Quotes"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes.translation_table","text":"","title":"translation_table"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes.__init__","text":"Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 19 20 21 22 23 24 25 26 def __init__ ( self , quotes : Optional [ List [ Tuple [ str , str ]]]) -> None : if quotes is None : quotes = quotes_and_apostrophes self . translation_table = str . maketrans ( \"\" . join ( quote_group for quote_group , _ in quotes ), \"\" . join ( rep * len ( quote_group ) for quote_group , rep in quotes ), )","title":"__init__()"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes.__call__","text":"Normalises quotes. PARAMETER DESCRIPTION doc Document to process. TYPE: Doc RETURNS DESCRIPTION Doc Same document, with quotes normalised. Source code in edsnlp/pipelines/core/normalizer/quotes/quotes.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Normalises quotes. Parameters ---------- doc : Doc Document to process. Returns ------- Doc Same document, with quotes normalised. \"\"\" for token in doc : token . norm_ = token . norm_ . translate ( self . translation_table ) return doc","title":"__call__()"},{"location":"reference/pipelines/core/sentences/","text":"edsnlp.pipelines.core.sentences","title":"`edsnlp.pipelines.core.sentences`"},{"location":"reference/pipelines/core/sentences/#edsnlppipelinescoresentences","text":"","title":"edsnlp.pipelines.core.sentences"},{"location":"reference/pipelines/core/sentences/factory/","text":"edsnlp.pipelines.core.sentences.factory DEFAULT_CONFIG = dict ( punct_chars = None , ignore_excluded = True , use_endlines = None ) module-attribute create_component ( nlp , name , punct_chars , use_endlines , ignore_excluded ) Source code in edsnlp/pipelines/core/sentences/factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @deprecated_factory ( \"sentences\" , \"eds.sentences\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.is_sent_start\" ], ) @Language . factory ( \"eds.sentences\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.is_sent_start\" ], ) def create_component ( nlp : Language , name : str , punct_chars : Optional [ List [ str ]], use_endlines : Optional [ bool ], ignore_excluded : bool , ): return SentenceSegmenter ( nlp . vocab , punct_chars = punct_chars , use_endlines = use_endlines , ignore_excluded = ignore_excluded , )","title":"factory"},{"location":"reference/pipelines/core/sentences/factory/#edsnlppipelinescoresentencesfactory","text":"","title":"edsnlp.pipelines.core.sentences.factory"},{"location":"reference/pipelines/core/sentences/factory/#edsnlp.pipelines.core.sentences.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/sentences/factory/#edsnlp.pipelines.core.sentences.factory.create_component","text":"Source code in edsnlp/pipelines/core/sentences/factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @deprecated_factory ( \"sentences\" , \"eds.sentences\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.is_sent_start\" ], ) @Language . factory ( \"eds.sentences\" , default_config = DEFAULT_CONFIG , assigns = [ \"token.is_sent_start\" ], ) def create_component ( nlp : Language , name : str , punct_chars : Optional [ List [ str ]], use_endlines : Optional [ bool ], ignore_excluded : bool , ): return SentenceSegmenter ( nlp . vocab , punct_chars = punct_chars , use_endlines = use_endlines , ignore_excluded = ignore_excluded , )","title":"create_component()"},{"location":"reference/pipelines/core/sentences/terms/","text":"edsnlp.pipelines.core.sentences.terms punctuation = [ '!' , '.' , '?' , '\u0589' , '\u061f' , '\u06d4' , '\u0700' , '\u0701' , '\u0702' , '\u07f9' , '\u0964' , '\u0965' , '\u104a' , '\u104b' , '\u1362' , '\u1367' , '\u1368' , '\u166e' , '\u1735' , '\u1736' , '\u1803' , '\u1809' , '\u1944' , '\u1945' , '\u1aa8' , '\u1aa9' , '\u1aaa' , '\u1aab' , '\u1b5a' , '\u1b5b' , '\u1b5e' , '\u1b5f' , '\u1c3b' , '\u1c3c' , '\u1c7e' , '\u1c7f' , '\u203c' , '\u203d' , '\u2047' , '\u2048' , '\u2049' , '\u2e2e' , '\u2e3c' , '\ua4ff' , '\ua60e' , '\ua60f' , '\ua6f3' , '\ua6f7' , '\ua876' , '\ua877' , '\ua8ce' , '\ua8cf' , '\ua92f' , '\ua9c8' , '\ua9c9' , '\uaa5d' , '\uaa5e' , '\uaa5f' , '\uaaf0' , '\uaaf1' , '\uabeb' , '\ufe52' , '\ufe56' , '\ufe57' , '\uff01' , '\uff0e' , '\uff1f' , '\ud802\ude56' , '\ud802\ude57' , '\ud804\udc47' , '\ud804\udc48' , '\ud804\udcbe' , '\ud804\udcbf' , '\ud804\udcc0' , '\ud804\udcc1' , '\ud804\udd41' , '\ud804\udd42' , '\ud804\udd43' , '\ud804\uddc5' , '\ud804\uddc6' , '\ud804\uddcd' , '\ud804\uddde' , '\ud804\udddf' , '\ud804\ude38' , '\ud804\ude39' , '\ud804\ude3b' , '\ud804\ude3c' , '\ud804\udea9' , '\ud805\udc4b' , '\ud805\udc4c' , '\ud805\uddc2' , '\ud805\uddc3' , '\ud805\uddc9' , '\ud805\uddca' , '\ud805\uddcb' , '\ud805\uddcc' , '\ud805\uddcd' , '\ud805\uddce' , '\ud805\uddcf' , '\ud805\uddd0' , '\ud805\uddd1' , '\ud805\uddd2' , '\ud805\uddd3' , '\ud805\uddd4' , '\ud805\uddd5' , '\ud805\uddd6' , '\ud805\uddd7' , '\ud805\ude41' , '\ud805\ude42' , '\ud805\udf3c' , '\ud805\udf3d' , '\ud805\udf3e' , '\ud806\ude42' , '\ud806\ude43' , '\ud806\ude9b' , '\ud806\ude9c' , '\ud807\udc41' , '\ud807\udc42' , '\ud81a\ude6e' , '\ud81a\ude6f' , '\ud81a\udef5' , '\ud81a\udf37' , '\ud81a\udf38' , '\ud81a\udf44' , '\ud82f\udc9f' , '\ud836\ude88' , '\uff61' , '\u3002' ] module-attribute","title":"terms"},{"location":"reference/pipelines/core/sentences/terms/#edsnlppipelinescoresentencesterms","text":"","title":"edsnlp.pipelines.core.sentences.terms"},{"location":"reference/pipelines/core/sentences/terms/#edsnlp.pipelines.core.sentences.terms.punctuation","text":"","title":"punctuation"},{"location":"reference/pipelines/core/terminology/","text":"edsnlp.pipelines.core.terminology","title":"`edsnlp.pipelines.core.terminology`"},{"location":"reference/pipelines/core/terminology/#edsnlppipelinescoreterminology","text":"","title":"edsnlp.pipelines.core.terminology"},{"location":"reference/pipelines/core/terminology/factory/","text":"edsnlp.pipelines.core.terminology.factory DEFAULT_CONFIG = dict ( terms = None , regex = None , attr = 'TEXT' , ignore_excluded = False , term_matcher = 'exact' , term_matcher_config = {}) module-attribute create_component ( nlp , name , label , terms , attr , regex , ignore_excluded , term_matcher , term_matcher_config ) Source code in edsnlp/pipelines/core/terminology/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @Language . factory ( \"eds.terminology\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , label : str , terms : Optional [ Dict [ str , Union [ str , List [ str ]]]], attr : Union [ str , Dict [ str , str ]], regex : Optional [ Dict [ str , Union [ str , List [ str ]]]], ignore_excluded : bool , term_matcher : TerminologyTermMatcher , term_matcher_config : Dict [ str , Any ], ): assert not ( terms is None and regex is None ) if terms is None : terms = dict () if regex is None : regex = dict () return TerminologyMatcher ( nlp , label = label , terms = terms , attr = attr , regex = regex , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"factory"},{"location":"reference/pipelines/core/terminology/factory/#edsnlppipelinescoreterminologyfactory","text":"","title":"edsnlp.pipelines.core.terminology.factory"},{"location":"reference/pipelines/core/terminology/factory/#edsnlp.pipelines.core.terminology.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/core/terminology/factory/#edsnlp.pipelines.core.terminology.factory.create_component","text":"Source code in edsnlp/pipelines/core/terminology/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @Language . factory ( \"eds.terminology\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , label : str , terms : Optional [ Dict [ str , Union [ str , List [ str ]]]], attr : Union [ str , Dict [ str , str ]], regex : Optional [ Dict [ str , Union [ str , List [ str ]]]], ignore_excluded : bool , term_matcher : TerminologyTermMatcher , term_matcher_config : Dict [ str , Any ], ): assert not ( terms is None and regex is None ) if terms is None : terms = dict () if regex is None : regex = dict () return TerminologyMatcher ( nlp , label = label , terms = terms , attr = attr , regex = regex , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"create_component()"},{"location":"reference/pipelines/core/terminology/terminology/","text":"edsnlp.pipelines.core.terminology.terminology TerminologyTermMatcher Bases: str , Enum Source code in edsnlp/pipelines/core/terminology/terminology.py 16 17 18 class TerminologyTermMatcher ( str , Enum ): exact = \"exact\" simstring = \"simstring\" exact = 'exact' class-attribute simstring = 'simstring' class-attribute TerminologyMatcher Bases: BaseComponent Provides a terminology matching component. The terminology matching component differs from the simple matcher component in that the regex and terms keys are used as spaCy's kb_id . All matched entities have the same label, defined in the top-level constructor (argument label ). PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language label Top-level label TYPE: str terms A dictionary of terms. TYPE: Optional[Patterns] regex A dictionary of regular expressions. TYPE: Optional[Patterns] attr The default attribute to use for matching. Can be overridden using the terms and regex configurations. TYPE: str ignore_excluded Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). TYPE: bool term_matcher The matcher to use for matching phrases ? One of (exact, simstring) term_matcher_config Parameters of the matcher class Source code in edsnlp/pipelines/core/terminology/terminology.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class TerminologyMatcher ( BaseComponent ): \"\"\" Provides a terminology matching component. The terminology matching component differs from the simple matcher component in that the `regex` and `terms` keys are used as spaCy's `kb_id`. All matched entities have the same label, defined in the top-level constructor (argument `label`). Parameters ---------- nlp : Language The spaCy object. label : str Top-level label terms : Optional[Patterns] A dictionary of terms. regex : Optional[Patterns] A dictionary of regular expressions. attr : str The default attribute to use for matching. Can be overridden using the `terms` and `regex` configurations. ignore_excluded : bool Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). term_matcher: TerminologyTermMatcher The matcher to use for matching phrases ? One of (exact, simstring) term_matcher_config: Dict[str,Any] Parameters of the matcher class \"\"\" def __init__ ( self , nlp : Language , label : str , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , term_matcher : TerminologyTermMatcher = TerminologyTermMatcher . exact , term_matcher_config = None , ): self . nlp = nlp self . label = label self . attr = attr if term_matcher == TerminologyTermMatcher . exact : self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) elif term_matcher == TerminologyTermMatcher . simstring : self . phrase_matcher = SimstringMatcher ( vocab = self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) else : raise ValueError ( f \"Algorithm { repr ( term_matcher ) } does not belong to\" f \" known matchers [exact, simstring].\" ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () def set_extensions ( self ) -> None : super () . set_extensions () if not Span . has_extension ( self . label ): Span . set_extension ( self . label , default = None ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Post-process matches to account for terminology. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = [] for match in chain ( matches , regex_matches ): span = Span ( doc = match . doc , start = match . start , end = match . end , label = self . label , kb_id = match . label , ) span . _ . set ( self . label , match . label_ ) spans . append ( span ) return spans def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) if self . label not in doc . spans : doc . spans [ self . label ] = matches ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc nlp = nlp instance-attribute label = label instance-attribute attr = attr instance-attribute phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , None = term_matcher_config or {}) instance-attribute regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded ) instance-attribute __init__ ( nlp , label , terms , regex , attr , ignore_excluded , term_matcher = TerminologyTermMatcher . exact , term_matcher_config = None ) Source code in edsnlp/pipelines/core/terminology/terminology.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , nlp : Language , label : str , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , term_matcher : TerminologyTermMatcher = TerminologyTermMatcher . exact , term_matcher_config = None , ): self . nlp = nlp self . label = label self . attr = attr if term_matcher == TerminologyTermMatcher . exact : self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) elif term_matcher == TerminologyTermMatcher . simstring : self . phrase_matcher = SimstringMatcher ( vocab = self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) else : raise ValueError ( f \"Algorithm { repr ( term_matcher ) } does not belong to\" f \" known matchers [exact, simstring].\" ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () set_extensions () Source code in edsnlp/pipelines/core/terminology/terminology.py 100 101 102 103 def set_extensions ( self ) -> None : super () . set_extensions () if not Span . has_extension ( self . label ): Span . set_extension ( self . label , default = None ) process ( doc ) Find matching spans in doc. Post-process matches to account for terminology. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION spans List of Spans returned by the matchers. Source code in edsnlp/pipelines/core/terminology/terminology.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Post-process matches to account for terminology. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = [] for match in chain ( matches , regex_matches ): span = Span ( doc = match . doc , start = match . start , end = match . end , label = self . label , kb_id = match . label , ) span . _ . set ( self . label , match . label_ ) spans . append ( span ) return spans __call__ ( doc ) Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/terminology/terminology.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) if self . label not in doc . spans : doc . spans [ self . label ] = matches ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"terminology"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlppipelinescoreterminologyterminology","text":"","title":"edsnlp.pipelines.core.terminology.terminology"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyTermMatcher","text":"Bases: str , Enum Source code in edsnlp/pipelines/core/terminology/terminology.py 16 17 18 class TerminologyTermMatcher ( str , Enum ): exact = \"exact\" simstring = \"simstring\"","title":"TerminologyTermMatcher"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyTermMatcher.exact","text":"","title":"exact"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyTermMatcher.simstring","text":"","title":"simstring"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher","text":"Bases: BaseComponent Provides a terminology matching component. The terminology matching component differs from the simple matcher component in that the regex and terms keys are used as spaCy's kb_id . All matched entities have the same label, defined in the top-level constructor (argument label ). PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language label Top-level label TYPE: str terms A dictionary of terms. TYPE: Optional[Patterns] regex A dictionary of regular expressions. TYPE: Optional[Patterns] attr The default attribute to use for matching. Can be overridden using the terms and regex configurations. TYPE: str ignore_excluded Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). TYPE: bool term_matcher The matcher to use for matching phrases ? One of (exact, simstring) term_matcher_config Parameters of the matcher class Source code in edsnlp/pipelines/core/terminology/terminology.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class TerminologyMatcher ( BaseComponent ): \"\"\" Provides a terminology matching component. The terminology matching component differs from the simple matcher component in that the `regex` and `terms` keys are used as spaCy's `kb_id`. All matched entities have the same label, defined in the top-level constructor (argument `label`). Parameters ---------- nlp : Language The spaCy object. label : str Top-level label terms : Optional[Patterns] A dictionary of terms. regex : Optional[Patterns] A dictionary of regular expressions. attr : str The default attribute to use for matching. Can be overridden using the `terms` and `regex` configurations. ignore_excluded : bool Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens). term_matcher: TerminologyTermMatcher The matcher to use for matching phrases ? One of (exact, simstring) term_matcher_config: Dict[str,Any] Parameters of the matcher class \"\"\" def __init__ ( self , nlp : Language , label : str , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , term_matcher : TerminologyTermMatcher = TerminologyTermMatcher . exact , term_matcher_config = None , ): self . nlp = nlp self . label = label self . attr = attr if term_matcher == TerminologyTermMatcher . exact : self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) elif term_matcher == TerminologyTermMatcher . simstring : self . phrase_matcher = SimstringMatcher ( vocab = self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) else : raise ValueError ( f \"Algorithm { repr ( term_matcher ) } does not belong to\" f \" known matchers [exact, simstring].\" ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions () def set_extensions ( self ) -> None : super () . set_extensions () if not Span . has_extension ( self . label ): Span . set_extension ( self . label , default = None ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Post-process matches to account for terminology. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = [] for match in chain ( matches , regex_matches ): span = Span ( doc = match . doc , start = match . start , end = match . end , label = self . label , kb_id = match . label , ) span . _ . set ( self . label , match . label_ ) spans . append ( span ) return spans def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) if self . label not in doc . spans : doc . spans [ self . label ] = matches ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"TerminologyMatcher"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.label","text":"","title":"label"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.attr","text":"","title":"attr"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.phrase_matcher","text":"","title":"phrase_matcher"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.regex_matcher","text":"","title":"regex_matcher"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.__init__","text":"Source code in edsnlp/pipelines/core/terminology/terminology.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , nlp : Language , label : str , terms : Optional [ Patterns ], regex : Optional [ Patterns ], attr : str , ignore_excluded : bool , term_matcher : TerminologyTermMatcher = TerminologyTermMatcher . exact , term_matcher_config = None , ): self . nlp = nlp self . label = label self . attr = attr if term_matcher == TerminologyTermMatcher . exact : self . phrase_matcher = EDSPhraseMatcher ( self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) elif term_matcher == TerminologyTermMatcher . simstring : self . phrase_matcher = SimstringMatcher ( vocab = self . nlp . vocab , attr = attr , ignore_excluded = ignore_excluded , ** ( term_matcher_config or {}), ) else : raise ValueError ( f \"Algorithm { repr ( term_matcher ) } does not belong to\" f \" known matchers [exact, simstring].\" ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = ignore_excluded , ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . regex_matcher . build_patterns ( regex = regex ) self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.set_extensions","text":"Source code in edsnlp/pipelines/core/terminology/terminology.py 100 101 102 103 def set_extensions ( self ) -> None : super () . set_extensions () if not Span . has_extension ( self . label ): Span . set_extension ( self . label , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.process","text":"Find matching spans in doc. Post-process matches to account for terminology. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION spans List of Spans returned by the matchers. Source code in edsnlp/pipelines/core/terminology/terminology.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find matching spans in doc. Post-process matches to account for terminology. Parameters ---------- doc: spaCy Doc object. Returns ------- spans: List of Spans returned by the matchers. \"\"\" matches = self . phrase_matcher ( doc , as_spans = True ) regex_matches = self . regex_matcher ( doc , as_spans = True ) spans = [] for match in chain ( matches , regex_matches ): span = Span ( doc = match . doc , start = match . start , end = match . end , label = self . label , kb_id = match . label , ) span . _ . set ( self . label , match . label_ ) spans . append ( span ) return spans","title":"process()"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.__call__","text":"Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/core/terminology/terminology.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" matches = self . process ( doc ) if self . label not in doc . spans : doc . spans [ self . label ] = matches ents , discarded = filter_spans ( list ( doc . ents ) + matches , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/misc/","text":"edsnlp.pipelines.misc","title":"`edsnlp.pipelines.misc`"},{"location":"reference/pipelines/misc/#edsnlppipelinesmisc","text":"","title":"edsnlp.pipelines.misc"},{"location":"reference/pipelines/misc/consultation_dates/","text":"edsnlp.pipelines.misc.consultation_dates","title":"`edsnlp.pipelines.misc.consultation_dates`"},{"location":"reference/pipelines/misc/consultation_dates/#edsnlppipelinesmiscconsultation_dates","text":"","title":"edsnlp.pipelines.misc.consultation_dates"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/","text":"edsnlp.pipelines.misc.consultation_dates.consultation_dates ConsultationDates Bases: GenericMatcher Class to extract consultation dates from \"CR-CONS\" documents. The pipeline populates the doc . spans [ 'consultation_dates' ] list. For each extraction s in this list, the corresponding date is available as s._.consultation_date . PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language consultation_mention List of RegEx for consultation mentions. If type==list : Overrides the default list If type==bool : Uses the default list of True, disable if False TYPE: Union[List[str], bool] town_mention : Union[List[str], bool] List of RegEx for all AP-HP hospitals' towns mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False document_date_mention : Union[List[str], bool] List of RegEx for document date. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class ConsultationDates ( GenericMatcher ): \"\"\" Class to extract consultation dates from \"CR-CONS\" documents. The pipeline populates the `#!python doc.spans['consultation_dates']` list. For each extraction `s` in this list, the corresponding date is available as `s._.consultation_date`. Parameters ---------- nlp : Language Language pipeline object consultation_mention : Union[List[str], bool] List of RegEx for consultation mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False town_mention : Union[List[str], bool] List of RegEx for all AP-HP hospitals' towns mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False document_date_mention : Union[List[str], bool] List of RegEx for document date. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False \"\"\" def __init__ ( self , nlp : Language , consultation_mention : Union [ List [ str ], bool ], town_mention : Union [ List [ str ], bool ], document_date_mention : Union [ List [ str ], bool ], attr : str , ** kwargs , ): logger . warning ( \"This pipeline is still in beta\" ) logger . warning ( \"This pipeline should ONLY be used on notes \" \"where `note_class_source_value == 'CR-CONS'`\" ) logger . warning ( \"\"\"This pipeline requires to use the normalizer pipeline with: lowercase=True, accents=True, quotes=True\"\"\" ) if not ( nlp . has_pipe ( \"dates\" ) and nlp . get_pipe ( \"dates\" ) . on_ents_only is False ): config = dict ( ** DEFAULT_CONFIG ) config [ \"on_ents_only\" ] = \"consultation_mentions\" self . date_matcher = Dates ( nlp , ** config ) else : self . date_matcher = None if not consultation_mention : consultation_mention = [] elif consultation_mention is True : consultation_mention = consult_regex . consultation_mention if not document_date_mention : document_date_mention = [] elif document_date_mention is True : document_date_mention = consult_regex . document_date_mention if not town_mention : town_mention = [] elif town_mention is True : town_mention = consult_regex . town_mention regex = dict ( consultation_mention = consultation_mention , town_mention = town_mention , document_date_mention = document_date_mention , ) super () . __init__ ( nlp , regex = regex , terms = dict (), attr = attr , ignore_excluded = False , ** kwargs , ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"consultation_date\" ): Span . set_extension ( \"consultation_date\" , default = None ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Finds entities Parameters ---------- doc: spaCy Doc object Returns ------- doc: Doc spaCy Doc object with additional `doc.spans['consultation_dates]` `SpanGroup` \"\"\" ents = self . process ( doc ) doc . spans [ \"consultation_mentions\" ] = ents doc . spans [ \"consultation_dates\" ] = [] if self . date_matcher is not None : doc = self . date_matcher ( doc ) for mention in ents : # Looking for a date # - In the same sentence # - Not less than 10 tokens AFTER the consultation mention matching_dates = [ date for date in doc . spans [ \"dates\" ] if ( ( mention . sent == date . sent ) and ( date . start > mention . start ) and ( date . start - mention . end <= 10 ) ) ] if matching_dates : # We keep the first mention of a date kept_date = min ( matching_dates , key = lambda d : d . start ) span = doc [ mention . start : kept_date . end ] span . label_ = mention . label_ span . _ . consultation_date = kept_date . _ . date doc . spans [ \"consultation_dates\" ] . append ( span ) del doc . spans [ \"consultation_mentions\" ] return doc date_matcher = Dates ( nlp , None = config ) instance-attribute __init__ ( nlp , consultation_mention , town_mention , document_date_mention , attr , ** kwargs ) Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , nlp : Language , consultation_mention : Union [ List [ str ], bool ], town_mention : Union [ List [ str ], bool ], document_date_mention : Union [ List [ str ], bool ], attr : str , ** kwargs , ): logger . warning ( \"This pipeline is still in beta\" ) logger . warning ( \"This pipeline should ONLY be used on notes \" \"where `note_class_source_value == 'CR-CONS'`\" ) logger . warning ( \"\"\"This pipeline requires to use the normalizer pipeline with: lowercase=True, accents=True, quotes=True\"\"\" ) if not ( nlp . has_pipe ( \"dates\" ) and nlp . get_pipe ( \"dates\" ) . on_ents_only is False ): config = dict ( ** DEFAULT_CONFIG ) config [ \"on_ents_only\" ] = \"consultation_mentions\" self . date_matcher = Dates ( nlp , ** config ) else : self . date_matcher = None if not consultation_mention : consultation_mention = [] elif consultation_mention is True : consultation_mention = consult_regex . consultation_mention if not document_date_mention : document_date_mention = [] elif document_date_mention is True : document_date_mention = consult_regex . document_date_mention if not town_mention : town_mention = [] elif town_mention is True : town_mention = consult_regex . town_mention regex = dict ( consultation_mention = consultation_mention , town_mention = town_mention , document_date_mention = document_date_mention , ) super () . __init__ ( nlp , regex = regex , terms = dict (), attr = attr , ignore_excluded = False , ** kwargs , ) self . set_extensions () set_extensions () Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 109 110 111 112 @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"consultation_date\" ): Span . set_extension ( \"consultation_date\" , default = None ) __call__ ( doc ) Finds entities PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object with additional doc.spans['consultation_dates] SpanGroup Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Finds entities Parameters ---------- doc: spaCy Doc object Returns ------- doc: Doc spaCy Doc object with additional `doc.spans['consultation_dates]` `SpanGroup` \"\"\" ents = self . process ( doc ) doc . spans [ \"consultation_mentions\" ] = ents doc . spans [ \"consultation_dates\" ] = [] if self . date_matcher is not None : doc = self . date_matcher ( doc ) for mention in ents : # Looking for a date # - In the same sentence # - Not less than 10 tokens AFTER the consultation mention matching_dates = [ date for date in doc . spans [ \"dates\" ] if ( ( mention . sent == date . sent ) and ( date . start > mention . start ) and ( date . start - mention . end <= 10 ) ) ] if matching_dates : # We keep the first mention of a date kept_date = min ( matching_dates , key = lambda d : d . start ) span = doc [ mention . start : kept_date . end ] span . label_ = mention . label_ span . _ . consultation_date = kept_date . _ . date doc . spans [ \"consultation_dates\" ] . append ( span ) del doc . spans [ \"consultation_mentions\" ] return doc","title":"consultation_dates"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlppipelinesmiscconsultation_datesconsultation_dates","text":"","title":"edsnlp.pipelines.misc.consultation_dates.consultation_dates"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates","text":"Bases: GenericMatcher Class to extract consultation dates from \"CR-CONS\" documents. The pipeline populates the doc . spans [ 'consultation_dates' ] list. For each extraction s in this list, the corresponding date is available as s._.consultation_date . PARAMETER DESCRIPTION nlp Language pipeline object TYPE: Language consultation_mention List of RegEx for consultation mentions. If type==list : Overrides the default list If type==bool : Uses the default list of True, disable if False TYPE: Union[List[str], bool] town_mention : Union[List[str], bool] List of RegEx for all AP-HP hospitals' towns mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False document_date_mention : Union[List[str], bool] List of RegEx for document date. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class ConsultationDates ( GenericMatcher ): \"\"\" Class to extract consultation dates from \"CR-CONS\" documents. The pipeline populates the `#!python doc.spans['consultation_dates']` list. For each extraction `s` in this list, the corresponding date is available as `s._.consultation_date`. Parameters ---------- nlp : Language Language pipeline object consultation_mention : Union[List[str], bool] List of RegEx for consultation mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False town_mention : Union[List[str], bool] List of RegEx for all AP-HP hospitals' towns mentions. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False document_date_mention : Union[List[str], bool] List of RegEx for document date. - If `type==list`: Overrides the default list - If `type==bool`: Uses the default list of True, disable if False \"\"\" def __init__ ( self , nlp : Language , consultation_mention : Union [ List [ str ], bool ], town_mention : Union [ List [ str ], bool ], document_date_mention : Union [ List [ str ], bool ], attr : str , ** kwargs , ): logger . warning ( \"This pipeline is still in beta\" ) logger . warning ( \"This pipeline should ONLY be used on notes \" \"where `note_class_source_value == 'CR-CONS'`\" ) logger . warning ( \"\"\"This pipeline requires to use the normalizer pipeline with: lowercase=True, accents=True, quotes=True\"\"\" ) if not ( nlp . has_pipe ( \"dates\" ) and nlp . get_pipe ( \"dates\" ) . on_ents_only is False ): config = dict ( ** DEFAULT_CONFIG ) config [ \"on_ents_only\" ] = \"consultation_mentions\" self . date_matcher = Dates ( nlp , ** config ) else : self . date_matcher = None if not consultation_mention : consultation_mention = [] elif consultation_mention is True : consultation_mention = consult_regex . consultation_mention if not document_date_mention : document_date_mention = [] elif document_date_mention is True : document_date_mention = consult_regex . document_date_mention if not town_mention : town_mention = [] elif town_mention is True : town_mention = consult_regex . town_mention regex = dict ( consultation_mention = consultation_mention , town_mention = town_mention , document_date_mention = document_date_mention , ) super () . __init__ ( nlp , regex = regex , terms = dict (), attr = attr , ignore_excluded = False , ** kwargs , ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"consultation_date\" ): Span . set_extension ( \"consultation_date\" , default = None ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Finds entities Parameters ---------- doc: spaCy Doc object Returns ------- doc: Doc spaCy Doc object with additional `doc.spans['consultation_dates]` `SpanGroup` \"\"\" ents = self . process ( doc ) doc . spans [ \"consultation_mentions\" ] = ents doc . spans [ \"consultation_dates\" ] = [] if self . date_matcher is not None : doc = self . date_matcher ( doc ) for mention in ents : # Looking for a date # - In the same sentence # - Not less than 10 tokens AFTER the consultation mention matching_dates = [ date for date in doc . spans [ \"dates\" ] if ( ( mention . sent == date . sent ) and ( date . start > mention . start ) and ( date . start - mention . end <= 10 ) ) ] if matching_dates : # We keep the first mention of a date kept_date = min ( matching_dates , key = lambda d : d . start ) span = doc [ mention . start : kept_date . end ] span . label_ = mention . label_ span . _ . consultation_date = kept_date . _ . date doc . spans [ \"consultation_dates\" ] . append ( span ) del doc . spans [ \"consultation_mentions\" ] return doc","title":"ConsultationDates"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates.date_matcher","text":"","title":"date_matcher"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates.__init__","text":"Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , nlp : Language , consultation_mention : Union [ List [ str ], bool ], town_mention : Union [ List [ str ], bool ], document_date_mention : Union [ List [ str ], bool ], attr : str , ** kwargs , ): logger . warning ( \"This pipeline is still in beta\" ) logger . warning ( \"This pipeline should ONLY be used on notes \" \"where `note_class_source_value == 'CR-CONS'`\" ) logger . warning ( \"\"\"This pipeline requires to use the normalizer pipeline with: lowercase=True, accents=True, quotes=True\"\"\" ) if not ( nlp . has_pipe ( \"dates\" ) and nlp . get_pipe ( \"dates\" ) . on_ents_only is False ): config = dict ( ** DEFAULT_CONFIG ) config [ \"on_ents_only\" ] = \"consultation_mentions\" self . date_matcher = Dates ( nlp , ** config ) else : self . date_matcher = None if not consultation_mention : consultation_mention = [] elif consultation_mention is True : consultation_mention = consult_regex . consultation_mention if not document_date_mention : document_date_mention = [] elif document_date_mention is True : document_date_mention = consult_regex . document_date_mention if not town_mention : town_mention = [] elif town_mention is True : town_mention = consult_regex . town_mention regex = dict ( consultation_mention = consultation_mention , town_mention = town_mention , document_date_mention = document_date_mention , ) super () . __init__ ( nlp , regex = regex , terms = dict (), attr = attr , ignore_excluded = False , ** kwargs , ) self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates.set_extensions","text":"Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 109 110 111 112 @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"consultation_date\" ): Span . set_extension ( \"consultation_date\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates.__call__","text":"Finds entities PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object with additional doc.spans['consultation_dates] SpanGroup Source code in edsnlp/pipelines/misc/consultation_dates/consultation_dates.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Finds entities Parameters ---------- doc: spaCy Doc object Returns ------- doc: Doc spaCy Doc object with additional `doc.spans['consultation_dates]` `SpanGroup` \"\"\" ents = self . process ( doc ) doc . spans [ \"consultation_mentions\" ] = ents doc . spans [ \"consultation_dates\" ] = [] if self . date_matcher is not None : doc = self . date_matcher ( doc ) for mention in ents : # Looking for a date # - In the same sentence # - Not less than 10 tokens AFTER the consultation mention matching_dates = [ date for date in doc . spans [ \"dates\" ] if ( ( mention . sent == date . sent ) and ( date . start > mention . start ) and ( date . start - mention . end <= 10 ) ) ] if matching_dates : # We keep the first mention of a date kept_date = min ( matching_dates , key = lambda d : d . start ) span = doc [ mention . start : kept_date . end ] span . label_ = mention . label_ span . _ . consultation_date = kept_date . _ . date doc . spans [ \"consultation_dates\" ] . append ( span ) del doc . spans [ \"consultation_mentions\" ] return doc","title":"__call__()"},{"location":"reference/pipelines/misc/consultation_dates/factory/","text":"edsnlp.pipelines.misc.consultation_dates.factory DEFAULT_CONFIG = dict ( consultation_mention = True , town_mention = False , document_date_mention = False , attr = 'NORM' ) module-attribute create_component ( nlp , name , attr , consultation_mention , town_mention , document_date_mention ) Source code in edsnlp/pipelines/misc/consultation_dates/factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @deprecated_factory ( \"consultation_dates\" , \"eds.consultation_dates\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc._.consultation_dates\" ], ) @Language . factory ( \"eds.consultation_dates\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc._.consultation_dates\" ], ) def create_component ( nlp : Language , name : str , attr : str , consultation_mention : Union [ List [ str ], bool ], town_mention : Union [ List [ str ], bool ], document_date_mention : Union [ List [ str ], bool ], ): return ConsultationDates ( nlp , attr = attr , consultation_mention = consultation_mention , document_date_mention = document_date_mention , town_mention = town_mention , )","title":"factory"},{"location":"reference/pipelines/misc/consultation_dates/factory/#edsnlppipelinesmiscconsultation_datesfactory","text":"","title":"edsnlp.pipelines.misc.consultation_dates.factory"},{"location":"reference/pipelines/misc/consultation_dates/factory/#edsnlp.pipelines.misc.consultation_dates.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/misc/consultation_dates/factory/#edsnlp.pipelines.misc.consultation_dates.factory.create_component","text":"Source code in edsnlp/pipelines/misc/consultation_dates/factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @deprecated_factory ( \"consultation_dates\" , \"eds.consultation_dates\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc._.consultation_dates\" ], ) @Language . factory ( \"eds.consultation_dates\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc._.consultation_dates\" ], ) def create_component ( nlp : Language , name : str , attr : str , consultation_mention : Union [ List [ str ], bool ], town_mention : Union [ List [ str ], bool ], document_date_mention : Union [ List [ str ], bool ], ): return ConsultationDates ( nlp , attr = attr , consultation_mention = consultation_mention , document_date_mention = document_date_mention , town_mention = town_mention , )","title":"create_component()"},{"location":"reference/pipelines/misc/consultation_dates/patterns/","text":"edsnlp.pipelines.misc.consultation_dates.patterns consultation_mention = [ 'rendez-vous pris' , 'consultation' , 'consultation.{1,8}examen' , ' \\\\ bcs \\\\ b' , 'examen clinique' , 'de compte rendu' , \"date de l'examen\" , 'examen realise le' , 'date de la visite' ] module-attribute town_mention = [ 'paris' , 'kremlin.bicetre' , 'creteil' , 'boulogne.billancourt' , 'villejuif' , 'clamart' , 'bobigny' , 'clichy' , 'ivry.sur.seine' , 'issy.les.moulineaux' , 'draveil' , 'limeil' , 'champcueil' , 'roche.guyon' , 'bondy' , 'colombes' , 'hendaye' , 'herck.sur.mer' , 'labruyere' , 'garches' , 'sevran' , 'hyeres' ] module-attribute document_date_mention = [ 'imprime le' , 'signe electroniquement' , 'signe le' , 'saisi le' , 'dicte le' , 'tape le' , 'date de reference' , 'date \\\\ s*:' , 'dactylographie le' , 'date du rapport' ] module-attribute","title":"patterns"},{"location":"reference/pipelines/misc/consultation_dates/patterns/#edsnlppipelinesmiscconsultation_datespatterns","text":"","title":"edsnlp.pipelines.misc.consultation_dates.patterns"},{"location":"reference/pipelines/misc/consultation_dates/patterns/#edsnlp.pipelines.misc.consultation_dates.patterns.consultation_mention","text":"","title":"consultation_mention"},{"location":"reference/pipelines/misc/consultation_dates/patterns/#edsnlp.pipelines.misc.consultation_dates.patterns.town_mention","text":"","title":"town_mention"},{"location":"reference/pipelines/misc/consultation_dates/patterns/#edsnlp.pipelines.misc.consultation_dates.patterns.document_date_mention","text":"","title":"document_date_mention"},{"location":"reference/pipelines/misc/dates/","text":"edsnlp.pipelines.misc.dates","title":"`edsnlp.pipelines.misc.dates`"},{"location":"reference/pipelines/misc/dates/#edsnlppipelinesmiscdates","text":"","title":"edsnlp.pipelines.misc.dates"},{"location":"reference/pipelines/misc/dates/dates/","text":"edsnlp.pipelines.misc.dates.dates eds.dates pipeline. PERIOD_PROXIMITY_THRESHOLD = 3 module-attribute Dates Bases: BaseComponent Tags and normalizes dates, using the open-source dateparser library. The pipeline uses spaCy's filter_spans function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: spacy.language.Language absolute List of regular expressions for absolute dates. TYPE: Union[List[str], str] relative List of regular expressions for relative dates (eg hier , la semaine prochaine ). TYPE: Union[List[str], str] duration List of regular expressions for durations (eg pendant trois mois ). TYPE: Union[List[str], str] false_positive List of regular expressions for false positive (eg phone numbers, etc). TYPE: Union[List[str], str] on_ents_only Wether to look on dates in the whole document or in specific sentences: If True : Only look in the sentences of each entity in doc.ents If False: Look in the whole document If given a string key or list of string: Only look in the sentences of each entity in doc . spans [ key ] TYPE: Union[bool, str, List[str]] detect_periods Whether to detect periods (experimental) TYPE: bool as_ents Whether to treat dates as entities TYPE: bool attr spaCy attribute to use TYPE: str Source code in edsnlp/pipelines/misc/dates/dates.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 class Dates ( BaseComponent ): \"\"\" Tags and normalizes dates, using the open-source `dateparser` library. The pipeline uses spaCy's `filter_spans` function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. Parameters ---------- nlp : spacy.language.Language Language pipeline object absolute : Union[List[str], str] List of regular expressions for absolute dates. relative : Union[List[str], str] List of regular expressions for relative dates (eg `hier`, `la semaine prochaine`). duration : Union[List[str], str] List of regular expressions for durations (eg `pendant trois mois`). false_positive : Union[List[str], str] List of regular expressions for false positive (eg phone numbers, etc). on_ents_only : Union[bool, str, List[str]] Wether to look on dates in the whole document or in specific sentences: - If `True`: Only look in the sentences of each entity in doc.ents - If False: Look in the whole document - If given a string `key` or list of string: Only look in the sentences of each entity in `#!python doc.spans[key]` detect_periods : bool Whether to detect periods (experimental) as_ents : bool Whether to treat dates as entities attr : str spaCy attribute to use \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , absolute : Optional [ List [ str ]], relative : Optional [ List [ str ]], duration : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : Union [ bool , List [ str ]], detect_periods : bool , as_ents : bool , attr : str , ): self . nlp = nlp if absolute is None : absolute = patterns . absolute_pattern if relative is None : relative = patterns . relative_pattern if duration is None : duration = patterns . duration_pattern if false_positive is None : false_positive = patterns . false_positive_pattern if isinstance ( absolute , str ): absolute = [ absolute ] if isinstance ( relative , str ): relative = [ relative ] if isinstance ( duration , str ): relative = [ duration ] if isinstance ( false_positive , str ): false_positive = [ false_positive ] self . on_ents_only = on_ents_only self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"false_positive\" , false_positive ) self . regex_matcher . add ( \"absolute\" , absolute ) self . regex_matcher . add ( \"relative\" , relative ) self . regex_matcher . add ( \"duration\" , duration ) self . detect_periods = detect_periods self . as_ents = as_ents if detect_periods : logger . warning ( \"The period extractor is experimental.\" ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the dates pipeline. \"\"\" if not Span . has_extension ( \"datetime\" ): Span . set_extension ( \"datetime\" , default = None ) if not Span . has_extension ( \"date\" ): Span . set_extension ( \"date\" , default = None ) if not Span . has_extension ( \"period\" ): Span . set_extension ( \"period\" , default = None ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , return_groupdict = True , ), ) else : dates = self . regex_matcher ( doc , as_spans = True , return_groupdict = True , ) dates = filter_spans ( dates ) dates = [ date for date in dates if date [ 0 ] . label_ != \"false_positive\" ] return dates def parse ( self , dates : List [ Tuple [ Span , Dict [ str , str ]]]) -> List [ Span ]: \"\"\" Parse dates using the groupdict returned by the matcher. Parameters ---------- dates : List[Tuple[Span, Dict[str, str]]] List of tuples containing the spans and groupdict returned by the matcher. Returns ------- List[Span] List of processed spans, with the date parsed. \"\"\" for span , groupdict in dates : if span . label_ == \"relative\" : parsed = RelativeDate . parse_obj ( groupdict ) elif span . label_ == \"absolute\" : parsed = AbsoluteDate . parse_obj ( groupdict ) else : parsed = Duration . parse_obj ( groupdict ) span . _ . date = parsed return [ span for span , _ in dates ] def process_periods ( self , dates : List [ Span ]) -> List [ Span ]: \"\"\" Experimental period detection. Parameters ---------- dates : List[Span] List of detected dates. Returns ------- List[Span] List of detected periods. \"\"\" if len ( dates ) < 2 : return [] periods = [] seen = set () dates = list ( sorted ( dates , key = lambda d : d . start )) for d1 , d2 in zip ( dates [: - 1 ], dates [ 1 :]): if d1 . _ . date . mode == Mode . DURATION or d2 . _ . date . mode == Mode . DURATION : pass elif d1 in seen or d1 . _ . date . mode is None or d2 . _ . date . mode is None : continue if ( d1 . end - d2 . start < PERIOD_PROXIMITY_THRESHOLD and d1 . _ . date . mode != d2 . _ . date . mode ): period = Span ( d1 . doc , d1 . start , d2 . end , label = \"period\" ) # If one date is a duration, # the other may not have a registered mode. m1 = d1 . _ . date . mode or Mode . FROM m2 = d2 . _ . date . mode or Mode . FROM period . _ . period = Period . parse_obj ( { m1 . value : d1 , m2 . value : d2 , } ) seen . add ( d1 ) seen . add ( d2 ) periods . append ( period ) return periods def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) dates = self . parse ( dates ) doc . spans [ \"dates\" ] = dates if self . detect_periods : doc . spans [ \"periods\" ] = self . process_periods ( dates ) if self . as_ents : ents , discarded = filter_spans ( list ( doc . ents ) + dates , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc nlp = nlp instance-attribute on_ents_only = on_ents_only instance-attribute regex_matcher = RegexMatcher ( attr = attr , alignment_mode = 'strict' ) instance-attribute detect_periods = detect_periods instance-attribute as_ents = as_ents instance-attribute __init__ ( nlp , absolute , relative , duration , false_positive , on_ents_only , detect_periods , as_ents , attr ) Source code in edsnlp/pipelines/misc/dates/dates.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , nlp : Language , absolute : Optional [ List [ str ]], relative : Optional [ List [ str ]], duration : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : Union [ bool , List [ str ]], detect_periods : bool , as_ents : bool , attr : str , ): self . nlp = nlp if absolute is None : absolute = patterns . absolute_pattern if relative is None : relative = patterns . relative_pattern if duration is None : duration = patterns . duration_pattern if false_positive is None : false_positive = patterns . false_positive_pattern if isinstance ( absolute , str ): absolute = [ absolute ] if isinstance ( relative , str ): relative = [ relative ] if isinstance ( duration , str ): relative = [ duration ] if isinstance ( false_positive , str ): false_positive = [ false_positive ] self . on_ents_only = on_ents_only self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"false_positive\" , false_positive ) self . regex_matcher . add ( \"absolute\" , absolute ) self . regex_matcher . add ( \"relative\" , relative ) self . regex_matcher . add ( \"duration\" , duration ) self . detect_periods = detect_periods self . as_ents = as_ents if detect_periods : logger . warning ( \"The period extractor is experimental.\" ) self . set_extensions () set_extensions () Set extensions for the dates pipeline. Source code in edsnlp/pipelines/misc/dates/dates.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the dates pipeline. \"\"\" if not Span . has_extension ( \"datetime\" ): Span . set_extension ( \"datetime\" , default = None ) if not Span . has_extension ( \"date\" ): Span . set_extension ( \"date\" , default = None ) if not Span . has_extension ( \"period\" ): Span . set_extension ( \"period\" , default = None ) process ( doc ) Find dates in doc. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION dates list of date spans Source code in edsnlp/pipelines/misc/dates/dates.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , return_groupdict = True , ), ) else : dates = self . regex_matcher ( doc , as_spans = True , return_groupdict = True , ) dates = filter_spans ( dates ) dates = [ date for date in dates if date [ 0 ] . label_ != \"false_positive\" ] return dates parse ( dates ) Parse dates using the groupdict returned by the matcher. PARAMETER DESCRIPTION dates List of tuples containing the spans and groupdict returned by the matcher. TYPE: List[Tuple[Span, Dict[str, str]]] RETURNS DESCRIPTION List[Span] List of processed spans, with the date parsed. Source code in edsnlp/pipelines/misc/dates/dates.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def parse ( self , dates : List [ Tuple [ Span , Dict [ str , str ]]]) -> List [ Span ]: \"\"\" Parse dates using the groupdict returned by the matcher. Parameters ---------- dates : List[Tuple[Span, Dict[str, str]]] List of tuples containing the spans and groupdict returned by the matcher. Returns ------- List[Span] List of processed spans, with the date parsed. \"\"\" for span , groupdict in dates : if span . label_ == \"relative\" : parsed = RelativeDate . parse_obj ( groupdict ) elif span . label_ == \"absolute\" : parsed = AbsoluteDate . parse_obj ( groupdict ) else : parsed = Duration . parse_obj ( groupdict ) span . _ . date = parsed return [ span for span , _ in dates ] process_periods ( dates ) Experimental period detection. PARAMETER DESCRIPTION dates List of detected dates. TYPE: List[Span] RETURNS DESCRIPTION List[Span] List of detected periods. Source code in edsnlp/pipelines/misc/dates/dates.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def process_periods ( self , dates : List [ Span ]) -> List [ Span ]: \"\"\" Experimental period detection. Parameters ---------- dates : List[Span] List of detected dates. Returns ------- List[Span] List of detected periods. \"\"\" if len ( dates ) < 2 : return [] periods = [] seen = set () dates = list ( sorted ( dates , key = lambda d : d . start )) for d1 , d2 in zip ( dates [: - 1 ], dates [ 1 :]): if d1 . _ . date . mode == Mode . DURATION or d2 . _ . date . mode == Mode . DURATION : pass elif d1 in seen or d1 . _ . date . mode is None or d2 . _ . date . mode is None : continue if ( d1 . end - d2 . start < PERIOD_PROXIMITY_THRESHOLD and d1 . _ . date . mode != d2 . _ . date . mode ): period = Span ( d1 . doc , d1 . start , d2 . end , label = \"period\" ) # If one date is a duration, # the other may not have a registered mode. m1 = d1 . _ . date . mode or Mode . FROM m2 = d2 . _ . date . mode or Mode . FROM period . _ . period = Period . parse_obj ( { m1 . value : d1 , m2 . value : d2 , } ) seen . add ( d1 ) seen . add ( d2 ) periods . append ( period ) return periods __call__ ( doc ) Tags dates. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for dates TYPE: Doc Source code in edsnlp/pipelines/misc/dates/dates.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) dates = self . parse ( dates ) doc . spans [ \"dates\" ] = dates if self . detect_periods : doc . spans [ \"periods\" ] = self . process_periods ( dates ) if self . as_ents : ents , discarded = filter_spans ( list ( doc . ents ) + dates , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"dates"},{"location":"reference/pipelines/misc/dates/dates/#edsnlppipelinesmiscdatesdates","text":"eds.dates pipeline.","title":"edsnlp.pipelines.misc.dates.dates"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.PERIOD_PROXIMITY_THRESHOLD","text":"","title":"PERIOD_PROXIMITY_THRESHOLD"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates","text":"Bases: BaseComponent Tags and normalizes dates, using the open-source dateparser library. The pipeline uses spaCy's filter_spans function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: spacy.language.Language absolute List of regular expressions for absolute dates. TYPE: Union[List[str], str] relative List of regular expressions for relative dates (eg hier , la semaine prochaine ). TYPE: Union[List[str], str] duration List of regular expressions for durations (eg pendant trois mois ). TYPE: Union[List[str], str] false_positive List of regular expressions for false positive (eg phone numbers, etc). TYPE: Union[List[str], str] on_ents_only Wether to look on dates in the whole document or in specific sentences: If True : Only look in the sentences of each entity in doc.ents If False: Look in the whole document If given a string key or list of string: Only look in the sentences of each entity in doc . spans [ key ] TYPE: Union[bool, str, List[str]] detect_periods Whether to detect periods (experimental) TYPE: bool as_ents Whether to treat dates as entities TYPE: bool attr spaCy attribute to use TYPE: str Source code in edsnlp/pipelines/misc/dates/dates.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 class Dates ( BaseComponent ): \"\"\" Tags and normalizes dates, using the open-source `dateparser` library. The pipeline uses spaCy's `filter_spans` function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day. Parameters ---------- nlp : spacy.language.Language Language pipeline object absolute : Union[List[str], str] List of regular expressions for absolute dates. relative : Union[List[str], str] List of regular expressions for relative dates (eg `hier`, `la semaine prochaine`). duration : Union[List[str], str] List of regular expressions for durations (eg `pendant trois mois`). false_positive : Union[List[str], str] List of regular expressions for false positive (eg phone numbers, etc). on_ents_only : Union[bool, str, List[str]] Wether to look on dates in the whole document or in specific sentences: - If `True`: Only look in the sentences of each entity in doc.ents - If False: Look in the whole document - If given a string `key` or list of string: Only look in the sentences of each entity in `#!python doc.spans[key]` detect_periods : bool Whether to detect periods (experimental) as_ents : bool Whether to treat dates as entities attr : str spaCy attribute to use \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , absolute : Optional [ List [ str ]], relative : Optional [ List [ str ]], duration : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : Union [ bool , List [ str ]], detect_periods : bool , as_ents : bool , attr : str , ): self . nlp = nlp if absolute is None : absolute = patterns . absolute_pattern if relative is None : relative = patterns . relative_pattern if duration is None : duration = patterns . duration_pattern if false_positive is None : false_positive = patterns . false_positive_pattern if isinstance ( absolute , str ): absolute = [ absolute ] if isinstance ( relative , str ): relative = [ relative ] if isinstance ( duration , str ): relative = [ duration ] if isinstance ( false_positive , str ): false_positive = [ false_positive ] self . on_ents_only = on_ents_only self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"false_positive\" , false_positive ) self . regex_matcher . add ( \"absolute\" , absolute ) self . regex_matcher . add ( \"relative\" , relative ) self . regex_matcher . add ( \"duration\" , duration ) self . detect_periods = detect_periods self . as_ents = as_ents if detect_periods : logger . warning ( \"The period extractor is experimental.\" ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the dates pipeline. \"\"\" if not Span . has_extension ( \"datetime\" ): Span . set_extension ( \"datetime\" , default = None ) if not Span . has_extension ( \"date\" ): Span . set_extension ( \"date\" , default = None ) if not Span . has_extension ( \"period\" ): Span . set_extension ( \"period\" , default = None ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , return_groupdict = True , ), ) else : dates = self . regex_matcher ( doc , as_spans = True , return_groupdict = True , ) dates = filter_spans ( dates ) dates = [ date for date in dates if date [ 0 ] . label_ != \"false_positive\" ] return dates def parse ( self , dates : List [ Tuple [ Span , Dict [ str , str ]]]) -> List [ Span ]: \"\"\" Parse dates using the groupdict returned by the matcher. Parameters ---------- dates : List[Tuple[Span, Dict[str, str]]] List of tuples containing the spans and groupdict returned by the matcher. Returns ------- List[Span] List of processed spans, with the date parsed. \"\"\" for span , groupdict in dates : if span . label_ == \"relative\" : parsed = RelativeDate . parse_obj ( groupdict ) elif span . label_ == \"absolute\" : parsed = AbsoluteDate . parse_obj ( groupdict ) else : parsed = Duration . parse_obj ( groupdict ) span . _ . date = parsed return [ span for span , _ in dates ] def process_periods ( self , dates : List [ Span ]) -> List [ Span ]: \"\"\" Experimental period detection. Parameters ---------- dates : List[Span] List of detected dates. Returns ------- List[Span] List of detected periods. \"\"\" if len ( dates ) < 2 : return [] periods = [] seen = set () dates = list ( sorted ( dates , key = lambda d : d . start )) for d1 , d2 in zip ( dates [: - 1 ], dates [ 1 :]): if d1 . _ . date . mode == Mode . DURATION or d2 . _ . date . mode == Mode . DURATION : pass elif d1 in seen or d1 . _ . date . mode is None or d2 . _ . date . mode is None : continue if ( d1 . end - d2 . start < PERIOD_PROXIMITY_THRESHOLD and d1 . _ . date . mode != d2 . _ . date . mode ): period = Span ( d1 . doc , d1 . start , d2 . end , label = \"period\" ) # If one date is a duration, # the other may not have a registered mode. m1 = d1 . _ . date . mode or Mode . FROM m2 = d2 . _ . date . mode or Mode . FROM period . _ . period = Period . parse_obj ( { m1 . value : d1 , m2 . value : d2 , } ) seen . add ( d1 ) seen . add ( d2 ) periods . append ( period ) return periods def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) dates = self . parse ( dates ) doc . spans [ \"dates\" ] = dates if self . detect_periods : doc . spans [ \"periods\" ] = self . process_periods ( dates ) if self . as_ents : ents , discarded = filter_spans ( list ( doc . ents ) + dates , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"Dates"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.on_ents_only","text":"","title":"on_ents_only"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.regex_matcher","text":"","title":"regex_matcher"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.detect_periods","text":"","title":"detect_periods"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.as_ents","text":"","title":"as_ents"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.__init__","text":"Source code in edsnlp/pipelines/misc/dates/dates.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , nlp : Language , absolute : Optional [ List [ str ]], relative : Optional [ List [ str ]], duration : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : Union [ bool , List [ str ]], detect_periods : bool , as_ents : bool , attr : str , ): self . nlp = nlp if absolute is None : absolute = patterns . absolute_pattern if relative is None : relative = patterns . relative_pattern if duration is None : duration = patterns . duration_pattern if false_positive is None : false_positive = patterns . false_positive_pattern if isinstance ( absolute , str ): absolute = [ absolute ] if isinstance ( relative , str ): relative = [ relative ] if isinstance ( duration , str ): relative = [ duration ] if isinstance ( false_positive , str ): false_positive = [ false_positive ] self . on_ents_only = on_ents_only self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"false_positive\" , false_positive ) self . regex_matcher . add ( \"absolute\" , absolute ) self . regex_matcher . add ( \"relative\" , relative ) self . regex_matcher . add ( \"duration\" , duration ) self . detect_periods = detect_periods self . as_ents = as_ents if detect_periods : logger . warning ( \"The period extractor is experimental.\" ) self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.set_extensions","text":"Set extensions for the dates pipeline. Source code in edsnlp/pipelines/misc/dates/dates.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the dates pipeline. \"\"\" if not Span . has_extension ( \"datetime\" ): Span . set_extension ( \"datetime\" , default = None ) if not Span . has_extension ( \"date\" ): Span . set_extension ( \"date\" , default = None ) if not Span . has_extension ( \"period\" ): Span . set_extension ( \"period\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.process","text":"Find dates in doc. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION dates list of date spans Source code in edsnlp/pipelines/misc/dates/dates.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find dates in doc. Parameters ---------- doc: spaCy Doc object Returns ------- dates: list of date spans \"\"\" if self . on_ents_only : if type ( self . on_ents_only ) == bool : ents = doc . ents else : if type ( self . on_ents_only ) == str : self . on_ents_only = [ self . on_ents_only ] ents = [] for key in self . on_ents_only : ents . extend ( list ( doc . spans [ key ])) dates = [] for sent in set ([ ent . sent for ent in ents ]): dates = chain ( dates , self . regex_matcher ( sent , as_spans = True , return_groupdict = True , ), ) else : dates = self . regex_matcher ( doc , as_spans = True , return_groupdict = True , ) dates = filter_spans ( dates ) dates = [ date for date in dates if date [ 0 ] . label_ != \"false_positive\" ] return dates","title":"process()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.parse","text":"Parse dates using the groupdict returned by the matcher. PARAMETER DESCRIPTION dates List of tuples containing the spans and groupdict returned by the matcher. TYPE: List[Tuple[Span, Dict[str, str]]] RETURNS DESCRIPTION List[Span] List of processed spans, with the date parsed. Source code in edsnlp/pipelines/misc/dates/dates.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def parse ( self , dates : List [ Tuple [ Span , Dict [ str , str ]]]) -> List [ Span ]: \"\"\" Parse dates using the groupdict returned by the matcher. Parameters ---------- dates : List[Tuple[Span, Dict[str, str]]] List of tuples containing the spans and groupdict returned by the matcher. Returns ------- List[Span] List of processed spans, with the date parsed. \"\"\" for span , groupdict in dates : if span . label_ == \"relative\" : parsed = RelativeDate . parse_obj ( groupdict ) elif span . label_ == \"absolute\" : parsed = AbsoluteDate . parse_obj ( groupdict ) else : parsed = Duration . parse_obj ( groupdict ) span . _ . date = parsed return [ span for span , _ in dates ]","title":"parse()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.process_periods","text":"Experimental period detection. PARAMETER DESCRIPTION dates List of detected dates. TYPE: List[Span] RETURNS DESCRIPTION List[Span] List of detected periods. Source code in edsnlp/pipelines/misc/dates/dates.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def process_periods ( self , dates : List [ Span ]) -> List [ Span ]: \"\"\" Experimental period detection. Parameters ---------- dates : List[Span] List of detected dates. Returns ------- List[Span] List of detected periods. \"\"\" if len ( dates ) < 2 : return [] periods = [] seen = set () dates = list ( sorted ( dates , key = lambda d : d . start )) for d1 , d2 in zip ( dates [: - 1 ], dates [ 1 :]): if d1 . _ . date . mode == Mode . DURATION or d2 . _ . date . mode == Mode . DURATION : pass elif d1 in seen or d1 . _ . date . mode is None or d2 . _ . date . mode is None : continue if ( d1 . end - d2 . start < PERIOD_PROXIMITY_THRESHOLD and d1 . _ . date . mode != d2 . _ . date . mode ): period = Span ( d1 . doc , d1 . start , d2 . end , label = \"period\" ) # If one date is a duration, # the other may not have a registered mode. m1 = d1 . _ . date . mode or Mode . FROM m2 = d2 . _ . date . mode or Mode . FROM period . _ . period = Period . parse_obj ( { m1 . value : d1 , m2 . value : d2 , } ) seen . add ( d1 ) seen . add ( d2 ) periods . append ( period ) return periods","title":"process_periods()"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.__call__","text":"Tags dates. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for dates TYPE: Doc Source code in edsnlp/pipelines/misc/dates/dates.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags dates. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for dates \"\"\" dates = self . process ( doc ) dates = self . parse ( dates ) doc . spans [ \"dates\" ] = dates if self . detect_periods : doc . spans [ \"periods\" ] = self . process_periods ( dates ) if self . as_ents : ents , discarded = filter_spans ( list ( doc . ents ) + dates , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/misc/dates/factory/","text":"edsnlp.pipelines.misc.dates.factory DEFAULT_CONFIG = dict ( absolute = None , relative = None , duration = None , false_positive = None , detect_periods = False , on_ents_only = False , as_ents = False , attr = 'LOWER' ) module-attribute create_component ( nlp , name , absolute , relative , duration , false_positive , on_ents_only , detect_periods , as_ents , attr ) Source code in edsnlp/pipelines/misc/dates/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @deprecated_factory ( \"dates\" , \"eds.dates\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.spans\" ] ) @Language . factory ( \"eds.dates\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.spans\" ]) def create_component ( nlp : Language , name : str , absolute : Optional [ List [ str ]], relative : Optional [ List [ str ]], duration : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : Union [ bool , List [ str ]], detect_periods : bool , as_ents : bool , attr : str , ): return Dates ( nlp , absolute = absolute , relative = relative , duration = duration , false_positive = false_positive , on_ents_only = on_ents_only , detect_periods = detect_periods , as_ents = as_ents , attr = attr , )","title":"factory"},{"location":"reference/pipelines/misc/dates/factory/#edsnlppipelinesmiscdatesfactory","text":"","title":"edsnlp.pipelines.misc.dates.factory"},{"location":"reference/pipelines/misc/dates/factory/#edsnlp.pipelines.misc.dates.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/misc/dates/factory/#edsnlp.pipelines.misc.dates.factory.create_component","text":"Source code in edsnlp/pipelines/misc/dates/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @deprecated_factory ( \"dates\" , \"eds.dates\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.spans\" ] ) @Language . factory ( \"eds.dates\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.spans\" ]) def create_component ( nlp : Language , name : str , absolute : Optional [ List [ str ]], relative : Optional [ List [ str ]], duration : Optional [ List [ str ]], false_positive : Optional [ List [ str ]], on_ents_only : Union [ bool , List [ str ]], detect_periods : bool , as_ents : bool , attr : str , ): return Dates ( nlp , absolute = absolute , relative = relative , duration = duration , false_positive = false_positive , on_ents_only = on_ents_only , detect_periods = detect_periods , as_ents = as_ents , attr = attr , )","title":"create_component()"},{"location":"reference/pipelines/misc/dates/models/","text":"edsnlp.pipelines.misc.dates.models Direction Bases: Enum Source code in edsnlp/pipelines/misc/dates/models.py 13 14 15 16 17 class Direction ( Enum ): FUTURE = \"FUTURE\" PAST = \"PAST\" CURRENT = \"CURRENT\" FUTURE = 'FUTURE' class-attribute PAST = 'PAST' class-attribute CURRENT = 'CURRENT' class-attribute Mode Bases: Enum Source code in edsnlp/pipelines/misc/dates/models.py 20 21 22 23 24 class Mode ( Enum ): FROM = \"FROM\" UNTIL = \"UNTIL\" DURATION = \"DURATION\" FROM = 'FROM' class-attribute UNTIL = 'UNTIL' class-attribute DURATION = 'DURATION' class-attribute Period Bases: BaseModel Source code in edsnlp/pipelines/misc/dates/models.py 27 28 29 30 31 32 33 class Period ( BaseModel ): FROM : Optional [ Span ] = None UNTIL : Optional [ Span ] = None DURATION : Optional [ Span ] = None class Config : arbitrary_types_allowed = True FROM : Optional [ Span ] = None class-attribute UNTIL : Optional [ Span ] = None class-attribute DURATION : Optional [ Span ] = None class-attribute Config Source code in edsnlp/pipelines/misc/dates/models.py 32 33 class Config : arbitrary_types_allowed = True arbitrary_types_allowed = True class-attribute BaseDate Bases: BaseModel Source code in edsnlp/pipelines/misc/dates/models.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BaseDate ( BaseModel ): mode : Optional [ Mode ] = None @validator ( \"*\" , pre = True ) def remove_space ( cls , v ): \"\"\"Remove spaces. Useful for coping with ill-formatted PDF extractions.\"\"\" if isinstance ( v , str ): return v . replace ( \" \" , \"\" ) return v @root_validator ( pre = True ) def validate_strings ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: result = d . copy () for k , v in d . items (): if v is not None and \"_\" in k : key , value = k . split ( \"_\" ) result . update ({ key : value }) return result mode : Optional [ Mode ] = None class-attribute remove_space ( v ) Remove spaces. Useful for coping with ill-formatted PDF extractions. Source code in edsnlp/pipelines/misc/dates/models.py 40 41 42 43 44 45 @validator ( \"*\" , pre = True ) def remove_space ( cls , v ): \"\"\"Remove spaces. Useful for coping with ill-formatted PDF extractions.\"\"\" if isinstance ( v , str ): return v . replace ( \" \" , \"\" ) return v validate_strings ( d ) Source code in edsnlp/pipelines/misc/dates/models.py 47 48 49 50 51 52 53 54 55 56 @root_validator ( pre = True ) def validate_strings ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: result = d . copy () for k , v in d . items (): if v is not None and \"_\" in k : key , value = k . split ( \"_\" ) result . update ({ key : value }) return result AbsoluteDate Bases: BaseDate Source code in edsnlp/pipelines/misc/dates/models.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class AbsoluteDate ( BaseDate ): year : Optional [ int ] = None month : Optional [ int ] = None day : Optional [ int ] = None hour : Optional [ int ] = None minute : Optional [ int ] = None second : Optional [ int ] = None def to_datetime ( self , tz : Union [ str , pendulum . tz . timezone ] = \"Europe/Paris\" , note_datetime : Optional [ datetime ] = None , infer_from_context : bool = False , default_day = 1 , default_month = 1 , ** kwargs , ) -> Optional [ pendulum . datetime ]: d = self . dict ( exclude_none = True ) d . pop ( \"mode\" , None ) if self . year and self . month and self . day : try : return pendulum . datetime ( ** d , tz = tz ) except ValueError : return None elif infer_from_context : # no year if ( not self . year and self . month and self . day and note_datetime and not isinstance ( note_datetime , NaTType ) ): d [ \"year\" ] = note_datetime . year return pendulum . datetime ( ** d , tz = tz ) # no day elif self . year and self . month and not self . day : d [ \"day\" ] = default_day return pendulum . datetime ( ** d , tz = tz ) # year only elif self . year and not self . month and not self . day : d [ \"day\" ] = default_day d [ \"month\" ] = default_month return pendulum . datetime ( ** d , tz = tz ) # month only elif ( not self . year and self . month and not self . day and note_datetime and not isinstance ( note_datetime , NaTType ) ): d [ \"day\" ] = default_day d [ \"year\" ] = note_datetime . year return pendulum . datetime ( ** d , tz = tz ) return None return None def norm ( self ) -> str : year = str ( self . year ) if self . year else \"????\" month = f \" { self . month : 02 } \" if self . month else \"??\" day = f \" { self . day : 02 } \" if self . day else \"??\" norm = \"-\" . join ([ year , month , day ]) if self . hour : norm += f \" { self . hour : 02 } h\" if self . minute : norm += f \" { self . minute : 02 } m\" if self . second : norm += f \" { self . second : 02 } s\" return norm @validator ( \"year\" ) def validate_year ( cls , v ): if v > 100 : return v if v < 25 : return 2000 + v year : Optional [ int ] = None class-attribute month : Optional [ int ] = None class-attribute day : Optional [ int ] = None class-attribute hour : Optional [ int ] = None class-attribute minute : Optional [ int ] = None class-attribute second : Optional [ int ] = None class-attribute to_datetime ( tz = 'Europe/Paris' , note_datetime = None , infer_from_context = False , default_day = 1 , default_month = 1 , ** kwargs ) Source code in edsnlp/pipelines/misc/dates/models.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def to_datetime ( self , tz : Union [ str , pendulum . tz . timezone ] = \"Europe/Paris\" , note_datetime : Optional [ datetime ] = None , infer_from_context : bool = False , default_day = 1 , default_month = 1 , ** kwargs , ) -> Optional [ pendulum . datetime ]: d = self . dict ( exclude_none = True ) d . pop ( \"mode\" , None ) if self . year and self . month and self . day : try : return pendulum . datetime ( ** d , tz = tz ) except ValueError : return None elif infer_from_context : # no year if ( not self . year and self . month and self . day and note_datetime and not isinstance ( note_datetime , NaTType ) ): d [ \"year\" ] = note_datetime . year return pendulum . datetime ( ** d , tz = tz ) # no day elif self . year and self . month and not self . day : d [ \"day\" ] = default_day return pendulum . datetime ( ** d , tz = tz ) # year only elif self . year and not self . month and not self . day : d [ \"day\" ] = default_day d [ \"month\" ] = default_month return pendulum . datetime ( ** d , tz = tz ) # month only elif ( not self . year and self . month and not self . day and note_datetime and not isinstance ( note_datetime , NaTType ) ): d [ \"day\" ] = default_day d [ \"year\" ] = note_datetime . year return pendulum . datetime ( ** d , tz = tz ) return None return None norm () Source code in edsnlp/pipelines/misc/dates/models.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def norm ( self ) -> str : year = str ( self . year ) if self . year else \"????\" month = f \" { self . month : 02 } \" if self . month else \"??\" day = f \" { self . day : 02 } \" if self . day else \"??\" norm = \"-\" . join ([ year , month , day ]) if self . hour : norm += f \" { self . hour : 02 } h\" if self . minute : norm += f \" { self . minute : 02 } m\" if self . second : norm += f \" { self . second : 02 } s\" return norm validate_year ( v ) Source code in edsnlp/pipelines/misc/dates/models.py 143 144 145 146 147 148 149 @validator ( \"year\" ) def validate_year ( cls , v ): if v > 100 : return v if v < 25 : return 2000 + v Relative Bases: BaseDate Source code in edsnlp/pipelines/misc/dates/models.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 class Relative ( BaseDate ): year : Optional [ int ] = None month : Optional [ int ] = None week : Optional [ int ] = None day : Optional [ int ] = None hour : Optional [ int ] = None minute : Optional [ int ] = None second : Optional [ int ] = None @root_validator ( pre = True ) def parse_unit ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: \"\"\" Units need to be handled separately. This validator modifies the key corresponding to the unit with the detected value Parameters ---------- d : Dict[str, str] Original data Returns ------- Dict[str, str] Transformed data \"\"\" unit = d . get ( \"unit\" ) if unit : d [ unit ] = d . get ( \"number\" ) return d def to_datetime ( self , ** kwargs ) -> pendulum . Duration : d = self . dict ( exclude_none = True ) direction = d . pop ( \"direction\" , None ) dir = - 1 if direction == Direction . PAST else 1 d . pop ( \"mode\" , None ) d = { f \" { k } s\" : v for k , v in d . items ()} td = dir * pendulum . duration ( ** d ) return td year : Optional [ int ] = None class-attribute month : Optional [ int ] = None class-attribute week : Optional [ int ] = None class-attribute day : Optional [ int ] = None class-attribute hour : Optional [ int ] = None class-attribute minute : Optional [ int ] = None class-attribute second : Optional [ int ] = None class-attribute parse_unit ( d ) Units need to be handled separately. This validator modifies the key corresponding to the unit with the detected value PARAMETER DESCRIPTION d Original data TYPE: Dict[str, str] RETURNS DESCRIPTION Dict[str, str] Transformed data Source code in edsnlp/pipelines/misc/dates/models.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @root_validator ( pre = True ) def parse_unit ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: \"\"\" Units need to be handled separately. This validator modifies the key corresponding to the unit with the detected value Parameters ---------- d : Dict[str, str] Original data Returns ------- Dict[str, str] Transformed data \"\"\" unit = d . get ( \"unit\" ) if unit : d [ unit ] = d . get ( \"number\" ) return d to_datetime ( ** kwargs ) Source code in edsnlp/pipelines/misc/dates/models.py 187 188 189 190 191 192 193 194 195 196 197 198 def to_datetime ( self , ** kwargs ) -> pendulum . Duration : d = self . dict ( exclude_none = True ) direction = d . pop ( \"direction\" , None ) dir = - 1 if direction == Direction . PAST else 1 d . pop ( \"mode\" , None ) d = { f \" { k } s\" : v for k , v in d . items ()} td = dir * pendulum . duration ( ** d ) return td RelativeDate Bases: Relative Source code in edsnlp/pipelines/misc/dates/models.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 class RelativeDate ( Relative ): direction : Direction = Direction . CURRENT def to_datetime ( self , note_datetime : Optional [ datetime ] = None , ** kwargs , ) -> pendulum . Duration : td = super ( RelativeDate , self ) . to_datetime () if note_datetime is not None and not isinstance ( note_datetime , NaTType ): return note_datetime + td return td def norm ( self ) -> str : if self . direction == Direction . CURRENT : d = self . dict ( exclude_none = True ) d . pop ( \"direction\" , None ) d . pop ( \"mode\" , None ) key = next ( iter ( d . keys ()), \"day\" ) norm = f \"~0 { key } \" else : td = self . to_datetime () norm = str ( td ) if td . in_seconds () > 0 : norm = f \"+ { norm } \" return norm @root_validator ( pre = True ) def handle_specifics ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: \"\"\" Specific patterns such as `aujourd'hui`, `hier`, etc, need to be handled separately. Parameters ---------- d : Dict[str, str] Original data. Returns ------- Dict[str, str] Modified data. \"\"\" specific = d . get ( \"specific\" ) specific = specific_dict . get ( specific ) if specific : d . update ( specific ) return d direction : Direction = Direction . CURRENT class-attribute to_datetime ( note_datetime = None , ** kwargs ) Source code in edsnlp/pipelines/misc/dates/models.py 204 205 206 207 208 209 210 211 212 213 214 def to_datetime ( self , note_datetime : Optional [ datetime ] = None , ** kwargs , ) -> pendulum . Duration : td = super ( RelativeDate , self ) . to_datetime () if note_datetime is not None and not isinstance ( note_datetime , NaTType ): return note_datetime + td return td norm () Source code in edsnlp/pipelines/misc/dates/models.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def norm ( self ) -> str : if self . direction == Direction . CURRENT : d = self . dict ( exclude_none = True ) d . pop ( \"direction\" , None ) d . pop ( \"mode\" , None ) key = next ( iter ( d . keys ()), \"day\" ) norm = f \"~0 { key } \" else : td = self . to_datetime () norm = str ( td ) if td . in_seconds () > 0 : norm = f \"+ { norm } \" return norm handle_specifics ( d ) Specific patterns such as aujourd'hui , hier , etc, need to be handled separately. PARAMETER DESCRIPTION d Original data. TYPE: Dict[str, str] RETURNS DESCRIPTION Dict[str, str] Modified data. Source code in edsnlp/pipelines/misc/dates/models.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 @root_validator ( pre = True ) def handle_specifics ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: \"\"\" Specific patterns such as `aujourd'hui`, `hier`, etc, need to be handled separately. Parameters ---------- d : Dict[str, str] Original data. Returns ------- Dict[str, str] Modified data. \"\"\" specific = d . get ( \"specific\" ) specific = specific_dict . get ( specific ) if specific : d . update ( specific ) return d Duration Bases: Relative Source code in edsnlp/pipelines/misc/dates/models.py 260 261 262 263 264 265 266 class Duration ( Relative ): mode : Mode = Mode . DURATION def norm ( self ) -> str : td = self . to_datetime () return f \"during { td } \" mode : Mode = Mode . DURATION class-attribute norm () Source code in edsnlp/pipelines/misc/dates/models.py 263 264 265 266 def norm ( self ) -> str : td = self . to_datetime () return f \"during { td } \"","title":"models"},{"location":"reference/pipelines/misc/dates/models/#edsnlppipelinesmiscdatesmodels","text":"","title":"edsnlp.pipelines.misc.dates.models"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Direction","text":"Bases: Enum Source code in edsnlp/pipelines/misc/dates/models.py 13 14 15 16 17 class Direction ( Enum ): FUTURE = \"FUTURE\" PAST = \"PAST\" CURRENT = \"CURRENT\"","title":"Direction"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Direction.FUTURE","text":"","title":"FUTURE"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Direction.PAST","text":"","title":"PAST"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Direction.CURRENT","text":"","title":"CURRENT"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Mode","text":"Bases: Enum Source code in edsnlp/pipelines/misc/dates/models.py 20 21 22 23 24 class Mode ( Enum ): FROM = \"FROM\" UNTIL = \"UNTIL\" DURATION = \"DURATION\"","title":"Mode"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Mode.FROM","text":"","title":"FROM"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Mode.UNTIL","text":"","title":"UNTIL"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Mode.DURATION","text":"","title":"DURATION"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Period","text":"Bases: BaseModel Source code in edsnlp/pipelines/misc/dates/models.py 27 28 29 30 31 32 33 class Period ( BaseModel ): FROM : Optional [ Span ] = None UNTIL : Optional [ Span ] = None DURATION : Optional [ Span ] = None class Config : arbitrary_types_allowed = True","title":"Period"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Period.FROM","text":"","title":"FROM"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Period.UNTIL","text":"","title":"UNTIL"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Period.DURATION","text":"","title":"DURATION"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Period.Config","text":"Source code in edsnlp/pipelines/misc/dates/models.py 32 33 class Config : arbitrary_types_allowed = True","title":"Config"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Period.Config.arbitrary_types_allowed","text":"","title":"arbitrary_types_allowed"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.BaseDate","text":"Bases: BaseModel Source code in edsnlp/pipelines/misc/dates/models.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BaseDate ( BaseModel ): mode : Optional [ Mode ] = None @validator ( \"*\" , pre = True ) def remove_space ( cls , v ): \"\"\"Remove spaces. Useful for coping with ill-formatted PDF extractions.\"\"\" if isinstance ( v , str ): return v . replace ( \" \" , \"\" ) return v @root_validator ( pre = True ) def validate_strings ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: result = d . copy () for k , v in d . items (): if v is not None and \"_\" in k : key , value = k . split ( \"_\" ) result . update ({ key : value }) return result","title":"BaseDate"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.BaseDate.mode","text":"","title":"mode"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.BaseDate.remove_space","text":"Remove spaces. Useful for coping with ill-formatted PDF extractions. Source code in edsnlp/pipelines/misc/dates/models.py 40 41 42 43 44 45 @validator ( \"*\" , pre = True ) def remove_space ( cls , v ): \"\"\"Remove spaces. Useful for coping with ill-formatted PDF extractions.\"\"\" if isinstance ( v , str ): return v . replace ( \" \" , \"\" ) return v","title":"remove_space()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.BaseDate.validate_strings","text":"Source code in edsnlp/pipelines/misc/dates/models.py 47 48 49 50 51 52 53 54 55 56 @root_validator ( pre = True ) def validate_strings ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: result = d . copy () for k , v in d . items (): if v is not None and \"_\" in k : key , value = k . split ( \"_\" ) result . update ({ key : value }) return result","title":"validate_strings()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate","text":"Bases: BaseDate Source code in edsnlp/pipelines/misc/dates/models.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class AbsoluteDate ( BaseDate ): year : Optional [ int ] = None month : Optional [ int ] = None day : Optional [ int ] = None hour : Optional [ int ] = None minute : Optional [ int ] = None second : Optional [ int ] = None def to_datetime ( self , tz : Union [ str , pendulum . tz . timezone ] = \"Europe/Paris\" , note_datetime : Optional [ datetime ] = None , infer_from_context : bool = False , default_day = 1 , default_month = 1 , ** kwargs , ) -> Optional [ pendulum . datetime ]: d = self . dict ( exclude_none = True ) d . pop ( \"mode\" , None ) if self . year and self . month and self . day : try : return pendulum . datetime ( ** d , tz = tz ) except ValueError : return None elif infer_from_context : # no year if ( not self . year and self . month and self . day and note_datetime and not isinstance ( note_datetime , NaTType ) ): d [ \"year\" ] = note_datetime . year return pendulum . datetime ( ** d , tz = tz ) # no day elif self . year and self . month and not self . day : d [ \"day\" ] = default_day return pendulum . datetime ( ** d , tz = tz ) # year only elif self . year and not self . month and not self . day : d [ \"day\" ] = default_day d [ \"month\" ] = default_month return pendulum . datetime ( ** d , tz = tz ) # month only elif ( not self . year and self . month and not self . day and note_datetime and not isinstance ( note_datetime , NaTType ) ): d [ \"day\" ] = default_day d [ \"year\" ] = note_datetime . year return pendulum . datetime ( ** d , tz = tz ) return None return None def norm ( self ) -> str : year = str ( self . year ) if self . year else \"????\" month = f \" { self . month : 02 } \" if self . month else \"??\" day = f \" { self . day : 02 } \" if self . day else \"??\" norm = \"-\" . join ([ year , month , day ]) if self . hour : norm += f \" { self . hour : 02 } h\" if self . minute : norm += f \" { self . minute : 02 } m\" if self . second : norm += f \" { self . second : 02 } s\" return norm @validator ( \"year\" ) def validate_year ( cls , v ): if v > 100 : return v if v < 25 : return 2000 + v","title":"AbsoluteDate"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.year","text":"","title":"year"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.month","text":"","title":"month"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.day","text":"","title":"day"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.hour","text":"","title":"hour"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.minute","text":"","title":"minute"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.second","text":"","title":"second"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.to_datetime","text":"Source code in edsnlp/pipelines/misc/dates/models.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def to_datetime ( self , tz : Union [ str , pendulum . tz . timezone ] = \"Europe/Paris\" , note_datetime : Optional [ datetime ] = None , infer_from_context : bool = False , default_day = 1 , default_month = 1 , ** kwargs , ) -> Optional [ pendulum . datetime ]: d = self . dict ( exclude_none = True ) d . pop ( \"mode\" , None ) if self . year and self . month and self . day : try : return pendulum . datetime ( ** d , tz = tz ) except ValueError : return None elif infer_from_context : # no year if ( not self . year and self . month and self . day and note_datetime and not isinstance ( note_datetime , NaTType ) ): d [ \"year\" ] = note_datetime . year return pendulum . datetime ( ** d , tz = tz ) # no day elif self . year and self . month and not self . day : d [ \"day\" ] = default_day return pendulum . datetime ( ** d , tz = tz ) # year only elif self . year and not self . month and not self . day : d [ \"day\" ] = default_day d [ \"month\" ] = default_month return pendulum . datetime ( ** d , tz = tz ) # month only elif ( not self . year and self . month and not self . day and note_datetime and not isinstance ( note_datetime , NaTType ) ): d [ \"day\" ] = default_day d [ \"year\" ] = note_datetime . year return pendulum . datetime ( ** d , tz = tz ) return None return None","title":"to_datetime()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.norm","text":"Source code in edsnlp/pipelines/misc/dates/models.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def norm ( self ) -> str : year = str ( self . year ) if self . year else \"????\" month = f \" { self . month : 02 } \" if self . month else \"??\" day = f \" { self . day : 02 } \" if self . day else \"??\" norm = \"-\" . join ([ year , month , day ]) if self . hour : norm += f \" { self . hour : 02 } h\" if self . minute : norm += f \" { self . minute : 02 } m\" if self . second : norm += f \" { self . second : 02 } s\" return norm","title":"norm()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.AbsoluteDate.validate_year","text":"Source code in edsnlp/pipelines/misc/dates/models.py 143 144 145 146 147 148 149 @validator ( \"year\" ) def validate_year ( cls , v ): if v > 100 : return v if v < 25 : return 2000 + v","title":"validate_year()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative","text":"Bases: BaseDate Source code in edsnlp/pipelines/misc/dates/models.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 class Relative ( BaseDate ): year : Optional [ int ] = None month : Optional [ int ] = None week : Optional [ int ] = None day : Optional [ int ] = None hour : Optional [ int ] = None minute : Optional [ int ] = None second : Optional [ int ] = None @root_validator ( pre = True ) def parse_unit ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: \"\"\" Units need to be handled separately. This validator modifies the key corresponding to the unit with the detected value Parameters ---------- d : Dict[str, str] Original data Returns ------- Dict[str, str] Transformed data \"\"\" unit = d . get ( \"unit\" ) if unit : d [ unit ] = d . get ( \"number\" ) return d def to_datetime ( self , ** kwargs ) -> pendulum . Duration : d = self . dict ( exclude_none = True ) direction = d . pop ( \"direction\" , None ) dir = - 1 if direction == Direction . PAST else 1 d . pop ( \"mode\" , None ) d = { f \" { k } s\" : v for k , v in d . items ()} td = dir * pendulum . duration ( ** d ) return td","title":"Relative"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.year","text":"","title":"year"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.month","text":"","title":"month"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.week","text":"","title":"week"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.day","text":"","title":"day"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.hour","text":"","title":"hour"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.minute","text":"","title":"minute"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.second","text":"","title":"second"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.parse_unit","text":"Units need to be handled separately. This validator modifies the key corresponding to the unit with the detected value PARAMETER DESCRIPTION d Original data TYPE: Dict[str, str] RETURNS DESCRIPTION Dict[str, str] Transformed data Source code in edsnlp/pipelines/misc/dates/models.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @root_validator ( pre = True ) def parse_unit ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: \"\"\" Units need to be handled separately. This validator modifies the key corresponding to the unit with the detected value Parameters ---------- d : Dict[str, str] Original data Returns ------- Dict[str, str] Transformed data \"\"\" unit = d . get ( \"unit\" ) if unit : d [ unit ] = d . get ( \"number\" ) return d","title":"parse_unit()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.to_datetime","text":"Source code in edsnlp/pipelines/misc/dates/models.py 187 188 189 190 191 192 193 194 195 196 197 198 def to_datetime ( self , ** kwargs ) -> pendulum . Duration : d = self . dict ( exclude_none = True ) direction = d . pop ( \"direction\" , None ) dir = - 1 if direction == Direction . PAST else 1 d . pop ( \"mode\" , None ) d = { f \" { k } s\" : v for k , v in d . items ()} td = dir * pendulum . duration ( ** d ) return td","title":"to_datetime()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.RelativeDate","text":"Bases: Relative Source code in edsnlp/pipelines/misc/dates/models.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 class RelativeDate ( Relative ): direction : Direction = Direction . CURRENT def to_datetime ( self , note_datetime : Optional [ datetime ] = None , ** kwargs , ) -> pendulum . Duration : td = super ( RelativeDate , self ) . to_datetime () if note_datetime is not None and not isinstance ( note_datetime , NaTType ): return note_datetime + td return td def norm ( self ) -> str : if self . direction == Direction . CURRENT : d = self . dict ( exclude_none = True ) d . pop ( \"direction\" , None ) d . pop ( \"mode\" , None ) key = next ( iter ( d . keys ()), \"day\" ) norm = f \"~0 { key } \" else : td = self . to_datetime () norm = str ( td ) if td . in_seconds () > 0 : norm = f \"+ { norm } \" return norm @root_validator ( pre = True ) def handle_specifics ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: \"\"\" Specific patterns such as `aujourd'hui`, `hier`, etc, need to be handled separately. Parameters ---------- d : Dict[str, str] Original data. Returns ------- Dict[str, str] Modified data. \"\"\" specific = d . get ( \"specific\" ) specific = specific_dict . get ( specific ) if specific : d . update ( specific ) return d","title":"RelativeDate"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.RelativeDate.direction","text":"","title":"direction"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.RelativeDate.to_datetime","text":"Source code in edsnlp/pipelines/misc/dates/models.py 204 205 206 207 208 209 210 211 212 213 214 def to_datetime ( self , note_datetime : Optional [ datetime ] = None , ** kwargs , ) -> pendulum . Duration : td = super ( RelativeDate , self ) . to_datetime () if note_datetime is not None and not isinstance ( note_datetime , NaTType ): return note_datetime + td return td","title":"to_datetime()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.RelativeDate.norm","text":"Source code in edsnlp/pipelines/misc/dates/models.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def norm ( self ) -> str : if self . direction == Direction . CURRENT : d = self . dict ( exclude_none = True ) d . pop ( \"direction\" , None ) d . pop ( \"mode\" , None ) key = next ( iter ( d . keys ()), \"day\" ) norm = f \"~0 { key } \" else : td = self . to_datetime () norm = str ( td ) if td . in_seconds () > 0 : norm = f \"+ { norm } \" return norm","title":"norm()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.RelativeDate.handle_specifics","text":"Specific patterns such as aujourd'hui , hier , etc, need to be handled separately. PARAMETER DESCRIPTION d Original data. TYPE: Dict[str, str] RETURNS DESCRIPTION Dict[str, str] Modified data. Source code in edsnlp/pipelines/misc/dates/models.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 @root_validator ( pre = True ) def handle_specifics ( cls , d : Dict [ str , str ]) -> Dict [ str , str ]: \"\"\" Specific patterns such as `aujourd'hui`, `hier`, etc, need to be handled separately. Parameters ---------- d : Dict[str, str] Original data. Returns ------- Dict[str, str] Modified data. \"\"\" specific = d . get ( \"specific\" ) specific = specific_dict . get ( specific ) if specific : d . update ( specific ) return d","title":"handle_specifics()"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Duration","text":"Bases: Relative Source code in edsnlp/pipelines/misc/dates/models.py 260 261 262 263 264 265 266 class Duration ( Relative ): mode : Mode = Mode . DURATION def norm ( self ) -> str : td = self . to_datetime () return f \"during { td } \"","title":"Duration"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Duration.mode","text":"","title":"mode"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Duration.norm","text":"Source code in edsnlp/pipelines/misc/dates/models.py 263 264 265 266 def norm ( self ) -> str : td = self . to_datetime () return f \"during { td } \"","title":"norm()"},{"location":"reference/pipelines/misc/dates/patterns/","text":"edsnlp.pipelines.misc.dates.patterns","title":"`edsnlp.pipelines.misc.dates.patterns`"},{"location":"reference/pipelines/misc/dates/patterns/#edsnlppipelinesmiscdatespatterns","text":"","title":"edsnlp.pipelines.misc.dates.patterns"},{"location":"reference/pipelines/misc/dates/patterns/absolute/","text":"edsnlp.pipelines.misc.dates.patterns.absolute no_year_pattern = [ day + raw_delimiter_with_spaces_pattern + month + time_pattern + post_num_pattern for day in [ ante_num_pattern + numeric_day_pattern , letter_day_pattern ] for month in [ numeric_month_pattern + post_num_pattern , letter_month_pattern ]] module-attribute no_day_pattern = [ letter_month_pattern + raw_delimiter_with_spaces_pattern + year_pattern + post_num_pattern , ante_num_pattern + lz_numeric_month_pattern + raw_delimiter_with_spaces_pattern + year_pattern + post_num_pattern ] module-attribute no_day_no_year_pattern = [ letter_month_pattern ] module-attribute full_year_pattern = ante_num_pattern + fy_pattern + post_num_pattern module-attribute absolute_pattern = [ '(?<=' + mode_pattern + '.{,3})?' + p for p in absolute_pattern ] module-attribute","title":"absolute"},{"location":"reference/pipelines/misc/dates/patterns/absolute/#edsnlppipelinesmiscdatespatternsabsolute","text":"","title":"edsnlp.pipelines.misc.dates.patterns.absolute"},{"location":"reference/pipelines/misc/dates/patterns/absolute/#edsnlp.pipelines.misc.dates.patterns.absolute.no_year_pattern","text":"","title":"no_year_pattern"},{"location":"reference/pipelines/misc/dates/patterns/absolute/#edsnlp.pipelines.misc.dates.patterns.absolute.no_day_pattern","text":"","title":"no_day_pattern"},{"location":"reference/pipelines/misc/dates/patterns/absolute/#edsnlp.pipelines.misc.dates.patterns.absolute.no_day_no_year_pattern","text":"","title":"no_day_no_year_pattern"},{"location":"reference/pipelines/misc/dates/patterns/absolute/#edsnlp.pipelines.misc.dates.patterns.absolute.full_year_pattern","text":"","title":"full_year_pattern"},{"location":"reference/pipelines/misc/dates/patterns/absolute/#edsnlp.pipelines.misc.dates.patterns.absolute.absolute_pattern","text":"","title":"absolute_pattern"},{"location":"reference/pipelines/misc/dates/patterns/current/","text":"edsnlp.pipelines.misc.dates.patterns.current current_patterns : List [ str ] = [ '(?P<year_0>cette \\\\ s+ann[\u00e9e]e)(?![- \\\\ s]l[\u00e0a])' , \"(?P<day_0>ce \\\\ s+jour|aujourd[' \\\\ s]?hui)\" , '(?P<week_0>cette \\\\ s+semaine|ces \\\\ sjours[- \\\\ s]ci)' , '(?P<month_0>ce \\\\ smois([- \\\\ s]ci)?)' ] module-attribute current_pattern = make_pattern ( current_patterns , with_breaks = True ) module-attribute","title":"current"},{"location":"reference/pipelines/misc/dates/patterns/current/#edsnlppipelinesmiscdatespatternscurrent","text":"","title":"edsnlp.pipelines.misc.dates.patterns.current"},{"location":"reference/pipelines/misc/dates/patterns/current/#edsnlp.pipelines.misc.dates.patterns.current.current_patterns","text":"","title":"current_patterns"},{"location":"reference/pipelines/misc/dates/patterns/current/#edsnlp.pipelines.misc.dates.patterns.current.current_pattern","text":"","title":"current_pattern"},{"location":"reference/pipelines/misc/dates/patterns/duration/","text":"edsnlp.pipelines.misc.dates.patterns.duration cue_pattern = '(pendant|durant|pdt)' module-attribute duration_pattern = [ cue_pattern + '.{,3}' + numbers . number_pattern + ' \\\\ s*' + units . unit_pattern ] module-attribute","title":"duration"},{"location":"reference/pipelines/misc/dates/patterns/duration/#edsnlppipelinesmiscdatespatternsduration","text":"","title":"edsnlp.pipelines.misc.dates.patterns.duration"},{"location":"reference/pipelines/misc/dates/patterns/duration/#edsnlp.pipelines.misc.dates.patterns.duration.cue_pattern","text":"","title":"cue_pattern"},{"location":"reference/pipelines/misc/dates/patterns/duration/#edsnlp.pipelines.misc.dates.patterns.duration.duration_pattern","text":"","title":"duration_pattern"},{"location":"reference/pipelines/misc/dates/patterns/false_positive/","text":"edsnlp.pipelines.misc.dates.patterns.false_positive page_patterns = [ ' \\\\ d \\\\ / \\\\ d' ] module-attribute phone_patterns = [ '( \\\\ d \\\\ d' + delimiter + '){3,} \\\\ d \\\\ d' for delimiter in delimiters ] module-attribute false_positive_pattern = make_pattern ( page_patterns + phone_patterns ) module-attribute","title":"false_positive"},{"location":"reference/pipelines/misc/dates/patterns/false_positive/#edsnlppipelinesmiscdatespatternsfalse_positive","text":"","title":"edsnlp.pipelines.misc.dates.patterns.false_positive"},{"location":"reference/pipelines/misc/dates/patterns/false_positive/#edsnlp.pipelines.misc.dates.patterns.false_positive.page_patterns","text":"","title":"page_patterns"},{"location":"reference/pipelines/misc/dates/patterns/false_positive/#edsnlp.pipelines.misc.dates.patterns.false_positive.phone_patterns","text":"","title":"phone_patterns"},{"location":"reference/pipelines/misc/dates/patterns/false_positive/#edsnlp.pipelines.misc.dates.patterns.false_positive.false_positive_pattern","text":"","title":"false_positive_pattern"},{"location":"reference/pipelines/misc/dates/patterns/relative/","text":"edsnlp.pipelines.misc.dates.patterns.relative specific = { 'minus1' : ( 'hier' , dict ( direction = 'PAST' , day = 1 )), 'minus2' : ( 'avant[- \\\\ s]hier' , dict ( direction = 'PAST' , day = 2 )), 'plus1' : ( 'demain' , dict ( direction = 'FUTURE' , day = 1 )), 'plus2' : ( 'apr\u00e8s[- \\\\ s]demain' , dict ( direction = 'FUTURE' , day = 2 ))} module-attribute specific_pattern = make_pattern ([ '(?P<specific_ {k} > {p} )' for ( k , ( p , _ )) in specific . items ()]) module-attribute specific_dict = { k : v for ( k , ( _ , v )) in specific . items ()} module-attribute relative_pattern = [ '(?<=' + mode_pattern + '.{,3})?' + p for p in relative_pattern ] module-attribute make_specific_pattern ( mode = 'forward' ) Source code in edsnlp/pipelines/misc/dates/patterns/relative.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def make_specific_pattern ( mode : str = \"forward\" ): if mode == \"forward\" : p = directions . preceding_direction_pattern p += r \"\\s+\" p += numbers . number_pattern p += r \"\\s*\" p += units . unit_pattern elif mode == \"backward\" : p = numbers . number_pattern p += r \"\\s*\" p += units . unit_pattern p += r \"\\s+\" p += directions . following_direction_pattern else : p = directions . preceding_direction_pattern p += r \"\\s+\" p += numbers . number_pattern p += r \"\\s*\" p += units . unit_pattern p += r \"\\s+\" p += directions . following_direction_pattern return p","title":"relative"},{"location":"reference/pipelines/misc/dates/patterns/relative/#edsnlppipelinesmiscdatespatternsrelative","text":"","title":"edsnlp.pipelines.misc.dates.patterns.relative"},{"location":"reference/pipelines/misc/dates/patterns/relative/#edsnlp.pipelines.misc.dates.patterns.relative.specific","text":"","title":"specific"},{"location":"reference/pipelines/misc/dates/patterns/relative/#edsnlp.pipelines.misc.dates.patterns.relative.specific_pattern","text":"","title":"specific_pattern"},{"location":"reference/pipelines/misc/dates/patterns/relative/#edsnlp.pipelines.misc.dates.patterns.relative.specific_dict","text":"","title":"specific_dict"},{"location":"reference/pipelines/misc/dates/patterns/relative/#edsnlp.pipelines.misc.dates.patterns.relative.relative_pattern","text":"","title":"relative_pattern"},{"location":"reference/pipelines/misc/dates/patterns/relative/#edsnlp.pipelines.misc.dates.patterns.relative.make_specific_pattern","text":"Source code in edsnlp/pipelines/misc/dates/patterns/relative.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def make_specific_pattern ( mode : str = \"forward\" ): if mode == \"forward\" : p = directions . preceding_direction_pattern p += r \"\\s+\" p += numbers . number_pattern p += r \"\\s*\" p += units . unit_pattern elif mode == \"backward\" : p = numbers . number_pattern p += r \"\\s*\" p += units . unit_pattern p += r \"\\s+\" p += directions . following_direction_pattern else : p = directions . preceding_direction_pattern p += r \"\\s+\" p += numbers . number_pattern p += r \"\\s*\" p += units . unit_pattern p += r \"\\s+\" p += directions . following_direction_pattern return p","title":"make_specific_pattern()"},{"location":"reference/pipelines/misc/dates/patterns/atomic/","text":"edsnlp.pipelines.misc.dates.patterns.atomic","title":"`edsnlp.pipelines.misc.dates.patterns.atomic`"},{"location":"reference/pipelines/misc/dates/patterns/atomic/#edsnlppipelinesmiscdatespatternsatomic","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.days letter_days = [ '(?P<day_01>premier|1 \\\\ s*er)' , '(?P<day_02>deux)' , '(?P<day_03>trois)' , '(?P<day_04>quatre)' , '(?P<day_05>cinq)' , '(?P<day_06>six)' , '(?P<day_07>sept)' , '(?P<day_08>huit)' , '(?P<day_09>neuf)' , '(?P<day_10>dix)' , '(?P<day_11>onze)' , '(?P<day_12>douze)' , '(?P<day_13>treize)' , '(?P<day_14>quatorze)' , '(?P<day_15>quinze)' , '(?P<day_16>seize)' , '(?P<day_17>dix \\\\ -? \\\\ s*sept)' , '(?P<day_18>dix \\\\ -? \\\\ s*huit)' , '(?P<day_19>dix \\\\ -? \\\\ s*neuf)' , '(?P<day_20>vingt)' , '(?P<day_21>vingt \\\\ -? \\\\ s*et \\\\ -? \\\\ s*un)' , '(?P<day_22>vingt \\\\ -? \\\\ s*deux)' , '(?P<day_23>vingt \\\\ -? \\\\ s*trois)' , '(?P<day_24>vingt \\\\ -? \\\\ s*quatre)' , '(?P<day_25>vingt \\\\ -? \\\\ s*cinq)' , '(?P<day_26>vingt \\\\ -? \\\\ s*six)' , '(?P<day_27>vingt \\\\ -? \\\\ s*sept)' , '(?P<day_28>vingt \\\\ -? \\\\ s*huit)' , '(?P<day_29>vingt \\\\ -? \\\\ s*neuf)' , '(?P<day_30>trente)' , '(?P<day_31>trente \\\\ -? \\\\ s*et \\\\ -? \\\\ s*un)' ] module-attribute letter_day_pattern = make_pattern ( letter_days ) module-attribute nlz_numeric_day_pattern = '(?<! \\\\ d)([1-9]|[12] \\\\ d|3[01])(?! \\\\ d)' module-attribute numeric_day_pattern = '(?P<day> {numeric_day_pattern} )' module-attribute lz_numeric_day_pattern = '(?P<day> {lz_numeric_day_pattern} )' module-attribute day_pattern = '( {letter_day_pattern} | {numeric_day_pattern} )' module-attribute","title":"days"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/#edsnlppipelinesmiscdatespatternsatomicdays","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.days"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/#edsnlp.pipelines.misc.dates.patterns.atomic.days.letter_days","text":"","title":"letter_days"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/#edsnlp.pipelines.misc.dates.patterns.atomic.days.letter_day_pattern","text":"","title":"letter_day_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/#edsnlp.pipelines.misc.dates.patterns.atomic.days.nlz_numeric_day_pattern","text":"","title":"nlz_numeric_day_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/#edsnlp.pipelines.misc.dates.patterns.atomic.days.numeric_day_pattern","text":"","title":"numeric_day_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/#edsnlp.pipelines.misc.dates.patterns.atomic.days.lz_numeric_day_pattern","text":"","title":"lz_numeric_day_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/#edsnlp.pipelines.misc.dates.patterns.atomic.days.day_pattern","text":"","title":"day_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.delimiters raw_delimiters = [ ' \\\\ /' , ' \\\\ -' ] module-attribute delimiters = raw_delimiters + [ ' \\\\ .' , '[^ \\\\ S]+' ] module-attribute raw_delimiter_pattern = make_pattern ( raw_delimiters ) module-attribute raw_delimiter_with_spaces_pattern = make_pattern ( raw_delimiters + [ '[^ \\\\ S]+' ]) module-attribute delimiter_pattern = make_pattern ( delimiters ) module-attribute ante_num_pattern = '(?<!.(?: {raw_delimiter_pattern} )|[0-9][.,])' module-attribute post_num_pattern = '(?! {raw_delimiter_pattern} )' module-attribute","title":"delimiters"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/#edsnlppipelinesmiscdatespatternsatomicdelimiters","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.delimiters"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/#edsnlp.pipelines.misc.dates.patterns.atomic.delimiters.raw_delimiters","text":"","title":"raw_delimiters"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/#edsnlp.pipelines.misc.dates.patterns.atomic.delimiters.delimiters","text":"","title":"delimiters"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/#edsnlp.pipelines.misc.dates.patterns.atomic.delimiters.raw_delimiter_pattern","text":"","title":"raw_delimiter_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/#edsnlp.pipelines.misc.dates.patterns.atomic.delimiters.raw_delimiter_with_spaces_pattern","text":"","title":"raw_delimiter_with_spaces_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/#edsnlp.pipelines.misc.dates.patterns.atomic.delimiters.delimiter_pattern","text":"","title":"delimiter_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/#edsnlp.pipelines.misc.dates.patterns.atomic.delimiters.ante_num_pattern","text":"","title":"ante_num_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/#edsnlp.pipelines.misc.dates.patterns.atomic.delimiters.post_num_pattern","text":"","title":"post_num_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/directions/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.directions preceding_directions = [ '(?P<direction_PAST>depuis|depuis \\\\ s+le|il \\\\ s+y \\\\ s+a|\u00e0)' , '(?P<direction_FUTURE>dans)' ] module-attribute following_directions = [ '(?P<direction_FUTURE>prochaine?s?|suivante?s?|plus \\\\ s+tard)' , '(?P<direction_PAST>derni[e\u00e8]re?s?|pass\u00e9e?s?|pr[\u00e9e]c[\u00e9e]dente?s?|plus \\\\ s+t[\u00f4o]t)' ] module-attribute preceding_direction_pattern = make_pattern ( preceding_directions , with_breaks = True ) module-attribute following_direction_pattern = make_pattern ( following_directions , with_breaks = True ) module-attribute","title":"directions"},{"location":"reference/pipelines/misc/dates/patterns/atomic/directions/#edsnlppipelinesmiscdatespatternsatomicdirections","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.directions"},{"location":"reference/pipelines/misc/dates/patterns/atomic/directions/#edsnlp.pipelines.misc.dates.patterns.atomic.directions.preceding_directions","text":"","title":"preceding_directions"},{"location":"reference/pipelines/misc/dates/patterns/atomic/directions/#edsnlp.pipelines.misc.dates.patterns.atomic.directions.following_directions","text":"","title":"following_directions"},{"location":"reference/pipelines/misc/dates/patterns/atomic/directions/#edsnlp.pipelines.misc.dates.patterns.atomic.directions.preceding_direction_pattern","text":"","title":"preceding_direction_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/directions/#edsnlp.pipelines.misc.dates.patterns.atomic.directions.following_direction_pattern","text":"","title":"following_direction_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/modes/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.modes modes = [ '(?P<mode_FROM>depuis|depuis \\\\ s+le|[\u00e0a] \\\\ s+partir \\\\ s+d[eu]|du)' , \"(?P<mode_UNTIL>jusqu'[\u00e0a]u?|au)\" ] module-attribute mode_pattern = make_pattern ( modes , with_breaks = True ) module-attribute","title":"modes"},{"location":"reference/pipelines/misc/dates/patterns/atomic/modes/#edsnlppipelinesmiscdatespatternsatomicmodes","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.modes"},{"location":"reference/pipelines/misc/dates/patterns/atomic/modes/#edsnlp.pipelines.misc.dates.patterns.atomic.modes.modes","text":"","title":"modes"},{"location":"reference/pipelines/misc/dates/patterns/atomic/modes/#edsnlp.pipelines.misc.dates.patterns.atomic.modes.mode_pattern","text":"","title":"mode_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.months letter_months = [ '(?P<month_01>janvier|janv \\\\ .?)' , '(?P<month_02>f[\u00e9e]vrier|f[\u00e9e]v \\\\ .?)' , '(?P<month_03>mars|mar \\\\ .?)' , '(?P<month_04>avril|avr \\\\ .?)' , '(?P<month_05>mai)' , '(?P<month_06>juin)' , '(?P<month_07>juillet|juill? \\\\ .?)' , '(?P<month_08>ao[u\u00fb]t)' , '(?P<month_09>septembre|sept? \\\\ .?)' , '(?P<month_10>octobre|oct \\\\ .?)' , '(?P<month_11>novembre|nov \\\\ .?)' , '(?P<month_12>d[\u00e9e]cembre|d[\u00e9e]c \\\\ .?)' ] module-attribute letter_month_pattern = make_pattern ( letter_months , with_breaks = True ) module-attribute numeric_month_pattern = '(?P<month> {numeric_month_pattern} )' module-attribute lz_numeric_month_pattern = '(?P<month> {lz_numeric_month_pattern} )' module-attribute month_pattern = '( {letter_month_pattern} | {numeric_month_pattern} )' module-attribute","title":"months"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/#edsnlppipelinesmiscdatespatternsatomicmonths","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.months"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/#edsnlp.pipelines.misc.dates.patterns.atomic.months.letter_months","text":"","title":"letter_months"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/#edsnlp.pipelines.misc.dates.patterns.atomic.months.letter_month_pattern","text":"","title":"letter_month_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/#edsnlp.pipelines.misc.dates.patterns.atomic.months.numeric_month_pattern","text":"","title":"numeric_month_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/#edsnlp.pipelines.misc.dates.patterns.atomic.months.lz_numeric_month_pattern","text":"","title":"lz_numeric_month_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/#edsnlp.pipelines.misc.dates.patterns.atomic.months.month_pattern","text":"","title":"month_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/numbers/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.numbers letter_numbers = [ \"(?P<number_01>l'|le|la|une?|ce|cette|cet)\" , '(?P<number_02>deux)' , '(?P<number_03>trois)' , '(?P<number_04>quatre)' , '(?P<number_05>cinq)' , '(?P<number_06>six)' , '(?P<number_07>sept)' , '(?P<number_08>huit)' , '(?P<number_09>neuf)' , '(?P<number_10>dix)' , '(?P<number_11>onze)' , '(?P<number_12>douze)' , '(?P<number_12>treize)' , '(?P<number_13>quatorze)' , '(?P<number_14>quinze)' , '(?P<number_15>seize)' , '(?P<number_16>dix[- \\\\ s]sept)' , '(?P<number_17>dix[- \\\\ s]huit)' , '(?P<number_18>dix[- \\\\ s]neuf)' , '(?P<number_20>vingt)' , '(?P<number_21>vingt[- \\\\ s]et[- \\\\ s]un)' , '(?P<number_22>vingt[- \\\\ s]deux)' , '(?P<number_23>vingt[- \\\\ s]trois)' , '(?P<number_24>vingt[- \\\\ s]quatre)' , '(?P<number_25>vingt[- \\\\ s]cinq)' , '(?P<number_26>vingt[- \\\\ s]six)' , '(?P<number_27>vingt[- \\\\ s]sept)' , '(?P<number_28>vingt[- \\\\ s]huit)' , '(?P<number_29>vingt[- \\\\ s]neuf)' , '(?P<number_30>trente)' ] module-attribute numeric_numbers = [ str ( i ) for i in range ( 1 , 100 )] module-attribute letter_number_pattern = make_pattern ( letter_numbers , with_breaks = True ) module-attribute numeric_number_pattern = make_pattern ( numeric_numbers , name = 'number' ) module-attribute number_pattern = '( {letter_number_pattern} | {numeric_number_pattern} )' module-attribute","title":"numbers"},{"location":"reference/pipelines/misc/dates/patterns/atomic/numbers/#edsnlppipelinesmiscdatespatternsatomicnumbers","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.numbers"},{"location":"reference/pipelines/misc/dates/patterns/atomic/numbers/#edsnlp.pipelines.misc.dates.patterns.atomic.numbers.letter_numbers","text":"","title":"letter_numbers"},{"location":"reference/pipelines/misc/dates/patterns/atomic/numbers/#edsnlp.pipelines.misc.dates.patterns.atomic.numbers.numeric_numbers","text":"","title":"numeric_numbers"},{"location":"reference/pipelines/misc/dates/patterns/atomic/numbers/#edsnlp.pipelines.misc.dates.patterns.atomic.numbers.letter_number_pattern","text":"","title":"letter_number_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/numbers/#edsnlp.pipelines.misc.dates.patterns.atomic.numbers.numeric_number_pattern","text":"","title":"numeric_number_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/numbers/#edsnlp.pipelines.misc.dates.patterns.atomic.numbers.number_pattern","text":"","title":"number_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.time hour_pattern = '(?<! \\\\ d)(?P<hour>0?[1-9]|1 \\\\ d|2[0-3])(?! \\\\ d)' module-attribute lz_hour_pattern = '(?<! \\\\ d)(?P<hour>0[1-9]|[12] \\\\ d|3[01])(?! \\\\ d)' module-attribute minute_pattern = '(?<! \\\\ d)(?P<minute>0?[1-9]|[1-5] \\\\ d)(?! \\\\ d)' module-attribute lz_minute_pattern = '(?<! \\\\ d)(?P<minute>0[1-9]|[1-5] \\\\ d)(?! \\\\ d)' module-attribute second_pattern = '(?<! \\\\ d)(?P<second>0?[1-9]|[1-5] \\\\ d)(?! \\\\ d)' module-attribute lz_second_pattern = '(?<! \\\\ d)(?P<second>0[1-9]|[1-5] \\\\ d)(?! \\\\ d)' module-attribute time_pattern = '( \\\\ s.{,3}' + ' {hour_pattern} [h:]( {lz_minute_pattern} )?' + '((:|m|min) {lz_second_pattern} )?' + ')?' module-attribute","title":"time"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlppipelinesmiscdatespatternsatomictime","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.time"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlp.pipelines.misc.dates.patterns.atomic.time.hour_pattern","text":"","title":"hour_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlp.pipelines.misc.dates.patterns.atomic.time.lz_hour_pattern","text":"","title":"lz_hour_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlp.pipelines.misc.dates.patterns.atomic.time.minute_pattern","text":"","title":"minute_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlp.pipelines.misc.dates.patterns.atomic.time.lz_minute_pattern","text":"","title":"lz_minute_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlp.pipelines.misc.dates.patterns.atomic.time.second_pattern","text":"","title":"second_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlp.pipelines.misc.dates.patterns.atomic.time.lz_second_pattern","text":"","title":"lz_second_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/#edsnlp.pipelines.misc.dates.patterns.atomic.time.time_pattern","text":"","title":"time_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/units/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.units units = [ '(?P<unit_year>ans?|ann[\u00e9e]es?)' , '(?P<unit_semester>semestres?)' , '(?P<unit_trimester>trimestres?)' , '(?P<unit_month>mois)' , '(?P<unit_week>semaines?)' , '(?P<unit_day>jours?|journ[\u00e9e]es?)' , '(?P<unit_hour>h|heures?)' , '(?P<unit_minute>min|minutes?)' , '(?P<unit_second>sec|secondes?|s)' ] module-attribute unit_pattern = make_pattern ( units , with_breaks = True ) module-attribute","title":"units"},{"location":"reference/pipelines/misc/dates/patterns/atomic/units/#edsnlppipelinesmiscdatespatternsatomicunits","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.units"},{"location":"reference/pipelines/misc/dates/patterns/atomic/units/#edsnlp.pipelines.misc.dates.patterns.atomic.units.units","text":"","title":"units"},{"location":"reference/pipelines/misc/dates/patterns/atomic/units/#edsnlp.pipelines.misc.dates.patterns.atomic.units.unit_pattern","text":"","title":"unit_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/years/","text":"edsnlp.pipelines.misc.dates.patterns.atomic.years year_patterns : List [ str ] = [ '19 \\\\ d \\\\ d' ] + [ str ( year ) for year in range ( 2000 , date . today () . year + 2 )] module-attribute full_year_pattern = '(?<! \\\\ d)' + full_year_pattern + '(?! \\\\ d)' module-attribute year_pattern = '(?<! \\\\ d)' + year_pattern + '(?! \\\\ d)' module-attribute","title":"years"},{"location":"reference/pipelines/misc/dates/patterns/atomic/years/#edsnlppipelinesmiscdatespatternsatomicyears","text":"","title":"edsnlp.pipelines.misc.dates.patterns.atomic.years"},{"location":"reference/pipelines/misc/dates/patterns/atomic/years/#edsnlp.pipelines.misc.dates.patterns.atomic.years.year_patterns","text":"","title":"year_patterns"},{"location":"reference/pipelines/misc/dates/patterns/atomic/years/#edsnlp.pipelines.misc.dates.patterns.atomic.years.full_year_pattern","text":"","title":"full_year_pattern"},{"location":"reference/pipelines/misc/dates/patterns/atomic/years/#edsnlp.pipelines.misc.dates.patterns.atomic.years.year_pattern","text":"","title":"year_pattern"},{"location":"reference/pipelines/misc/measurements/","text":"edsnlp.pipelines.misc.measurements","title":"`edsnlp.pipelines.misc.measurements`"},{"location":"reference/pipelines/misc/measurements/#edsnlppipelinesmiscmeasurements","text":"","title":"edsnlp.pipelines.misc.measurements"},{"location":"reference/pipelines/misc/measurements/factory/","text":"edsnlp.pipelines.misc.measurements.factory DEFAULT_CONFIG = dict ( attr = 'NORM' , ignore_excluded = True , units_config = patterns . units_config , number_terms = patterns . number_terms , unit_divisors = patterns . unit_divisors , measurements = None , stopwords = patterns . stopwords ) module-attribute create_component ( nlp , name , measurements , units_config , number_terms , stopwords , unit_divisors , ignore_excluded , attr ) Source code in edsnlp/pipelines/misc/measurements/factory.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @Language . factory ( \"eds.measurements\" , default_config = DEFAULT_CONFIG ) @deprecated_factory ( \"eds.measures\" , \"eds.measurements\" , default_config = DEFAULT_CONFIG ) def create_component ( nlp : Language , name : str , measurements : Optional [ Union [ Dict [ str , MeasureConfig ], List [ str ]]], units_config : Dict [ str , UnitConfig ], number_terms : Dict [ str , List [ str ]], stopwords : List [ str ], unit_divisors : List [ str ], ignore_excluded : bool , attr : str , ): return MeasurementsMatcher ( nlp , units_config = units_config , number_terms = number_terms , unit_divisors = unit_divisors , measurements = measurements , stopwords = stopwords , attr = attr , ignore_excluded = ignore_excluded , )","title":"factory"},{"location":"reference/pipelines/misc/measurements/factory/#edsnlppipelinesmiscmeasurementsfactory","text":"","title":"edsnlp.pipelines.misc.measurements.factory"},{"location":"reference/pipelines/misc/measurements/factory/#edsnlp.pipelines.misc.measurements.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/misc/measurements/factory/#edsnlp.pipelines.misc.measurements.factory.create_component","text":"Source code in edsnlp/pipelines/misc/measurements/factory.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @Language . factory ( \"eds.measurements\" , default_config = DEFAULT_CONFIG ) @deprecated_factory ( \"eds.measures\" , \"eds.measurements\" , default_config = DEFAULT_CONFIG ) def create_component ( nlp : Language , name : str , measurements : Optional [ Union [ Dict [ str , MeasureConfig ], List [ str ]]], units_config : Dict [ str , UnitConfig ], number_terms : Dict [ str , List [ str ]], stopwords : List [ str ], unit_divisors : List [ str ], ignore_excluded : bool , attr : str , ): return MeasurementsMatcher ( nlp , units_config = units_config , number_terms = number_terms , unit_divisors = unit_divisors , measurements = measurements , stopwords = stopwords , attr = attr , ignore_excluded = ignore_excluded , )","title":"create_component()"},{"location":"reference/pipelines/misc/measurements/measurements/","text":"edsnlp.pipelines.misc.measurements.measurements __all__ = [ 'MeasurementsMatcher' ] module-attribute AFTER_SNIPPET_LIMIT = 6 module-attribute BEFORE_SNIPPET_LIMIT = 10 module-attribute UnitConfig Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 26 27 28 29 30 31 class UnitConfig ( TypedDict ): dim : str degree : int scale : float terms : List [ str ] followed_by : Optional [ str ] = None dim : str = None class-attribute degree : int = None class-attribute scale : float = None class-attribute terms : List [ str ] = None class-attribute followed_by : Optional [ str ] = None class-attribute UnitlessRange Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 34 35 36 37 class UnitlessRange ( TypedDict ): min : int max : int unit : str min : int = None class-attribute max : int = None class-attribute unit : str = None class-attribute UnitlessPatternConfig Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 40 41 42 class UnitlessPatternConfig ( TypedDict ): terms : List [ str ] ranges : List [ UnitlessRange ] terms : List [ str ] = None class-attribute ranges : List [ UnitlessRange ] = None class-attribute UnitlessPatternConfigWithName Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 45 46 47 48 class UnitlessPatternConfigWithName ( TypedDict ): terms : List [ str ] ranges : List [ UnitlessRange ] name : str terms : List [ str ] = None class-attribute ranges : List [ UnitlessRange ] = None class-attribute name : str = None class-attribute MeasureConfig Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 51 52 53 class MeasureConfig ( TypedDict ): unit : str unitless_patterns : List [ UnitlessPatternConfig ] unit : str = None class-attribute unitless_patterns : List [ UnitlessPatternConfig ] = None class-attribute Measurement Bases: abc . ABC Source code in edsnlp/pipelines/misc/measurements/measurements.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class Measurement ( abc . ABC ): @abc . abstractmethod def __iter__ ( self ) -> Iterable [ \"SimpleMeasurement\" ]: \"\"\" Iter over items of the measure (only one for SimpleMeasurement) Returns ------- iterable : Iterable[\"SimpleMeasurement\"] \"\"\" @abc . abstractmethod def __getitem__ ( self , item ) -> \"SimpleMeasurement\" : \"\"\" Access items of the measure (only one for SimpleMeasurement) Parameters ---------- item : int Returns ------- measure : SimpleMeasurement \"\"\" __iter__ () Iter over items of the measure (only one for SimpleMeasurement) RETURNS DESCRIPTION iterable TYPE: Iterable[\"SimpleMeasurement\"] Source code in edsnlp/pipelines/misc/measurements/measurements.py 57 58 59 60 61 62 63 64 65 @abc . abstractmethod def __iter__ ( self ) -> Iterable [ \"SimpleMeasurement\" ]: \"\"\" Iter over items of the measure (only one for SimpleMeasurement) Returns ------- iterable : Iterable[\"SimpleMeasurement\"] \"\"\" __getitem__ ( item ) Access items of the measure (only one for SimpleMeasurement) PARAMETER DESCRIPTION item TYPE: int RETURNS DESCRIPTION measure TYPE: SimpleMeasurement Source code in edsnlp/pipelines/misc/measurements/measurements.py 67 68 69 70 71 72 73 74 75 76 77 78 79 @abc . abstractmethod def __getitem__ ( self , item ) -> \"SimpleMeasurement\" : \"\"\" Access items of the measure (only one for SimpleMeasurement) Parameters ---------- item : int Returns ------- measure : SimpleMeasurement \"\"\" UnitRegistry Source code in edsnlp/pipelines/misc/measurements/measurements.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class UnitRegistry : def __init__ ( self , config : Dict [ str , UnitConfig ]): self . config = { unicodedata . normalize ( \"NFKC\" , k ): v for k , v in config . items ()} for unit , unit_config in list ( self . config . items ()): if not unit . startswith ( \"per_\" ) and \"per_\" + unit not in unit_config : self . config [ \"per_\" + unit ] = { \"dim\" : unit_config [ \"dim\" ], \"degree\" : - unit_config [ \"degree\" ], \"scale\" : 1 / unit_config [ \"scale\" ], } @lru_cache ( maxsize =- 1 ) def parse_unit ( self , unit : str ) -> Tuple [ str , float ]: degrees = defaultdict ( lambda : 0 ) scale = 1 for part in regex . split ( \"(?<!per)_\" , unit ): unit_config = self . config [ unicodedata . normalize ( \"NFKC\" , part )] degrees [ unit_config [ \"dim\" ]] += unit_config [ \"degree\" ] scale *= unit_config [ \"scale\" ] return str ( dict ( sorted ( degrees . items ()))), scale config = { unicodedata . normalize ( 'NFKC' , k ): v for ( k , v ) in config . items ()} instance-attribute __init__ ( config ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 83 84 85 86 87 88 89 90 91 def __init__ ( self , config : Dict [ str , UnitConfig ]): self . config = { unicodedata . normalize ( \"NFKC\" , k ): v for k , v in config . items ()} for unit , unit_config in list ( self . config . items ()): if not unit . startswith ( \"per_\" ) and \"per_\" + unit not in unit_config : self . config [ \"per_\" + unit ] = { \"dim\" : unit_config [ \"dim\" ], \"degree\" : - unit_config [ \"degree\" ], \"scale\" : 1 / unit_config [ \"scale\" ], } parse_unit ( unit ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 93 94 95 96 97 98 99 100 101 @lru_cache ( maxsize =- 1 ) def parse_unit ( self , unit : str ) -> Tuple [ str , float ]: degrees = defaultdict ( lambda : 0 ) scale = 1 for part in regex . split ( \"(?<!per)_\" , unit ): unit_config = self . config [ unicodedata . normalize ( \"NFKC\" , part )] degrees [ unit_config [ \"dim\" ]] += unit_config [ \"degree\" ] scale *= unit_config [ \"scale\" ] return str ( dict ( sorted ( degrees . items ()))), scale SimpleMeasurement Bases: Measurement Source code in edsnlp/pipelines/misc/measurements/measurements.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class SimpleMeasurement ( Measurement ): def __init__ ( self , value , unit , registry ): \"\"\" The SimpleMeasurement class contains the value and unit for a single non-composite measure Parameters ---------- value : float unit : str \"\"\" super () . __init__ () self . value = value self . unit = unit self . registry = registry def __iter__ ( self ): return iter (( self ,)) def __getitem__ ( self , item : int ): assert isinstance ( item , int ) return [ self ][ item ] def __str__ ( self ): return f \" { self . value } { self . unit } \" def __repr__ ( self ): return f \"Measurement( { self . value } , { repr ( self . unit ) } )\" def __eq__ ( self , other : Any ): if isinstance ( other , SimpleMeasurement ): return self . convert_to ( other . unit ) == other . value return False def __add__ ( self , other : \"SimpleMeasurement\" ): if other . unit == self . unit : return self . __class__ ( self . value + other . value , self . unit , self . registry ) return self . __class__ ( self . value + other . convert_to ( self . unit ), self . unit , self . registry ) def __lt__ ( self , other : \"SimpleMeasurement\" ): return self . convert_to ( other . unit ) < other . value def __le__ ( self , other : \"SimpleMeasurement\" ): return self . convert_to ( other . unit ) <= other . value def convert_to ( self , other_unit ): self_degrees , self_scale = self . registry . parse_unit ( self . unit ) other_degrees , other_scale = self . registry . parse_unit ( other_unit ) if self_degrees != other_degrees : raise AttributeError ( f \"Units { self . unit } and { other_unit } are not homogenous\" ) new_value = self . value * self_scale / other_scale return new_value def __getattr__ ( self , other_unit ): return self . convert_to ( other_unit ) @classmethod def verify ( cls , ent ): return True value = value instance-attribute unit = unit instance-attribute registry = registry instance-attribute __init__ ( value , unit , registry ) The SimpleMeasurement class contains the value and unit for a single non-composite measure PARAMETER DESCRIPTION value TYPE: float unit TYPE: str Source code in edsnlp/pipelines/misc/measurements/measurements.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , value , unit , registry ): \"\"\" The SimpleMeasurement class contains the value and unit for a single non-composite measure Parameters ---------- value : float unit : str \"\"\" super () . __init__ () self . value = value self . unit = unit self . registry = registry __iter__ () Source code in edsnlp/pipelines/misc/measurements/measurements.py 120 121 def __iter__ ( self ): return iter (( self ,)) __getitem__ ( item ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 123 124 125 def __getitem__ ( self , item : int ): assert isinstance ( item , int ) return [ self ][ item ] __str__ () Source code in edsnlp/pipelines/misc/measurements/measurements.py 127 128 def __str__ ( self ): return f \" { self . value } { self . unit } \" __repr__ () Source code in edsnlp/pipelines/misc/measurements/measurements.py 130 131 def __repr__ ( self ): return f \"Measurement( { self . value } , { repr ( self . unit ) } )\" __eq__ ( other ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 133 134 135 136 def __eq__ ( self , other : Any ): if isinstance ( other , SimpleMeasurement ): return self . convert_to ( other . unit ) == other . value return False __add__ ( other ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 138 139 140 141 142 143 def __add__ ( self , other : \"SimpleMeasurement\" ): if other . unit == self . unit : return self . __class__ ( self . value + other . value , self . unit , self . registry ) return self . __class__ ( self . value + other . convert_to ( self . unit ), self . unit , self . registry ) __lt__ ( other ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 145 146 def __lt__ ( self , other : \"SimpleMeasurement\" ): return self . convert_to ( other . unit ) < other . value __le__ ( other ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 148 149 def __le__ ( self , other : \"SimpleMeasurement\" ): return self . convert_to ( other . unit ) <= other . value convert_to ( other_unit ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 151 152 153 154 155 156 157 158 159 160 def convert_to ( self , other_unit ): self_degrees , self_scale = self . registry . parse_unit ( self . unit ) other_degrees , other_scale = self . registry . parse_unit ( other_unit ) if self_degrees != other_degrees : raise AttributeError ( f \"Units { self . unit } and { other_unit } are not homogenous\" ) new_value = self . value * self_scale / other_scale return new_value __getattr__ ( other_unit ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 162 163 def __getattr__ ( self , other_unit ): return self . convert_to ( other_unit ) verify ( ent ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 165 166 167 @classmethod def verify ( cls , ent ): return True MeasurementsMatcher Source code in edsnlp/pipelines/misc/measurements/measurements.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 class MeasurementsMatcher : def __init__ ( self , nlp : spacy . Language , measurements : Union [ List [ str ], Tuple [ str ], Dict [ str , MeasureConfig ]], units_config : Dict [ str , UnitConfig ], number_terms : Dict [ str , List [ str ]], stopwords : List [ str ] = ( \"par\" , \"sur\" , \"de\" , \"a\" , \":\" ), unit_divisors : List [ str ] = ( \"par\" , \"/\" ), name : str = \"measurements\" , ignore_excluded : bool = True , attr : str = \"NORM\" , ): \"\"\" Matcher component to extract measurements. A measurements is most often composed of a number and a unit like > 1,26 cm The unit can also be positioned in place of the decimal dot/comma > 1 cm 26 Some measurements can be composite > 1,26 cm x 2,34 mm And sometimes they are factorized > Les trois kystes mesurent 1, 2 et 3cm. The recognized measurements are stored in the \"measurements\" SpanGroup. Each span has a `Measurement` object stored in the \"value\" extension attribute. Parameters ---------- nlp : Language The SpaCy object. measurements : Dict[str, MeasureConfig] A mapping from measure names to MeasureConfig Each measure's configuration has the following shape: { \"unit\": str, # the unit of the measure (like \"kg\"), \"unitless_patterns\": { # optional patterns to handle unitless cases \"terms\": List[str], # list of preceding terms used to trigger the measure # Mapping from ranges to unit to handle cases like # (\"Taille: 1.2\" -> 1.20 m vs \"Taille: 120\" -> 120cm) \"ranges\": List[{ \"min\": int, \"max\": int, \"unit\": str, }, { \"min\": int, \"unit\": str, }, ...], } number_terms: Dict[str, List[str] A mapping of numbers to their lexical variants stopwords: List[str] A list of stopwords that do not matter when placed between a unitless trigger and a number unit_divisors: List[str] A list of terms used to divide two units (like: m / s) attr : str Whether to match on the text ('TEXT') or on the normalized text ('NORM') ignore_excluded : bool Whether to exclude pollution patterns when matching in the text \"\"\" if measurements is None : measurements = common_measurements elif isinstance ( measurements , ( list , tuple )): measurements = { m : common_measurements [ m ] for m in measurements } self . nlp = nlp self . name = name self . unit_registry = UnitRegistry ( units_config ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = True ) self . term_matcher = EDSPhraseMatcher ( nlp . vocab , attr = attr , ignore_excluded = True ) self . unitless_patterns : Dict [ str , UnitlessPatternConfigWithName ] = {} self . unit_part_label_hashes : Set [ int ] = set () self . unitless_label_hashes : Set [ int ] = set () self . unit_followers : Dict [ str , str ] = {} self . measure_names : Dict [ str , str ] = {} # NUMBER PATTERNS self . regex_matcher . add ( \"number\" , [ r \"(?<![a-z-])\\d+([ ]\\d {3} )*[ ]+(?:[,.][ ]+\\d+)?\" , r \"(?<![a-z-])\\d+([ ]\\d {3} )*(?:[,.]\\d+)?\" , ], ) self . number_label_hashes = { nlp . vocab . strings [ \"number\" ]} for number , terms in number_terms . items (): self . term_matcher . build_patterns ( nlp , { number : terms }) self . number_label_hashes . add ( nlp . vocab . strings [ number ]) # UNIT PATTERNS for unit_name , unit_config in units_config . items (): self . term_matcher . build_patterns ( nlp , { unit_name : unit_config [ \"terms\" ]}) if unit_config . get ( \"followed_by\" ) is not None : self . unit_followers [ unit_name ] = unit_config [ \"followed_by\" ] self . unit_part_label_hashes . add ( nlp . vocab . strings [ unit_name ]) self . unit_part_label_hashes . add ( nlp . vocab . strings [ \"per\" ]) self . term_matcher . build_patterns ( nlp , { \"per\" : unit_divisors }) self . term_matcher . add ( \"stopword\" , list ( nlp . pipe ( stopwords ))) # MEASURES for name , measure_config in measurements . items (): unit = measure_config [ \"unit\" ] self . measure_names [ self . unit_registry . parse_unit ( unit )[ 0 ]] = name if \"unitless_patterns\" in measure_config : for pattern in measure_config [ \"unitless_patterns\" ]: pattern_name = f \"unitless_ { len ( self . unitless_patterns ) } \" self . term_matcher . add ( pattern_name , list ( nlp . pipe ( pattern [ \"terms\" ])) ) self . unitless_label_hashes . add ( nlp . vocab . strings [ pattern_name ]) self . unitless_patterns [ pattern_name ] = { \"name\" : name , ** pattern } self . set_extensions () @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the measurements pipeline. \"\"\" if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) def extract_units ( self , term_matches : Iterable [ Span ]) -> Iterable [ Span ]: \"\"\" Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" => we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) => we aggregate these adjacent matches together to compose a new unit g_per_day Parameters ---------- term_matches: Iterable[Span] Returns ------- Iterable[Span] \"\"\" last = None units = [] current = [] unit_label_hashes = set () for unit_part in filter_spans ( term_matches ): if unit_part . label not in self . unit_part_label_hashes : continue if last is not None and unit_part . start != last . end and len ( current ): doc = current [ 0 ] . doc # Last non \"per\" match: we don't want our units to be like `g_per` end = next ( ( i for i , e in list ( enumerate ( current ))[:: - 1 ] if e . label_ != \"per\" ), None , ) if end is not None : unit = \"_\" . join ( part . label_ for part in current [: end + 1 ]) units . append ( Span ( doc , current [ 0 ] . start , current [ end ] . end , unit )) unit_label_hashes . add ( units [ - 1 ] . label ) current = [] last = None if len ( current ) > 0 or unit_part . label_ != \"per\" : current . append ( unit_part ) last = unit_part end = next ( ( i for i , e in list ( enumerate ( current ))[:: - 1 ] if e . label_ != \"per\" ), None ) if end is not None : doc = current [ 0 ] . doc unit = \"_\" . join ( part . label_ for part in current [: end + 1 ]) units . append ( Span ( doc , current [ 0 ] . start , current [ end ] . end , unit )) unit_label_hashes . add ( units [ - 1 ] . label ) return units @classmethod def make_pseudo_sentence ( cls , doc : Doc , matches : List [ Tuple [ Span , bool ]], pseudo_mapping : Dict [ int , str ], ) -> Tuple [ str , List [ int ]]: \"\"\" Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font [1][,] [2] [et] [3] [cm] de long[.]\" is transformed into \"wn,n,nuw.\" Parameters ---------- doc: Doc matches: List[(Span, bool)] List of tuple of span and whether the span represents a sentence end pseudo_mapping: Dict[int, str] A mapping from label to char in the pseudo sentence Returns ------- (str, List[int]) - the pseudo sentence - a list of offsets to convert match indices into pseudo sent char indices \"\"\" pseudo = [] last = 0 offsets = [] for ent , is_sent_split in matches : if ent . start != last : pseudo . append ( \"w\" ) offsets . append ( len ( pseudo )) if is_sent_split : pseudo . append ( \".\" ) else : pseudo . append ( pseudo_mapping . get ( ent . label , \"w\" )) last = ent . end if len ( doc ) != last : pseudo . append ( \"w\" ) pseudo = \"\" . join ( pseudo ) return pseudo , offsets def get_matches ( self , doc ): \"\"\" Extract and filter regex and phrase matches in the document to prepare the measurement extraction. Returns the matches and a list of hashes to quickly find unit matches Parameters ---------- doc: Doc Returns ------- Tuple[List[(Span, bool)], Set[int]] - List of tuples of spans and whether the spans represents a sentence end - List of hash label to distinguish unit from other matches \"\"\" sent_ends = [ doc [ i : i + 1 ] for i in range ( len ( doc )) if doc [ i ] . is_sent_end ] regex_matches = list ( self . regex_matcher ( doc , as_spans = True )) term_matches = list ( self . term_matcher ( doc , as_spans = True )) # Detect unit parts and compose them into units units = self . extract_units ( term_matches ) unit_label_hashes = { unit . label for unit in units } # Filter matches to prevent matches over dates or doc entities non_unit_terms = [ term for term in term_matches if term . label not in self . unit_part_label_hashes ] # Filter out measurement-related spans that overlap already matched # entities (in doc.ents or doc.spans[\"dates\"]) # Note: we also include sentence ends tokens as 1-token spans in those matches spans__keep__is_sent_end = filter_spans ( [ # Tuples (span, keep = is measurement related, is sentence end) * zip ( doc . spans . get ( \"dates\" , ()), repeat ( False ), repeat ( False )), * zip ( regex_matches , repeat ( True ), repeat ( False )), * zip ( non_unit_terms , repeat ( True ), repeat ( False )), * zip ( units , repeat ( True ), repeat ( False )), * zip ( doc . ents , repeat ( False ), repeat ( False )), * zip ( sent_ends , repeat ( True ), repeat ( True )), ], # filter entities to keep only the ... sort_key = measurements_match_tuples_sort_key , ) # Remove non-measurement related spans (keep = False) and sort the matches matches_and_is_sentence_end : List [( Span , bool )] = sorted ( [ ( span , is_sent_end ) for span , keep , is_sent_end in spans__keep__is_sent_end # and remove entities that are not relevant to this pipeline if keep ] ) return matches_and_is_sentence_end , unit_label_hashes def extract_measurements ( self , doc : Doc ): \"\"\" Extracts measure entities from the document Parameters ---------- doc: Doc Returns ------- List[Span] \"\"\" matches , unit_label_hashes = self . get_matches ( doc ) # Make match slice function to query them def get_matches_after ( i ): anchor = matches [ i ][ 0 ] for j , ( ent , is_sent_end ) in enumerate ( matches [ i + 1 :]): if not is_sent_end and ent . start > anchor . end + AFTER_SNIPPET_LIMIT : return yield j + i + 1 , ent def get_matches_before ( i ): anchor = matches [ i ][ 0 ] for j , ( ent , is_sent_end ) in enumerate ( matches [ i :: - 1 ]): if not is_sent_end and ent . end < anchor . start - BEFORE_SNIPPET_LIMIT : return yield i - j , ent # Make a pseudo sentence to query higher order patterns in the main loop # `offsets` is a mapping from matches indices (ie match n\u00b0i) to # char indices in the pseudo sentence pseudo , offsets = self . make_pseudo_sentence ( doc , matches , { self . nlp . vocab . strings [ \"stopword\" ]: \",\" , self . nlp . vocab . strings [ \"number\" ]: \"n\" , ** { name : \"u\" for name in unit_label_hashes }, ** { name : \"n\" for name in self . number_label_hashes }, }, ) measurements = [] matched_unit_indices = set () # Iterate through the number matches for number_idx , ( number , is_sent_split ) in enumerate ( matches ): if not is_sent_split and number . label not in self . number_label_hashes : continue # Detect the measure value try : if number . label_ == \"number\" : value = float ( number . text . replace ( \" \" , \"\" ) . replace ( \",\" , \".\" ) . replace ( \" \" , \"\" ) ) else : value = float ( number . label_ ) except ValueError : continue unit_idx = unit_text = unit_norm = None # Find the closest unit after the number try : unit_idx , unit_text = next ( ( j , ent ) for j , ent in get_matches_after ( number_idx ) if ent . label in unit_label_hashes ) unit_norm = unit_text . label_ except ( AttributeError , StopIteration ): pass # Try to pair the number with this next unit if the two are only separated # by numbers and separators alternatively (as in [1][,] [2] [and] [3] cm) try : pseudo_sent = pseudo [ offsets [ number_idx ] + 1 : offsets [ unit_idx ]] if not re . fullmatch ( r \"(,n)*\" , pseudo_sent ): unit_text , unit_norm = None , None except TypeError : pass # Otherwise, try to infer the unit from the preceding unit to handle cases # like (1 meter 50) if unit_norm is None and number_idx - 1 in matched_unit_indices : try : unit_before = matches [ number_idx - 1 ][ 0 ] if unit_before . end == number . start : unit_norm = self . unit_followers [ unit_before . label_ ] except ( KeyError , AttributeError , IndexError ): pass # If no unit was matched, try to detect unitless patterns before # the number to handle cases like (\"Weight: 63, Height: 170\") if not unit_norm : try : ( unitless_idx , unitless_text ) = next ( ( j , e ) for j , e in get_matches_before ( number_idx ) if e . label in self . unitless_label_hashes ) unit_norm = None if re . fullmatch ( r \"[,n]*\" , pseudo [ offsets [ unitless_idx ] + 1 : offsets [ number_idx ]], ): unitless_pattern = self . unitless_patterns [ unitless_text . label_ ] unit_norm = next ( scope [ \"unit\" ] for scope in unitless_pattern [ \"ranges\" ] if ( \"min\" not in scope or value >= scope [ \"min\" ]) and ( \"max\" not in scope or value < scope [ \"max\" ]) ) except StopIteration : pass # Otherwise, skip this number if not unit_norm : continue # Compute the final entity if unit_text and unit_text . end == number . start : ent = doc [ unit_text . start : number . end ] elif unit_text and unit_text . start == number . end : ent = doc [ number . start : unit_text . end ] else : ent = number # Compute the dimensionality of the parsed unit try : dims = self . unit_registry . parse_unit ( unit_norm )[ 0 ] except KeyError : continue # If the measure was not requested, dismiss it # Otherwise, relabel the entity and create the value attribute if dims not in self . measure_names : continue ent . _ . value = SimpleMeasurement ( value , unit_norm , self . unit_registry ) ent . label_ = self . measure_names [ dims ] measurements . append ( ent ) if unit_idx is not None : matched_unit_indices . add ( unit_idx ) return measurements @classmethod def merge_adjacent_measurements ( cls , measurements : List [ Span ]) -> List [ Span ]: \"\"\" Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\" Parameters ---------- measurements: List[Span] Returns ------- List[Span] \"\"\" merged = measurements [: 1 ] for ent in measurements [ 1 :]: last = merged [ - 1 ] if last . end == ent . start and last . _ . value . unit != ent . _ . value . unit : try : new_value = last . _ . value + ent . _ . value merged [ - 1 ] = last = last . doc [ last . start : ent . end ] last . _ . value = new_value last . label_ = ent . label_ except ( AttributeError , TypeError ): merged . append ( ent ) else : merged . append ( ent ) return merged def __call__ ( self , doc ): \"\"\" Adds measurements to document's \"measurements\" SpanGroup. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted measurements. \"\"\" measurements = self . extract_measurements ( doc ) measurements = self . merge_adjacent_measurements ( measurements ) doc . spans [ \"measurements\" ] = measurements # for backward compatibility doc . spans [ \"measures\" ] = doc . spans [ \"measurements\" ] return doc nlp = nlp instance-attribute name = name instance-attribute unit_registry = UnitRegistry ( units_config ) instance-attribute regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = True ) instance-attribute term_matcher = EDSPhraseMatcher ( nlp . vocab , attr = attr , ignore_excluded = True ) instance-attribute unitless_patterns : Dict [ str , UnitlessPatternConfigWithName ] = {} instance-attribute unit_part_label_hashes : Set [ int ] = set () instance-attribute unitless_label_hashes : Set [ int ] = set () instance-attribute unit_followers : Dict [ str , str ] = {} instance-attribute measure_names : Dict [ str , str ] = {} instance-attribute number_label_hashes = { nlp . vocab . strings [ 'number' ]} instance-attribute __init__ ( nlp , measurements , units_config , number_terms , stopwords = ( \"par\" , \"sur\" , \"de\" , \"a\" , \":\" ), unit_divisors = ( \"par\" , \"/\" ), name = 'measurements' , ignore_excluded = True , attr = 'NORM' ) Matcher component to extract measurements. A measurements is most often composed of a number and a unit like 1,26 cm The unit can also be positioned in place of the decimal dot/comma 1 cm 26 Some measurements can be composite 1,26 cm x 2,34 mm And sometimes they are factorized Les trois kystes mesurent 1, 2 et 3cm. The recognized measurements are stored in the \"measurements\" SpanGroup. Each span has a Measurement object stored in the \"value\" extension attribute. PARAMETER DESCRIPTION nlp The SpaCy object. TYPE: Language measurements A mapping from measure names to MeasureConfig Each measure's configuration has the following shape: { \"unit\": str, # the unit of the measure (like \"kg\"), \"unitless_patterns\": { # optional patterns to handle unitless cases \"terms\": List[str], # list of preceding terms used to trigger the measure Mapping from ranges to unit to handle cases like (\"Taille: 1.2\" -> 1.20 m vs \"Taille: 120\" -> 120cm) \"ranges\": List[{ \"min\": int, \"max\": int, \"unit\": str, }, { \"min\": int, \"unit\": str, }, ...], } TYPE: Dict[str, MeasureConfig] number_terms A mapping of numbers to their lexical variants TYPE: Dict [ str , List [ str ]] stopwords A list of stopwords that do not matter when placed between a unitless trigger and a number TYPE: List [ str ] DEFAULT: (\"par\", \"sur\", \"de\", \"a\", \":\") unit_divisors A list of terms used to divide two units (like: m / s) TYPE: List [ str ] DEFAULT: (\"par\", \"/\") attr Whether to match on the text ('TEXT') or on the normalized text ('NORM') TYPE: str DEFAULT: 'NORM' ignore_excluded Whether to exclude pollution patterns when matching in the text TYPE: bool DEFAULT: True Source code in edsnlp/pipelines/misc/measurements/measurements.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def __init__ ( self , nlp : spacy . Language , measurements : Union [ List [ str ], Tuple [ str ], Dict [ str , MeasureConfig ]], units_config : Dict [ str , UnitConfig ], number_terms : Dict [ str , List [ str ]], stopwords : List [ str ] = ( \"par\" , \"sur\" , \"de\" , \"a\" , \":\" ), unit_divisors : List [ str ] = ( \"par\" , \"/\" ), name : str = \"measurements\" , ignore_excluded : bool = True , attr : str = \"NORM\" , ): \"\"\" Matcher component to extract measurements. A measurements is most often composed of a number and a unit like > 1,26 cm The unit can also be positioned in place of the decimal dot/comma > 1 cm 26 Some measurements can be composite > 1,26 cm x 2,34 mm And sometimes they are factorized > Les trois kystes mesurent 1, 2 et 3cm. The recognized measurements are stored in the \"measurements\" SpanGroup. Each span has a `Measurement` object stored in the \"value\" extension attribute. Parameters ---------- nlp : Language The SpaCy object. measurements : Dict[str, MeasureConfig] A mapping from measure names to MeasureConfig Each measure's configuration has the following shape: { \"unit\": str, # the unit of the measure (like \"kg\"), \"unitless_patterns\": { # optional patterns to handle unitless cases \"terms\": List[str], # list of preceding terms used to trigger the measure # Mapping from ranges to unit to handle cases like # (\"Taille: 1.2\" -> 1.20 m vs \"Taille: 120\" -> 120cm) \"ranges\": List[{ \"min\": int, \"max\": int, \"unit\": str, }, { \"min\": int, \"unit\": str, }, ...], } number_terms: Dict[str, List[str] A mapping of numbers to their lexical variants stopwords: List[str] A list of stopwords that do not matter when placed between a unitless trigger and a number unit_divisors: List[str] A list of terms used to divide two units (like: m / s) attr : str Whether to match on the text ('TEXT') or on the normalized text ('NORM') ignore_excluded : bool Whether to exclude pollution patterns when matching in the text \"\"\" if measurements is None : measurements = common_measurements elif isinstance ( measurements , ( list , tuple )): measurements = { m : common_measurements [ m ] for m in measurements } self . nlp = nlp self . name = name self . unit_registry = UnitRegistry ( units_config ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = True ) self . term_matcher = EDSPhraseMatcher ( nlp . vocab , attr = attr , ignore_excluded = True ) self . unitless_patterns : Dict [ str , UnitlessPatternConfigWithName ] = {} self . unit_part_label_hashes : Set [ int ] = set () self . unitless_label_hashes : Set [ int ] = set () self . unit_followers : Dict [ str , str ] = {} self . measure_names : Dict [ str , str ] = {} # NUMBER PATTERNS self . regex_matcher . add ( \"number\" , [ r \"(?<![a-z-])\\d+([ ]\\d {3} )*[ ]+(?:[,.][ ]+\\d+)?\" , r \"(?<![a-z-])\\d+([ ]\\d {3} )*(?:[,.]\\d+)?\" , ], ) self . number_label_hashes = { nlp . vocab . strings [ \"number\" ]} for number , terms in number_terms . items (): self . term_matcher . build_patterns ( nlp , { number : terms }) self . number_label_hashes . add ( nlp . vocab . strings [ number ]) # UNIT PATTERNS for unit_name , unit_config in units_config . items (): self . term_matcher . build_patterns ( nlp , { unit_name : unit_config [ \"terms\" ]}) if unit_config . get ( \"followed_by\" ) is not None : self . unit_followers [ unit_name ] = unit_config [ \"followed_by\" ] self . unit_part_label_hashes . add ( nlp . vocab . strings [ unit_name ]) self . unit_part_label_hashes . add ( nlp . vocab . strings [ \"per\" ]) self . term_matcher . build_patterns ( nlp , { \"per\" : unit_divisors }) self . term_matcher . add ( \"stopword\" , list ( nlp . pipe ( stopwords ))) # MEASURES for name , measure_config in measurements . items (): unit = measure_config [ \"unit\" ] self . measure_names [ self . unit_registry . parse_unit ( unit )[ 0 ]] = name if \"unitless_patterns\" in measure_config : for pattern in measure_config [ \"unitless_patterns\" ]: pattern_name = f \"unitless_ { len ( self . unitless_patterns ) } \" self . term_matcher . add ( pattern_name , list ( nlp . pipe ( pattern [ \"terms\" ])) ) self . unitless_label_hashes . add ( nlp . vocab . strings [ pattern_name ]) self . unitless_patterns [ pattern_name ] = { \"name\" : name , ** pattern } self . set_extensions () set_extensions () Set extensions for the measurements pipeline. Source code in edsnlp/pipelines/misc/measurements/measurements.py 290 291 292 293 294 295 296 297 @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the measurements pipeline. \"\"\" if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) extract_units ( term_matches ) Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" => we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) => we aggregate these adjacent matches together to compose a new unit g_per_day PARAMETER DESCRIPTION term_matches TYPE: Iterable [ Span ] RETURNS DESCRIPTION Iterable[Span] Source code in edsnlp/pipelines/misc/measurements/measurements.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def extract_units ( self , term_matches : Iterable [ Span ]) -> Iterable [ Span ]: \"\"\" Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" => we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) => we aggregate these adjacent matches together to compose a new unit g_per_day Parameters ---------- term_matches: Iterable[Span] Returns ------- Iterable[Span] \"\"\" last = None units = [] current = [] unit_label_hashes = set () for unit_part in filter_spans ( term_matches ): if unit_part . label not in self . unit_part_label_hashes : continue if last is not None and unit_part . start != last . end and len ( current ): doc = current [ 0 ] . doc # Last non \"per\" match: we don't want our units to be like `g_per` end = next ( ( i for i , e in list ( enumerate ( current ))[:: - 1 ] if e . label_ != \"per\" ), None , ) if end is not None : unit = \"_\" . join ( part . label_ for part in current [: end + 1 ]) units . append ( Span ( doc , current [ 0 ] . start , current [ end ] . end , unit )) unit_label_hashes . add ( units [ - 1 ] . label ) current = [] last = None if len ( current ) > 0 or unit_part . label_ != \"per\" : current . append ( unit_part ) last = unit_part end = next ( ( i for i , e in list ( enumerate ( current ))[:: - 1 ] if e . label_ != \"per\" ), None ) if end is not None : doc = current [ 0 ] . doc unit = \"_\" . join ( part . label_ for part in current [: end + 1 ]) units . append ( Span ( doc , current [ 0 ] . start , current [ end ] . end , unit )) unit_label_hashes . add ( units [ - 1 ] . label ) return units make_pseudo_sentence ( doc , matches , pseudo_mapping ) Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font 1 2 3 de long[.]\" is transformed into \"wn,n,nuw.\" PARAMETER DESCRIPTION doc TYPE: Doc matches List of tuple of span and whether the span represents a sentence end TYPE: List [ Tuple [ Span , bool ]] pseudo_mapping A mapping from label to char in the pseudo sentence TYPE: Dict [ int , str ] RETURNS DESCRIPTION (str, List[int]) the pseudo sentence a list of offsets to convert match indices into pseudo sent char indices Source code in edsnlp/pipelines/misc/measurements/measurements.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 @classmethod def make_pseudo_sentence ( cls , doc : Doc , matches : List [ Tuple [ Span , bool ]], pseudo_mapping : Dict [ int , str ], ) -> Tuple [ str , List [ int ]]: \"\"\" Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font [1][,] [2] [et] [3] [cm] de long[.]\" is transformed into \"wn,n,nuw.\" Parameters ---------- doc: Doc matches: List[(Span, bool)] List of tuple of span and whether the span represents a sentence end pseudo_mapping: Dict[int, str] A mapping from label to char in the pseudo sentence Returns ------- (str, List[int]) - the pseudo sentence - a list of offsets to convert match indices into pseudo sent char indices \"\"\" pseudo = [] last = 0 offsets = [] for ent , is_sent_split in matches : if ent . start != last : pseudo . append ( \"w\" ) offsets . append ( len ( pseudo )) if is_sent_split : pseudo . append ( \".\" ) else : pseudo . append ( pseudo_mapping . get ( ent . label , \"w\" )) last = ent . end if len ( doc ) != last : pseudo . append ( \"w\" ) pseudo = \"\" . join ( pseudo ) return pseudo , offsets get_matches ( doc ) Extract and filter regex and phrase matches in the document to prepare the measurement extraction. Returns the matches and a list of hashes to quickly find unit matches PARAMETER DESCRIPTION doc RETURNS DESCRIPTION Tuple[List[(Span, bool)], Set[int]] List of tuples of spans and whether the spans represents a sentence end List of hash label to distinguish unit from other matches Source code in edsnlp/pipelines/misc/measurements/measurements.py 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 def get_matches ( self , doc ): \"\"\" Extract and filter regex and phrase matches in the document to prepare the measurement extraction. Returns the matches and a list of hashes to quickly find unit matches Parameters ---------- doc: Doc Returns ------- Tuple[List[(Span, bool)], Set[int]] - List of tuples of spans and whether the spans represents a sentence end - List of hash label to distinguish unit from other matches \"\"\" sent_ends = [ doc [ i : i + 1 ] for i in range ( len ( doc )) if doc [ i ] . is_sent_end ] regex_matches = list ( self . regex_matcher ( doc , as_spans = True )) term_matches = list ( self . term_matcher ( doc , as_spans = True )) # Detect unit parts and compose them into units units = self . extract_units ( term_matches ) unit_label_hashes = { unit . label for unit in units } # Filter matches to prevent matches over dates or doc entities non_unit_terms = [ term for term in term_matches if term . label not in self . unit_part_label_hashes ] # Filter out measurement-related spans that overlap already matched # entities (in doc.ents or doc.spans[\"dates\"]) # Note: we also include sentence ends tokens as 1-token spans in those matches spans__keep__is_sent_end = filter_spans ( [ # Tuples (span, keep = is measurement related, is sentence end) * zip ( doc . spans . get ( \"dates\" , ()), repeat ( False ), repeat ( False )), * zip ( regex_matches , repeat ( True ), repeat ( False )), * zip ( non_unit_terms , repeat ( True ), repeat ( False )), * zip ( units , repeat ( True ), repeat ( False )), * zip ( doc . ents , repeat ( False ), repeat ( False )), * zip ( sent_ends , repeat ( True ), repeat ( True )), ], # filter entities to keep only the ... sort_key = measurements_match_tuples_sort_key , ) # Remove non-measurement related spans (keep = False) and sort the matches matches_and_is_sentence_end : List [( Span , bool )] = sorted ( [ ( span , is_sent_end ) for span , keep , is_sent_end in spans__keep__is_sent_end # and remove entities that are not relevant to this pipeline if keep ] ) return matches_and_is_sentence_end , unit_label_hashes extract_measurements ( doc ) Extracts measure entities from the document PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION List[Span] Source code in edsnlp/pipelines/misc/measurements/measurements.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 def extract_measurements ( self , doc : Doc ): \"\"\" Extracts measure entities from the document Parameters ---------- doc: Doc Returns ------- List[Span] \"\"\" matches , unit_label_hashes = self . get_matches ( doc ) # Make match slice function to query them def get_matches_after ( i ): anchor = matches [ i ][ 0 ] for j , ( ent , is_sent_end ) in enumerate ( matches [ i + 1 :]): if not is_sent_end and ent . start > anchor . end + AFTER_SNIPPET_LIMIT : return yield j + i + 1 , ent def get_matches_before ( i ): anchor = matches [ i ][ 0 ] for j , ( ent , is_sent_end ) in enumerate ( matches [ i :: - 1 ]): if not is_sent_end and ent . end < anchor . start - BEFORE_SNIPPET_LIMIT : return yield i - j , ent # Make a pseudo sentence to query higher order patterns in the main loop # `offsets` is a mapping from matches indices (ie match n\u00b0i) to # char indices in the pseudo sentence pseudo , offsets = self . make_pseudo_sentence ( doc , matches , { self . nlp . vocab . strings [ \"stopword\" ]: \",\" , self . nlp . vocab . strings [ \"number\" ]: \"n\" , ** { name : \"u\" for name in unit_label_hashes }, ** { name : \"n\" for name in self . number_label_hashes }, }, ) measurements = [] matched_unit_indices = set () # Iterate through the number matches for number_idx , ( number , is_sent_split ) in enumerate ( matches ): if not is_sent_split and number . label not in self . number_label_hashes : continue # Detect the measure value try : if number . label_ == \"number\" : value = float ( number . text . replace ( \" \" , \"\" ) . replace ( \",\" , \".\" ) . replace ( \" \" , \"\" ) ) else : value = float ( number . label_ ) except ValueError : continue unit_idx = unit_text = unit_norm = None # Find the closest unit after the number try : unit_idx , unit_text = next ( ( j , ent ) for j , ent in get_matches_after ( number_idx ) if ent . label in unit_label_hashes ) unit_norm = unit_text . label_ except ( AttributeError , StopIteration ): pass # Try to pair the number with this next unit if the two are only separated # by numbers and separators alternatively (as in [1][,] [2] [and] [3] cm) try : pseudo_sent = pseudo [ offsets [ number_idx ] + 1 : offsets [ unit_idx ]] if not re . fullmatch ( r \"(,n)*\" , pseudo_sent ): unit_text , unit_norm = None , None except TypeError : pass # Otherwise, try to infer the unit from the preceding unit to handle cases # like (1 meter 50) if unit_norm is None and number_idx - 1 in matched_unit_indices : try : unit_before = matches [ number_idx - 1 ][ 0 ] if unit_before . end == number . start : unit_norm = self . unit_followers [ unit_before . label_ ] except ( KeyError , AttributeError , IndexError ): pass # If no unit was matched, try to detect unitless patterns before # the number to handle cases like (\"Weight: 63, Height: 170\") if not unit_norm : try : ( unitless_idx , unitless_text ) = next ( ( j , e ) for j , e in get_matches_before ( number_idx ) if e . label in self . unitless_label_hashes ) unit_norm = None if re . fullmatch ( r \"[,n]*\" , pseudo [ offsets [ unitless_idx ] + 1 : offsets [ number_idx ]], ): unitless_pattern = self . unitless_patterns [ unitless_text . label_ ] unit_norm = next ( scope [ \"unit\" ] for scope in unitless_pattern [ \"ranges\" ] if ( \"min\" not in scope or value >= scope [ \"min\" ]) and ( \"max\" not in scope or value < scope [ \"max\" ]) ) except StopIteration : pass # Otherwise, skip this number if not unit_norm : continue # Compute the final entity if unit_text and unit_text . end == number . start : ent = doc [ unit_text . start : number . end ] elif unit_text and unit_text . start == number . end : ent = doc [ number . start : unit_text . end ] else : ent = number # Compute the dimensionality of the parsed unit try : dims = self . unit_registry . parse_unit ( unit_norm )[ 0 ] except KeyError : continue # If the measure was not requested, dismiss it # Otherwise, relabel the entity and create the value attribute if dims not in self . measure_names : continue ent . _ . value = SimpleMeasurement ( value , unit_norm , self . unit_registry ) ent . label_ = self . measure_names [ dims ] measurements . append ( ent ) if unit_idx is not None : matched_unit_indices . add ( unit_idx ) return measurements merge_adjacent_measurements ( measurements ) Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\" PARAMETER DESCRIPTION measurements TYPE: List [ Span ] RETURNS DESCRIPTION List[Span] Source code in edsnlp/pipelines/misc/measurements/measurements.py 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 @classmethod def merge_adjacent_measurements ( cls , measurements : List [ Span ]) -> List [ Span ]: \"\"\" Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\" Parameters ---------- measurements: List[Span] Returns ------- List[Span] \"\"\" merged = measurements [: 1 ] for ent in measurements [ 1 :]: last = merged [ - 1 ] if last . end == ent . start and last . _ . value . unit != ent . _ . value . unit : try : new_value = last . _ . value + ent . _ . value merged [ - 1 ] = last = last . doc [ last . start : ent . end ] last . _ . value = new_value last . label_ = ent . label_ except ( AttributeError , TypeError ): merged . append ( ent ) else : merged . append ( ent ) return merged __call__ ( doc ) Adds measurements to document's \"measurements\" SpanGroup. PARAMETER DESCRIPTION doc spaCy Doc object RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted measurements. Source code in edsnlp/pipelines/misc/measurements/measurements.py 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def __call__ ( self , doc ): \"\"\" Adds measurements to document's \"measurements\" SpanGroup. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted measurements. \"\"\" measurements = self . extract_measurements ( doc ) measurements = self . merge_adjacent_measurements ( measurements ) doc . spans [ \"measurements\" ] = measurements # for backward compatibility doc . spans [ \"measures\" ] = doc . spans [ \"measurements\" ] return doc measurements_match_tuples_sort_key ( span__keep__is_sent_end ) Source code in edsnlp/pipelines/misc/measurements/measurements.py 666 667 668 669 670 671 672 673 def measurements_match_tuples_sort_key ( span__keep__is_sent_end : Tuple [ Span , bool , bool ] ) -> Tuple [ int , int , bool ]: span , _ , is_sent_end = span__keep__is_sent_end length = span . end - span . start return length , span . end , not is_sent_end","title":"measurements"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlppipelinesmiscmeasurementsmeasurements","text":"","title":"edsnlp.pipelines.misc.measurements.measurements"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.__all__","text":"","title":"__all__"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.AFTER_SNIPPET_LIMIT","text":"","title":"AFTER_SNIPPET_LIMIT"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.BEFORE_SNIPPET_LIMIT","text":"","title":"BEFORE_SNIPPET_LIMIT"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitConfig","text":"Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 26 27 28 29 30 31 class UnitConfig ( TypedDict ): dim : str degree : int scale : float terms : List [ str ] followed_by : Optional [ str ] = None","title":"UnitConfig"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitConfig.dim","text":"","title":"dim"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitConfig.degree","text":"","title":"degree"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitConfig.scale","text":"","title":"scale"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitConfig.terms","text":"","title":"terms"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitConfig.followed_by","text":"","title":"followed_by"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessRange","text":"Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 34 35 36 37 class UnitlessRange ( TypedDict ): min : int max : int unit : str","title":"UnitlessRange"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessRange.min","text":"","title":"min"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessRange.max","text":"","title":"max"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessRange.unit","text":"","title":"unit"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessPatternConfig","text":"Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 40 41 42 class UnitlessPatternConfig ( TypedDict ): terms : List [ str ] ranges : List [ UnitlessRange ]","title":"UnitlessPatternConfig"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessPatternConfig.terms","text":"","title":"terms"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessPatternConfig.ranges","text":"","title":"ranges"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessPatternConfigWithName","text":"Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 45 46 47 48 class UnitlessPatternConfigWithName ( TypedDict ): terms : List [ str ] ranges : List [ UnitlessRange ] name : str","title":"UnitlessPatternConfigWithName"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessPatternConfigWithName.terms","text":"","title":"terms"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessPatternConfigWithName.ranges","text":"","title":"ranges"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitlessPatternConfigWithName.name","text":"","title":"name"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasureConfig","text":"Bases: TypedDict Source code in edsnlp/pipelines/misc/measurements/measurements.py 51 52 53 class MeasureConfig ( TypedDict ): unit : str unitless_patterns : List [ UnitlessPatternConfig ]","title":"MeasureConfig"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasureConfig.unit","text":"","title":"unit"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasureConfig.unitless_patterns","text":"","title":"unitless_patterns"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.Measurement","text":"Bases: abc . ABC Source code in edsnlp/pipelines/misc/measurements/measurements.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 class Measurement ( abc . ABC ): @abc . abstractmethod def __iter__ ( self ) -> Iterable [ \"SimpleMeasurement\" ]: \"\"\" Iter over items of the measure (only one for SimpleMeasurement) Returns ------- iterable : Iterable[\"SimpleMeasurement\"] \"\"\" @abc . abstractmethod def __getitem__ ( self , item ) -> \"SimpleMeasurement\" : \"\"\" Access items of the measure (only one for SimpleMeasurement) Parameters ---------- item : int Returns ------- measure : SimpleMeasurement \"\"\"","title":"Measurement"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.Measurement.__iter__","text":"Iter over items of the measure (only one for SimpleMeasurement) RETURNS DESCRIPTION iterable TYPE: Iterable[\"SimpleMeasurement\"] Source code in edsnlp/pipelines/misc/measurements/measurements.py 57 58 59 60 61 62 63 64 65 @abc . abstractmethod def __iter__ ( self ) -> Iterable [ \"SimpleMeasurement\" ]: \"\"\" Iter over items of the measure (only one for SimpleMeasurement) Returns ------- iterable : Iterable[\"SimpleMeasurement\"] \"\"\"","title":"__iter__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.Measurement.__getitem__","text":"Access items of the measure (only one for SimpleMeasurement) PARAMETER DESCRIPTION item TYPE: int RETURNS DESCRIPTION measure TYPE: SimpleMeasurement Source code in edsnlp/pipelines/misc/measurements/measurements.py 67 68 69 70 71 72 73 74 75 76 77 78 79 @abc . abstractmethod def __getitem__ ( self , item ) -> \"SimpleMeasurement\" : \"\"\" Access items of the measure (only one for SimpleMeasurement) Parameters ---------- item : int Returns ------- measure : SimpleMeasurement \"\"\"","title":"__getitem__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitRegistry","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class UnitRegistry : def __init__ ( self , config : Dict [ str , UnitConfig ]): self . config = { unicodedata . normalize ( \"NFKC\" , k ): v for k , v in config . items ()} for unit , unit_config in list ( self . config . items ()): if not unit . startswith ( \"per_\" ) and \"per_\" + unit not in unit_config : self . config [ \"per_\" + unit ] = { \"dim\" : unit_config [ \"dim\" ], \"degree\" : - unit_config [ \"degree\" ], \"scale\" : 1 / unit_config [ \"scale\" ], } @lru_cache ( maxsize =- 1 ) def parse_unit ( self , unit : str ) -> Tuple [ str , float ]: degrees = defaultdict ( lambda : 0 ) scale = 1 for part in regex . split ( \"(?<!per)_\" , unit ): unit_config = self . config [ unicodedata . normalize ( \"NFKC\" , part )] degrees [ unit_config [ \"dim\" ]] += unit_config [ \"degree\" ] scale *= unit_config [ \"scale\" ] return str ( dict ( sorted ( degrees . items ()))), scale","title":"UnitRegistry"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitRegistry.config","text":"","title":"config"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitRegistry.__init__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 83 84 85 86 87 88 89 90 91 def __init__ ( self , config : Dict [ str , UnitConfig ]): self . config = { unicodedata . normalize ( \"NFKC\" , k ): v for k , v in config . items ()} for unit , unit_config in list ( self . config . items ()): if not unit . startswith ( \"per_\" ) and \"per_\" + unit not in unit_config : self . config [ \"per_\" + unit ] = { \"dim\" : unit_config [ \"dim\" ], \"degree\" : - unit_config [ \"degree\" ], \"scale\" : 1 / unit_config [ \"scale\" ], }","title":"__init__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.UnitRegistry.parse_unit","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 93 94 95 96 97 98 99 100 101 @lru_cache ( maxsize =- 1 ) def parse_unit ( self , unit : str ) -> Tuple [ str , float ]: degrees = defaultdict ( lambda : 0 ) scale = 1 for part in regex . split ( \"(?<!per)_\" , unit ): unit_config = self . config [ unicodedata . normalize ( \"NFKC\" , part )] degrees [ unit_config [ \"dim\" ]] += unit_config [ \"degree\" ] scale *= unit_config [ \"scale\" ] return str ( dict ( sorted ( degrees . items ()))), scale","title":"parse_unit()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement","text":"Bases: Measurement Source code in edsnlp/pipelines/misc/measurements/measurements.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class SimpleMeasurement ( Measurement ): def __init__ ( self , value , unit , registry ): \"\"\" The SimpleMeasurement class contains the value and unit for a single non-composite measure Parameters ---------- value : float unit : str \"\"\" super () . __init__ () self . value = value self . unit = unit self . registry = registry def __iter__ ( self ): return iter (( self ,)) def __getitem__ ( self , item : int ): assert isinstance ( item , int ) return [ self ][ item ] def __str__ ( self ): return f \" { self . value } { self . unit } \" def __repr__ ( self ): return f \"Measurement( { self . value } , { repr ( self . unit ) } )\" def __eq__ ( self , other : Any ): if isinstance ( other , SimpleMeasurement ): return self . convert_to ( other . unit ) == other . value return False def __add__ ( self , other : \"SimpleMeasurement\" ): if other . unit == self . unit : return self . __class__ ( self . value + other . value , self . unit , self . registry ) return self . __class__ ( self . value + other . convert_to ( self . unit ), self . unit , self . registry ) def __lt__ ( self , other : \"SimpleMeasurement\" ): return self . convert_to ( other . unit ) < other . value def __le__ ( self , other : \"SimpleMeasurement\" ): return self . convert_to ( other . unit ) <= other . value def convert_to ( self , other_unit ): self_degrees , self_scale = self . registry . parse_unit ( self . unit ) other_degrees , other_scale = self . registry . parse_unit ( other_unit ) if self_degrees != other_degrees : raise AttributeError ( f \"Units { self . unit } and { other_unit } are not homogenous\" ) new_value = self . value * self_scale / other_scale return new_value def __getattr__ ( self , other_unit ): return self . convert_to ( other_unit ) @classmethod def verify ( cls , ent ): return True","title":"SimpleMeasurement"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.value","text":"","title":"value"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.unit","text":"","title":"unit"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.registry","text":"","title":"registry"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__init__","text":"The SimpleMeasurement class contains the value and unit for a single non-composite measure PARAMETER DESCRIPTION value TYPE: float unit TYPE: str Source code in edsnlp/pipelines/misc/measurements/measurements.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , value , unit , registry ): \"\"\" The SimpleMeasurement class contains the value and unit for a single non-composite measure Parameters ---------- value : float unit : str \"\"\" super () . __init__ () self . value = value self . unit = unit self . registry = registry","title":"__init__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__iter__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 120 121 def __iter__ ( self ): return iter (( self ,))","title":"__iter__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__getitem__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 123 124 125 def __getitem__ ( self , item : int ): assert isinstance ( item , int ) return [ self ][ item ]","title":"__getitem__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__str__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 127 128 def __str__ ( self ): return f \" { self . value } { self . unit } \"","title":"__str__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__repr__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 130 131 def __repr__ ( self ): return f \"Measurement( { self . value } , { repr ( self . unit ) } )\"","title":"__repr__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__eq__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 133 134 135 136 def __eq__ ( self , other : Any ): if isinstance ( other , SimpleMeasurement ): return self . convert_to ( other . unit ) == other . value return False","title":"__eq__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__add__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 138 139 140 141 142 143 def __add__ ( self , other : \"SimpleMeasurement\" ): if other . unit == self . unit : return self . __class__ ( self . value + other . value , self . unit , self . registry ) return self . __class__ ( self . value + other . convert_to ( self . unit ), self . unit , self . registry )","title":"__add__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__lt__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 145 146 def __lt__ ( self , other : \"SimpleMeasurement\" ): return self . convert_to ( other . unit ) < other . value","title":"__lt__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__le__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 148 149 def __le__ ( self , other : \"SimpleMeasurement\" ): return self . convert_to ( other . unit ) <= other . value","title":"__le__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.convert_to","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 151 152 153 154 155 156 157 158 159 160 def convert_to ( self , other_unit ): self_degrees , self_scale = self . registry . parse_unit ( self . unit ) other_degrees , other_scale = self . registry . parse_unit ( other_unit ) if self_degrees != other_degrees : raise AttributeError ( f \"Units { self . unit } and { other_unit } are not homogenous\" ) new_value = self . value * self_scale / other_scale return new_value","title":"convert_to()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__getattr__","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 162 163 def __getattr__ ( self , other_unit ): return self . convert_to ( other_unit )","title":"__getattr__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.verify","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 165 166 167 @classmethod def verify ( cls , ent ): return True","title":"verify()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 class MeasurementsMatcher : def __init__ ( self , nlp : spacy . Language , measurements : Union [ List [ str ], Tuple [ str ], Dict [ str , MeasureConfig ]], units_config : Dict [ str , UnitConfig ], number_terms : Dict [ str , List [ str ]], stopwords : List [ str ] = ( \"par\" , \"sur\" , \"de\" , \"a\" , \":\" ), unit_divisors : List [ str ] = ( \"par\" , \"/\" ), name : str = \"measurements\" , ignore_excluded : bool = True , attr : str = \"NORM\" , ): \"\"\" Matcher component to extract measurements. A measurements is most often composed of a number and a unit like > 1,26 cm The unit can also be positioned in place of the decimal dot/comma > 1 cm 26 Some measurements can be composite > 1,26 cm x 2,34 mm And sometimes they are factorized > Les trois kystes mesurent 1, 2 et 3cm. The recognized measurements are stored in the \"measurements\" SpanGroup. Each span has a `Measurement` object stored in the \"value\" extension attribute. Parameters ---------- nlp : Language The SpaCy object. measurements : Dict[str, MeasureConfig] A mapping from measure names to MeasureConfig Each measure's configuration has the following shape: { \"unit\": str, # the unit of the measure (like \"kg\"), \"unitless_patterns\": { # optional patterns to handle unitless cases \"terms\": List[str], # list of preceding terms used to trigger the measure # Mapping from ranges to unit to handle cases like # (\"Taille: 1.2\" -> 1.20 m vs \"Taille: 120\" -> 120cm) \"ranges\": List[{ \"min\": int, \"max\": int, \"unit\": str, }, { \"min\": int, \"unit\": str, }, ...], } number_terms: Dict[str, List[str] A mapping of numbers to their lexical variants stopwords: List[str] A list of stopwords that do not matter when placed between a unitless trigger and a number unit_divisors: List[str] A list of terms used to divide two units (like: m / s) attr : str Whether to match on the text ('TEXT') or on the normalized text ('NORM') ignore_excluded : bool Whether to exclude pollution patterns when matching in the text \"\"\" if measurements is None : measurements = common_measurements elif isinstance ( measurements , ( list , tuple )): measurements = { m : common_measurements [ m ] for m in measurements } self . nlp = nlp self . name = name self . unit_registry = UnitRegistry ( units_config ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = True ) self . term_matcher = EDSPhraseMatcher ( nlp . vocab , attr = attr , ignore_excluded = True ) self . unitless_patterns : Dict [ str , UnitlessPatternConfigWithName ] = {} self . unit_part_label_hashes : Set [ int ] = set () self . unitless_label_hashes : Set [ int ] = set () self . unit_followers : Dict [ str , str ] = {} self . measure_names : Dict [ str , str ] = {} # NUMBER PATTERNS self . regex_matcher . add ( \"number\" , [ r \"(?<![a-z-])\\d+([ ]\\d {3} )*[ ]+(?:[,.][ ]+\\d+)?\" , r \"(?<![a-z-])\\d+([ ]\\d {3} )*(?:[,.]\\d+)?\" , ], ) self . number_label_hashes = { nlp . vocab . strings [ \"number\" ]} for number , terms in number_terms . items (): self . term_matcher . build_patterns ( nlp , { number : terms }) self . number_label_hashes . add ( nlp . vocab . strings [ number ]) # UNIT PATTERNS for unit_name , unit_config in units_config . items (): self . term_matcher . build_patterns ( nlp , { unit_name : unit_config [ \"terms\" ]}) if unit_config . get ( \"followed_by\" ) is not None : self . unit_followers [ unit_name ] = unit_config [ \"followed_by\" ] self . unit_part_label_hashes . add ( nlp . vocab . strings [ unit_name ]) self . unit_part_label_hashes . add ( nlp . vocab . strings [ \"per\" ]) self . term_matcher . build_patterns ( nlp , { \"per\" : unit_divisors }) self . term_matcher . add ( \"stopword\" , list ( nlp . pipe ( stopwords ))) # MEASURES for name , measure_config in measurements . items (): unit = measure_config [ \"unit\" ] self . measure_names [ self . unit_registry . parse_unit ( unit )[ 0 ]] = name if \"unitless_patterns\" in measure_config : for pattern in measure_config [ \"unitless_patterns\" ]: pattern_name = f \"unitless_ { len ( self . unitless_patterns ) } \" self . term_matcher . add ( pattern_name , list ( nlp . pipe ( pattern [ \"terms\" ])) ) self . unitless_label_hashes . add ( nlp . vocab . strings [ pattern_name ]) self . unitless_patterns [ pattern_name ] = { \"name\" : name , ** pattern } self . set_extensions () @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the measurements pipeline. \"\"\" if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) def extract_units ( self , term_matches : Iterable [ Span ]) -> Iterable [ Span ]: \"\"\" Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" => we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) => we aggregate these adjacent matches together to compose a new unit g_per_day Parameters ---------- term_matches: Iterable[Span] Returns ------- Iterable[Span] \"\"\" last = None units = [] current = [] unit_label_hashes = set () for unit_part in filter_spans ( term_matches ): if unit_part . label not in self . unit_part_label_hashes : continue if last is not None and unit_part . start != last . end and len ( current ): doc = current [ 0 ] . doc # Last non \"per\" match: we don't want our units to be like `g_per` end = next ( ( i for i , e in list ( enumerate ( current ))[:: - 1 ] if e . label_ != \"per\" ), None , ) if end is not None : unit = \"_\" . join ( part . label_ for part in current [: end + 1 ]) units . append ( Span ( doc , current [ 0 ] . start , current [ end ] . end , unit )) unit_label_hashes . add ( units [ - 1 ] . label ) current = [] last = None if len ( current ) > 0 or unit_part . label_ != \"per\" : current . append ( unit_part ) last = unit_part end = next ( ( i for i , e in list ( enumerate ( current ))[:: - 1 ] if e . label_ != \"per\" ), None ) if end is not None : doc = current [ 0 ] . doc unit = \"_\" . join ( part . label_ for part in current [: end + 1 ]) units . append ( Span ( doc , current [ 0 ] . start , current [ end ] . end , unit )) unit_label_hashes . add ( units [ - 1 ] . label ) return units @classmethod def make_pseudo_sentence ( cls , doc : Doc , matches : List [ Tuple [ Span , bool ]], pseudo_mapping : Dict [ int , str ], ) -> Tuple [ str , List [ int ]]: \"\"\" Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font [1][,] [2] [et] [3] [cm] de long[.]\" is transformed into \"wn,n,nuw.\" Parameters ---------- doc: Doc matches: List[(Span, bool)] List of tuple of span and whether the span represents a sentence end pseudo_mapping: Dict[int, str] A mapping from label to char in the pseudo sentence Returns ------- (str, List[int]) - the pseudo sentence - a list of offsets to convert match indices into pseudo sent char indices \"\"\" pseudo = [] last = 0 offsets = [] for ent , is_sent_split in matches : if ent . start != last : pseudo . append ( \"w\" ) offsets . append ( len ( pseudo )) if is_sent_split : pseudo . append ( \".\" ) else : pseudo . append ( pseudo_mapping . get ( ent . label , \"w\" )) last = ent . end if len ( doc ) != last : pseudo . append ( \"w\" ) pseudo = \"\" . join ( pseudo ) return pseudo , offsets def get_matches ( self , doc ): \"\"\" Extract and filter regex and phrase matches in the document to prepare the measurement extraction. Returns the matches and a list of hashes to quickly find unit matches Parameters ---------- doc: Doc Returns ------- Tuple[List[(Span, bool)], Set[int]] - List of tuples of spans and whether the spans represents a sentence end - List of hash label to distinguish unit from other matches \"\"\" sent_ends = [ doc [ i : i + 1 ] for i in range ( len ( doc )) if doc [ i ] . is_sent_end ] regex_matches = list ( self . regex_matcher ( doc , as_spans = True )) term_matches = list ( self . term_matcher ( doc , as_spans = True )) # Detect unit parts and compose them into units units = self . extract_units ( term_matches ) unit_label_hashes = { unit . label for unit in units } # Filter matches to prevent matches over dates or doc entities non_unit_terms = [ term for term in term_matches if term . label not in self . unit_part_label_hashes ] # Filter out measurement-related spans that overlap already matched # entities (in doc.ents or doc.spans[\"dates\"]) # Note: we also include sentence ends tokens as 1-token spans in those matches spans__keep__is_sent_end = filter_spans ( [ # Tuples (span, keep = is measurement related, is sentence end) * zip ( doc . spans . get ( \"dates\" , ()), repeat ( False ), repeat ( False )), * zip ( regex_matches , repeat ( True ), repeat ( False )), * zip ( non_unit_terms , repeat ( True ), repeat ( False )), * zip ( units , repeat ( True ), repeat ( False )), * zip ( doc . ents , repeat ( False ), repeat ( False )), * zip ( sent_ends , repeat ( True ), repeat ( True )), ], # filter entities to keep only the ... sort_key = measurements_match_tuples_sort_key , ) # Remove non-measurement related spans (keep = False) and sort the matches matches_and_is_sentence_end : List [( Span , bool )] = sorted ( [ ( span , is_sent_end ) for span , keep , is_sent_end in spans__keep__is_sent_end # and remove entities that are not relevant to this pipeline if keep ] ) return matches_and_is_sentence_end , unit_label_hashes def extract_measurements ( self , doc : Doc ): \"\"\" Extracts measure entities from the document Parameters ---------- doc: Doc Returns ------- List[Span] \"\"\" matches , unit_label_hashes = self . get_matches ( doc ) # Make match slice function to query them def get_matches_after ( i ): anchor = matches [ i ][ 0 ] for j , ( ent , is_sent_end ) in enumerate ( matches [ i + 1 :]): if not is_sent_end and ent . start > anchor . end + AFTER_SNIPPET_LIMIT : return yield j + i + 1 , ent def get_matches_before ( i ): anchor = matches [ i ][ 0 ] for j , ( ent , is_sent_end ) in enumerate ( matches [ i :: - 1 ]): if not is_sent_end and ent . end < anchor . start - BEFORE_SNIPPET_LIMIT : return yield i - j , ent # Make a pseudo sentence to query higher order patterns in the main loop # `offsets` is a mapping from matches indices (ie match n\u00b0i) to # char indices in the pseudo sentence pseudo , offsets = self . make_pseudo_sentence ( doc , matches , { self . nlp . vocab . strings [ \"stopword\" ]: \",\" , self . nlp . vocab . strings [ \"number\" ]: \"n\" , ** { name : \"u\" for name in unit_label_hashes }, ** { name : \"n\" for name in self . number_label_hashes }, }, ) measurements = [] matched_unit_indices = set () # Iterate through the number matches for number_idx , ( number , is_sent_split ) in enumerate ( matches ): if not is_sent_split and number . label not in self . number_label_hashes : continue # Detect the measure value try : if number . label_ == \"number\" : value = float ( number . text . replace ( \" \" , \"\" ) . replace ( \",\" , \".\" ) . replace ( \" \" , \"\" ) ) else : value = float ( number . label_ ) except ValueError : continue unit_idx = unit_text = unit_norm = None # Find the closest unit after the number try : unit_idx , unit_text = next ( ( j , ent ) for j , ent in get_matches_after ( number_idx ) if ent . label in unit_label_hashes ) unit_norm = unit_text . label_ except ( AttributeError , StopIteration ): pass # Try to pair the number with this next unit if the two are only separated # by numbers and separators alternatively (as in [1][,] [2] [and] [3] cm) try : pseudo_sent = pseudo [ offsets [ number_idx ] + 1 : offsets [ unit_idx ]] if not re . fullmatch ( r \"(,n)*\" , pseudo_sent ): unit_text , unit_norm = None , None except TypeError : pass # Otherwise, try to infer the unit from the preceding unit to handle cases # like (1 meter 50) if unit_norm is None and number_idx - 1 in matched_unit_indices : try : unit_before = matches [ number_idx - 1 ][ 0 ] if unit_before . end == number . start : unit_norm = self . unit_followers [ unit_before . label_ ] except ( KeyError , AttributeError , IndexError ): pass # If no unit was matched, try to detect unitless patterns before # the number to handle cases like (\"Weight: 63, Height: 170\") if not unit_norm : try : ( unitless_idx , unitless_text ) = next ( ( j , e ) for j , e in get_matches_before ( number_idx ) if e . label in self . unitless_label_hashes ) unit_norm = None if re . fullmatch ( r \"[,n]*\" , pseudo [ offsets [ unitless_idx ] + 1 : offsets [ number_idx ]], ): unitless_pattern = self . unitless_patterns [ unitless_text . label_ ] unit_norm = next ( scope [ \"unit\" ] for scope in unitless_pattern [ \"ranges\" ] if ( \"min\" not in scope or value >= scope [ \"min\" ]) and ( \"max\" not in scope or value < scope [ \"max\" ]) ) except StopIteration : pass # Otherwise, skip this number if not unit_norm : continue # Compute the final entity if unit_text and unit_text . end == number . start : ent = doc [ unit_text . start : number . end ] elif unit_text and unit_text . start == number . end : ent = doc [ number . start : unit_text . end ] else : ent = number # Compute the dimensionality of the parsed unit try : dims = self . unit_registry . parse_unit ( unit_norm )[ 0 ] except KeyError : continue # If the measure was not requested, dismiss it # Otherwise, relabel the entity and create the value attribute if dims not in self . measure_names : continue ent . _ . value = SimpleMeasurement ( value , unit_norm , self . unit_registry ) ent . label_ = self . measure_names [ dims ] measurements . append ( ent ) if unit_idx is not None : matched_unit_indices . add ( unit_idx ) return measurements @classmethod def merge_adjacent_measurements ( cls , measurements : List [ Span ]) -> List [ Span ]: \"\"\" Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\" Parameters ---------- measurements: List[Span] Returns ------- List[Span] \"\"\" merged = measurements [: 1 ] for ent in measurements [ 1 :]: last = merged [ - 1 ] if last . end == ent . start and last . _ . value . unit != ent . _ . value . unit : try : new_value = last . _ . value + ent . _ . value merged [ - 1 ] = last = last . doc [ last . start : ent . end ] last . _ . value = new_value last . label_ = ent . label_ except ( AttributeError , TypeError ): merged . append ( ent ) else : merged . append ( ent ) return merged def __call__ ( self , doc ): \"\"\" Adds measurements to document's \"measurements\" SpanGroup. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted measurements. \"\"\" measurements = self . extract_measurements ( doc ) measurements = self . merge_adjacent_measurements ( measurements ) doc . spans [ \"measurements\" ] = measurements # for backward compatibility doc . spans [ \"measures\" ] = doc . spans [ \"measurements\" ] return doc","title":"MeasurementsMatcher"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.name","text":"","title":"name"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.unit_registry","text":"","title":"unit_registry"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.regex_matcher","text":"","title":"regex_matcher"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.term_matcher","text":"","title":"term_matcher"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.unitless_patterns","text":"","title":"unitless_patterns"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.unit_part_label_hashes","text":"","title":"unit_part_label_hashes"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.unitless_label_hashes","text":"","title":"unitless_label_hashes"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.unit_followers","text":"","title":"unit_followers"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.measure_names","text":"","title":"measure_names"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.number_label_hashes","text":"","title":"number_label_hashes"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.__init__","text":"Matcher component to extract measurements. A measurements is most often composed of a number and a unit like 1,26 cm The unit can also be positioned in place of the decimal dot/comma 1 cm 26 Some measurements can be composite 1,26 cm x 2,34 mm And sometimes they are factorized Les trois kystes mesurent 1, 2 et 3cm. The recognized measurements are stored in the \"measurements\" SpanGroup. Each span has a Measurement object stored in the \"value\" extension attribute. PARAMETER DESCRIPTION nlp The SpaCy object. TYPE: Language measurements A mapping from measure names to MeasureConfig Each measure's configuration has the following shape: { \"unit\": str, # the unit of the measure (like \"kg\"), \"unitless_patterns\": { # optional patterns to handle unitless cases \"terms\": List[str], # list of preceding terms used to trigger the measure","title":"__init__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.__init__--mapping-from-ranges-to-unit-to-handle-cases-like","text":"","title":"Mapping from ranges to unit to handle cases like"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.__init__--taille-12-120-m-vs-taille-120-120cm","text":"\"ranges\": List[{ \"min\": int, \"max\": int, \"unit\": str, }, { \"min\": int, \"unit\": str, }, ...], } TYPE: Dict[str, MeasureConfig] number_terms A mapping of numbers to their lexical variants TYPE: Dict [ str , List [ str ]] stopwords A list of stopwords that do not matter when placed between a unitless trigger and a number TYPE: List [ str ] DEFAULT: (\"par\", \"sur\", \"de\", \"a\", \":\") unit_divisors A list of terms used to divide two units (like: m / s) TYPE: List [ str ] DEFAULT: (\"par\", \"/\") attr Whether to match on the text ('TEXT') or on the normalized text ('NORM') TYPE: str DEFAULT: 'NORM' ignore_excluded Whether to exclude pollution patterns when matching in the text TYPE: bool DEFAULT: True Source code in edsnlp/pipelines/misc/measurements/measurements.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def __init__ ( self , nlp : spacy . Language , measurements : Union [ List [ str ], Tuple [ str ], Dict [ str , MeasureConfig ]], units_config : Dict [ str , UnitConfig ], number_terms : Dict [ str , List [ str ]], stopwords : List [ str ] = ( \"par\" , \"sur\" , \"de\" , \"a\" , \":\" ), unit_divisors : List [ str ] = ( \"par\" , \"/\" ), name : str = \"measurements\" , ignore_excluded : bool = True , attr : str = \"NORM\" , ): \"\"\" Matcher component to extract measurements. A measurements is most often composed of a number and a unit like > 1,26 cm The unit can also be positioned in place of the decimal dot/comma > 1 cm 26 Some measurements can be composite > 1,26 cm x 2,34 mm And sometimes they are factorized > Les trois kystes mesurent 1, 2 et 3cm. The recognized measurements are stored in the \"measurements\" SpanGroup. Each span has a `Measurement` object stored in the \"value\" extension attribute. Parameters ---------- nlp : Language The SpaCy object. measurements : Dict[str, MeasureConfig] A mapping from measure names to MeasureConfig Each measure's configuration has the following shape: { \"unit\": str, # the unit of the measure (like \"kg\"), \"unitless_patterns\": { # optional patterns to handle unitless cases \"terms\": List[str], # list of preceding terms used to trigger the measure # Mapping from ranges to unit to handle cases like # (\"Taille: 1.2\" -> 1.20 m vs \"Taille: 120\" -> 120cm) \"ranges\": List[{ \"min\": int, \"max\": int, \"unit\": str, }, { \"min\": int, \"unit\": str, }, ...], } number_terms: Dict[str, List[str] A mapping of numbers to their lexical variants stopwords: List[str] A list of stopwords that do not matter when placed between a unitless trigger and a number unit_divisors: List[str] A list of terms used to divide two units (like: m / s) attr : str Whether to match on the text ('TEXT') or on the normalized text ('NORM') ignore_excluded : bool Whether to exclude pollution patterns when matching in the text \"\"\" if measurements is None : measurements = common_measurements elif isinstance ( measurements , ( list , tuple )): measurements = { m : common_measurements [ m ] for m in measurements } self . nlp = nlp self . name = name self . unit_registry = UnitRegistry ( units_config ) self . regex_matcher = RegexMatcher ( attr = attr , ignore_excluded = True ) self . term_matcher = EDSPhraseMatcher ( nlp . vocab , attr = attr , ignore_excluded = True ) self . unitless_patterns : Dict [ str , UnitlessPatternConfigWithName ] = {} self . unit_part_label_hashes : Set [ int ] = set () self . unitless_label_hashes : Set [ int ] = set () self . unit_followers : Dict [ str , str ] = {} self . measure_names : Dict [ str , str ] = {} # NUMBER PATTERNS self . regex_matcher . add ( \"number\" , [ r \"(?<![a-z-])\\d+([ ]\\d {3} )*[ ]+(?:[,.][ ]+\\d+)?\" , r \"(?<![a-z-])\\d+([ ]\\d {3} )*(?:[,.]\\d+)?\" , ], ) self . number_label_hashes = { nlp . vocab . strings [ \"number\" ]} for number , terms in number_terms . items (): self . term_matcher . build_patterns ( nlp , { number : terms }) self . number_label_hashes . add ( nlp . vocab . strings [ number ]) # UNIT PATTERNS for unit_name , unit_config in units_config . items (): self . term_matcher . build_patterns ( nlp , { unit_name : unit_config [ \"terms\" ]}) if unit_config . get ( \"followed_by\" ) is not None : self . unit_followers [ unit_name ] = unit_config [ \"followed_by\" ] self . unit_part_label_hashes . add ( nlp . vocab . strings [ unit_name ]) self . unit_part_label_hashes . add ( nlp . vocab . strings [ \"per\" ]) self . term_matcher . build_patterns ( nlp , { \"per\" : unit_divisors }) self . term_matcher . add ( \"stopword\" , list ( nlp . pipe ( stopwords ))) # MEASURES for name , measure_config in measurements . items (): unit = measure_config [ \"unit\" ] self . measure_names [ self . unit_registry . parse_unit ( unit )[ 0 ]] = name if \"unitless_patterns\" in measure_config : for pattern in measure_config [ \"unitless_patterns\" ]: pattern_name = f \"unitless_ { len ( self . unitless_patterns ) } \" self . term_matcher . add ( pattern_name , list ( nlp . pipe ( pattern [ \"terms\" ])) ) self . unitless_label_hashes . add ( nlp . vocab . strings [ pattern_name ]) self . unitless_patterns [ pattern_name ] = { \"name\" : name , ** pattern } self . set_extensions ()","title":"(\"Taille: 1.2\" -&gt; 1.20 m vs \"Taille: 120\" -&gt; 120cm)"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.set_extensions","text":"Set extensions for the measurements pipeline. Source code in edsnlp/pipelines/misc/measurements/measurements.py 290 291 292 293 294 295 296 297 @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the measurements pipeline. \"\"\" if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.extract_units","text":"Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" => we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) => we aggregate these adjacent matches together to compose a new unit g_per_day PARAMETER DESCRIPTION term_matches TYPE: Iterable [ Span ] RETURNS DESCRIPTION Iterable[Span] Source code in edsnlp/pipelines/misc/measurements/measurements.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def extract_units ( self , term_matches : Iterable [ Span ]) -> Iterable [ Span ]: \"\"\" Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" => we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) => we aggregate these adjacent matches together to compose a new unit g_per_day Parameters ---------- term_matches: Iterable[Span] Returns ------- Iterable[Span] \"\"\" last = None units = [] current = [] unit_label_hashes = set () for unit_part in filter_spans ( term_matches ): if unit_part . label not in self . unit_part_label_hashes : continue if last is not None and unit_part . start != last . end and len ( current ): doc = current [ 0 ] . doc # Last non \"per\" match: we don't want our units to be like `g_per` end = next ( ( i for i , e in list ( enumerate ( current ))[:: - 1 ] if e . label_ != \"per\" ), None , ) if end is not None : unit = \"_\" . join ( part . label_ for part in current [: end + 1 ]) units . append ( Span ( doc , current [ 0 ] . start , current [ end ] . end , unit )) unit_label_hashes . add ( units [ - 1 ] . label ) current = [] last = None if len ( current ) > 0 or unit_part . label_ != \"per\" : current . append ( unit_part ) last = unit_part end = next ( ( i for i , e in list ( enumerate ( current ))[:: - 1 ] if e . label_ != \"per\" ), None ) if end is not None : doc = current [ 0 ] . doc unit = \"_\" . join ( part . label_ for part in current [: end + 1 ]) units . append ( Span ( doc , current [ 0 ] . start , current [ end ] . end , unit )) unit_label_hashes . add ( units [ - 1 ] . label ) return units","title":"extract_units()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.make_pseudo_sentence","text":"Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font 1 2 3 de long[.]\" is transformed into \"wn,n,nuw.\" PARAMETER DESCRIPTION doc TYPE: Doc matches List of tuple of span and whether the span represents a sentence end TYPE: List [ Tuple [ Span , bool ]] pseudo_mapping A mapping from label to char in the pseudo sentence TYPE: Dict [ int , str ] RETURNS DESCRIPTION (str, List[int]) the pseudo sentence a list of offsets to convert match indices into pseudo sent char indices Source code in edsnlp/pipelines/misc/measurements/measurements.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 @classmethod def make_pseudo_sentence ( cls , doc : Doc , matches : List [ Tuple [ Span , bool ]], pseudo_mapping : Dict [ int , str ], ) -> Tuple [ str , List [ int ]]: \"\"\" Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font [1][,] [2] [et] [3] [cm] de long[.]\" is transformed into \"wn,n,nuw.\" Parameters ---------- doc: Doc matches: List[(Span, bool)] List of tuple of span and whether the span represents a sentence end pseudo_mapping: Dict[int, str] A mapping from label to char in the pseudo sentence Returns ------- (str, List[int]) - the pseudo sentence - a list of offsets to convert match indices into pseudo sent char indices \"\"\" pseudo = [] last = 0 offsets = [] for ent , is_sent_split in matches : if ent . start != last : pseudo . append ( \"w\" ) offsets . append ( len ( pseudo )) if is_sent_split : pseudo . append ( \".\" ) else : pseudo . append ( pseudo_mapping . get ( ent . label , \"w\" )) last = ent . end if len ( doc ) != last : pseudo . append ( \"w\" ) pseudo = \"\" . join ( pseudo ) return pseudo , offsets","title":"make_pseudo_sentence()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.get_matches","text":"Extract and filter regex and phrase matches in the document to prepare the measurement extraction. Returns the matches and a list of hashes to quickly find unit matches PARAMETER DESCRIPTION doc RETURNS DESCRIPTION Tuple[List[(Span, bool)], Set[int]] List of tuples of spans and whether the spans represents a sentence end List of hash label to distinguish unit from other matches Source code in edsnlp/pipelines/misc/measurements/measurements.py 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 def get_matches ( self , doc ): \"\"\" Extract and filter regex and phrase matches in the document to prepare the measurement extraction. Returns the matches and a list of hashes to quickly find unit matches Parameters ---------- doc: Doc Returns ------- Tuple[List[(Span, bool)], Set[int]] - List of tuples of spans and whether the spans represents a sentence end - List of hash label to distinguish unit from other matches \"\"\" sent_ends = [ doc [ i : i + 1 ] for i in range ( len ( doc )) if doc [ i ] . is_sent_end ] regex_matches = list ( self . regex_matcher ( doc , as_spans = True )) term_matches = list ( self . term_matcher ( doc , as_spans = True )) # Detect unit parts and compose them into units units = self . extract_units ( term_matches ) unit_label_hashes = { unit . label for unit in units } # Filter matches to prevent matches over dates or doc entities non_unit_terms = [ term for term in term_matches if term . label not in self . unit_part_label_hashes ] # Filter out measurement-related spans that overlap already matched # entities (in doc.ents or doc.spans[\"dates\"]) # Note: we also include sentence ends tokens as 1-token spans in those matches spans__keep__is_sent_end = filter_spans ( [ # Tuples (span, keep = is measurement related, is sentence end) * zip ( doc . spans . get ( \"dates\" , ()), repeat ( False ), repeat ( False )), * zip ( regex_matches , repeat ( True ), repeat ( False )), * zip ( non_unit_terms , repeat ( True ), repeat ( False )), * zip ( units , repeat ( True ), repeat ( False )), * zip ( doc . ents , repeat ( False ), repeat ( False )), * zip ( sent_ends , repeat ( True ), repeat ( True )), ], # filter entities to keep only the ... sort_key = measurements_match_tuples_sort_key , ) # Remove non-measurement related spans (keep = False) and sort the matches matches_and_is_sentence_end : List [( Span , bool )] = sorted ( [ ( span , is_sent_end ) for span , keep , is_sent_end in spans__keep__is_sent_end # and remove entities that are not relevant to this pipeline if keep ] ) return matches_and_is_sentence_end , unit_label_hashes","title":"get_matches()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.extract_measurements","text":"Extracts measure entities from the document PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION List[Span] Source code in edsnlp/pipelines/misc/measurements/measurements.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 def extract_measurements ( self , doc : Doc ): \"\"\" Extracts measure entities from the document Parameters ---------- doc: Doc Returns ------- List[Span] \"\"\" matches , unit_label_hashes = self . get_matches ( doc ) # Make match slice function to query them def get_matches_after ( i ): anchor = matches [ i ][ 0 ] for j , ( ent , is_sent_end ) in enumerate ( matches [ i + 1 :]): if not is_sent_end and ent . start > anchor . end + AFTER_SNIPPET_LIMIT : return yield j + i + 1 , ent def get_matches_before ( i ): anchor = matches [ i ][ 0 ] for j , ( ent , is_sent_end ) in enumerate ( matches [ i :: - 1 ]): if not is_sent_end and ent . end < anchor . start - BEFORE_SNIPPET_LIMIT : return yield i - j , ent # Make a pseudo sentence to query higher order patterns in the main loop # `offsets` is a mapping from matches indices (ie match n\u00b0i) to # char indices in the pseudo sentence pseudo , offsets = self . make_pseudo_sentence ( doc , matches , { self . nlp . vocab . strings [ \"stopword\" ]: \",\" , self . nlp . vocab . strings [ \"number\" ]: \"n\" , ** { name : \"u\" for name in unit_label_hashes }, ** { name : \"n\" for name in self . number_label_hashes }, }, ) measurements = [] matched_unit_indices = set () # Iterate through the number matches for number_idx , ( number , is_sent_split ) in enumerate ( matches ): if not is_sent_split and number . label not in self . number_label_hashes : continue # Detect the measure value try : if number . label_ == \"number\" : value = float ( number . text . replace ( \" \" , \"\" ) . replace ( \",\" , \".\" ) . replace ( \" \" , \"\" ) ) else : value = float ( number . label_ ) except ValueError : continue unit_idx = unit_text = unit_norm = None # Find the closest unit after the number try : unit_idx , unit_text = next ( ( j , ent ) for j , ent in get_matches_after ( number_idx ) if ent . label in unit_label_hashes ) unit_norm = unit_text . label_ except ( AttributeError , StopIteration ): pass # Try to pair the number with this next unit if the two are only separated # by numbers and separators alternatively (as in [1][,] [2] [and] [3] cm) try : pseudo_sent = pseudo [ offsets [ number_idx ] + 1 : offsets [ unit_idx ]] if not re . fullmatch ( r \"(,n)*\" , pseudo_sent ): unit_text , unit_norm = None , None except TypeError : pass # Otherwise, try to infer the unit from the preceding unit to handle cases # like (1 meter 50) if unit_norm is None and number_idx - 1 in matched_unit_indices : try : unit_before = matches [ number_idx - 1 ][ 0 ] if unit_before . end == number . start : unit_norm = self . unit_followers [ unit_before . label_ ] except ( KeyError , AttributeError , IndexError ): pass # If no unit was matched, try to detect unitless patterns before # the number to handle cases like (\"Weight: 63, Height: 170\") if not unit_norm : try : ( unitless_idx , unitless_text ) = next ( ( j , e ) for j , e in get_matches_before ( number_idx ) if e . label in self . unitless_label_hashes ) unit_norm = None if re . fullmatch ( r \"[,n]*\" , pseudo [ offsets [ unitless_idx ] + 1 : offsets [ number_idx ]], ): unitless_pattern = self . unitless_patterns [ unitless_text . label_ ] unit_norm = next ( scope [ \"unit\" ] for scope in unitless_pattern [ \"ranges\" ] if ( \"min\" not in scope or value >= scope [ \"min\" ]) and ( \"max\" not in scope or value < scope [ \"max\" ]) ) except StopIteration : pass # Otherwise, skip this number if not unit_norm : continue # Compute the final entity if unit_text and unit_text . end == number . start : ent = doc [ unit_text . start : number . end ] elif unit_text and unit_text . start == number . end : ent = doc [ number . start : unit_text . end ] else : ent = number # Compute the dimensionality of the parsed unit try : dims = self . unit_registry . parse_unit ( unit_norm )[ 0 ] except KeyError : continue # If the measure was not requested, dismiss it # Otherwise, relabel the entity and create the value attribute if dims not in self . measure_names : continue ent . _ . value = SimpleMeasurement ( value , unit_norm , self . unit_registry ) ent . label_ = self . measure_names [ dims ] measurements . append ( ent ) if unit_idx is not None : matched_unit_indices . add ( unit_idx ) return measurements","title":"extract_measurements()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.merge_adjacent_measurements","text":"Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\" PARAMETER DESCRIPTION measurements TYPE: List [ Span ] RETURNS DESCRIPTION List[Span] Source code in edsnlp/pipelines/misc/measurements/measurements.py 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 @classmethod def merge_adjacent_measurements ( cls , measurements : List [ Span ]) -> List [ Span ]: \"\"\" Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\" Parameters ---------- measurements: List[Span] Returns ------- List[Span] \"\"\" merged = measurements [: 1 ] for ent in measurements [ 1 :]: last = merged [ - 1 ] if last . end == ent . start and last . _ . value . unit != ent . _ . value . unit : try : new_value = last . _ . value + ent . _ . value merged [ - 1 ] = last = last . doc [ last . start : ent . end ] last . _ . value = new_value last . label_ = ent . label_ except ( AttributeError , TypeError ): merged . append ( ent ) else : merged . append ( ent ) return merged","title":"merge_adjacent_measurements()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.__call__","text":"Adds measurements to document's \"measurements\" SpanGroup. PARAMETER DESCRIPTION doc spaCy Doc object RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted measurements. Source code in edsnlp/pipelines/misc/measurements/measurements.py 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 def __call__ ( self , doc ): \"\"\" Adds measurements to document's \"measurements\" SpanGroup. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted measurements. \"\"\" measurements = self . extract_measurements ( doc ) measurements = self . merge_adjacent_measurements ( measurements ) doc . spans [ \"measurements\" ] = measurements # for backward compatibility doc . spans [ \"measures\" ] = doc . spans [ \"measurements\" ] return doc","title":"__call__()"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.measurements_match_tuples_sort_key","text":"Source code in edsnlp/pipelines/misc/measurements/measurements.py 666 667 668 669 670 671 672 673 def measurements_match_tuples_sort_key ( span__keep__is_sent_end : Tuple [ Span , bool , bool ] ) -> Tuple [ int , int , bool ]: span , _ , is_sent_end = span__keep__is_sent_end length = span . end - span . start return length , span . end , not is_sent_end","title":"measurements_match_tuples_sort_key()"},{"location":"reference/pipelines/misc/measurements/patterns/","text":"edsnlp.pipelines.misc.measurements.patterns number_terms = { '1' : [ 'un' , 'une' ], '2' : [ 'deux' ], '3' : [ 'trois' ], '4' : [ 'quatre' ], '5' : [ 'cinq' ], '6' : [ 'six' ], '7' : [ 'sept' ], '8' : [ 'huit' ], '9' : [ 'neuf' ], '10' : [ 'dix' ], '11' : [ 'onze' ], '12' : [ 'douze' ], '13' : [ 'treize' ], '14' : [ 'quatorze' ], '15' : [ 'quinze' ], '16' : [ 'seize' ], '17' : [ 'dix-sept' , 'dix sept' ], '18' : [ 'dix-huit' , 'dix huit' ], '19' : [ 'dix-neuf' , 'dix neuf' ], '20' : [ 'vingt' , 'vingts' ], '30' : [ 'trente' ], '40' : [ 'quarante' ], '50' : [ 'cinquante' ], '60' : [ 'soixante' ], '70' : [ 'soixante dix' , 'soixante-dix' ], '80' : [ 'quatre vingt' , 'quatre-vingt' , 'quatre vingts' , 'quatre-vingts' ], '90' : [ 'quatre vingt dix' , 'quatre-vingt-dix' ], '100' : [ 'cent' ], '500' : [ 'cinq cent' , 'cinq-cent' ], '1000' : [ 'mille' , 'milles' ]} module-attribute units_config = { '\u00b5m' : { 'dim' : 'length' , 'degree' : 1 , 'scale' : 0.0001 , 'terms' : [ 'micrometre' , 'micrometres' , 'micro-metre' , 'micrometres' , '\u00b5m' , 'um' ], 'followed_by' : None }, 'mm' : { 'dim' : 'length' , 'degree' : 1 , 'scale' : 0.1 , 'terms' : [ 'millimetre' , 'millimetres' , 'milimetre' , 'milimetres' , 'mm' ], 'followed_by' : None }, 'cm' : { 'dim' : 'length' , 'degree' : 1 , 'scale' : 1.0 , 'terms' : [ 'centimetre' , 'centimetres' , 'cm' ], 'followed_by' : None }, 'dm' : { 'dim' : 'length' , 'degree' : 1 , 'scale' : 10.0 , 'terms' : [ 'decimetre' , 'decimetres' , 'dm' ], 'followed_by' : None }, 'm' : { 'dim' : 'length' , 'degree' : 1 , 'scale' : 100.0 , 'terms' : [ 'metre' , 'metres' , 'm' ], 'followed_by' : 'cm' }, 'mg' : { 'dim' : 'mass' , 'degree' : 1 , 'scale' : 1.0 , 'terms' : [ 'milligramme' , 'miligramme' , 'milligrammes' , 'miligrammes' , 'mgr' , 'mg' ], 'followed_by' : None }, 'cg' : { 'dim' : 'mass' , 'degree' : 1 , 'scale' : 10.0 , 'terms' : [ 'centigramme' , 'centigrammes' , 'cg' , 'cgr' ], 'followed_by' : None }, 'dg' : { 'dim' : 'mass' , 'degree' : 1 , 'scale' : 100.0 , 'terms' : [ 'decigramme' , 'decigrammes' , 'dgr' , 'dg' ], 'followed_by' : None }, 'g' : { 'dim' : 'mass' , 'degree' : 1 , 'scale' : 1000.0 , 'terms' : [ 'gramme' , 'grammes' , 'gr' , 'g' ], 'followed_by' : None }, 'kg' : { 'dim' : 'mass' , 'degree' : 1 , 'scale' : 1000000.0 , 'terms' : [ 'kilo' , 'kilogramme' , 'kilogrammes' , 'kgr' , 'kg' ], 'followed_by' : 'g' }, 'second' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 1 , 'terms' : [ 'seconde' , 'secondes' , 's' ], 'followed_by' : None }, 'minute' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 60 , 'terms' : [ 'mn' , 'min' , 'minute' , 'minutes' ], 'followed_by' : 'second' }, 'hour' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 3600 , 'terms' : [ 'heure' , 'h' ], 'followed_by' : 'minute' }, 'day' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 3600 * 1 , 'terms' : [ 'jour' , 'jours' , 'j' ], 'followed_by' : None }, 'month' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 3600 * 30.4167 , 'terms' : [ 'mois' ], 'followed_by' : None }, 'week' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 3600 * 7 , 'terms' : [ 'semaine' , 'semaines' ], 'followed_by' : None }, 'year' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 3600 * 365.25 , 'terms' : [ 'an' , 'ann\u00e9e' , 'ans' , 'ann\u00e9es' ], 'followed_by' : None }, 'arc-second' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 2 / 60.0 , 'terms' : [ '\"' , \"''\" ], 'followed_by' : None }, 'arc-minute' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 2 , 'terms' : [ \"'\" ], 'followed_by' : 'arc-second' }, 'degree' : { 'dim' : 'time' , 'degree' : 1 , 'scale' : 120 , 'terms' : [ 'degre' , '\u00b0' , 'deg' ], 'followed_by' : 'arc-minute' }, 'celcius' : { 'dim' : 'temperature' , 'degree' : 1 , 'scale' : 1 , 'terms' : [ '\u00b0C' , '\u00b0 celcius' , 'celcius' ], 'followed_by' : None }, 'ml' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 1.0 , 'terms' : [ 'mililitre' , 'millilitre' , 'mililitres' , 'millilitres' , 'ml' ], 'followed_by' : None }, 'cl' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 10.0 , 'terms' : [ 'centilitre' , 'centilitres' , 'cl' ], 'followed_by' : None }, 'dl' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 100.0 , 'terms' : [ 'decilitre' , 'decilitres' , 'dl' ], 'followed_by' : None }, 'l' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 1000.0 , 'terms' : [ 'litre' , 'litres' , 'l' , 'dm3' ], 'followed_by' : 'ml' }, 'cac' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 0.005 , 'terms' : [ 'cac' , 'c.a.c' , 'cuillere \u00e0 caf\u00e9' , 'cuill\u00e8res \u00e0 caf\u00e9' ], 'followed_by' : None }, 'goutte' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 5e-05 , 'terms' : [ 'gt' , 'goutte' ], 'followed_by' : None }, 'mm3' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 0.001 , 'terms' : [ 'mm3' , 'mm\u00b3' ], 'followed_by' : None }, 'cm3' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 1.0 , 'terms' : [ 'cm3' , 'cm\u00b3' , 'cc' ], 'followed_by' : None }, 'dm3' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 1000.0 , 'terms' : [ 'dm3' , 'dm\u00b3' ], 'followed_by' : None }, 'm3' : { 'dim' : 'length' , 'degree' : 3 , 'scale' : 1000000.0 , 'terms' : [ 'm3' , 'm\u00b3' ], 'followed_by' : None }, '\u00b5m2' : { 'dim' : 'length' , 'degree' : 2 , 'scale' : 1e-08 , 'terms' : [ '\u00b5m2' , '\u00b5m\u00b2' ], 'followed_by' : None }, 'mm2' : { 'dim' : 'length' , 'degree' : 2 , 'scale' : 0.01 , 'terms' : [ 'mm2' , 'mm\u00b2' ], 'followed_by' : None }, 'cm2' : { 'dim' : 'length' , 'degree' : 2 , 'scale' : 1.0 , 'terms' : [ 'cm2' , 'cm\u00b2' ], 'followed_by' : None }, 'dm2' : { 'dim' : 'length' , 'degree' : 2 , 'scale' : 100.0 , 'terms' : [ 'dm2' , 'dm\u00b2' ], 'followed_by' : None }, 'm2' : { 'dim' : 'length' , 'degree' : 2 , 'scale' : 10000.0 , 'terms' : [ 'm2' , 'm\u00b2' ], 'followed_by' : None }, 'mui' : { 'dim' : 'ui' , 'degree' : 1 , 'scale' : 1.0 , 'terms' : [ 'mui' , 'm ui' ], 'followed_by' : None }, 'dui' : { 'dim' : 'ui' , 'degree' : 1 , 'scale' : 10.0 , 'terms' : [ 'dui' , 'd ui' ], 'followed_by' : None }, 'cui' : { 'dim' : 'ui' , 'degree' : 1 , 'scale' : 100.0 , 'terms' : [ 'cui' , 'c ui' ], 'followed_by' : None }, 'ui' : { 'dim' : 'ui' , 'degree' : 1 , 'scale' : 1000.0 , 'terms' : [ 'ui' ], 'followed_by' : None }, 'per_\u00b5m' : { 'dim' : 'length' , 'degree' : - 1 , 'scale' : 10000.0 , 'terms' : [ '\u00b5m-1' ], 'followed_by' : None }, 'per_mm' : { 'dim' : 'length' , 'degree' : - 1 , 'scale' : 10.0 , 'terms' : [ 'mm-1' ], 'followed_by' : None }, 'per_cm' : { 'dim' : 'length' , 'degree' : - 1 , 'scale' : 1.0 , 'terms' : [ 'cm-1' ], 'followed_by' : None }, 'per_dm' : { 'dim' : 'length' , 'degree' : - 1 , 'scale' : 0.1 , 'terms' : [ 'dm-1' ], 'followed_by' : None }, 'per_m' : { 'dim' : 'length' , 'degree' : - 1 , 'scale' : 0.001 , 'terms' : [ 'm-1' ], 'followed_by' : None }, 'per_mg' : { 'dim' : 'mass' , 'degree' : - 1 , 'scale' : 1.0 , 'terms' : [ 'mgr-1' , 'mg-1' , 'mgr\u207b\u00b9' , 'mg\u207b\u00b9' ], 'followed_by' : None }, 'per_cg' : { 'dim' : 'mass' , 'degree' : - 1 , 'scale' : 0.1 , 'terms' : [ 'cg-1' , 'cgr-1' , 'cg\u207b\u00b9' , 'cgr\u207b\u00b9' ], 'followed_by' : None }, 'per_dg' : { 'dim' : 'mass' , 'degree' : - 1 , 'scale' : 0.01 , 'terms' : [ 'dgr-1' , 'dg-1' , 'dgr\u207b\u00b9' , 'dg\u207b\u00b9' ], 'followed_by' : None }, 'per_g' : { 'dim' : 'mass' , 'degree' : - 1 , 'scale' : 0.001 , 'terms' : [ 'gr-1' , 'g-1' , 'gr\u207b\u00b9' , 'g\u207b\u00b9' ], 'followed_by' : None }, 'per_kg' : { 'dim' : 'mass' , 'degree' : - 1 , 'scale' : 1e-06 , 'terms' : [ 'kgr-1' , 'kg-1' , 'kgr\u207b\u00b9' , 'kg\u207b\u00b9' ], 'followed_by' : None }, 'per_ml' : { 'dim' : 'length' , 'degree' : - 3 , 'scale' : 1.0 , 'terms' : [ 'ml-1' , 'ml\u207b\u00b9' ], 'followed_by' : None }, 'per_cl' : { 'dim' : 'length' , 'degree' : - 3 , 'scale' : 0.1 , 'terms' : [ 'cl-1' , 'cl\u207b\u00b9' ], 'followed_by' : None }, 'per_dl' : { 'dim' : 'length' , 'degree' : - 3 , 'scale' : 0.01 , 'terms' : [ 'dl-1' , 'dl\u207b\u00b9' ], 'followed_by' : None }, 'per_l' : { 'dim' : 'length' , 'degree' : - 3 , 'scale' : 0.001 , 'terms' : [ 'l-1' , 'l\u207b\u00b9' ], 'followed_by' : None }, 'per_mm3' : { 'dim' : 'length' , 'degree' : - 3 , 'scale' : 1000.0 , 'terms' : [ 'mm-3' , 'mm\u207b\u00b3' ], 'followed_by' : None }, 'per_cm3' : { 'dim' : 'length' , 'degree' : - 3 , 'scale' : 1.0 , 'terms' : [ 'cm-3' , 'cm\u207b\u00b3' , 'cc-1' , 'cc\u207b\u00b9' ], 'followed_by' : None }, 'per_dm3' : { 'dim' : 'length' , 'degree' : - 3 , 'scale' : 0.001 , 'terms' : [ 'dm-3' , 'dm\u207b\u00b3' ], 'followed_by' : None }, 'per_m3' : { 'dim' : 'length' , 'degree' : - 3 , 'scale' : 1e-06 , 'terms' : [ 'm-3' , 'm\u207b\u00b3' ], 'followed_by' : None }, 'per_mui' : { 'dim' : 'ui' , 'degree' : - 1 , 'scale' : 1.0 , 'terms' : [ 'mui-1' , 'mui\u207b\u00b9' ], 'followed_by' : None }, 'per_dui' : { 'dim' : 'ui' , 'degree' : - 1 , 'scale' : 0.1 , 'terms' : [ 'dui-1' , 'dui\u207b\u00b9' ], 'followed_by' : None }, 'per_cui' : { 'dim' : 'ui' , 'degree' : - 1 , 'scale' : 0.01 , 'terms' : [ 'cui-1' , 'cui\u207b\u00b9' ], 'followed_by' : None }, 'per_ui' : { 'dim' : 'ui' , 'degree' : - 1 , 'scale' : 0.001 , 'terms' : [ 'ui-1' , 'ui\u207b\u00b9' ], 'followed_by' : None }, 'per_\u00b5m2' : { 'dim' : 'length' , 'degree' : - 2 , 'scale' : 100000000.0 , 'terms' : [ '\u00b5m-2' , '\u00b5m\u207b\u00b2' ], 'followed_by' : None }, 'per_mm2' : { 'dim' : 'length' , 'degree' : - 2 , 'scale' : 100.0 , 'terms' : [ 'mm-2' , 'mm\u207b\u00b2' ], 'followed_by' : None }, 'per_cm2' : { 'dim' : 'length' , 'degree' : - 2 , 'scale' : 1.0 , 'terms' : [ 'cm-2' , 'cm\u207b\u00b2' ], 'followed_by' : None }, 'per_dm2' : { 'dim' : 'length' , 'degree' : - 2 , 'scale' : 0.01 , 'terms' : [ 'dm-2' , 'dm\u207b\u00b2' ], 'followed_by' : None }, 'per_m2' : { 'dim' : 'length' , 'degree' : - 2 , 'scale' : 0.0001 , 'terms' : [ 'm-2' , 'm\u207b\u00b2' ], 'followed_by' : None }} module-attribute common_measurements = { 'eds.weight' : { 'unit' : 'kg' , 'unitless_patterns' : [{ 'terms' : [ 'poids' , 'poid' , 'pese' , 'pesant' , 'pesait' , 'pesent' ], 'ranges' : [{ 'min' : 0 , 'max' : 200 , 'unit' : 'kg' }, { 'min' : 200 , 'unit' : 'g' }]}]}, 'eds.size' : { 'unit' : 'm' , 'unitless_patterns' : [{ 'terms' : [ 'mesure' , 'taille' , 'mesurant' , 'mesurent' , 'mesurait' , 'mesuree' , 'hauteur' , 'largeur' , 'longueur' ], 'ranges' : [{ 'min' : 0 , 'max' : 3 , 'unit' : 'm' }, { 'min' : 3 , 'unit' : 'cm' }]}]}, 'eds.bmi' : { 'unit' : 'kg_per_m2' , 'unitless_patterns' : [{ 'terms' : [ 'imc' , 'bmi' ], 'ranges' : [{ 'unit' : 'kg_per_m2' }]}]}, 'eds.volume' : { 'unit' : 'm3' , 'unitless_patterns' : []}} module-attribute unit_divisors = [ '/' , 'par' ] module-attribute stopwords = [ 'par' , 'sur' , 'de' , 'a' , ':' , ',' , 'et' ] module-attribute","title":"patterns"},{"location":"reference/pipelines/misc/measurements/patterns/#edsnlppipelinesmiscmeasurementspatterns","text":"","title":"edsnlp.pipelines.misc.measurements.patterns"},{"location":"reference/pipelines/misc/measurements/patterns/#edsnlp.pipelines.misc.measurements.patterns.number_terms","text":"","title":"number_terms"},{"location":"reference/pipelines/misc/measurements/patterns/#edsnlp.pipelines.misc.measurements.patterns.units_config","text":"","title":"units_config"},{"location":"reference/pipelines/misc/measurements/patterns/#edsnlp.pipelines.misc.measurements.patterns.common_measurements","text":"","title":"common_measurements"},{"location":"reference/pipelines/misc/measurements/patterns/#edsnlp.pipelines.misc.measurements.patterns.unit_divisors","text":"","title":"unit_divisors"},{"location":"reference/pipelines/misc/measurements/patterns/#edsnlp.pipelines.misc.measurements.patterns.stopwords","text":"","title":"stopwords"},{"location":"reference/pipelines/misc/reason/","text":"edsnlp.pipelines.misc.reason","title":"`edsnlp.pipelines.misc.reason`"},{"location":"reference/pipelines/misc/reason/#edsnlppipelinesmiscreason","text":"","title":"edsnlp.pipelines.misc.reason"},{"location":"reference/pipelines/misc/reason/factory/","text":"edsnlp.pipelines.misc.reason.factory DEFAULT_CONFIG = dict ( reasons = None , attr = 'TEXT' , use_sections = False , ignore_excluded = False ) module-attribute create_component ( nlp , name , reasons , attr , use_sections , ignore_excluded ) Source code in edsnlp/pipelines/misc/reason/factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @deprecated_factory ( \"reason\" , \"eds.reason\" , default_config = DEFAULT_CONFIG ) @Language . factory ( \"eds.reason\" , default_config = DEFAULT_CONFIG ) def create_component ( nlp : Language , name : str , reasons : Optional [ Dict [ str , Union [ List [ str ], str ]]], attr : str , use_sections : bool , ignore_excluded : bool , ): return Reason ( nlp , reasons = reasons , attr = attr , use_sections = use_sections , ignore_excluded = ignore_excluded , )","title":"factory"},{"location":"reference/pipelines/misc/reason/factory/#edsnlppipelinesmiscreasonfactory","text":"","title":"edsnlp.pipelines.misc.reason.factory"},{"location":"reference/pipelines/misc/reason/factory/#edsnlp.pipelines.misc.reason.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/misc/reason/factory/#edsnlp.pipelines.misc.reason.factory.create_component","text":"Source code in edsnlp/pipelines/misc/reason/factory.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @deprecated_factory ( \"reason\" , \"eds.reason\" , default_config = DEFAULT_CONFIG ) @Language . factory ( \"eds.reason\" , default_config = DEFAULT_CONFIG ) def create_component ( nlp : Language , name : str , reasons : Optional [ Dict [ str , Union [ List [ str ], str ]]], attr : str , use_sections : bool , ignore_excluded : bool , ): return Reason ( nlp , reasons = reasons , attr = attr , use_sections = use_sections , ignore_excluded = ignore_excluded , )","title":"create_component()"},{"location":"reference/pipelines/misc/reason/patterns/","text":"edsnlp.pipelines.misc.reason.patterns reasons = dict ( reasons = [ '(?i)motif de l.?hospitalisation : .+' , '(?i)hospitalis[\u00e9e].?.*(pour|. cause|suite [\u00e0a]).+' , '(?i)(consulte|prise en charge(?! \\\\ set \\\\ svous \\\\ sassurer \\\\ sun \\\\ straitement \\\\ sadapt\u00e9)).*pour.+' , '(?i)motif \\\\ sd.hospitalisation \\\\ s:.+' , '(?i)au total \\\\ s? \\\\ :? \\\\ s? \\\\ n?.+' , '(?i)motif \\\\ sde \\\\ sla \\\\ sconsultation' , '(?i)motif \\\\ sd.admission' , '(?i)conclusion \\\\ smedicale' ]) module-attribute sections_reason = [ 'motif' , 'conclusion' ] module-attribute section_exclude = [ 'ant\u00e9c\u00e9dents' , 'ant\u00e9c\u00e9dents familiaux' , 'histoire de la maladie' ] module-attribute","title":"patterns"},{"location":"reference/pipelines/misc/reason/patterns/#edsnlppipelinesmiscreasonpatterns","text":"","title":"edsnlp.pipelines.misc.reason.patterns"},{"location":"reference/pipelines/misc/reason/patterns/#edsnlp.pipelines.misc.reason.patterns.reasons","text":"","title":"reasons"},{"location":"reference/pipelines/misc/reason/patterns/#edsnlp.pipelines.misc.reason.patterns.sections_reason","text":"","title":"sections_reason"},{"location":"reference/pipelines/misc/reason/patterns/#edsnlp.pipelines.misc.reason.patterns.section_exclude","text":"","title":"section_exclude"},{"location":"reference/pipelines/misc/reason/reason/","text":"edsnlp.pipelines.misc.reason.reason Reason Bases: GenericMatcher Pipeline to identify the reason of the hospitalisation. It declares a Span extension called ents_reason and adds the key reasons to doc.spans. It also declares the boolean extension is_reason . This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language reasons The terminology of reasons. TYPE: Optional[Dict[str, Union[List[str], str]]] attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex. TYPE: str use_sections whether or not use the sections pipeline to improve results. TYPE: bool, ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/reason/reason.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Reason ( GenericMatcher ): \"\"\"Pipeline to identify the reason of the hospitalisation. It declares a Span extension called `ents_reason` and adds the key `reasons` to doc.spans. It also declares the boolean extension `is_reason`. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. reasons : Optional[Dict[str, Union[List[str], str]]] The terminology of reasons. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex. use_sections : bool, whether or not use the `sections` pipeline to improve results. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , reasons : Optional [ Dict [ str , Union [ List [ str ], str ]]], attr : Union [ Dict [ str , str ], str ], use_sections : bool , ignore_excluded : bool , ): if reasons is None : reasons = patterns . reasons super () . __init__ ( nlp , terms = None , regex = reasons , attr = attr , ignore_excluded = ignore_excluded , ) self . use_sections = use_sections and ( \"eds.sections\" in self . nlp . pipe_names or \"sections\" in self . nlp . pipe_names ) if use_sections and not self . use_sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `eds.section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"ents_reason\" ): Span . set_extension ( \"ents_reason\" , default = None ) if not Span . has_extension ( \"is_reason\" ): Span . set_extension ( \"is_reason\" , default = False ) def _enhance_with_sections ( self , sections : Iterable , reasons : Iterable ) -> List : \"\"\"Enhance the list of reasons with the section information. If the reason overlaps with history, so it will be removed from the list Parameters ---------- sections : Iterable Spans of sections identified with the `sections` pipeline reasons : Iterable Reasons list identified by the regex Returns ------- List Updated list of spans reasons \"\"\" for section in sections : if section . label_ in patterns . sections_reason : reasons . append ( section ) if section . label_ in patterns . section_exclude : for reason in reasons : if check_inclusion ( reason , section . start , section . end ): reasons . remove ( reason ) return reasons def __call__ ( self , doc : Doc ) -> Doc : \"\"\"Find spans related to the reasons of the hospitalisation Parameters ---------- doc : Doc Returns ------- Doc \"\"\" matches = self . process ( doc ) reasons = get_spans ( matches , \"reasons\" ) if self . use_sections : sections = doc . spans [ \"sections\" ] reasons = self . _enhance_with_sections ( sections = sections , reasons = reasons ) doc . spans [ \"reasons\" ] = reasons # Entities if len ( doc . ents ) > 0 : for reason in reasons : # TODO optimize this iteration ent_list = [] for ent in doc . ents : if check_inclusion ( ent , reason . start , reason . end ): ent_list . append ( ent ) ent . _ . is_reason = True reason . _ . ents_reason = ent_list reason . _ . is_reason = True return doc use_sections = use_sections and 'eds.sections' in self . nlp . pipe_names or 'sections' in self . nlp . pipe_names instance-attribute __init__ ( nlp , reasons , attr , use_sections , ignore_excluded ) Source code in edsnlp/pipelines/misc/reason/reason.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , nlp : Language , reasons : Optional [ Dict [ str , Union [ List [ str ], str ]]], attr : Union [ Dict [ str , str ], str ], use_sections : bool , ignore_excluded : bool , ): if reasons is None : reasons = patterns . reasons super () . __init__ ( nlp , terms = None , regex = reasons , attr = attr , ignore_excluded = ignore_excluded , ) self . use_sections = use_sections and ( \"eds.sections\" in self . nlp . pipe_names or \"sections\" in self . nlp . pipe_names ) if use_sections and not self . use_sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `eds.section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . set_extensions () set_extensions () Source code in edsnlp/pipelines/misc/reason/reason.py 71 72 73 74 75 76 77 78 @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"ents_reason\" ): Span . set_extension ( \"ents_reason\" , default = None ) if not Span . has_extension ( \"is_reason\" ): Span . set_extension ( \"is_reason\" , default = False ) __call__ ( doc ) Find spans related to the reasons of the hospitalisation PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION Doc Source code in edsnlp/pipelines/misc/reason/reason.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __call__ ( self , doc : Doc ) -> Doc : \"\"\"Find spans related to the reasons of the hospitalisation Parameters ---------- doc : Doc Returns ------- Doc \"\"\" matches = self . process ( doc ) reasons = get_spans ( matches , \"reasons\" ) if self . use_sections : sections = doc . spans [ \"sections\" ] reasons = self . _enhance_with_sections ( sections = sections , reasons = reasons ) doc . spans [ \"reasons\" ] = reasons # Entities if len ( doc . ents ) > 0 : for reason in reasons : # TODO optimize this iteration ent_list = [] for ent in doc . ents : if check_inclusion ( ent , reason . start , reason . end ): ent_list . append ( ent ) ent . _ . is_reason = True reason . _ . ents_reason = ent_list reason . _ . is_reason = True return doc","title":"reason"},{"location":"reference/pipelines/misc/reason/reason/#edsnlppipelinesmiscreasonreason","text":"","title":"edsnlp.pipelines.misc.reason.reason"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason","text":"Bases: GenericMatcher Pipeline to identify the reason of the hospitalisation. It declares a Span extension called ents_reason and adds the key reasons to doc.spans. It also declares the boolean extension is_reason . This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language reasons The terminology of reasons. TYPE: Optional[Dict[str, Union[List[str], str]]] attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex. TYPE: str use_sections whether or not use the sections pipeline to improve results. TYPE: bool, ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/reason/reason.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Reason ( GenericMatcher ): \"\"\"Pipeline to identify the reason of the hospitalisation. It declares a Span extension called `ents_reason` and adds the key `reasons` to doc.spans. It also declares the boolean extension `is_reason`. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. reasons : Optional[Dict[str, Union[List[str], str]]] The terminology of reasons. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex. use_sections : bool, whether or not use the `sections` pipeline to improve results. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , reasons : Optional [ Dict [ str , Union [ List [ str ], str ]]], attr : Union [ Dict [ str , str ], str ], use_sections : bool , ignore_excluded : bool , ): if reasons is None : reasons = patterns . reasons super () . __init__ ( nlp , terms = None , regex = reasons , attr = attr , ignore_excluded = ignore_excluded , ) self . use_sections = use_sections and ( \"eds.sections\" in self . nlp . pipe_names or \"sections\" in self . nlp . pipe_names ) if use_sections and not self . use_sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `eds.section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"ents_reason\" ): Span . set_extension ( \"ents_reason\" , default = None ) if not Span . has_extension ( \"is_reason\" ): Span . set_extension ( \"is_reason\" , default = False ) def _enhance_with_sections ( self , sections : Iterable , reasons : Iterable ) -> List : \"\"\"Enhance the list of reasons with the section information. If the reason overlaps with history, so it will be removed from the list Parameters ---------- sections : Iterable Spans of sections identified with the `sections` pipeline reasons : Iterable Reasons list identified by the regex Returns ------- List Updated list of spans reasons \"\"\" for section in sections : if section . label_ in patterns . sections_reason : reasons . append ( section ) if section . label_ in patterns . section_exclude : for reason in reasons : if check_inclusion ( reason , section . start , section . end ): reasons . remove ( reason ) return reasons def __call__ ( self , doc : Doc ) -> Doc : \"\"\"Find spans related to the reasons of the hospitalisation Parameters ---------- doc : Doc Returns ------- Doc \"\"\" matches = self . process ( doc ) reasons = get_spans ( matches , \"reasons\" ) if self . use_sections : sections = doc . spans [ \"sections\" ] reasons = self . _enhance_with_sections ( sections = sections , reasons = reasons ) doc . spans [ \"reasons\" ] = reasons # Entities if len ( doc . ents ) > 0 : for reason in reasons : # TODO optimize this iteration ent_list = [] for ent in doc . ents : if check_inclusion ( ent , reason . start , reason . end ): ent_list . append ( ent ) ent . _ . is_reason = True reason . _ . ents_reason = ent_list reason . _ . is_reason = True return doc","title":"Reason"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason.use_sections","text":"","title":"use_sections"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason.__init__","text":"Source code in edsnlp/pipelines/misc/reason/reason.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , nlp : Language , reasons : Optional [ Dict [ str , Union [ List [ str ], str ]]], attr : Union [ Dict [ str , str ], str ], use_sections : bool , ignore_excluded : bool , ): if reasons is None : reasons = patterns . reasons super () . __init__ ( nlp , terms = None , regex = reasons , attr = attr , ignore_excluded = ignore_excluded , ) self . use_sections = use_sections and ( \"eds.sections\" in self . nlp . pipe_names or \"sections\" in self . nlp . pipe_names ) if use_sections and not self . use_sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `eds.section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason.set_extensions","text":"Source code in edsnlp/pipelines/misc/reason/reason.py 71 72 73 74 75 76 77 78 @classmethod def set_extensions ( cls ) -> None : if not Span . has_extension ( \"ents_reason\" ): Span . set_extension ( \"ents_reason\" , default = None ) if not Span . has_extension ( \"is_reason\" ): Span . set_extension ( \"is_reason\" , default = False )","title":"set_extensions()"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason.__call__","text":"Find spans related to the reasons of the hospitalisation PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION Doc Source code in edsnlp/pipelines/misc/reason/reason.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __call__ ( self , doc : Doc ) -> Doc : \"\"\"Find spans related to the reasons of the hospitalisation Parameters ---------- doc : Doc Returns ------- Doc \"\"\" matches = self . process ( doc ) reasons = get_spans ( matches , \"reasons\" ) if self . use_sections : sections = doc . spans [ \"sections\" ] reasons = self . _enhance_with_sections ( sections = sections , reasons = reasons ) doc . spans [ \"reasons\" ] = reasons # Entities if len ( doc . ents ) > 0 : for reason in reasons : # TODO optimize this iteration ent_list = [] for ent in doc . ents : if check_inclusion ( ent , reason . start , reason . end ): ent_list . append ( ent ) ent . _ . is_reason = True reason . _ . ents_reason = ent_list reason . _ . is_reason = True return doc","title":"__call__()"},{"location":"reference/pipelines/misc/sections/","text":"edsnlp.pipelines.misc.sections","title":"`edsnlp.pipelines.misc.sections`"},{"location":"reference/pipelines/misc/sections/#edsnlppipelinesmiscsections","text":"","title":"edsnlp.pipelines.misc.sections"},{"location":"reference/pipelines/misc/sections/factory/","text":"edsnlp.pipelines.misc.sections.factory DEFAULT_CONFIG = dict ( sections = None , add_patterns = True , attr = 'NORM' , ignore_excluded = True ) module-attribute create_component ( nlp , name , sections , add_patterns , attr , ignore_excluded ) Source code in edsnlp/pipelines/misc/sections/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @deprecated_factory ( \"sections\" , \"eds.sections\" , default_config = DEFAULT_CONFIG ) @Language . factory ( \"eds.sections\" , default_config = DEFAULT_CONFIG ) def create_component ( nlp : Language , name : str , sections : Optional [ Dict [ str , List [ str ]]], add_patterns : bool , attr : str , ignore_excluded : bool , ): return Sections ( nlp , sections = sections , add_patterns = add_patterns , attr = attr , ignore_excluded = ignore_excluded , )","title":"factory"},{"location":"reference/pipelines/misc/sections/factory/#edsnlppipelinesmiscsectionsfactory","text":"","title":"edsnlp.pipelines.misc.sections.factory"},{"location":"reference/pipelines/misc/sections/factory/#edsnlp.pipelines.misc.sections.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/misc/sections/factory/#edsnlp.pipelines.misc.sections.factory.create_component","text":"Source code in edsnlp/pipelines/misc/sections/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @deprecated_factory ( \"sections\" , \"eds.sections\" , default_config = DEFAULT_CONFIG ) @Language . factory ( \"eds.sections\" , default_config = DEFAULT_CONFIG ) def create_component ( nlp : Language , name : str , sections : Optional [ Dict [ str , List [ str ]]], add_patterns : bool , attr : str , ignore_excluded : bool , ): return Sections ( nlp , sections = sections , add_patterns = add_patterns , attr = attr , ignore_excluded = ignore_excluded , )","title":"create_component()"},{"location":"reference/pipelines/misc/sections/patterns/","text":"edsnlp.pipelines.misc.sections.patterns These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles. The section titles were reviewed by Gilles Chatellier, who gave meaningful insights. See sections/section-dataset notebook for detail. allergies = [ 'allergies' ] module-attribute antecedents = [ 'antecedents' , 'antecedents medicaux et chirurgicaux' , 'antecedents personnels' , 'antecedents medicaux' , 'antecedents chirurgicaux' , 'atcd' ] module-attribute antecedents_familiaux = [ 'antecedents familiaux' ] module-attribute traitements_entree = [ 'attitude therapeutique initiale' , \"traitement a l'entree\" , 'traitement actuel' , 'traitement en cours' , \"traitements a l'entree\" ] module-attribute conclusion = [ 'au total' , 'conclusion' , 'conclusion de sortie' , 'syntese medicale / conclusion' , 'synthese' , 'synthese medicale' , 'synthese medicale/conclusion' , 'conclusion medicale' ] module-attribute conclusion_entree = [ \"conclusion a l'entree\" ] module-attribute habitus = [ 'contexte familial et social' , 'habitus' , 'mode de vie' , 'mode de vie - scolarite' , 'situation sociale, mode de vie' ] module-attribute correspondants = [ 'correspondants' ] module-attribute diagnostic = [ 'diagnostic retenu' ] module-attribute donnees_biometriques_entree = [ \"donnees biometriques et parametres vitaux a l'entree\" , \"parametres vitaux et donnees biometriques a l'entree\" ] module-attribute examens = [ 'examen clinique' , \"examen clinique a l'entree\" ] module-attribute examens_complementaires = [ 'examen(s) complementaire(s)' , 'examens complementaires' , \"examens complementaires a l'entree\" , \"examens complementaires realises a l'entree\" , 'examens complementaires realises pendant le sejour' , 'examens para-cliniques' , 'imagerie post-operatoire' ] module-attribute facteurs_de_risques = [ 'facteurs de risque' , 'facteurs de risques' ] module-attribute histoire_de_la_maladie = [ 'histoire de la maladie' , 'histoire de la maladie - explorations' , 'histoire de la maladie actuelle' , 'histoire du poids' , 'histoire recente' , 'histoire recente de la maladie' , 'rappel clinique' , 'resume' , 'resume clinique' , 'resume clinique - histoire de la maladie' , 'antecedents et histoire de la maladie' ] module-attribute actes = [ 'intervention' ] module-attribute motif = [ 'motif' , \"motif d'hospitalisation\" , \"motif de l'hospitalisation\" , 'motif medical' ] module-attribute prescriptions = [ 'prescriptions de sortie' , 'prescriptions medicales de sortie' ] module-attribute traitements_sortie = [ 'traitement de sortie' ] module-attribute evolution = [ 'evolution' , 'evolution et examen clinique aux lits portes :' ] module-attribute modalites_sortie = [ 'modalites de sortie' , 'devenir du patient' ] module-attribute vaccinations = [ 'vaccinations' , 'vaccination' ] module-attribute introduction = [ \"compte.?rendu d'hospitalisation.{0,30}\" ] module-attribute sections = { 'allergies' : allergies , 'ant\u00e9c\u00e9dents' : antecedents , 'ant\u00e9c\u00e9dents familiaux' : antecedents_familiaux , 'traitements entr\u00e9e' : traitements_entree , 'conclusion' : conclusion , 'conclusion entr\u00e9e' : conclusion_entree , 'habitus' : habitus , 'correspondants' : correspondants , 'diagnostic' : diagnostic , 'donn\u00e9es biom\u00e9triques entr\u00e9e' : donnees_biometriques_entree , 'examens' : examens , 'examens compl\u00e9mentaires' : examens_complementaires , 'facteurs de risques' : facteurs_de_risques , 'histoire de la maladie' : histoire_de_la_maladie , 'actes' : actes , 'motif' : motif , 'prescriptions' : prescriptions , 'traitements sortie' : traitements_sortie , 'evolution' : evolution , 'modalites sortie' : modalites_sortie , 'vaccinations' : vaccinations , 'introduction' : introduction } module-attribute","title":"patterns"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlppipelinesmiscsectionspatterns","text":"These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles. The section titles were reviewed by Gilles Chatellier, who gave meaningful insights. See sections/section-dataset notebook for detail.","title":"edsnlp.pipelines.misc.sections.patterns"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.allergies","text":"","title":"allergies"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.antecedents","text":"","title":"antecedents"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.antecedents_familiaux","text":"","title":"antecedents_familiaux"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.traitements_entree","text":"","title":"traitements_entree"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.conclusion","text":"","title":"conclusion"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.conclusion_entree","text":"","title":"conclusion_entree"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.habitus","text":"","title":"habitus"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.correspondants","text":"","title":"correspondants"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.diagnostic","text":"","title":"diagnostic"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.donnees_biometriques_entree","text":"","title":"donnees_biometriques_entree"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.examens","text":"","title":"examens"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.examens_complementaires","text":"","title":"examens_complementaires"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.facteurs_de_risques","text":"","title":"facteurs_de_risques"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.histoire_de_la_maladie","text":"","title":"histoire_de_la_maladie"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.actes","text":"","title":"actes"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.motif","text":"","title":"motif"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.prescriptions","text":"","title":"prescriptions"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.traitements_sortie","text":"","title":"traitements_sortie"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.evolution","text":"","title":"evolution"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.modalites_sortie","text":"","title":"modalites_sortie"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.vaccinations","text":"","title":"vaccinations"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.introduction","text":"","title":"introduction"},{"location":"reference/pipelines/misc/sections/patterns/#edsnlp.pipelines.misc.sections.patterns.sections","text":"","title":"sections"},{"location":"reference/pipelines/misc/sections/sections/","text":"edsnlp.pipelines.misc.sections.sections Sections Bases: GenericMatcher Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : allergies ; ant\u00e9c\u00e9dents ; ant\u00e9c\u00e9dents familiaux ; traitements entr\u00e9e ; conclusion ; conclusion entr\u00e9e ; habitus ; correspondants ; diagnostic ; donn\u00e9es biom\u00e9triques entr\u00e9e ; examens ; examens compl\u00e9mentaires ; facteurs de risques ; histoire de la maladie ; actes ; motif ; prescriptions ; traitements sortie. The component looks for section titles within the document, and stores them in the section_title extension. For ease-of-use, the component also populates a section extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. PARAMETER DESCRIPTION nlp spaCy pipeline object. TYPE: Language sections Dictionary of terms to look for. TYPE: Dict[str, List[str]] attr Default attribute to match on. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/sections/sections.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class Sections ( GenericMatcher ): \"\"\" Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : - allergies ; - ant\u00e9c\u00e9dents ; - ant\u00e9c\u00e9dents familiaux ; - traitements entr\u00e9e ; - conclusion ; - conclusion entr\u00e9e ; - habitus ; - correspondants ; - diagnostic ; - donn\u00e9es biom\u00e9triques entr\u00e9e ; - examens ; - examens compl\u00e9mentaires ; - facteurs de risques ; - histoire de la maladie ; - actes ; - motif ; - prescriptions ; - traitements sortie. The component looks for section titles within the document, and stores them in the `section_title` extension. For ease-of-use, the component also populates a `section` extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. Parameters ---------- nlp : Language spaCy pipeline object. sections : Dict[str, List[str]] Dictionary of terms to look for. attr : str Default attribute to match on. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , sections : Dict [ str , List [ str ]], add_patterns : bool , attr : str , ignore_excluded : bool , ): logger . warning ( \"The component Sections is still in Beta. Use at your own risks.\" ) if sections is None : sections = patterns . sections sections = dict ( sections ) self . add_patterns = add_patterns if add_patterns : for k , v in sections . items (): sections [ k ] = [ r \"(?<=(?:\\n|^)[^\\n]{0,5})\" + ent + r \"(?=[^\\n]{0,5}\\n)\" for ent in v ] super () . __init__ ( nlp , terms = None , regex = sections , attr = attr , ignore_excluded = ignore_excluded , ) self . set_extensions () if not nlp . has_pipe ( \"normalizer\" ) and not nlp . has_pipe ( \"eds.normalizer\" ): logger . warning ( \"You should add pipe `eds.normalizer`\" ) @classmethod def set_extensions ( cls ): if not Span . has_extension ( \"section_title\" ): Span . set_extension ( \"section_title\" , default = None ) if not Span . has_extension ( \"section\" ): Span . set_extension ( \"section\" , default = None ) @classmethod def tag_ents ( cls , sections : List [ Span ]): for section in sections : for e in section . ents : e . _ . section = section . label_ # noinspection PyProtectedMember def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles self . tag_ents ( sections ) return doc add_patterns = add_patterns instance-attribute __init__ ( nlp , sections , add_patterns , attr , ignore_excluded ) Source code in edsnlp/pipelines/misc/sections/sections.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __init__ ( self , nlp : Language , sections : Dict [ str , List [ str ]], add_patterns : bool , attr : str , ignore_excluded : bool , ): logger . warning ( \"The component Sections is still in Beta. Use at your own risks.\" ) if sections is None : sections = patterns . sections sections = dict ( sections ) self . add_patterns = add_patterns if add_patterns : for k , v in sections . items (): sections [ k ] = [ r \"(?<=(?:\\n|^)[^\\n]{0,5})\" + ent + r \"(?=[^\\n]{0,5}\\n)\" for ent in v ] super () . __init__ ( nlp , terms = None , regex = sections , attr = attr , ignore_excluded = ignore_excluded , ) self . set_extensions () if not nlp . has_pipe ( \"normalizer\" ) and not nlp . has_pipe ( \"eds.normalizer\" ): logger . warning ( \"You should add pipe `eds.normalizer`\" ) set_extensions () Source code in edsnlp/pipelines/misc/sections/sections.py 101 102 103 104 105 106 107 108 @classmethod def set_extensions ( cls ): if not Span . has_extension ( \"section_title\" ): Span . set_extension ( \"section_title\" , default = None ) if not Span . has_extension ( \"section\" ): Span . set_extension ( \"section\" , default = None ) tag_ents ( sections ) Source code in edsnlp/pipelines/misc/sections/sections.py 110 111 112 113 114 @classmethod def tag_ents ( cls , sections : List [ Span ]): for section in sections : for e in section . ents : e . _ . section = section . label_ __call__ ( doc ) Divides the doc into sections PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for sections Source code in edsnlp/pipelines/misc/sections/sections.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles self . tag_ents ( sections ) return doc","title":"sections"},{"location":"reference/pipelines/misc/sections/sections/#edsnlppipelinesmiscsectionssections","text":"","title":"edsnlp.pipelines.misc.sections.sections"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections","text":"Bases: GenericMatcher Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : allergies ; ant\u00e9c\u00e9dents ; ant\u00e9c\u00e9dents familiaux ; traitements entr\u00e9e ; conclusion ; conclusion entr\u00e9e ; habitus ; correspondants ; diagnostic ; donn\u00e9es biom\u00e9triques entr\u00e9e ; examens ; examens compl\u00e9mentaires ; facteurs de risques ; histoire de la maladie ; actes ; motif ; prescriptions ; traitements sortie. The component looks for section titles within the document, and stores them in the section_title extension. For ease-of-use, the component also populates a section extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. PARAMETER DESCRIPTION nlp spaCy pipeline object. TYPE: Language sections Dictionary of terms to look for. TYPE: Dict[str, List[str]] attr Default attribute to match on. TYPE: str ignore_excluded Whether to skip excluded tokens. TYPE: bool Source code in edsnlp/pipelines/misc/sections/sections.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class Sections ( GenericMatcher ): \"\"\" Divides the document into sections. By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier. Detected sections are : - allergies ; - ant\u00e9c\u00e9dents ; - ant\u00e9c\u00e9dents familiaux ; - traitements entr\u00e9e ; - conclusion ; - conclusion entr\u00e9e ; - habitus ; - correspondants ; - diagnostic ; - donn\u00e9es biom\u00e9triques entr\u00e9e ; - examens ; - examens compl\u00e9mentaires ; - facteurs de risques ; - histoire de la maladie ; - actes ; - motif ; - prescriptions ; - traitements sortie. The component looks for section titles within the document, and stores them in the `section_title` extension. For ease-of-use, the component also populates a `section` extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected. Parameters ---------- nlp : Language spaCy pipeline object. sections : Dict[str, List[str]] Dictionary of terms to look for. attr : str Default attribute to match on. ignore_excluded : bool Whether to skip excluded tokens. \"\"\" def __init__ ( self , nlp : Language , sections : Dict [ str , List [ str ]], add_patterns : bool , attr : str , ignore_excluded : bool , ): logger . warning ( \"The component Sections is still in Beta. Use at your own risks.\" ) if sections is None : sections = patterns . sections sections = dict ( sections ) self . add_patterns = add_patterns if add_patterns : for k , v in sections . items (): sections [ k ] = [ r \"(?<=(?:\\n|^)[^\\n]{0,5})\" + ent + r \"(?=[^\\n]{0,5}\\n)\" for ent in v ] super () . __init__ ( nlp , terms = None , regex = sections , attr = attr , ignore_excluded = ignore_excluded , ) self . set_extensions () if not nlp . has_pipe ( \"normalizer\" ) and not nlp . has_pipe ( \"eds.normalizer\" ): logger . warning ( \"You should add pipe `eds.normalizer`\" ) @classmethod def set_extensions ( cls ): if not Span . has_extension ( \"section_title\" ): Span . set_extension ( \"section_title\" , default = None ) if not Span . has_extension ( \"section\" ): Span . set_extension ( \"section\" , default = None ) @classmethod def tag_ents ( cls , sections : List [ Span ]): for section in sections : for e in section . ents : e . _ . section = section . label_ # noinspection PyProtectedMember def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles self . tag_ents ( sections ) return doc","title":"Sections"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections.add_patterns","text":"","title":"add_patterns"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections.__init__","text":"Source code in edsnlp/pipelines/misc/sections/sections.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def __init__ ( self , nlp : Language , sections : Dict [ str , List [ str ]], add_patterns : bool , attr : str , ignore_excluded : bool , ): logger . warning ( \"The component Sections is still in Beta. Use at your own risks.\" ) if sections is None : sections = patterns . sections sections = dict ( sections ) self . add_patterns = add_patterns if add_patterns : for k , v in sections . items (): sections [ k ] = [ r \"(?<=(?:\\n|^)[^\\n]{0,5})\" + ent + r \"(?=[^\\n]{0,5}\\n)\" for ent in v ] super () . __init__ ( nlp , terms = None , regex = sections , attr = attr , ignore_excluded = ignore_excluded , ) self . set_extensions () if not nlp . has_pipe ( \"normalizer\" ) and not nlp . has_pipe ( \"eds.normalizer\" ): logger . warning ( \"You should add pipe `eds.normalizer`\" )","title":"__init__()"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections.set_extensions","text":"Source code in edsnlp/pipelines/misc/sections/sections.py 101 102 103 104 105 106 107 108 @classmethod def set_extensions ( cls ): if not Span . has_extension ( \"section_title\" ): Span . set_extension ( \"section_title\" , default = None ) if not Span . has_extension ( \"section\" ): Span . set_extension ( \"section\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections.tag_ents","text":"Source code in edsnlp/pipelines/misc/sections/sections.py 110 111 112 113 114 @classmethod def tag_ents ( cls , sections : List [ Span ]): for section in sections : for e in section . ents : e . _ . section = section . label_","title":"tag_ents()"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections.__call__","text":"Divides the doc into sections PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for sections Source code in edsnlp/pipelines/misc/sections/sections.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Divides the doc into sections Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for sections \"\"\" titles = filter_spans ( self . process ( doc )) sections = [] for t1 , t2 in zip ( titles [: - 1 ], titles [ 1 :]): section = Span ( doc , t1 . start , t2 . start , label = t1 . label ) section . _ . section_title = t1 sections . append ( section ) if titles : t = titles [ - 1 ] section = Span ( doc , t . start , len ( doc ), label = t . label ) section . _ . section_title = t sections . append ( section ) doc . spans [ \"sections\" ] = sections doc . spans [ \"section_titles\" ] = titles self . tag_ents ( sections ) return doc","title":"__call__()"},{"location":"reference/pipelines/ner/","text":"edsnlp.pipelines.ner","title":"`edsnlp.pipelines.ner`"},{"location":"reference/pipelines/ner/#edsnlppipelinesner","text":"","title":"edsnlp.pipelines.ner"},{"location":"reference/pipelines/ner/adicap/","text":"edsnlp.pipelines.ner.adicap","title":"`edsnlp.pipelines.ner.adicap`"},{"location":"reference/pipelines/ner/adicap/#edsnlppipelinesneradicap","text":"","title":"edsnlp.pipelines.ner.adicap"},{"location":"reference/pipelines/ner/adicap/adicap/","text":"edsnlp.pipelines.ner.adicap.adicap eds.adicap pipeline Adicap Bases: ContextualMatcher Source code in edsnlp/pipelines/ner/adicap/adicap.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 class Adicap ( ContextualMatcher ): def __init__ ( self , nlp , pattern , attr , prefix , window ): self . nlp = nlp if pattern is None : pattern = patterns . base_code if prefix is None : prefix = patterns . adicap_prefix adicap_pattern = dict ( source = \"adicap\" , regex = prefix , regex_attr = attr , assign = [ dict ( name = \"code\" , regex = pattern , window = window , replace_entity = True , reduce_mode = None , ), ], ) super () . __init__ ( nlp = nlp , name = \"adicap\" , attr = attr , patterns = adicap_pattern , ignore_excluded = False , regex_flags = 0 , alignment_mode = \"expand\" , include_assigned = False , assign_as_span = False , ) self . decode_dict = get_adicap_dict () self . set_extensions () @classmethod def set_extensions ( cls ) -> None : super () . set_extensions () if not Span . has_extension ( \"adicap\" ): Span . set_extension ( \"adicap\" , default = None ) if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) def decode ( self , code ): exploded = list ( code ) adicap = AdicapCode ( code = code , sampling_mode = self . decode_dict [ \"D1\" ][ \"codes\" ] . get ( exploded [ 0 ]), technic = self . decode_dict [ \"D2\" ][ \"codes\" ] . get ( exploded [ 1 ]), organ = self . decode_dict [ \"D3\" ][ \"codes\" ] . get ( \"\" . join ( exploded [ 2 : 4 ])), ) for d in [ \"D4\" , \"D5\" , \"D6\" , \"D7\" ]: adicap_short = self . decode_dict [ d ][ \"codes\" ] . get ( \"\" . join ( exploded [ 4 : 8 ])) adicap_long = self . decode_dict [ d ][ \"codes\" ] . get ( \"\" . join ( exploded [ 2 : 8 ])) if ( adicap_short is not None ) | ( adicap_long is not None ): adicap . pathology = self . decode_dict [ d ][ \"label\" ] adicap . behaviour_type = self . decode_dict [ d ][ \"codes\" ] . get ( exploded [ 5 ]) if adicap_short is not None : adicap . pathology_type = adicap_short else : adicap . pathology_type = adicap_long return adicap def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags ADICAP mentions. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for ADICAP \"\"\" spans = self . process ( doc ) spans = filter_spans ( spans ) for span in spans : span . _ . adicap = self . decode ( span . _ . assigned [ \"code\" ]) span . _ . value = span . _ . adicap span . _ . assigned = None doc . spans [ \"adicap\" ] = spans ents , discarded = filter_spans ( list ( doc . ents ) + spans , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc nlp = nlp instance-attribute decode_dict = get_adicap_dict () instance-attribute __init__ ( nlp , pattern , attr , prefix , window ) Source code in edsnlp/pipelines/ner/adicap/adicap.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , nlp , pattern , attr , prefix , window ): self . nlp = nlp if pattern is None : pattern = patterns . base_code if prefix is None : prefix = patterns . adicap_prefix adicap_pattern = dict ( source = \"adicap\" , regex = prefix , regex_attr = attr , assign = [ dict ( name = \"code\" , regex = pattern , window = window , replace_entity = True , reduce_mode = None , ), ], ) super () . __init__ ( nlp = nlp , name = \"adicap\" , attr = attr , patterns = adicap_pattern , ignore_excluded = False , regex_flags = 0 , alignment_mode = \"expand\" , include_assigned = False , assign_as_span = False , ) self . decode_dict = get_adicap_dict () self . set_extensions () set_extensions () Source code in edsnlp/pipelines/ner/adicap/adicap.py 55 56 57 58 59 60 61 @classmethod def set_extensions ( cls ) -> None : super () . set_extensions () if not Span . has_extension ( \"adicap\" ): Span . set_extension ( \"adicap\" , default = None ) if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) decode ( code ) Source code in edsnlp/pipelines/ner/adicap/adicap.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def decode ( self , code ): exploded = list ( code ) adicap = AdicapCode ( code = code , sampling_mode = self . decode_dict [ \"D1\" ][ \"codes\" ] . get ( exploded [ 0 ]), technic = self . decode_dict [ \"D2\" ][ \"codes\" ] . get ( exploded [ 1 ]), organ = self . decode_dict [ \"D3\" ][ \"codes\" ] . get ( \"\" . join ( exploded [ 2 : 4 ])), ) for d in [ \"D4\" , \"D5\" , \"D6\" , \"D7\" ]: adicap_short = self . decode_dict [ d ][ \"codes\" ] . get ( \"\" . join ( exploded [ 4 : 8 ])) adicap_long = self . decode_dict [ d ][ \"codes\" ] . get ( \"\" . join ( exploded [ 2 : 8 ])) if ( adicap_short is not None ) | ( adicap_long is not None ): adicap . pathology = self . decode_dict [ d ][ \"label\" ] adicap . behaviour_type = self . decode_dict [ d ][ \"codes\" ] . get ( exploded [ 5 ]) if adicap_short is not None : adicap . pathology_type = adicap_short else : adicap . pathology_type = adicap_long return adicap __call__ ( doc ) Tags ADICAP mentions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for ADICAP TYPE: Doc Source code in edsnlp/pipelines/ner/adicap/adicap.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags ADICAP mentions. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for ADICAP \"\"\" spans = self . process ( doc ) spans = filter_spans ( spans ) for span in spans : span . _ . adicap = self . decode ( span . _ . assigned [ \"code\" ]) span . _ . value = span . _ . adicap span . _ . assigned = None doc . spans [ \"adicap\" ] = spans ents , discarded = filter_spans ( list ( doc . ents ) + spans , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"adicap"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlppipelinesneradicapadicap","text":"eds.adicap pipeline","title":"edsnlp.pipelines.ner.adicap.adicap"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap","text":"Bases: ContextualMatcher Source code in edsnlp/pipelines/ner/adicap/adicap.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 class Adicap ( ContextualMatcher ): def __init__ ( self , nlp , pattern , attr , prefix , window ): self . nlp = nlp if pattern is None : pattern = patterns . base_code if prefix is None : prefix = patterns . adicap_prefix adicap_pattern = dict ( source = \"adicap\" , regex = prefix , regex_attr = attr , assign = [ dict ( name = \"code\" , regex = pattern , window = window , replace_entity = True , reduce_mode = None , ), ], ) super () . __init__ ( nlp = nlp , name = \"adicap\" , attr = attr , patterns = adicap_pattern , ignore_excluded = False , regex_flags = 0 , alignment_mode = \"expand\" , include_assigned = False , assign_as_span = False , ) self . decode_dict = get_adicap_dict () self . set_extensions () @classmethod def set_extensions ( cls ) -> None : super () . set_extensions () if not Span . has_extension ( \"adicap\" ): Span . set_extension ( \"adicap\" , default = None ) if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) def decode ( self , code ): exploded = list ( code ) adicap = AdicapCode ( code = code , sampling_mode = self . decode_dict [ \"D1\" ][ \"codes\" ] . get ( exploded [ 0 ]), technic = self . decode_dict [ \"D2\" ][ \"codes\" ] . get ( exploded [ 1 ]), organ = self . decode_dict [ \"D3\" ][ \"codes\" ] . get ( \"\" . join ( exploded [ 2 : 4 ])), ) for d in [ \"D4\" , \"D5\" , \"D6\" , \"D7\" ]: adicap_short = self . decode_dict [ d ][ \"codes\" ] . get ( \"\" . join ( exploded [ 4 : 8 ])) adicap_long = self . decode_dict [ d ][ \"codes\" ] . get ( \"\" . join ( exploded [ 2 : 8 ])) if ( adicap_short is not None ) | ( adicap_long is not None ): adicap . pathology = self . decode_dict [ d ][ \"label\" ] adicap . behaviour_type = self . decode_dict [ d ][ \"codes\" ] . get ( exploded [ 5 ]) if adicap_short is not None : adicap . pathology_type = adicap_short else : adicap . pathology_type = adicap_long return adicap def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags ADICAP mentions. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for ADICAP \"\"\" spans = self . process ( doc ) spans = filter_spans ( spans ) for span in spans : span . _ . adicap = self . decode ( span . _ . assigned [ \"code\" ]) span . _ . value = span . _ . adicap span . _ . assigned = None doc . spans [ \"adicap\" ] = spans ents , discarded = filter_spans ( list ( doc . ents ) + spans , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"Adicap"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap.decode_dict","text":"","title":"decode_dict"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap.__init__","text":"Source code in edsnlp/pipelines/ner/adicap/adicap.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , nlp , pattern , attr , prefix , window ): self . nlp = nlp if pattern is None : pattern = patterns . base_code if prefix is None : prefix = patterns . adicap_prefix adicap_pattern = dict ( source = \"adicap\" , regex = prefix , regex_attr = attr , assign = [ dict ( name = \"code\" , regex = pattern , window = window , replace_entity = True , reduce_mode = None , ), ], ) super () . __init__ ( nlp = nlp , name = \"adicap\" , attr = attr , patterns = adicap_pattern , ignore_excluded = False , regex_flags = 0 , alignment_mode = \"expand\" , include_assigned = False , assign_as_span = False , ) self . decode_dict = get_adicap_dict () self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap.set_extensions","text":"Source code in edsnlp/pipelines/ner/adicap/adicap.py 55 56 57 58 59 60 61 @classmethod def set_extensions ( cls ) -> None : super () . set_extensions () if not Span . has_extension ( \"adicap\" ): Span . set_extension ( \"adicap\" , default = None ) if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap.decode","text":"Source code in edsnlp/pipelines/ner/adicap/adicap.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def decode ( self , code ): exploded = list ( code ) adicap = AdicapCode ( code = code , sampling_mode = self . decode_dict [ \"D1\" ][ \"codes\" ] . get ( exploded [ 0 ]), technic = self . decode_dict [ \"D2\" ][ \"codes\" ] . get ( exploded [ 1 ]), organ = self . decode_dict [ \"D3\" ][ \"codes\" ] . get ( \"\" . join ( exploded [ 2 : 4 ])), ) for d in [ \"D4\" , \"D5\" , \"D6\" , \"D7\" ]: adicap_short = self . decode_dict [ d ][ \"codes\" ] . get ( \"\" . join ( exploded [ 4 : 8 ])) adicap_long = self . decode_dict [ d ][ \"codes\" ] . get ( \"\" . join ( exploded [ 2 : 8 ])) if ( adicap_short is not None ) | ( adicap_long is not None ): adicap . pathology = self . decode_dict [ d ][ \"label\" ] adicap . behaviour_type = self . decode_dict [ d ][ \"codes\" ] . get ( exploded [ 5 ]) if adicap_short is not None : adicap . pathology_type = adicap_short else : adicap . pathology_type = adicap_long return adicap","title":"decode()"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap.__call__","text":"Tags ADICAP mentions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for ADICAP TYPE: Doc Source code in edsnlp/pipelines/ner/adicap/adicap.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags ADICAP mentions. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for ADICAP \"\"\" spans = self . process ( doc ) spans = filter_spans ( spans ) for span in spans : span . _ . adicap = self . decode ( span . _ . assigned [ \"code\" ]) span . _ . value = span . _ . adicap span . _ . assigned = None doc . spans [ \"adicap\" ] = spans ents , discarded = filter_spans ( list ( doc . ents ) + spans , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/ner/adicap/factory/","text":"edsnlp.pipelines.ner.adicap.factory DEFAULT_CONFIG = dict ( pattern = None , prefix = None , attr = 'TEXT' , window = 500 ) module-attribute create_component ( nlp , name , pattern , prefix , window , attr ) Source code in edsnlp/pipelines/ner/adicap/factory.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Language . factory ( \"eds.adicap\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , pattern : Optional [ Union [ List [ str ], str ]], prefix : Optional [ Union [ List [ str ], str ]], window : int , attr : str , ): return Adicap ( nlp , pattern = pattern , attr = attr , prefix = prefix , window = window )","title":"factory"},{"location":"reference/pipelines/ner/adicap/factory/#edsnlppipelinesneradicapfactory","text":"","title":"edsnlp.pipelines.ner.adicap.factory"},{"location":"reference/pipelines/ner/adicap/factory/#edsnlp.pipelines.ner.adicap.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/adicap/factory/#edsnlp.pipelines.ner.adicap.factory.create_component","text":"Source code in edsnlp/pipelines/ner/adicap/factory.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Language . factory ( \"eds.adicap\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , pattern : Optional [ Union [ List [ str ], str ]], prefix : Optional [ Union [ List [ str ], str ]], window : int , attr : str , ): return Adicap ( nlp , pattern = pattern , attr = attr , prefix = prefix , window = window )","title":"create_component()"},{"location":"reference/pipelines/ner/adicap/models/","text":"edsnlp.pipelines.ner.adicap.models AdicapCode Bases: BaseModel Source code in edsnlp/pipelines/ner/adicap/models.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class AdicapCode ( BaseModel ): code : str sampling_mode : Optional [ str ] technic : Optional [ str ] organ : Optional [ str ] pathology : Optional [ str ] pathology_type : Optional [ str ] behaviour_type : Optional [ str ] def norm ( self ) -> str : return self . code def __str__ ( self ): return self . norm () code : str = None class-attribute sampling_mode : Optional [ str ] = None class-attribute technic : Optional [ str ] = None class-attribute organ : Optional [ str ] = None class-attribute pathology : Optional [ str ] = None class-attribute pathology_type : Optional [ str ] = None class-attribute behaviour_type : Optional [ str ] = None class-attribute norm () Source code in edsnlp/pipelines/ner/adicap/models.py 15 16 def norm ( self ) -> str : return self . code __str__ () Source code in edsnlp/pipelines/ner/adicap/models.py 18 19 def __str__ ( self ): return self . norm ()","title":"models"},{"location":"reference/pipelines/ner/adicap/models/#edsnlppipelinesneradicapmodels","text":"","title":"edsnlp.pipelines.ner.adicap.models"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode","text":"Bases: BaseModel Source code in edsnlp/pipelines/ner/adicap/models.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class AdicapCode ( BaseModel ): code : str sampling_mode : Optional [ str ] technic : Optional [ str ] organ : Optional [ str ] pathology : Optional [ str ] pathology_type : Optional [ str ] behaviour_type : Optional [ str ] def norm ( self ) -> str : return self . code def __str__ ( self ): return self . norm ()","title":"AdicapCode"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.code","text":"","title":"code"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.sampling_mode","text":"","title":"sampling_mode"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.technic","text":"","title":"technic"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.organ","text":"","title":"organ"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.pathology","text":"","title":"pathology"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.pathology_type","text":"","title":"pathology_type"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.behaviour_type","text":"","title":"behaviour_type"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.norm","text":"Source code in edsnlp/pipelines/ner/adicap/models.py 15 16 def norm ( self ) -> str : return self . code","title":"norm()"},{"location":"reference/pipelines/ner/adicap/models/#edsnlp.pipelines.ner.adicap.models.AdicapCode.__str__","text":"Source code in edsnlp/pipelines/ner/adicap/models.py 18 19 def __str__ ( self ): return self . norm ()","title":"__str__()"},{"location":"reference/pipelines/ner/adicap/patterns/","text":"edsnlp.pipelines.ner.adicap.patterns Source : https://esante.gouv.fr/sites/default/files/media_entity/documents/cgts_sem_adicap_fiche-detaillee.pdf d1_4 = '[A-Z] {4} ' module-attribute d5_8_v1 = ' \\\\ d {4} ' module-attribute d5_8_v2 = ' \\\\ d {4} |[A-Z][0-9A-Z][A-Z][0-9]' module-attribute d5_8_v3 = '[0-9A-Z][0-9][09A-Z][0-9]' module-attribute d5_8_v4 = '0[A-Z][0-9] {2} ' module-attribute adicap_prefix = '(?i)(codification|adicap)' module-attribute base_code = '(' + d1_4 + '(?:' + d5_8_v1 + '|' + d5_8_v2 + '|' + d5_8_v3 + '|' + d5_8_v4 + '))' module-attribute","title":"patterns"},{"location":"reference/pipelines/ner/adicap/patterns/#edsnlppipelinesneradicappatterns","text":"Source : https://esante.gouv.fr/sites/default/files/media_entity/documents/cgts_sem_adicap_fiche-detaillee.pdf","title":"edsnlp.pipelines.ner.adicap.patterns"},{"location":"reference/pipelines/ner/adicap/patterns/#edsnlp.pipelines.ner.adicap.patterns.d1_4","text":"","title":"d1_4"},{"location":"reference/pipelines/ner/adicap/patterns/#edsnlp.pipelines.ner.adicap.patterns.d5_8_v1","text":"","title":"d5_8_v1"},{"location":"reference/pipelines/ner/adicap/patterns/#edsnlp.pipelines.ner.adicap.patterns.d5_8_v2","text":"","title":"d5_8_v2"},{"location":"reference/pipelines/ner/adicap/patterns/#edsnlp.pipelines.ner.adicap.patterns.d5_8_v3","text":"","title":"d5_8_v3"},{"location":"reference/pipelines/ner/adicap/patterns/#edsnlp.pipelines.ner.adicap.patterns.d5_8_v4","text":"","title":"d5_8_v4"},{"location":"reference/pipelines/ner/adicap/patterns/#edsnlp.pipelines.ner.adicap.patterns.adicap_prefix","text":"","title":"adicap_prefix"},{"location":"reference/pipelines/ner/adicap/patterns/#edsnlp.pipelines.ner.adicap.patterns.base_code","text":"","title":"base_code"},{"location":"reference/pipelines/ner/cim10/","text":"edsnlp.pipelines.ner.cim10","title":"`edsnlp.pipelines.ner.cim10`"},{"location":"reference/pipelines/ner/cim10/#edsnlppipelinesnercim10","text":"","title":"edsnlp.pipelines.ner.cim10"},{"location":"reference/pipelines/ner/cim10/factory/","text":"edsnlp.pipelines.ner.cim10.factory DEFAULT_CONFIG = dict ( attr = 'NORM' , ignore_excluded = False , term_matcher = TerminologyTermMatcher . exact , term_matcher_config = {}) module-attribute create_component ( nlp , name , attr , ignore_excluded , term_matcher , term_matcher_config ) Source code in edsnlp/pipelines/ner/cim10/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @Language . factory ( \"eds.cim10\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ] ) def create_component ( nlp : Language , name : str , attr : Union [ str , Dict [ str , str ]], ignore_excluded : bool , term_matcher : TerminologyTermMatcher , term_matcher_config : Dict [ str , Any ], ): return TerminologyMatcher ( nlp , label = \"cim10\" , regex = None , terms = patterns . get_patterns (), attr = attr , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"factory"},{"location":"reference/pipelines/ner/cim10/factory/#edsnlppipelinesnercim10factory","text":"","title":"edsnlp.pipelines.ner.cim10.factory"},{"location":"reference/pipelines/ner/cim10/factory/#edsnlp.pipelines.ner.cim10.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/cim10/factory/#edsnlp.pipelines.ner.cim10.factory.create_component","text":"Source code in edsnlp/pipelines/ner/cim10/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @Language . factory ( \"eds.cim10\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ] ) def create_component ( nlp : Language , name : str , attr : Union [ str , Dict [ str , str ]], ignore_excluded : bool , term_matcher : TerminologyTermMatcher , term_matcher_config : Dict [ str , Any ], ): return TerminologyMatcher ( nlp , label = \"cim10\" , regex = None , terms = patterns . get_patterns (), attr = attr , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"create_component()"},{"location":"reference/pipelines/ner/cim10/patterns/","text":"edsnlp.pipelines.ner.cim10.patterns get_patterns () Source code in edsnlp/pipelines/ner/cim10/patterns.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def get_patterns () -> Dict [ str , List [ str ]]: df = pd . read_csv ( BASE_DIR / \"resources\" / \"cim10.csv.gz\" ) df [ \"code_pattern\" ] = df [ \"code\" ] df [ \"code_point\" ] = df [ \"code\" ] . str [: 2 ] + \".\" + df [ \"code\" ] . str [ 2 :] df [ \"code_space\" ] = df [ \"code\" ] . str [ 0 ] + \" \" + df [ \"code\" ] . str [ 1 :] df [ \"code_space_point\" ] = ( df [ \"code\" ] . str [ 0 ] + \" \" + df [ \"code\" ] . str [ 1 ] + \".\" + df [ \"code\" ] . str [ 2 :] ) df = pd . concat ( [ df [[ \"code\" , \"short\" ]] . rename ( columns = { \"short\" : \"patterns\" }), df [[ \"code\" , \"long\" ]] . rename ( columns = { \"long\" : \"patterns\" }), df [[ \"code\" , \"code_pattern\" ]] . rename ( columns = { \"code_pattern\" : \"patterns\" }), df [[ \"code\" , \"code_point\" ]] . rename ( columns = { \"code_point\" : \"patterns\" }), df [[ \"code\" , \"code_space\" ]] . rename ( columns = { \"code_space\" : \"patterns\" }), df [[ \"code\" , \"code_space_point\" ]] . rename ( columns = { \"code_space_point\" : \"patterns\" } ), ] ) patterns = df . groupby ( \"code\" )[ \"patterns\" ] . agg ( list ) . to_dict () return patterns","title":"patterns"},{"location":"reference/pipelines/ner/cim10/patterns/#edsnlppipelinesnercim10patterns","text":"","title":"edsnlp.pipelines.ner.cim10.patterns"},{"location":"reference/pipelines/ner/cim10/patterns/#edsnlp.pipelines.ner.cim10.patterns.get_patterns","text":"Source code in edsnlp/pipelines/ner/cim10/patterns.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def get_patterns () -> Dict [ str , List [ str ]]: df = pd . read_csv ( BASE_DIR / \"resources\" / \"cim10.csv.gz\" ) df [ \"code_pattern\" ] = df [ \"code\" ] df [ \"code_point\" ] = df [ \"code\" ] . str [: 2 ] + \".\" + df [ \"code\" ] . str [ 2 :] df [ \"code_space\" ] = df [ \"code\" ] . str [ 0 ] + \" \" + df [ \"code\" ] . str [ 1 :] df [ \"code_space_point\" ] = ( df [ \"code\" ] . str [ 0 ] + \" \" + df [ \"code\" ] . str [ 1 ] + \".\" + df [ \"code\" ] . str [ 2 :] ) df = pd . concat ( [ df [[ \"code\" , \"short\" ]] . rename ( columns = { \"short\" : \"patterns\" }), df [[ \"code\" , \"long\" ]] . rename ( columns = { \"long\" : \"patterns\" }), df [[ \"code\" , \"code_pattern\" ]] . rename ( columns = { \"code_pattern\" : \"patterns\" }), df [[ \"code\" , \"code_point\" ]] . rename ( columns = { \"code_point\" : \"patterns\" }), df [[ \"code\" , \"code_space\" ]] . rename ( columns = { \"code_space\" : \"patterns\" }), df [[ \"code\" , \"code_space_point\" ]] . rename ( columns = { \"code_space_point\" : \"patterns\" } ), ] ) patterns = df . groupby ( \"code\" )[ \"patterns\" ] . agg ( list ) . to_dict () return patterns","title":"get_patterns()"},{"location":"reference/pipelines/ner/covid/","text":"edsnlp.pipelines.ner.covid","title":"`edsnlp.pipelines.ner.covid`"},{"location":"reference/pipelines/ner/covid/#edsnlppipelinesnercovid","text":"","title":"edsnlp.pipelines.ner.covid"},{"location":"reference/pipelines/ner/covid/factory/","text":"edsnlp.pipelines.ner.covid.factory DEFAULT_CONFIG = dict ( attr = 'LOWER' , ignore_excluded = False ) module-attribute create_component ( nlp , name , attr , ignore_excluded ) Source code in edsnlp/pipelines/ner/covid/factory.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @Language . factory ( \"eds.covid\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , attr : Union [ str , Dict [ str , str ]], ignore_excluded : bool , ): return GenericMatcher ( nlp , terms = None , regex = dict ( covid = patterns . pattern ), attr = attr , ignore_excluded = ignore_excluded , )","title":"factory"},{"location":"reference/pipelines/ner/covid/factory/#edsnlppipelinesnercovidfactory","text":"","title":"edsnlp.pipelines.ner.covid.factory"},{"location":"reference/pipelines/ner/covid/factory/#edsnlp.pipelines.ner.covid.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/covid/factory/#edsnlp.pipelines.ner.covid.factory.create_component","text":"Source code in edsnlp/pipelines/ner/covid/factory.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @Language . factory ( \"eds.covid\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , attr : Union [ str , Dict [ str , str ]], ignore_excluded : bool , ): return GenericMatcher ( nlp , terms = None , regex = dict ( covid = patterns . pattern ), attr = attr , ignore_excluded = ignore_excluded , )","title":"create_component()"},{"location":"reference/pipelines/ner/covid/patterns/","text":"edsnlp.pipelines.ner.covid.patterns covid = [ 'covid([- \\\\ s]?19)?' , 'sars[- \\\\ s]?cov[- \\\\ s]?2' , 'corona[- \\\\ s]?virus' ] module-attribute diseases = [ 'pneumopathies?' , 'infections?' ] module-attribute pattern = '(' + make_pattern ( diseases ) + ' \\\\ s[\u00e0a]u? \\\\ s)?' + make_pattern ( covid ) module-attribute","title":"patterns"},{"location":"reference/pipelines/ner/covid/patterns/#edsnlppipelinesnercovidpatterns","text":"","title":"edsnlp.pipelines.ner.covid.patterns"},{"location":"reference/pipelines/ner/covid/patterns/#edsnlp.pipelines.ner.covid.patterns.covid","text":"","title":"covid"},{"location":"reference/pipelines/ner/covid/patterns/#edsnlp.pipelines.ner.covid.patterns.diseases","text":"","title":"diseases"},{"location":"reference/pipelines/ner/covid/patterns/#edsnlp.pipelines.ner.covid.patterns.pattern","text":"","title":"pattern"},{"location":"reference/pipelines/ner/drugs/","text":"edsnlp.pipelines.ner.drugs","title":"`edsnlp.pipelines.ner.drugs`"},{"location":"reference/pipelines/ner/drugs/#edsnlppipelinesnerdrugs","text":"","title":"edsnlp.pipelines.ner.drugs"},{"location":"reference/pipelines/ner/drugs/factory/","text":"edsnlp.pipelines.ner.drugs.factory DEFAULT_CONFIG = dict ( attr = 'NORM' , ignore_excluded = False , term_matcher = TerminologyTermMatcher . exact , term_matcher_config = {}) module-attribute create_component ( nlp , name , attr , ignore_excluded , term_matcher , term_matcher_config ) Source code in edsnlp/pipelines/ner/drugs/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @Language . factory ( \"eds.drugs\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , attr : str , ignore_excluded : bool , term_matcher : TerminologyTermMatcher , term_matcher_config : Dict [ str , Any ], ): return TerminologyMatcher ( nlp , label = \"drug\" , terms = patterns . get_patterns (), regex = dict (), attr = attr , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"factory"},{"location":"reference/pipelines/ner/drugs/factory/#edsnlppipelinesnerdrugsfactory","text":"","title":"edsnlp.pipelines.ner.drugs.factory"},{"location":"reference/pipelines/ner/drugs/factory/#edsnlp.pipelines.ner.drugs.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/drugs/factory/#edsnlp.pipelines.ner.drugs.factory.create_component","text":"Source code in edsnlp/pipelines/ner/drugs/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @Language . factory ( \"eds.drugs\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , attr : str , ignore_excluded : bool , term_matcher : TerminologyTermMatcher , term_matcher_config : Dict [ str , Any ], ): return TerminologyMatcher ( nlp , label = \"drug\" , terms = patterns . get_patterns (), regex = dict (), attr = attr , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"create_component()"},{"location":"reference/pipelines/ner/drugs/patterns/","text":"edsnlp.pipelines.ner.drugs.patterns drugs_file = BASE_DIR / 'resources' / 'drugs.json' module-attribute get_patterns () Source code in edsnlp/pipelines/ner/drugs/patterns.py 9 10 11 12 def get_patterns () -> Dict [ str , List [ str ]]: with ( open ( drugs_file , \"r\" )) as f : return json . load ( f )","title":"patterns"},{"location":"reference/pipelines/ner/drugs/patterns/#edsnlppipelinesnerdrugspatterns","text":"","title":"edsnlp.pipelines.ner.drugs.patterns"},{"location":"reference/pipelines/ner/drugs/patterns/#edsnlp.pipelines.ner.drugs.patterns.drugs_file","text":"","title":"drugs_file"},{"location":"reference/pipelines/ner/drugs/patterns/#edsnlp.pipelines.ner.drugs.patterns.get_patterns","text":"Source code in edsnlp/pipelines/ner/drugs/patterns.py 9 10 11 12 def get_patterns () -> Dict [ str , List [ str ]]: with ( open ( drugs_file , \"r\" )) as f : return json . load ( f )","title":"get_patterns()"},{"location":"reference/pipelines/ner/scores/","text":"edsnlp.pipelines.ner.scores","title":"`edsnlp.pipelines.ner.scores`"},{"location":"reference/pipelines/ner/scores/#edsnlppipelinesnerscores","text":"","title":"edsnlp.pipelines.ner.scores"},{"location":"reference/pipelines/ner/scores/base_score/","text":"edsnlp.pipelines.ner.scores.base_score Score Bases: ContextualMatcher Matcher component to extract a numeric score PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language score_name The name of the extracted score TYPE: str regex A list of regexes to identify the score TYPE: List[str] attr Wether to match on the text ('TEXT') or on the normalized text ('NORM') TYPE: str value_extract Regex with capturing group to get the score value TYPE: str score_normalization Function that takes the \"raw\" value extracted from the value_extract regex, and should return - None if no score could be extracted - The desired score value else TYPE: Callable[[Union[str,None]], Any] window Number of token to include after the score's mention to find the score's value TYPE: int Source code in edsnlp/pipelines/ner/scores/base_score.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class Score ( ContextualMatcher ): \"\"\" Matcher component to extract a numeric score Parameters ---------- nlp : Language The spaCy object. score_name : str The name of the extracted score regex : List[str] A list of regexes to identify the score attr : str Wether to match on the text ('TEXT') or on the normalized text ('NORM') value_extract : str Regex with capturing group to get the score value score_normalization : Callable[[Union[str,None]], Any] Function that takes the \"raw\" value extracted from the `value_extract` regex, and should return - None if no score could be extracted - The desired score value else window : int Number of token to include after the score's mention to find the score's value \"\"\" def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , value_extract : Union [ str , Dict [ str , str ], List [ Dict [ str , str ]]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): if isinstance ( value_extract , str ): value_extract = [ dict ( name = \"value\" , regex = value_extract , window = window , ) ] if isinstance ( value_extract , dict ): value_extract = [ value_extract ] value_exists = False for i , extract in enumerate ( value_extract ): extract [ \"window\" ] = extract . get ( \"window\" , window ) if extract . get ( \"name\" , None ) == \"value\" : value_exists = True extract [ \"replace_entity\" ] = True extract [ \"reduce_mode\" ] = \"keep_first\" value_extract [ i ] = extract assert value_exists , \"You should provide a `value` regex in the `assign` dict.\" patterns = dict ( source = score_name , regex = regex , assign = value_extract , ) super () . __init__ ( nlp = nlp , name = score_name , patterns = patterns , assign_as_span = False , alignment_mode = \"expand\" , ignore_excluded = ignore_excluded , attr = attr , regex_flags = flags , include_assigned = False , ) self . score_name = score_name if isinstance ( score_normalization , str ): self . score_normalization = registry . get ( \"misc\" , score_normalization ) else : self . score_normalization = score_normalization self . set_extensions () @classmethod def set_extensions ( cls ) -> None : super ( Score , Score ) . set_extensions () if not Span . has_extension ( \"score_name\" ): Span . set_extension ( \"score_name\" , default = None ) if not Span . has_extension ( \"score_value\" ): Span . set_extension ( \"score_value\" , default = None ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = self . process ( doc ) ents = self . score_filtering ( ents ) ents , discarded = filter_spans ( list ( doc . ents ) + list ( ents ), return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" for ent in ents : value = ent . _ . assigned . get ( \"value\" , None ) if value is None : continue normalized_value = self . score_normalization ( value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = normalized_value yield ent score_name = score_name instance-attribute score_normalization = registry . get ( 'misc' , score_normalization ) instance-attribute __init__ ( nlp , score_name , regex , attr , value_extract , score_normalization , window , ignore_excluded , flags ) Source code in edsnlp/pipelines/ner/scores/base_score.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , value_extract : Union [ str , Dict [ str , str ], List [ Dict [ str , str ]]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): if isinstance ( value_extract , str ): value_extract = [ dict ( name = \"value\" , regex = value_extract , window = window , ) ] if isinstance ( value_extract , dict ): value_extract = [ value_extract ] value_exists = False for i , extract in enumerate ( value_extract ): extract [ \"window\" ] = extract . get ( \"window\" , window ) if extract . get ( \"name\" , None ) == \"value\" : value_exists = True extract [ \"replace_entity\" ] = True extract [ \"reduce_mode\" ] = \"keep_first\" value_extract [ i ] = extract assert value_exists , \"You should provide a `value` regex in the `assign` dict.\" patterns = dict ( source = score_name , regex = regex , assign = value_extract , ) super () . __init__ ( nlp = nlp , name = score_name , patterns = patterns , assign_as_span = False , alignment_mode = \"expand\" , ignore_excluded = ignore_excluded , attr = attr , regex_flags = flags , include_assigned = False , ) self . score_name = score_name if isinstance ( score_normalization , str ): self . score_normalization = registry . get ( \"misc\" , score_normalization ) else : self . score_normalization = score_normalization self . set_extensions () set_extensions () Source code in edsnlp/pipelines/ner/scores/base_score.py 100 101 102 103 104 105 106 @classmethod def set_extensions ( cls ) -> None : super ( Score , Score ) . set_extensions () if not Span . has_extension ( \"score_name\" ): Span . set_extension ( \"score_name\" , default = None ) if not Span . has_extension ( \"score_value\" ): Span . set_extension ( \"score_value\" , default = None ) __call__ ( doc ) Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/ner/scores/base_score.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = self . process ( doc ) ents = self . score_filtering ( ents ) ents , discarded = filter_spans ( list ( doc . ents ) + list ( ents ), return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc score_filtering ( ents ) Extracts, if available, the value of the score. Normalizes the score via the provided self.score_normalization method. PARAMETER DESCRIPTION ents List of spaCy's spans extracted by the score matcher TYPE: List [ Span ] RETURNS DESCRIPTION ents List of spaCy's spans, with, if found, an added score_value extension Source code in edsnlp/pipelines/ner/scores/base_score.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" for ent in ents : value = ent . _ . assigned . get ( \"value\" , None ) if value is None : continue normalized_value = self . score_normalization ( value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = normalized_value yield ent","title":"base_score"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlppipelinesnerscoresbase_score","text":"","title":"edsnlp.pipelines.ner.scores.base_score"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score","text":"Bases: ContextualMatcher Matcher component to extract a numeric score PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language score_name The name of the extracted score TYPE: str regex A list of regexes to identify the score TYPE: List[str] attr Wether to match on the text ('TEXT') or on the normalized text ('NORM') TYPE: str value_extract Regex with capturing group to get the score value TYPE: str score_normalization Function that takes the \"raw\" value extracted from the value_extract regex, and should return - None if no score could be extracted - The desired score value else TYPE: Callable[[Union[str,None]], Any] window Number of token to include after the score's mention to find the score's value TYPE: int Source code in edsnlp/pipelines/ner/scores/base_score.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class Score ( ContextualMatcher ): \"\"\" Matcher component to extract a numeric score Parameters ---------- nlp : Language The spaCy object. score_name : str The name of the extracted score regex : List[str] A list of regexes to identify the score attr : str Wether to match on the text ('TEXT') or on the normalized text ('NORM') value_extract : str Regex with capturing group to get the score value score_normalization : Callable[[Union[str,None]], Any] Function that takes the \"raw\" value extracted from the `value_extract` regex, and should return - None if no score could be extracted - The desired score value else window : int Number of token to include after the score's mention to find the score's value \"\"\" def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , value_extract : Union [ str , Dict [ str , str ], List [ Dict [ str , str ]]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): if isinstance ( value_extract , str ): value_extract = [ dict ( name = \"value\" , regex = value_extract , window = window , ) ] if isinstance ( value_extract , dict ): value_extract = [ value_extract ] value_exists = False for i , extract in enumerate ( value_extract ): extract [ \"window\" ] = extract . get ( \"window\" , window ) if extract . get ( \"name\" , None ) == \"value\" : value_exists = True extract [ \"replace_entity\" ] = True extract [ \"reduce_mode\" ] = \"keep_first\" value_extract [ i ] = extract assert value_exists , \"You should provide a `value` regex in the `assign` dict.\" patterns = dict ( source = score_name , regex = regex , assign = value_extract , ) super () . __init__ ( nlp = nlp , name = score_name , patterns = patterns , assign_as_span = False , alignment_mode = \"expand\" , ignore_excluded = ignore_excluded , attr = attr , regex_flags = flags , include_assigned = False , ) self . score_name = score_name if isinstance ( score_normalization , str ): self . score_normalization = registry . get ( \"misc\" , score_normalization ) else : self . score_normalization = score_normalization self . set_extensions () @classmethod def set_extensions ( cls ) -> None : super ( Score , Score ) . set_extensions () if not Span . has_extension ( \"score_name\" ): Span . set_extension ( \"score_name\" , default = None ) if not Span . has_extension ( \"score_value\" ): Span . set_extension ( \"score_value\" , default = None ) def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = self . process ( doc ) ents = self . score_filtering ( ents ) ents , discarded = filter_spans ( list ( doc . ents ) + list ( ents ), return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" for ent in ents : value = ent . _ . assigned . get ( \"value\" , None ) if value is None : continue normalized_value = self . score_normalization ( value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = normalized_value yield ent","title":"Score"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.score_name","text":"","title":"score_name"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.score_normalization","text":"","title":"score_normalization"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.__init__","text":"Source code in edsnlp/pipelines/ner/scores/base_score.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , value_extract : Union [ str , Dict [ str , str ], List [ Dict [ str , str ]]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): if isinstance ( value_extract , str ): value_extract = [ dict ( name = \"value\" , regex = value_extract , window = window , ) ] if isinstance ( value_extract , dict ): value_extract = [ value_extract ] value_exists = False for i , extract in enumerate ( value_extract ): extract [ \"window\" ] = extract . get ( \"window\" , window ) if extract . get ( \"name\" , None ) == \"value\" : value_exists = True extract [ \"replace_entity\" ] = True extract [ \"reduce_mode\" ] = \"keep_first\" value_extract [ i ] = extract assert value_exists , \"You should provide a `value` regex in the `assign` dict.\" patterns = dict ( source = score_name , regex = regex , assign = value_extract , ) super () . __init__ ( nlp = nlp , name = score_name , patterns = patterns , assign_as_span = False , alignment_mode = \"expand\" , ignore_excluded = ignore_excluded , attr = attr , regex_flags = flags , include_assigned = False , ) self . score_name = score_name if isinstance ( score_normalization , str ): self . score_normalization = registry . get ( \"misc\" , score_normalization ) else : self . score_normalization = score_normalization self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.set_extensions","text":"Source code in edsnlp/pipelines/ner/scores/base_score.py 100 101 102 103 104 105 106 @classmethod def set_extensions ( cls ) -> None : super ( Score , Score ) . set_extensions () if not Span . has_extension ( \"score_name\" ): Span . set_extension ( \"score_name\" , default = None ) if not Span . has_extension ( \"score_value\" ): Span . set_extension ( \"score_value\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.__call__","text":"Adds spans to document. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for extracted terms. Source code in edsnlp/pipelines/ner/scores/base_score.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Adds spans to document. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for extracted terms. \"\"\" ents = self . process ( doc ) ents = self . score_filtering ( ents ) ents , discarded = filter_spans ( list ( doc . ents ) + list ( ents ), return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.score_filtering","text":"Extracts, if available, the value of the score. Normalizes the score via the provided self.score_normalization method. PARAMETER DESCRIPTION ents List of spaCy's spans extracted by the score matcher TYPE: List [ Span ] RETURNS DESCRIPTION ents List of spaCy's spans, with, if found, an added score_value extension Source code in edsnlp/pipelines/ner/scores/base_score.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" for ent in ents : value = ent . _ . assigned . get ( \"value\" , None ) if value is None : continue normalized_value = self . score_normalization ( value ) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = normalized_value yield ent","title":"score_filtering()"},{"location":"reference/pipelines/ner/scores/factory/","text":"edsnlp.pipelines.ner.scores.factory DEFAULT_CONFIG = dict ( attr = 'NORM' , window = 7 , ignore_excluded = False , flags = 0 ) module-attribute create_component ( nlp , name , score_name , regex , value_extract , score_normalization , attr , window , flags , ignore_excluded ) Source code in edsnlp/pipelines/ner/scores/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @deprecated_factory ( \"score\" , \"eds.score\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.score\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , score_name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , ): return Score ( nlp , score_name = score_name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , flags = flags , window = window , ignore_excluded = ignore_excluded , )","title":"factory"},{"location":"reference/pipelines/ner/scores/factory/#edsnlppipelinesnerscoresfactory","text":"","title":"edsnlp.pipelines.ner.scores.factory"},{"location":"reference/pipelines/ner/scores/factory/#edsnlp.pipelines.ner.scores.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/scores/factory/#edsnlp.pipelines.ner.scores.factory.create_component","text":"Source code in edsnlp/pipelines/ner/scores/factory.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @deprecated_factory ( \"score\" , \"eds.score\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.score\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , score_name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , ): return Score ( nlp , score_name = score_name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , flags = flags , window = window , ignore_excluded = ignore_excluded , )","title":"create_component()"},{"location":"reference/pipelines/ner/scores/charlson/","text":"edsnlp.pipelines.ner.scores.charlson","title":"`edsnlp.pipelines.ner.scores.charlson`"},{"location":"reference/pipelines/ner/scores/charlson/#edsnlppipelinesnerscorescharlson","text":"","title":"edsnlp.pipelines.ner.scores.charlson"},{"location":"reference/pipelines/ner/scores/charlson/factory/","text":"edsnlp.pipelines.ner.scores.charlson.factory DEFAULT_CONFIG = dict ( regex = patterns . regex , value_extract = patterns . value_extract , score_normalization = patterns . score_normalization_str , attr = 'NORM' , window = 7 , ignore_excluded = False , flags = 0 ) module-attribute create_component ( nlp , name , regex , value_extract , score_normalization , attr , window , ignore_excluded , flags ) Source code in edsnlp/pipelines/ner/scores/charlson/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"charlson\" , \"eds.charlson\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.charlson\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"factory"},{"location":"reference/pipelines/ner/scores/charlson/factory/#edsnlppipelinesnerscorescharlsonfactory","text":"","title":"edsnlp.pipelines.ner.scores.charlson.factory"},{"location":"reference/pipelines/ner/scores/charlson/factory/#edsnlp.pipelines.ner.scores.charlson.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/scores/charlson/factory/#edsnlp.pipelines.ner.scores.charlson.factory.create_component","text":"Source code in edsnlp/pipelines/ner/scores/charlson/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"charlson\" , \"eds.charlson\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.charlson\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"create_component()"},{"location":"reference/pipelines/ner/scores/charlson/patterns/","text":"edsnlp.pipelines.ner.scores.charlson.patterns regex = [ 'charlson' ] module-attribute value_extract = '^.*?[ \\\\ n \\\\ W]*?( \\\\ d+)' module-attribute score_normalization_str = 'score_normalization.charlson' module-attribute score_normalization ( extracted_score ) Charlson score normalization. If available, returns the integer value of the Charlson score. Source code in edsnlp/pipelines/ner/scores/charlson/patterns.py 12 13 14 15 16 17 18 19 20 21 22 23 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Charlson score normalization. If available, returns the integer value of the Charlson score. \"\"\" score_range = list ( range ( 0 , 30 )) try : if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score ) except ValueError : return None","title":"patterns"},{"location":"reference/pipelines/ner/scores/charlson/patterns/#edsnlppipelinesnerscorescharlsonpatterns","text":"","title":"edsnlp.pipelines.ner.scores.charlson.patterns"},{"location":"reference/pipelines/ner/scores/charlson/patterns/#edsnlp.pipelines.ner.scores.charlson.patterns.regex","text":"","title":"regex"},{"location":"reference/pipelines/ner/scores/charlson/patterns/#edsnlp.pipelines.ner.scores.charlson.patterns.value_extract","text":"","title":"value_extract"},{"location":"reference/pipelines/ner/scores/charlson/patterns/#edsnlp.pipelines.ner.scores.charlson.patterns.score_normalization_str","text":"","title":"score_normalization_str"},{"location":"reference/pipelines/ner/scores/charlson/patterns/#edsnlp.pipelines.ner.scores.charlson.patterns.score_normalization","text":"Charlson score normalization. If available, returns the integer value of the Charlson score. Source code in edsnlp/pipelines/ner/scores/charlson/patterns.py 12 13 14 15 16 17 18 19 20 21 22 23 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Charlson score normalization. If available, returns the integer value of the Charlson score. \"\"\" score_range = list ( range ( 0 , 30 )) try : if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score ) except ValueError : return None","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/elstonellis/","text":"edsnlp.pipelines.ner.scores.elstonellis","title":"`edsnlp.pipelines.ner.scores.elstonellis`"},{"location":"reference/pipelines/ner/scores/elstonellis/#edsnlppipelinesnerscoreselstonellis","text":"","title":"edsnlp.pipelines.ner.scores.elstonellis"},{"location":"reference/pipelines/ner/scores/elstonellis/factory/","text":"edsnlp.pipelines.ner.scores.elstonellis.factory DEFAULT_CONFIG = dict ( regex = patterns . regex , value_extract = patterns . value_extract , score_normalization = patterns . score_normalization_str , attr = 'TEXT' , window = 20 , ignore_excluded = False , flags = 0 ) module-attribute create_component ( nlp , name , regex , value_extract , score_normalization , attr , window , ignore_excluded , flags ) Source code in edsnlp/pipelines/ner/scores/elstonellis/factory.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @Language . factory ( \"eds.elston-ellis\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"factory"},{"location":"reference/pipelines/ner/scores/elstonellis/factory/#edsnlppipelinesnerscoreselstonellisfactory","text":"","title":"edsnlp.pipelines.ner.scores.elstonellis.factory"},{"location":"reference/pipelines/ner/scores/elstonellis/factory/#edsnlp.pipelines.ner.scores.elstonellis.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/scores/elstonellis/factory/#edsnlp.pipelines.ner.scores.elstonellis.factory.create_component","text":"Source code in edsnlp/pipelines/ner/scores/elstonellis/factory.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @Language . factory ( \"eds.elston-ellis\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"create_component()"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/","text":"edsnlp.pipelines.ner.scores.elstonellis.patterns regex = [ '[Ee]lston (& |et |and )?[Ee]llis' , ' \\\\ b[Ee] {2} \\\\ b' ] module-attribute pattern1 = '[^ \\\\ d \\\\ ( \\\\ )]*[0-3]' module-attribute pattern2 = '.{0,2}[ \\\\ +,]' module-attribute value_extract = '(?s).( \\\\ ( {pattern1}{pattern2}{pattern1}{pattern2}{pattern1} \\\\ ))' module-attribute score_normalization_str = 'score_normalization.elstonellis' module-attribute score_normalization ( extracted_score ) Elston and Ellis score normalization. If available, returns the integer value of the Elston and Ellis score. Source code in edsnlp/pipelines/ner/scores/elstonellis/patterns.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Elston and Ellis score normalization. If available, returns the integer value of the Elston and Ellis score. \"\"\" try : x = 0 for i in re . findall ( r \"[0-3]\" , extracted_score ): x += int ( i ) if x <= 5 : return 1 elif x <= 7 : return 2 else : return 3 except ValueError : return None","title":"patterns"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/#edsnlppipelinesnerscoreselstonellispatterns","text":"","title":"edsnlp.pipelines.ner.scores.elstonellis.patterns"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/#edsnlp.pipelines.ner.scores.elstonellis.patterns.regex","text":"","title":"regex"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/#edsnlp.pipelines.ner.scores.elstonellis.patterns.pattern1","text":"","title":"pattern1"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/#edsnlp.pipelines.ner.scores.elstonellis.patterns.pattern2","text":"","title":"pattern2"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/#edsnlp.pipelines.ner.scores.elstonellis.patterns.value_extract","text":"","title":"value_extract"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/#edsnlp.pipelines.ner.scores.elstonellis.patterns.score_normalization_str","text":"","title":"score_normalization_str"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/#edsnlp.pipelines.ner.scores.elstonellis.patterns.score_normalization","text":"Elston and Ellis score normalization. If available, returns the integer value of the Elston and Ellis score. Source code in edsnlp/pipelines/ner/scores/elstonellis/patterns.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Elston and Ellis score normalization. If available, returns the integer value of the Elston and Ellis score. \"\"\" try : x = 0 for i in re . findall ( r \"[0-3]\" , extracted_score ): x += int ( i ) if x <= 5 : return 1 elif x <= 7 : return 2 else : return 3 except ValueError : return None","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/emergency/","text":"edsnlp.pipelines.ner.scores.emergency","title":"`edsnlp.pipelines.ner.scores.emergency`"},{"location":"reference/pipelines/ner/scores/emergency/#edsnlppipelinesnerscoresemergency","text":"","title":"edsnlp.pipelines.ner.scores.emergency"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/","text":"edsnlp.pipelines.ner.scores.emergency.ccmu","title":"`edsnlp.pipelines.ner.scores.emergency.ccmu`"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/#edsnlppipelinesnerscoresemergencyccmu","text":"","title":"edsnlp.pipelines.ner.scores.emergency.ccmu"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/factory/","text":"edsnlp.pipelines.ner.scores.emergency.ccmu.factory DEFAULT_CONFIG = dict ( regex = patterns . regex , value_extract = patterns . value_extract , score_normalization = patterns . score_normalization_str , attr = 'NORM' , window = 20 , ignore_excluded = False , flags = 0 ) module-attribute create_component ( nlp , name , regex , value_extract , score_normalization , attr , window , ignore_excluded , flags ) Source code in edsnlp/pipelines/ner/scores/emergency/ccmu/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"emergency.ccmu\" , \"eds.emergency.ccmu\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.emergency.ccmu\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"factory"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/factory/#edsnlppipelinesnerscoresemergencyccmufactory","text":"","title":"edsnlp.pipelines.ner.scores.emergency.ccmu.factory"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/factory/#edsnlp.pipelines.ner.scores.emergency.ccmu.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/factory/#edsnlp.pipelines.ner.scores.emergency.ccmu.factory.create_component","text":"Source code in edsnlp/pipelines/ner/scores/emergency/ccmu/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"emergency.ccmu\" , \"eds.emergency.ccmu\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.emergency.ccmu\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"create_component()"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/","text":"edsnlp.pipelines.ner.scores.emergency.ccmu.patterns regex = [ ' \\\\ bccmu \\\\ b' ] module-attribute value_extract = '^.*?[ \\\\ n \\\\ W]*?( \\\\ d+)' module-attribute score_normalization_str = 'score_normalization.ccmu' module-attribute score_normalization ( extracted_score ) CCMU score normalization. If available, returns the integer value of the CCMU score. Source code in edsnlp/pipelines/ner/scores/emergency/ccmu/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" CCMU score normalization. If available, returns the integer value of the CCMU score. \"\"\" score_range = [ 1 , 2 , 3 , 4 , 5 ] if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/#edsnlppipelinesnerscoresemergencyccmupatterns","text":"","title":"edsnlp.pipelines.ner.scores.emergency.ccmu.patterns"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/#edsnlp.pipelines.ner.scores.emergency.ccmu.patterns.regex","text":"","title":"regex"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/#edsnlp.pipelines.ner.scores.emergency.ccmu.patterns.value_extract","text":"","title":"value_extract"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/#edsnlp.pipelines.ner.scores.emergency.ccmu.patterns.score_normalization_str","text":"","title":"score_normalization_str"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/#edsnlp.pipelines.ner.scores.emergency.ccmu.patterns.score_normalization","text":"CCMU score normalization. If available, returns the integer value of the CCMU score. Source code in edsnlp/pipelines/ner/scores/emergency/ccmu/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" CCMU score normalization. If available, returns the integer value of the CCMU score. \"\"\" score_range = [ 1 , 2 , 3 , 4 , 5 ] if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/","text":"edsnlp.pipelines.ner.scores.emergency.gemsa","title":"`edsnlp.pipelines.ner.scores.emergency.gemsa`"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/#edsnlppipelinesnerscoresemergencygemsa","text":"","title":"edsnlp.pipelines.ner.scores.emergency.gemsa"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/factory/","text":"edsnlp.pipelines.ner.scores.emergency.gemsa.factory DEFAULT_CONFIG = dict ( regex = patterns . regex , value_extract = patterns . value_extract , score_normalization = patterns . score_normalization_str , attr = 'NORM' , window = 20 , ignore_excluded = False , flags = 0 ) module-attribute create_component ( nlp , name , regex , value_extract , score_normalization , attr , window , ignore_excluded , flags ) Source code in edsnlp/pipelines/ner/scores/emergency/gemsa/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"emergency.gemsa\" , \"eds.emergency.gemsa\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.emergency.gemsa\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"factory"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/factory/#edsnlppipelinesnerscoresemergencygemsafactory","text":"","title":"edsnlp.pipelines.ner.scores.emergency.gemsa.factory"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/factory/#edsnlp.pipelines.ner.scores.emergency.gemsa.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/factory/#edsnlp.pipelines.ner.scores.emergency.gemsa.factory.create_component","text":"Source code in edsnlp/pipelines/ner/scores/emergency/gemsa/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"emergency.gemsa\" , \"eds.emergency.gemsa\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.emergency.gemsa\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"create_component()"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/","text":"edsnlp.pipelines.ner.scores.emergency.gemsa.patterns regex = [ ' \\\\ bgemsa \\\\ b' ] module-attribute value_extract = '^.*?[ \\\\ n \\\\ W]*?( \\\\ d+)' module-attribute score_normalization_str = 'score_normalization.gemsa' module-attribute score_normalization ( extracted_score ) GEMSA score normalization. If available, returns the integer value of the GEMSA score. Source code in edsnlp/pipelines/ner/scores/emergency/gemsa/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" GEMSA score normalization. If available, returns the integer value of the GEMSA score. \"\"\" score_range = [ 1 , 2 , 3 , 4 , 5 , 6 ] if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/#edsnlppipelinesnerscoresemergencygemsapatterns","text":"","title":"edsnlp.pipelines.ner.scores.emergency.gemsa.patterns"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/#edsnlp.pipelines.ner.scores.emergency.gemsa.patterns.regex","text":"","title":"regex"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/#edsnlp.pipelines.ner.scores.emergency.gemsa.patterns.value_extract","text":"","title":"value_extract"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/#edsnlp.pipelines.ner.scores.emergency.gemsa.patterns.score_normalization_str","text":"","title":"score_normalization_str"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/#edsnlp.pipelines.ner.scores.emergency.gemsa.patterns.score_normalization","text":"GEMSA score normalization. If available, returns the integer value of the GEMSA score. Source code in edsnlp/pipelines/ner/scores/emergency/gemsa/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" GEMSA score normalization. If available, returns the integer value of the GEMSA score. \"\"\" score_range = [ 1 , 2 , 3 , 4 , 5 , 6 ] if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/emergency/priority/","text":"edsnlp.pipelines.ner.scores.emergency.priority","title":"`edsnlp.pipelines.ner.scores.emergency.priority`"},{"location":"reference/pipelines/ner/scores/emergency/priority/#edsnlppipelinesnerscoresemergencypriority","text":"","title":"edsnlp.pipelines.ner.scores.emergency.priority"},{"location":"reference/pipelines/ner/scores/emergency/priority/factory/","text":"edsnlp.pipelines.ner.scores.emergency.priority.factory DEFAULT_CONFIG = dict ( regex = patterns . regex , value_extract = patterns . value_extract , score_normalization = patterns . score_normalization_str , attr = 'NORM' , window = 7 , ignore_excluded = False , flags = 0 ) module-attribute create_component ( nlp , name , regex , value_extract , score_normalization , attr , window , ignore_excluded , flags ) Source code in edsnlp/pipelines/ner/scores/emergency/priority/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"emergency.priority\" , \"eds.emergency.priority\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.emergency.priority\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"factory"},{"location":"reference/pipelines/ner/scores/emergency/priority/factory/#edsnlppipelinesnerscoresemergencypriorityfactory","text":"","title":"edsnlp.pipelines.ner.scores.emergency.priority.factory"},{"location":"reference/pipelines/ner/scores/emergency/priority/factory/#edsnlp.pipelines.ner.scores.emergency.priority.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/scores/emergency/priority/factory/#edsnlp.pipelines.ner.scores.emergency.priority.factory.create_component","text":"Source code in edsnlp/pipelines/ner/scores/emergency/priority/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @deprecated_factory ( \"emergency.priority\" , \"eds.emergency.priority\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.emergency.priority\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : str , score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Score ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"create_component()"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/","text":"edsnlp.pipelines.ner.scores.emergency.priority.patterns regex = [ ' \\\\ bpriorite \\\\ b' ] module-attribute value_extract = '^.*?[ \\\\ n \\\\ W]*?( \\\\ d+)' module-attribute score_normalization_str = 'score_normalization.priority' module-attribute score_normalization ( extracted_score ) Priority score normalization. If available, returns the integer value of the priority score. Source code in edsnlp/pipelines/ner/scores/emergency/priority/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Priority score normalization. If available, returns the integer value of the priority score. \"\"\" score_range = list ( range ( 0 , 6 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/#edsnlppipelinesnerscoresemergencyprioritypatterns","text":"","title":"edsnlp.pipelines.ner.scores.emergency.priority.patterns"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/#edsnlp.pipelines.ner.scores.emergency.priority.patterns.regex","text":"","title":"regex"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/#edsnlp.pipelines.ner.scores.emergency.priority.patterns.value_extract","text":"","title":"value_extract"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/#edsnlp.pipelines.ner.scores.emergency.priority.patterns.score_normalization_str","text":"","title":"score_normalization_str"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/#edsnlp.pipelines.ner.scores.emergency.priority.patterns.score_normalization","text":"Priority score normalization. If available, returns the integer value of the priority score. Source code in edsnlp/pipelines/ner/scores/emergency/priority/patterns.py 12 13 14 15 16 17 18 19 20 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Priority score normalization. If available, returns the integer value of the priority score. \"\"\" score_range = list ( range ( 0 , 6 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/sofa/","text":"edsnlp.pipelines.ner.scores.sofa","title":"`edsnlp.pipelines.ner.scores.sofa`"},{"location":"reference/pipelines/ner/scores/sofa/#edsnlppipelinesnerscoressofa","text":"","title":"edsnlp.pipelines.ner.scores.sofa"},{"location":"reference/pipelines/ner/scores/sofa/factory/","text":"edsnlp.pipelines.ner.scores.sofa.factory DEFAULT_CONFIG = dict ( regex = patterns . regex , value_extract = patterns . value_extract , score_normalization = patterns . score_normalization_str , attr = 'NORM' , window = 10 , ignore_excluded = False , flags = 0 ) module-attribute create_component ( nlp , name , regex , value_extract , score_normalization , attr , window , ignore_excluded , flags ) Source code in edsnlp/pipelines/ner/scores/sofa/factory.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @deprecated_factory ( \"SOFA\" , \"eds.SOFA\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.SOFA\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : List [ Dict [ str , str ]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Sofa ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"factory"},{"location":"reference/pipelines/ner/scores/sofa/factory/#edsnlppipelinesnerscoressofafactory","text":"","title":"edsnlp.pipelines.ner.scores.sofa.factory"},{"location":"reference/pipelines/ner/scores/sofa/factory/#edsnlp.pipelines.ner.scores.sofa.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/scores/sofa/factory/#edsnlp.pipelines.ner.scores.sofa.factory.create_component","text":"Source code in edsnlp/pipelines/ner/scores/sofa/factory.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @deprecated_factory ( \"SOFA\" , \"eds.SOFA\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) @Language . factory ( \"eds.SOFA\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , regex : List [ str ], value_extract : List [ Dict [ str , str ]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], attr : str , window : int , ignore_excluded : bool , flags : Union [ re . RegexFlag , int ], ): return Sofa ( nlp , score_name = name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , ignore_excluded = ignore_excluded , flags = flags , )","title":"create_component()"},{"location":"reference/pipelines/ner/scores/sofa/patterns/","text":"edsnlp.pipelines.ner.scores.sofa.patterns regex = [ ' \\\\ bsofa \\\\ b' ] module-attribute digits = '[^ \\\\ d]*( \\\\ d*)' module-attribute value_extract = [ dict ( name = 'method_max' , regex = '(max)' , reduce_mode = 'keep_first' ), dict ( name = 'method_24h' , regex = '(24h)' , reduce_mode = 'keep_first' ), dict ( name = 'method_adm' , regex = '(admission)' , reduce_mode = 'keep_first' ), dict ( name = 'value' , regex = '^.*?[ \\\\ n \\\\ W]*?( \\\\ d+)(?![h0-9])' )] module-attribute score_normalization_str = 'score_normalization.sofa' module-attribute score_normalization ( extracted_score ) Sofa score normalization. If available, returns the integer value of the SOFA score. Source code in edsnlp/pipelines/ner/scores/sofa/patterns.py 34 35 36 37 38 39 40 41 42 43 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Sofa score normalization. If available, returns the integer value of the SOFA score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"patterns"},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlppipelinesnerscoressofapatterns","text":"","title":"edsnlp.pipelines.ner.scores.sofa.patterns"},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlp.pipelines.ner.scores.sofa.patterns.regex","text":"","title":"regex"},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlp.pipelines.ner.scores.sofa.patterns.digits","text":"","title":"digits"},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlp.pipelines.ner.scores.sofa.patterns.value_extract","text":"","title":"value_extract"},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlp.pipelines.ner.scores.sofa.patterns.score_normalization_str","text":"","title":"score_normalization_str"},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlp.pipelines.ner.scores.sofa.patterns.score_normalization","text":"Sofa score normalization. If available, returns the integer value of the SOFA score. Source code in edsnlp/pipelines/ner/scores/sofa/patterns.py 34 35 36 37 38 39 40 41 42 43 @spacy . registry . misc ( score_normalization_str ) def score_normalization ( extracted_score : Union [ str , None ]): \"\"\" Sofa score normalization. If available, returns the integer value of the SOFA score. \"\"\" score_range = list ( range ( 0 , 30 )) if ( extracted_score is not None ) and ( int ( extracted_score ) in score_range ): return int ( extracted_score )","title":"score_normalization()"},{"location":"reference/pipelines/ner/scores/sofa/sofa/","text":"edsnlp.pipelines.ner.scores.sofa.sofa Sofa Bases: Score Matcher component to extract the SOFA score PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language score_name The name of the extracted score TYPE: str regex A list of regexes to identify the SOFA score TYPE: List[str] attr Wether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM') TYPE: str method_regex Regex with capturing group to get the score extraction method (e.g. \"\u00e0 l'admission\", \"\u00e0 24H\", \"Maximum\") TYPE: str value_regex Regex to extract the score value TYPE: str score_normalization Function that takes the \"raw\" value extracted from the value_extract regex, and should return - None if no score could be extracted - The desired score value else TYPE: Callable[[Union[str,None]], Any] window Number of token to include after the score's mention to find the score's value TYPE: int Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class Sofa ( Score ): \"\"\" Matcher component to extract the SOFA score Parameters ---------- nlp : Language The spaCy object. score_name : str The name of the extracted score regex : List[str] A list of regexes to identify the SOFA score attr : str Wether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM') method_regex : str Regex with capturing group to get the score extraction method (e.g. \"\u00e0 l'admission\", \"\u00e0 24H\", \"Maximum\") value_regex : str Regex to extract the score value score_normalization : Callable[[Union[str,None]], Any] Function that takes the \"raw\" value extracted from the `value_extract` regex, and should return - None if no score could be extracted - The desired score value else window : int Number of token to include after the score's mention to find the score's value \"\"\" def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , value_extract : List [ Dict [ str , str ]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , ): super () . __init__ ( nlp , score_name = score_name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , flags = flags , ignore_excluded = ignore_excluded , ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : super ( Sofa , Sofa ) . set_extensions () if not Span . has_extension ( \"score_method\" ): Span . set_extension ( \"score_method\" , default = None ) def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" for ent in ents : assigned = ent . _ . assigned if not assigned : continue if assigned . get ( \"method_max\" ) is not None : method = \"Maximum\" elif assigned . get ( \"method_24h\" ) is not None : method = \"24H\" elif assigned . get ( \"method_adm\" ) is not None : method = \"A l'admission\" else : method = \"Non pr\u00e9cis\u00e9e\" normalized_value = self . score_normalization ( assigned [ \"value\" ]) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( normalized_value ) ent . _ . score_method = method yield ent __init__ ( nlp , score_name , regex , attr , value_extract , score_normalization , window , flags , ignore_excluded ) Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , value_extract : List [ Dict [ str , str ]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , ): super () . __init__ ( nlp , score_name = score_name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , flags = flags , ignore_excluded = ignore_excluded , ) self . set_extensions () set_extensions () Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 66 67 68 69 70 @classmethod def set_extensions ( cls ) -> None : super ( Sofa , Sofa ) . set_extensions () if not Span . has_extension ( \"score_method\" ): Span . set_extension ( \"score_method\" , default = None ) score_filtering ( ents ) Extracts, if available, the value of the score. Normalizes the score via the provided self.score_normalization method. PARAMETER DESCRIPTION ents List of spaCy's spans extracted by the score matcher TYPE: List [ Span ] RETURNS DESCRIPTION ents List of spaCy's spans, with, if found, an added score_value extension Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" for ent in ents : assigned = ent . _ . assigned if not assigned : continue if assigned . get ( \"method_max\" ) is not None : method = \"Maximum\" elif assigned . get ( \"method_24h\" ) is not None : method = \"24H\" elif assigned . get ( \"method_adm\" ) is not None : method = \"A l'admission\" else : method = \"Non pr\u00e9cis\u00e9e\" normalized_value = self . score_normalization ( assigned [ \"value\" ]) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( normalized_value ) ent . _ . score_method = method yield ent","title":"sofa"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlppipelinesnerscoressofasofa","text":"","title":"edsnlp.pipelines.ner.scores.sofa.sofa"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlp.pipelines.ner.scores.sofa.sofa.Sofa","text":"Bases: Score Matcher component to extract the SOFA score PARAMETER DESCRIPTION nlp The spaCy object. TYPE: Language score_name The name of the extracted score TYPE: str regex A list of regexes to identify the SOFA score TYPE: List[str] attr Wether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM') TYPE: str method_regex Regex with capturing group to get the score extraction method (e.g. \"\u00e0 l'admission\", \"\u00e0 24H\", \"Maximum\") TYPE: str value_regex Regex to extract the score value TYPE: str score_normalization Function that takes the \"raw\" value extracted from the value_extract regex, and should return - None if no score could be extracted - The desired score value else TYPE: Callable[[Union[str,None]], Any] window Number of token to include after the score's mention to find the score's value TYPE: int Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class Sofa ( Score ): \"\"\" Matcher component to extract the SOFA score Parameters ---------- nlp : Language The spaCy object. score_name : str The name of the extracted score regex : List[str] A list of regexes to identify the SOFA score attr : str Wether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM') method_regex : str Regex with capturing group to get the score extraction method (e.g. \"\u00e0 l'admission\", \"\u00e0 24H\", \"Maximum\") value_regex : str Regex to extract the score value score_normalization : Callable[[Union[str,None]], Any] Function that takes the \"raw\" value extracted from the `value_extract` regex, and should return - None if no score could be extracted - The desired score value else window : int Number of token to include after the score's mention to find the score's value \"\"\" def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , value_extract : List [ Dict [ str , str ]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , ): super () . __init__ ( nlp , score_name = score_name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , flags = flags , ignore_excluded = ignore_excluded , ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : super ( Sofa , Sofa ) . set_extensions () if not Span . has_extension ( \"score_method\" ): Span . set_extension ( \"score_method\" , default = None ) def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" for ent in ents : assigned = ent . _ . assigned if not assigned : continue if assigned . get ( \"method_max\" ) is not None : method = \"Maximum\" elif assigned . get ( \"method_24h\" ) is not None : method = \"24H\" elif assigned . get ( \"method_adm\" ) is not None : method = \"A l'admission\" else : method = \"Non pr\u00e9cis\u00e9e\" normalized_value = self . score_normalization ( assigned [ \"value\" ]) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( normalized_value ) ent . _ . score_method = method yield ent","title":"Sofa"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlp.pipelines.ner.scores.sofa.sofa.Sofa.__init__","text":"Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , nlp : Language , score_name : str , regex : List [ str ], attr : str , value_extract : List [ Dict [ str , str ]], score_normalization : Union [ str , Callable [[ Union [ str , None ]], Any ]], window : int , flags : Union [ re . RegexFlag , int ], ignore_excluded : bool , ): super () . __init__ ( nlp , score_name = score_name , regex = regex , value_extract = value_extract , score_normalization = score_normalization , attr = attr , window = window , flags = flags , ignore_excluded = ignore_excluded , ) self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlp.pipelines.ner.scores.sofa.sofa.Sofa.set_extensions","text":"Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 66 67 68 69 70 @classmethod def set_extensions ( cls ) -> None : super ( Sofa , Sofa ) . set_extensions () if not Span . has_extension ( \"score_method\" ): Span . set_extension ( \"score_method\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlp.pipelines.ner.scores.sofa.sofa.Sofa.score_filtering","text":"Extracts, if available, the value of the score. Normalizes the score via the provided self.score_normalization method. PARAMETER DESCRIPTION ents List of spaCy's spans extracted by the score matcher TYPE: List [ Span ] RETURNS DESCRIPTION ents List of spaCy's spans, with, if found, an added score_value extension Source code in edsnlp/pipelines/ner/scores/sofa/sofa.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def score_filtering ( self , ents : List [ Span ]) -> List [ Span ]: \"\"\" Extracts, if available, the value of the score. Normalizes the score via the provided `self.score_normalization` method. Parameters ---------- ents: List[Span] List of spaCy's spans extracted by the score matcher Returns ------- ents: List[Span] List of spaCy's spans, with, if found, an added `score_value` extension \"\"\" for ent in ents : assigned = ent . _ . assigned if not assigned : continue if assigned . get ( \"method_max\" ) is not None : method = \"Maximum\" elif assigned . get ( \"method_24h\" ) is not None : method = \"24H\" elif assigned . get ( \"method_adm\" ) is not None : method = \"A l'admission\" else : method = \"Non pr\u00e9cis\u00e9e\" normalized_value = self . score_normalization ( assigned [ \"value\" ]) if normalized_value is not None : ent . _ . score_name = self . score_name ent . _ . score_value = int ( normalized_value ) ent . _ . score_method = method yield ent","title":"score_filtering()"},{"location":"reference/pipelines/ner/scores/tnm/","text":"edsnlp.pipelines.ner.scores.tnm","title":"`edsnlp.pipelines.ner.scores.tnm`"},{"location":"reference/pipelines/ner/scores/tnm/#edsnlppipelinesnerscorestnm","text":"","title":"edsnlp.pipelines.ner.scores.tnm"},{"location":"reference/pipelines/ner/scores/tnm/factory/","text":"edsnlp.pipelines.ner.scores.tnm.factory DEFAULT_CONFIG = dict ( pattern = None , attr = 'TEXT' ) module-attribute create_component ( nlp , name , pattern , attr ) Source code in edsnlp/pipelines/ner/scores/tnm/factory.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Language . factory ( \"eds.TNM\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , pattern : Optional [ Union [ List [ str ], str ]], attr : str , ): return TNM ( nlp , pattern = pattern , attr = attr , )","title":"factory"},{"location":"reference/pipelines/ner/scores/tnm/factory/#edsnlppipelinesnerscorestnmfactory","text":"","title":"edsnlp.pipelines.ner.scores.tnm.factory"},{"location":"reference/pipelines/ner/scores/tnm/factory/#edsnlp.pipelines.ner.scores.tnm.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/scores/tnm/factory/#edsnlp.pipelines.ner.scores.tnm.factory.create_component","text":"Source code in edsnlp/pipelines/ner/scores/tnm/factory.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Language . factory ( \"eds.TNM\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ], ) def create_component ( nlp : Language , name : str , pattern : Optional [ Union [ List [ str ], str ]], attr : str , ): return TNM ( nlp , pattern = pattern , attr = attr , )","title":"create_component()"},{"location":"reference/pipelines/ner/scores/tnm/models/","text":"edsnlp.pipelines.ner.scores.tnm.models TnmEnum Bases: Enum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 16 17 18 class TnmEnum ( Enum ): def __str__ ( self ) -> str : return self . value __str__ () Source code in edsnlp/pipelines/ner/scores/tnm/models.py 17 18 def __str__ ( self ) -> str : return self . value Prefix Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 21 22 23 24 25 26 27 28 29 30 31 class Prefix ( TnmEnum ): clinical = \"c\" histopathology = \"p\" histopathology2 = \"P\" neoadjuvant_therapy = \"y\" recurrent = \"r\" autopsy = \"a\" ultrasonography = \"u\" multifocal = \"m\" py = \"yp\" mp = \"mp\" clinical = 'c' class-attribute histopathology = 'p' class-attribute histopathology2 = 'P' class-attribute neoadjuvant_therapy = 'y' class-attribute recurrent = 'r' class-attribute autopsy = 'a' class-attribute ultrasonography = 'u' class-attribute multifocal = 'm' class-attribute py = 'yp' class-attribute mp = 'mp' class-attribute Tumour Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 34 35 36 37 38 39 40 41 42 class Tumour ( TnmEnum ): unknown = \"x\" in_situ = \"is\" score_0 = \"0\" score_1 = \"1\" score_2 = \"2\" score_3 = \"3\" score_4 = \"4\" o = \"o\" unknown = 'x' class-attribute in_situ = 'is' class-attribute score_0 = '0' class-attribute score_1 = '1' class-attribute score_2 = '2' class-attribute score_3 = '3' class-attribute score_4 = '4' class-attribute o = 'o' class-attribute Specification Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 45 46 47 48 49 50 51 class Specification ( TnmEnum ): a = \"a\" b = \"b\" c = \"c\" d = \"d\" mi = \"mi\" x = \"x\" a = 'a' class-attribute b = 'b' class-attribute c = 'c' class-attribute d = 'd' class-attribute mi = 'mi' class-attribute x = 'x' class-attribute Node Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 54 55 56 57 58 59 60 class Node ( TnmEnum ): unknown = \"x\" score_0 = \"0\" score_1 = \"1\" score_2 = \"2\" score_3 = \"3\" o = \"o\" unknown = 'x' class-attribute score_0 = '0' class-attribute score_1 = '1' class-attribute score_2 = '2' class-attribute score_3 = '3' class-attribute o = 'o' class-attribute Metastasis Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 63 64 65 66 67 68 69 70 class Metastasis ( TnmEnum ): unknown = \"x\" score_0 = \"0\" score_1 = \"1\" o = \"o\" score_1x = \"1x\" score_2x = \"2x\" ox = \"ox\" unknown = 'x' class-attribute score_0 = '0' class-attribute score_1 = '1' class-attribute o = 'o' class-attribute score_1x = '1x' class-attribute score_2x = '2x' class-attribute ox = 'ox' class-attribute TNM Bases: BaseModel Source code in edsnlp/pipelines/ner/scores/tnm/models.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class TNM ( BaseModel ): prefix : Optional [ Prefix ] = None tumour : Optional [ Tumour ] = None tumour_specification : Optional [ Specification ] = None tumour_suffix : Optional [ str ] = None node : Optional [ Node ] = None node_specification : Optional [ Specification ] = None node_suffix : Optional [ str ] = None metastasis : Optional [ Metastasis ] = None resection_completeness : Optional [ int ] = None version : Optional [ str ] = None version_year : Optional [ int ] = None @validator ( \"*\" , pre = True ) def coerce_o ( cls , v ): if isinstance ( v , str ): v = v . replace ( \"o\" , \"0\" ) return v @validator ( \"version_year\" ) def validate_year ( cls , v ): if v is None : return v if v < 40 : v += 2000 elif v < 100 : v += 1900 return v def norm ( self ) -> str : norm = [] if self . prefix is not None : norm . append ( str ( self . prefix )) if ( ( self . tumour is not None ) | ( self . tumour_specification is not None ) | ( self . tumour_suffix is not None ) ): norm . append ( f \"T { str ( self . tumour or '' ) } \" ) norm . append ( f \" { str ( self . tumour_specification or '' ) } \" ) norm . append ( f \" { str ( self . tumour_suffix or '' ) } \" ) if ( ( self . node is not None ) | ( self . node_specification is not None ) | ( self . node_suffix is not None ) ): norm . append ( f \"N { str ( self . node or '' ) } \" ) norm . append ( f \" { str ( self . node_specification or '' ) } \" ) norm . append ( f \" { str ( self . node_suffix or '' ) } \" ) if self . metastasis is not None : norm . append ( f \"M { self . metastasis } \" ) if self . resection_completeness is not None : norm . append ( f \"R { self . resection_completeness } \" ) if self . version is not None and self . version_year is not None : norm . append ( f \" ( { self . version . upper () } { self . version_year } )\" ) return \"\" . join ( norm ) def dict ( self , * , include : Union [ \"AbstractSetIntStr\" , \"MappingIntStrAny\" ] = None , exclude : Union [ \"AbstractSetIntStr\" , \"MappingIntStrAny\" ] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , ) -> \"DictStrAny\" : \"\"\" Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. \"\"\" if skip_defaults is not None : warnings . warn ( f \"\"\" { self . __class__ . __name__ } .dict(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\" \"\"\" , DeprecationWarning , ) exclude_unset = skip_defaults d = dict ( self . _iter ( to_dict = True , by_alias = by_alias , include = include , exclude = exclude , exclude_unset = exclude_unset , exclude_defaults = exclude_defaults , exclude_none = exclude_none , ) ) set_keys = set ( d . keys ()) for k in set_keys . intersection ( { \"prefix\" , \"tumour\" , \"node\" , \"metastasis\" , \"tumour_specification\" , \"node_specification\" , \"tumour_suffix\" , \"node_suffix\" , } ): v = d [ k ] if isinstance ( v , TnmEnum ): d [ k ] = v . value return d prefix : Optional [ Prefix ] = None class-attribute tumour : Optional [ Tumour ] = None class-attribute tumour_specification : Optional [ Specification ] = None class-attribute tumour_suffix : Optional [ str ] = None class-attribute node : Optional [ Node ] = None class-attribute node_specification : Optional [ Specification ] = None class-attribute node_suffix : Optional [ str ] = None class-attribute metastasis : Optional [ Metastasis ] = None class-attribute resection_completeness : Optional [ int ] = None class-attribute version : Optional [ str ] = None class-attribute version_year : Optional [ int ] = None class-attribute coerce_o ( v ) Source code in edsnlp/pipelines/ner/scores/tnm/models.py 87 88 89 90 91 @validator ( \"*\" , pre = True ) def coerce_o ( cls , v ): if isinstance ( v , str ): v = v . replace ( \"o\" , \"0\" ) return v validate_year ( v ) Source code in edsnlp/pipelines/ner/scores/tnm/models.py 93 94 95 96 97 98 99 100 101 102 103 @validator ( \"version_year\" ) def validate_year ( cls , v ): if v is None : return v if v < 40 : v += 2000 elif v < 100 : v += 1900 return v norm () Source code in edsnlp/pipelines/ner/scores/tnm/models.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def norm ( self ) -> str : norm = [] if self . prefix is not None : norm . append ( str ( self . prefix )) if ( ( self . tumour is not None ) | ( self . tumour_specification is not None ) | ( self . tumour_suffix is not None ) ): norm . append ( f \"T { str ( self . tumour or '' ) } \" ) norm . append ( f \" { str ( self . tumour_specification or '' ) } \" ) norm . append ( f \" { str ( self . tumour_suffix or '' ) } \" ) if ( ( self . node is not None ) | ( self . node_specification is not None ) | ( self . node_suffix is not None ) ): norm . append ( f \"N { str ( self . node or '' ) } \" ) norm . append ( f \" { str ( self . node_specification or '' ) } \" ) norm . append ( f \" { str ( self . node_suffix or '' ) } \" ) if self . metastasis is not None : norm . append ( f \"M { self . metastasis } \" ) if self . resection_completeness is not None : norm . append ( f \"R { self . resection_completeness } \" ) if self . version is not None and self . version_year is not None : norm . append ( f \" ( { self . version . upper () } { self . version_year } )\" ) return \"\" . join ( norm ) dict ( * , include = None , exclude = None , by_alias = False , skip_defaults = None , exclude_unset = False , exclude_defaults = False , exclude_none = False ) Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. Source code in edsnlp/pipelines/ner/scores/tnm/models.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def dict ( self , * , include : Union [ \"AbstractSetIntStr\" , \"MappingIntStrAny\" ] = None , exclude : Union [ \"AbstractSetIntStr\" , \"MappingIntStrAny\" ] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , ) -> \"DictStrAny\" : \"\"\" Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. \"\"\" if skip_defaults is not None : warnings . warn ( f \"\"\" { self . __class__ . __name__ } .dict(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\" \"\"\" , DeprecationWarning , ) exclude_unset = skip_defaults d = dict ( self . _iter ( to_dict = True , by_alias = by_alias , include = include , exclude = exclude , exclude_unset = exclude_unset , exclude_defaults = exclude_defaults , exclude_none = exclude_none , ) ) set_keys = set ( d . keys ()) for k in set_keys . intersection ( { \"prefix\" , \"tumour\" , \"node\" , \"metastasis\" , \"tumour_specification\" , \"node_specification\" , \"tumour_suffix\" , \"node_suffix\" , } ): v = d [ k ] if isinstance ( v , TnmEnum ): d [ k ] = v . value return d","title":"models"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlppipelinesnerscorestnmmodels","text":"","title":"edsnlp.pipelines.ner.scores.tnm.models"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TnmEnum","text":"Bases: Enum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 16 17 18 class TnmEnum ( Enum ): def __str__ ( self ) -> str : return self . value","title":"TnmEnum"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TnmEnum.__str__","text":"Source code in edsnlp/pipelines/ner/scores/tnm/models.py 17 18 def __str__ ( self ) -> str : return self . value","title":"__str__()"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix","text":"Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 21 22 23 24 25 26 27 28 29 30 31 class Prefix ( TnmEnum ): clinical = \"c\" histopathology = \"p\" histopathology2 = \"P\" neoadjuvant_therapy = \"y\" recurrent = \"r\" autopsy = \"a\" ultrasonography = \"u\" multifocal = \"m\" py = \"yp\" mp = \"mp\"","title":"Prefix"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.clinical","text":"","title":"clinical"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.histopathology","text":"","title":"histopathology"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.histopathology2","text":"","title":"histopathology2"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.neoadjuvant_therapy","text":"","title":"neoadjuvant_therapy"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.recurrent","text":"","title":"recurrent"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.autopsy","text":"","title":"autopsy"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.ultrasonography","text":"","title":"ultrasonography"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.multifocal","text":"","title":"multifocal"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.py","text":"","title":"py"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Prefix.mp","text":"","title":"mp"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour","text":"Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 34 35 36 37 38 39 40 41 42 class Tumour ( TnmEnum ): unknown = \"x\" in_situ = \"is\" score_0 = \"0\" score_1 = \"1\" score_2 = \"2\" score_3 = \"3\" score_4 = \"4\" o = \"o\"","title":"Tumour"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour.unknown","text":"","title":"unknown"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour.in_situ","text":"","title":"in_situ"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour.score_0","text":"","title":"score_0"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour.score_1","text":"","title":"score_1"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour.score_2","text":"","title":"score_2"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour.score_3","text":"","title":"score_3"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour.score_4","text":"","title":"score_4"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Tumour.o","text":"","title":"o"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Specification","text":"Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 45 46 47 48 49 50 51 class Specification ( TnmEnum ): a = \"a\" b = \"b\" c = \"c\" d = \"d\" mi = \"mi\" x = \"x\"","title":"Specification"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Specification.a","text":"","title":"a"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Specification.b","text":"","title":"b"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Specification.c","text":"","title":"c"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Specification.d","text":"","title":"d"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Specification.mi","text":"","title":"mi"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Specification.x","text":"","title":"x"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Node","text":"Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 54 55 56 57 58 59 60 class Node ( TnmEnum ): unknown = \"x\" score_0 = \"0\" score_1 = \"1\" score_2 = \"2\" score_3 = \"3\" o = \"o\"","title":"Node"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Node.unknown","text":"","title":"unknown"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Node.score_0","text":"","title":"score_0"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Node.score_1","text":"","title":"score_1"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Node.score_2","text":"","title":"score_2"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Node.score_3","text":"","title":"score_3"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Node.o","text":"","title":"o"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Metastasis","text":"Bases: TnmEnum Source code in edsnlp/pipelines/ner/scores/tnm/models.py 63 64 65 66 67 68 69 70 class Metastasis ( TnmEnum ): unknown = \"x\" score_0 = \"0\" score_1 = \"1\" o = \"o\" score_1x = \"1x\" score_2x = \"2x\" ox = \"ox\"","title":"Metastasis"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Metastasis.unknown","text":"","title":"unknown"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Metastasis.score_0","text":"","title":"score_0"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Metastasis.score_1","text":"","title":"score_1"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Metastasis.o","text":"","title":"o"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Metastasis.score_1x","text":"","title":"score_1x"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Metastasis.score_2x","text":"","title":"score_2x"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.Metastasis.ox","text":"","title":"ox"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM","text":"Bases: BaseModel Source code in edsnlp/pipelines/ner/scores/tnm/models.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class TNM ( BaseModel ): prefix : Optional [ Prefix ] = None tumour : Optional [ Tumour ] = None tumour_specification : Optional [ Specification ] = None tumour_suffix : Optional [ str ] = None node : Optional [ Node ] = None node_specification : Optional [ Specification ] = None node_suffix : Optional [ str ] = None metastasis : Optional [ Metastasis ] = None resection_completeness : Optional [ int ] = None version : Optional [ str ] = None version_year : Optional [ int ] = None @validator ( \"*\" , pre = True ) def coerce_o ( cls , v ): if isinstance ( v , str ): v = v . replace ( \"o\" , \"0\" ) return v @validator ( \"version_year\" ) def validate_year ( cls , v ): if v is None : return v if v < 40 : v += 2000 elif v < 100 : v += 1900 return v def norm ( self ) -> str : norm = [] if self . prefix is not None : norm . append ( str ( self . prefix )) if ( ( self . tumour is not None ) | ( self . tumour_specification is not None ) | ( self . tumour_suffix is not None ) ): norm . append ( f \"T { str ( self . tumour or '' ) } \" ) norm . append ( f \" { str ( self . tumour_specification or '' ) } \" ) norm . append ( f \" { str ( self . tumour_suffix or '' ) } \" ) if ( ( self . node is not None ) | ( self . node_specification is not None ) | ( self . node_suffix is not None ) ): norm . append ( f \"N { str ( self . node or '' ) } \" ) norm . append ( f \" { str ( self . node_specification or '' ) } \" ) norm . append ( f \" { str ( self . node_suffix or '' ) } \" ) if self . metastasis is not None : norm . append ( f \"M { self . metastasis } \" ) if self . resection_completeness is not None : norm . append ( f \"R { self . resection_completeness } \" ) if self . version is not None and self . version_year is not None : norm . append ( f \" ( { self . version . upper () } { self . version_year } )\" ) return \"\" . join ( norm ) def dict ( self , * , include : Union [ \"AbstractSetIntStr\" , \"MappingIntStrAny\" ] = None , exclude : Union [ \"AbstractSetIntStr\" , \"MappingIntStrAny\" ] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , ) -> \"DictStrAny\" : \"\"\" Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. \"\"\" if skip_defaults is not None : warnings . warn ( f \"\"\" { self . __class__ . __name__ } .dict(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\" \"\"\" , DeprecationWarning , ) exclude_unset = skip_defaults d = dict ( self . _iter ( to_dict = True , by_alias = by_alias , include = include , exclude = exclude , exclude_unset = exclude_unset , exclude_defaults = exclude_defaults , exclude_none = exclude_none , ) ) set_keys = set ( d . keys ()) for k in set_keys . intersection ( { \"prefix\" , \"tumour\" , \"node\" , \"metastasis\" , \"tumour_specification\" , \"node_specification\" , \"tumour_suffix\" , \"node_suffix\" , } ): v = d [ k ] if isinstance ( v , TnmEnum ): d [ k ] = v . value return d","title":"TNM"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.prefix","text":"","title":"prefix"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.tumour","text":"","title":"tumour"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.tumour_specification","text":"","title":"tumour_specification"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.tumour_suffix","text":"","title":"tumour_suffix"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.node","text":"","title":"node"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.node_specification","text":"","title":"node_specification"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.node_suffix","text":"","title":"node_suffix"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.metastasis","text":"","title":"metastasis"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.resection_completeness","text":"","title":"resection_completeness"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.version","text":"","title":"version"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.version_year","text":"","title":"version_year"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.coerce_o","text":"Source code in edsnlp/pipelines/ner/scores/tnm/models.py 87 88 89 90 91 @validator ( \"*\" , pre = True ) def coerce_o ( cls , v ): if isinstance ( v , str ): v = v . replace ( \"o\" , \"0\" ) return v","title":"coerce_o()"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.validate_year","text":"Source code in edsnlp/pipelines/ner/scores/tnm/models.py 93 94 95 96 97 98 99 100 101 102 103 @validator ( \"version_year\" ) def validate_year ( cls , v ): if v is None : return v if v < 40 : v += 2000 elif v < 100 : v += 1900 return v","title":"validate_year()"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.norm","text":"Source code in edsnlp/pipelines/ner/scores/tnm/models.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def norm ( self ) -> str : norm = [] if self . prefix is not None : norm . append ( str ( self . prefix )) if ( ( self . tumour is not None ) | ( self . tumour_specification is not None ) | ( self . tumour_suffix is not None ) ): norm . append ( f \"T { str ( self . tumour or '' ) } \" ) norm . append ( f \" { str ( self . tumour_specification or '' ) } \" ) norm . append ( f \" { str ( self . tumour_suffix or '' ) } \" ) if ( ( self . node is not None ) | ( self . node_specification is not None ) | ( self . node_suffix is not None ) ): norm . append ( f \"N { str ( self . node or '' ) } \" ) norm . append ( f \" { str ( self . node_specification or '' ) } \" ) norm . append ( f \" { str ( self . node_suffix or '' ) } \" ) if self . metastasis is not None : norm . append ( f \"M { self . metastasis } \" ) if self . resection_completeness is not None : norm . append ( f \"R { self . resection_completeness } \" ) if self . version is not None and self . version_year is not None : norm . append ( f \" ( { self . version . upper () } { self . version_year } )\" ) return \"\" . join ( norm )","title":"norm()"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.dict","text":"Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. Source code in edsnlp/pipelines/ner/scores/tnm/models.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def dict ( self , * , include : Union [ \"AbstractSetIntStr\" , \"MappingIntStrAny\" ] = None , exclude : Union [ \"AbstractSetIntStr\" , \"MappingIntStrAny\" ] = None , by_alias : bool = False , skip_defaults : bool = None , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , ) -> \"DictStrAny\" : \"\"\" Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. \"\"\" if skip_defaults is not None : warnings . warn ( f \"\"\" { self . __class__ . __name__ } .dict(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\" \"\"\" , DeprecationWarning , ) exclude_unset = skip_defaults d = dict ( self . _iter ( to_dict = True , by_alias = by_alias , include = include , exclude = exclude , exclude_unset = exclude_unset , exclude_defaults = exclude_defaults , exclude_none = exclude_none , ) ) set_keys = set ( d . keys ()) for k in set_keys . intersection ( { \"prefix\" , \"tumour\" , \"node\" , \"metastasis\" , \"tumour_specification\" , \"node_specification\" , \"tumour_suffix\" , \"node_suffix\" , } ): v = d [ k ] if isinstance ( v , TnmEnum ): d [ k ] = v . value return d","title":"dict()"},{"location":"reference/pipelines/ner/scores/tnm/patterns/","text":"edsnlp.pipelines.ner.scores.tnm.patterns prefix_pattern = '(?P<prefix>[cpPyraum]p?)' module-attribute tumour_pattern = 'T \\\\ s?(?P<tumour>([0-4o]|is))?(?P<tumour_specification>[abcdx]|mi)?' module-attribute node_pattern = '( \\\\ s* \\\\ /? \\\\ s*([cpPyraum]p?)? \\\\ s*N \\\\ s?(?P<node>[0-3o]|x)' module-attribute metastasis_pattern = '( \\\\ s* \\\\ /? \\\\ s*([cpPyraum]p?)? \\\\ s*M \\\\ s?(?P<metastasis>([01o]|x))x?)?' module-attribute resection_completeness = '( \\\\ s* \\\\ /? \\\\ s*R \\\\ s?(?P<resection_completeness>[012]))?' module-attribute version_pattern = ' \\\\ (?(?P<version>uicc|accj|tnm|UICC|ACCJ|TNM) \\\\ s+([\u00e9eE]ditions|[\u00e9eE]d \\\\ .?)? \\\\ s*(?P<version_year> \\\\ d {4} | \\\\ d {2} ) \\\\ )?' module-attribute spacer = '(.| \\\\ n){1,5}' module-attribute tnm_pattern = '(?: \\\\ b|^)' + tnm_pattern + '(?: \\\\ b|$)' module-attribute","title":"patterns"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlppipelinesnerscorestnmpatterns","text":"","title":"edsnlp.pipelines.ner.scores.tnm.patterns"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlp.pipelines.ner.scores.tnm.patterns.prefix_pattern","text":"","title":"prefix_pattern"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlp.pipelines.ner.scores.tnm.patterns.tumour_pattern","text":"","title":"tumour_pattern"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlp.pipelines.ner.scores.tnm.patterns.node_pattern","text":"","title":"node_pattern"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlp.pipelines.ner.scores.tnm.patterns.metastasis_pattern","text":"","title":"metastasis_pattern"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlp.pipelines.ner.scores.tnm.patterns.resection_completeness","text":"","title":"resection_completeness"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlp.pipelines.ner.scores.tnm.patterns.version_pattern","text":"","title":"version_pattern"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlp.pipelines.ner.scores.tnm.patterns.spacer","text":"","title":"spacer"},{"location":"reference/pipelines/ner/scores/tnm/patterns/#edsnlp.pipelines.ner.scores.tnm.patterns.tnm_pattern","text":"","title":"tnm_pattern"},{"location":"reference/pipelines/ner/scores/tnm/tnm/","text":"edsnlp.pipelines.ner.scores.tnm.tnm eds.tnm pipeline. PERIOD_PROXIMITY_THRESHOLD = 3 module-attribute TNM Bases: BaseComponent Tags and normalizes TNM mentions. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: spacy.language.Language pattern List of regular expressions for TNM mentions. TYPE: Optional[Union[List[str], str]] attr spaCy attribute to use TYPE: str Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class TNM ( BaseComponent ): \"\"\" Tags and normalizes TNM mentions. Parameters ---------- nlp : spacy.language.Language Language pipeline object pattern : Optional[Union[List[str], str]] List of regular expressions for TNM mentions. attr : str spaCy attribute to use \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , pattern : Optional [ Union [ List [ str ], str ]], attr : str , ): self . nlp = nlp if pattern is None : pattern = patterns . tnm_pattern if isinstance ( pattern , str ): pattern = [ pattern ] self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"tnm\" , pattern ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the dates pipeline. \"\"\" if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find TNM mentions in doc. Parameters ---------- doc: spaCy Doc object Returns ------- spans: list of tnm spans \"\"\" spans = self . regex_matcher ( doc , as_spans = True , return_groupdict = True , ) spans = filter_spans ( spans ) return spans def parse ( self , spans : List [ Tuple [ Span , Dict [ str , str ]]]) -> List [ Span ]: \"\"\" Parse dates using the groupdict returned by the matcher. Parameters ---------- spans : List[Tuple[Span, Dict[str, str]]] List of tuples containing the spans and groupdict returned by the matcher. Returns ------- List[Span] List of processed spans, with the date parsed. \"\"\" for span , groupdict in spans : try : span . _ . value = models . TNM . parse_obj ( groupdict ) except ValidationError : span . _ . value = models . TNM . parse_obj ({}) span . kb_id_ = span . _ . value . norm () return [ span for span , _ in spans ] def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags TNM mentions. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for TNM \"\"\" spans = self . process ( doc ) spans = filter_spans ( spans ) spans = self . parse ( spans ) doc . spans [ \"tnm\" ] = spans ents , discarded = filter_spans ( list ( doc . ents ) + spans , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc nlp = nlp instance-attribute regex_matcher = RegexMatcher ( attr = attr , alignment_mode = 'strict' ) instance-attribute __init__ ( nlp , pattern , attr ) Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , nlp : Language , pattern : Optional [ Union [ List [ str ], str ]], attr : str , ): self . nlp = nlp if pattern is None : pattern = patterns . tnm_pattern if isinstance ( pattern , str ): pattern = [ pattern ] self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"tnm\" , pattern ) self . set_extensions () set_extensions () Set extensions for the dates pipeline. Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 52 53 54 55 56 57 58 59 @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the dates pipeline. \"\"\" if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) process ( doc ) Find TNM mentions in doc. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION spans list of tnm spans Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find TNM mentions in doc. Parameters ---------- doc: spaCy Doc object Returns ------- spans: list of tnm spans \"\"\" spans = self . regex_matcher ( doc , as_spans = True , return_groupdict = True , ) spans = filter_spans ( spans ) return spans parse ( spans ) Parse dates using the groupdict returned by the matcher. PARAMETER DESCRIPTION spans List of tuples containing the spans and groupdict returned by the matcher. TYPE: List[Tuple[Span, Dict[str, str]]] RETURNS DESCRIPTION List[Span] List of processed spans, with the date parsed. Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def parse ( self , spans : List [ Tuple [ Span , Dict [ str , str ]]]) -> List [ Span ]: \"\"\" Parse dates using the groupdict returned by the matcher. Parameters ---------- spans : List[Tuple[Span, Dict[str, str]]] List of tuples containing the spans and groupdict returned by the matcher. Returns ------- List[Span] List of processed spans, with the date parsed. \"\"\" for span , groupdict in spans : try : span . _ . value = models . TNM . parse_obj ( groupdict ) except ValidationError : span . _ . value = models . TNM . parse_obj ({}) span . kb_id_ = span . _ . value . norm () return [ span for span , _ in spans ] __call__ ( doc ) Tags TNM mentions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for TNM TYPE: Doc Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags TNM mentions. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for TNM \"\"\" spans = self . process ( doc ) spans = filter_spans ( spans ) spans = self . parse ( spans ) doc . spans [ \"tnm\" ] = spans ents , discarded = filter_spans ( list ( doc . ents ) + spans , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"tnm"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlppipelinesnerscorestnmtnm","text":"eds.tnm pipeline.","title":"edsnlp.pipelines.ner.scores.tnm.tnm"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.PERIOD_PROXIMITY_THRESHOLD","text":"","title":"PERIOD_PROXIMITY_THRESHOLD"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM","text":"Bases: BaseComponent Tags and normalizes TNM mentions. PARAMETER DESCRIPTION nlp Language pipeline object TYPE: spacy.language.Language pattern List of regular expressions for TNM mentions. TYPE: Optional[Union[List[str], str]] attr spaCy attribute to use TYPE: str Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class TNM ( BaseComponent ): \"\"\" Tags and normalizes TNM mentions. Parameters ---------- nlp : spacy.language.Language Language pipeline object pattern : Optional[Union[List[str], str]] List of regular expressions for TNM mentions. attr : str spaCy attribute to use \"\"\" # noinspection PyProtectedMember def __init__ ( self , nlp : Language , pattern : Optional [ Union [ List [ str ], str ]], attr : str , ): self . nlp = nlp if pattern is None : pattern = patterns . tnm_pattern if isinstance ( pattern , str ): pattern = [ pattern ] self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"tnm\" , pattern ) self . set_extensions () @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the dates pipeline. \"\"\" if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None ) def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find TNM mentions in doc. Parameters ---------- doc: spaCy Doc object Returns ------- spans: list of tnm spans \"\"\" spans = self . regex_matcher ( doc , as_spans = True , return_groupdict = True , ) spans = filter_spans ( spans ) return spans def parse ( self , spans : List [ Tuple [ Span , Dict [ str , str ]]]) -> List [ Span ]: \"\"\" Parse dates using the groupdict returned by the matcher. Parameters ---------- spans : List[Tuple[Span, Dict[str, str]]] List of tuples containing the spans and groupdict returned by the matcher. Returns ------- List[Span] List of processed spans, with the date parsed. \"\"\" for span , groupdict in spans : try : span . _ . value = models . TNM . parse_obj ( groupdict ) except ValidationError : span . _ . value = models . TNM . parse_obj ({}) span . kb_id_ = span . _ . value . norm () return [ span for span , _ in spans ] def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags TNM mentions. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for TNM \"\"\" spans = self . process ( doc ) spans = filter_spans ( spans ) spans = self . parse ( spans ) doc . spans [ \"tnm\" ] = spans ents , discarded = filter_spans ( list ( doc . ents ) + spans , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"TNM"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.nlp","text":"","title":"nlp"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.regex_matcher","text":"","title":"regex_matcher"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.__init__","text":"Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , nlp : Language , pattern : Optional [ Union [ List [ str ], str ]], attr : str , ): self . nlp = nlp if pattern is None : pattern = patterns . tnm_pattern if isinstance ( pattern , str ): pattern = [ pattern ] self . regex_matcher = RegexMatcher ( attr = attr , alignment_mode = \"strict\" ) self . regex_matcher . add ( \"tnm\" , pattern ) self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.set_extensions","text":"Set extensions for the dates pipeline. Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 52 53 54 55 56 57 58 59 @classmethod def set_extensions ( cls ) -> None : \"\"\" Set extensions for the dates pipeline. \"\"\" if not Span . has_extension ( \"value\" ): Span . set_extension ( \"value\" , default = None )","title":"set_extensions()"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.process","text":"Find TNM mentions in doc. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION spans list of tnm spans Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def process ( self , doc : Doc ) -> List [ Span ]: \"\"\" Find TNM mentions in doc. Parameters ---------- doc: spaCy Doc object Returns ------- spans: list of tnm spans \"\"\" spans = self . regex_matcher ( doc , as_spans = True , return_groupdict = True , ) spans = filter_spans ( spans ) return spans","title":"process()"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.parse","text":"Parse dates using the groupdict returned by the matcher. PARAMETER DESCRIPTION spans List of tuples containing the spans and groupdict returned by the matcher. TYPE: List[Tuple[Span, Dict[str, str]]] RETURNS DESCRIPTION List[Span] List of processed spans, with the date parsed. Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def parse ( self , spans : List [ Tuple [ Span , Dict [ str , str ]]]) -> List [ Span ]: \"\"\" Parse dates using the groupdict returned by the matcher. Parameters ---------- spans : List[Tuple[Span, Dict[str, str]]] List of tuples containing the spans and groupdict returned by the matcher. Returns ------- List[Span] List of processed spans, with the date parsed. \"\"\" for span , groupdict in spans : try : span . _ . value = models . TNM . parse_obj ( groupdict ) except ValidationError : span . _ . value = models . TNM . parse_obj ({}) span . kb_id_ = span . _ . value . norm () return [ span for span , _ in spans ]","title":"parse()"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.__call__","text":"Tags TNM mentions. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for TNM TYPE: Doc Source code in edsnlp/pipelines/ner/scores/tnm/tnm.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __call__ ( self , doc : Doc ) -> Doc : \"\"\" Tags TNM mentions. Parameters ---------- doc : Doc spaCy Doc object Returns ------- doc : Doc spaCy Doc object, annotated for TNM \"\"\" spans = self . process ( doc ) spans = filter_spans ( spans ) spans = self . parse ( spans ) doc . spans [ \"tnm\" ] = spans ents , discarded = filter_spans ( list ( doc . ents ) + spans , return_discarded = True ) doc . ents = ents if \"discarded\" not in doc . spans : doc . spans [ \"discarded\" ] = [] doc . spans [ \"discarded\" ] . extend ( discarded ) return doc","title":"__call__()"},{"location":"reference/pipelines/ner/umls/","text":"edsnlp.pipelines.ner.umls","title":"`edsnlp.pipelines.ner.umls`"},{"location":"reference/pipelines/ner/umls/#edsnlppipelinesnerumls","text":"","title":"edsnlp.pipelines.ner.umls"},{"location":"reference/pipelines/ner/umls/factory/","text":"edsnlp.pipelines.ner.umls.factory PATTERN_CONFIG = dict ( languages = [ 'FRE' ], sources = None ) module-attribute DEFAULT_CONFIG = dict ( attr = 'NORM' , ignore_excluded = False , term_matcher = TerminologyTermMatcher . exact , term_matcher_config = {}, pattern_config = PATTERN_CONFIG ) module-attribute create_component ( nlp , name , attr , ignore_excluded , term_matcher , term_matcher_config , pattern_config ) Source code in edsnlp/pipelines/ner/umls/factory.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @Language . factory ( \"eds.umls\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ] ) def create_component ( nlp : Language , name : str , attr : Union [ str , Dict [ str , str ]], ignore_excluded : bool , term_matcher : TerminologyTermMatcher , term_matcher_config : Dict [ str , Any ], pattern_config : Dict [ str , Any ], ): return TerminologyMatcher ( nlp , label = \"umls\" , regex = None , terms = patterns . get_patterns ( pattern_config ), attr = attr , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"factory"},{"location":"reference/pipelines/ner/umls/factory/#edsnlppipelinesnerumlsfactory","text":"","title":"edsnlp.pipelines.ner.umls.factory"},{"location":"reference/pipelines/ner/umls/factory/#edsnlp.pipelines.ner.umls.factory.PATTERN_CONFIG","text":"","title":"PATTERN_CONFIG"},{"location":"reference/pipelines/ner/umls/factory/#edsnlp.pipelines.ner.umls.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/ner/umls/factory/#edsnlp.pipelines.ner.umls.factory.create_component","text":"Source code in edsnlp/pipelines/ner/umls/factory.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @Language . factory ( \"eds.umls\" , default_config = DEFAULT_CONFIG , assigns = [ \"doc.ents\" , \"doc.spans\" ] ) def create_component ( nlp : Language , name : str , attr : Union [ str , Dict [ str , str ]], ignore_excluded : bool , term_matcher : TerminologyTermMatcher , term_matcher_config : Dict [ str , Any ], pattern_config : Dict [ str , Any ], ): return TerminologyMatcher ( nlp , label = \"umls\" , regex = None , terms = patterns . get_patterns ( pattern_config ), attr = attr , ignore_excluded = ignore_excluded , term_matcher = term_matcher , term_matcher_config = term_matcher_config , )","title":"create_component()"},{"location":"reference/pipelines/ner/umls/patterns/","text":"edsnlp.pipelines.ner.umls.patterns PATTERN_VERSION = '0.7.0' module-attribute UMLS_VERSION = '2022AA' module-attribute get_patterns ( config ) Load the UMLS terminology patterns. PARAMETER DESCRIPTION config Languages and sources to select from the whole terminology. For both keys, None will select all values. TYPE: dict[list] Return patterns : dict[list] The mapping between CUI codes and their synonyms. Notes When run for the first time, this method will download the entire UMLS file and store it at ~/.data/bio/umls/2022AA/. Therefore the second run will be significantly faster than the first one. Source code in edsnlp/pipelines/ner/umls/patterns.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def get_patterns ( config : Dict [ str , Any ]) -> Dict [ str , List [ str ]]: \"\"\"Load the UMLS terminology patterns. Parameters ---------- config : dict[list] Languages and sources to select from the whole terminology. For both keys, None will select all values. Return ------ patterns : dict[list] The mapping between CUI codes and their synonyms. Notes ----- When run for the first time, this method will download the entire UMLS file and store it at ~/.data/bio/umls/2022AA/. Therefore the second run will be significantly faster than the first one. \"\"\" path , module , filename = get_path ( config ) if path . exists (): print ( f \"Loading { filename } from { module . base } \" ) return module . load_pickle ( name = filename ) else : patterns = download_and_agg_umls ( config ) module . dump_pickle ( name = filename , obj = patterns ) print ( f \"Saved patterns into { module . base / filename } \" ) return patterns get_path ( config ) Get the path, module and filename of the UMLS file. PARAMETER DESCRIPTION config Languages and sources to select from the whole terminology. For both keys, None will select all values. TYPE: dict[list] Return path, module, filename : pathlib.Path, pystow.module, str Notes get_path will convert the config dict into a pretty filename. Examples: >>> config = { \"languages\" : [ \"FRE\" , \"ENG\" ], \"sources\" : None } >>> print ( get_path ( config )) .data/bio/umls/2022AA/languagesFRE-ENG_sourcesNone.pkl\" Source code in edsnlp/pipelines/ner/umls/patterns.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def get_path ( config : Dict [ str , Any ]) -> Tuple [ Path , pystow . impl . Module , str ]: \"\"\"Get the path, module and filename of the UMLS file. Parameters ---------- config : dict[list] Languages and sources to select from the whole terminology. For both keys, None will select all values. Return ------ path, module, filename : pathlib.Path, pystow.module, str Notes ----- `get_path` will convert the config dict into a pretty filename. Examples -------- >>> config = {\"languages\": [\"FRE\", \"ENG\"], \"sources\": None} >>> print(get_path(config)) .data/bio/umls/2022AA/languagesFRE-ENG_sourcesNone.pkl\" \"\"\" config_txt = \"\" for k , v in config . items (): if isinstance ( v , ( list , tuple )): v = \"-\" . join ( v ) _config_txt = f \" { k }{ v } \" if config_txt : _config_txt = f \"_ { _config_txt } \" config_txt += _config_txt filename = f \" { PATTERN_VERSION } _ { config_txt } .pkl\" module = pystow . module ( \"bio\" , \"umls\" , UMLS_VERSION ) path = module . base / filename return path , module , filename download_and_agg_umls ( config ) Download the UMLS if not exist and create a mapping between CUI code and synonyms. PARAMETER DESCRIPTION config Languages and sources to select from the whole terminology. For both keys, None will select all values. TYPE: dict[list] Return patterns : dict[list] The mapping between CUI codes and their synonyms. Notes Performs filtering on the returned mapping only, not the downloaded resource. Source code in edsnlp/pipelines/ner/umls/patterns.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def download_and_agg_umls ( config ) -> Dict [ str , List [ str ]]: \"\"\"Download the UMLS if not exist and create a mapping between CUI code and synonyms. Parameters ---------- config : dict[list] Languages and sources to select from the whole terminology. For both keys, None will select all values. Return ------ patterns : dict[list] The mapping between CUI codes and their synonyms. Notes ----- Performs filtering on the returned mapping only, not the downloaded resource. \"\"\" api_key = os . getenv ( \"UMLS_API_KEY\" ) if not api_key : warnings . warn ( \"You need to define UMLS_API_KEY to download the UMLS. \" \"Get a key by creating an account at \" \"https://uts.nlm.nih.gov/uts/signup-login\" ) path = download_umls ( version = UMLS_VERSION , api_key = api_key ) # https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.T.concept_names_and_sources_file_mr/ # noqa header = [ \"CUI\" , \"LAT\" , \"TS\" , \"LUI\" , \"STT\" , \"SUI\" , \"ISPREF\" , \"AUI\" , \"SAUI\" , \"SCUI\" , \"SDUI\" , \"SAB\" , \"TTY\" , \"CODE\" , \"STR\" , \"STRL\" , \"SUPPRESS\" , ] header_to_idx = dict ( zip ( header , range ( len ( header )))) patterns = defaultdict ( list ) languages = config . get ( \"languages\" ) sources = config . get ( \"sources\" ) with zipfile . ZipFile ( path ) as zip_file : with zip_file . open ( \"MRCONSO.RRF\" , mode = \"r\" ) as file : for row in tqdm ( file . readlines (), desc = \"Loading 'MRCONSO.RRF' into a dictionnary\" ): row = row . decode ( \"utf-8\" ) . split ( \"|\" ) if ( languages is None or row [ header_to_idx [ \"LAT\" ]] in languages ) and ( sources is None or row [ header_to_idx [ \"SAB\" ]] in sources ): cui = row [ header_to_idx [ \"CUI\" ]] synonym = row [ header_to_idx [ \"STR\" ]] patterns [ cui ] . append ( synonym ) return patterns","title":"patterns"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlppipelinesnerumlspatterns","text":"","title":"edsnlp.pipelines.ner.umls.patterns"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.PATTERN_VERSION","text":"","title":"PATTERN_VERSION"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.UMLS_VERSION","text":"","title":"UMLS_VERSION"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_patterns","text":"Load the UMLS terminology patterns. PARAMETER DESCRIPTION config Languages and sources to select from the whole terminology. For both keys, None will select all values. TYPE: dict[list]","title":"get_patterns()"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_patterns--return","text":"patterns : dict[list] The mapping between CUI codes and their synonyms.","title":"Return"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_patterns--notes","text":"When run for the first time, this method will download the entire UMLS file and store it at ~/.data/bio/umls/2022AA/. Therefore the second run will be significantly faster than the first one. Source code in edsnlp/pipelines/ner/umls/patterns.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def get_patterns ( config : Dict [ str , Any ]) -> Dict [ str , List [ str ]]: \"\"\"Load the UMLS terminology patterns. Parameters ---------- config : dict[list] Languages and sources to select from the whole terminology. For both keys, None will select all values. Return ------ patterns : dict[list] The mapping between CUI codes and their synonyms. Notes ----- When run for the first time, this method will download the entire UMLS file and store it at ~/.data/bio/umls/2022AA/. Therefore the second run will be significantly faster than the first one. \"\"\" path , module , filename = get_path ( config ) if path . exists (): print ( f \"Loading { filename } from { module . base } \" ) return module . load_pickle ( name = filename ) else : patterns = download_and_agg_umls ( config ) module . dump_pickle ( name = filename , obj = patterns ) print ( f \"Saved patterns into { module . base / filename } \" ) return patterns","title":"Notes"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_path","text":"Get the path, module and filename of the UMLS file. PARAMETER DESCRIPTION config Languages and sources to select from the whole terminology. For both keys, None will select all values. TYPE: dict[list]","title":"get_path()"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_path--return","text":"path, module, filename : pathlib.Path, pystow.module, str","title":"Return"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_path--notes","text":"get_path will convert the config dict into a pretty filename. Examples: >>> config = { \"languages\" : [ \"FRE\" , \"ENG\" ], \"sources\" : None } >>> print ( get_path ( config )) .data/bio/umls/2022AA/languagesFRE-ENG_sourcesNone.pkl\" Source code in edsnlp/pipelines/ner/umls/patterns.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def get_path ( config : Dict [ str , Any ]) -> Tuple [ Path , pystow . impl . Module , str ]: \"\"\"Get the path, module and filename of the UMLS file. Parameters ---------- config : dict[list] Languages and sources to select from the whole terminology. For both keys, None will select all values. Return ------ path, module, filename : pathlib.Path, pystow.module, str Notes ----- `get_path` will convert the config dict into a pretty filename. Examples -------- >>> config = {\"languages\": [\"FRE\", \"ENG\"], \"sources\": None} >>> print(get_path(config)) .data/bio/umls/2022AA/languagesFRE-ENG_sourcesNone.pkl\" \"\"\" config_txt = \"\" for k , v in config . items (): if isinstance ( v , ( list , tuple )): v = \"-\" . join ( v ) _config_txt = f \" { k }{ v } \" if config_txt : _config_txt = f \"_ { _config_txt } \" config_txt += _config_txt filename = f \" { PATTERN_VERSION } _ { config_txt } .pkl\" module = pystow . module ( \"bio\" , \"umls\" , UMLS_VERSION ) path = module . base / filename return path , module , filename","title":"Notes"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.download_and_agg_umls","text":"Download the UMLS if not exist and create a mapping between CUI code and synonyms. PARAMETER DESCRIPTION config Languages and sources to select from the whole terminology. For both keys, None will select all values. TYPE: dict[list]","title":"download_and_agg_umls()"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.download_and_agg_umls--return","text":"patterns : dict[list] The mapping between CUI codes and their synonyms.","title":"Return"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.download_and_agg_umls--notes","text":"Performs filtering on the returned mapping only, not the downloaded resource. Source code in edsnlp/pipelines/ner/umls/patterns.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def download_and_agg_umls ( config ) -> Dict [ str , List [ str ]]: \"\"\"Download the UMLS if not exist and create a mapping between CUI code and synonyms. Parameters ---------- config : dict[list] Languages and sources to select from the whole terminology. For both keys, None will select all values. Return ------ patterns : dict[list] The mapping between CUI codes and their synonyms. Notes ----- Performs filtering on the returned mapping only, not the downloaded resource. \"\"\" api_key = os . getenv ( \"UMLS_API_KEY\" ) if not api_key : warnings . warn ( \"You need to define UMLS_API_KEY to download the UMLS. \" \"Get a key by creating an account at \" \"https://uts.nlm.nih.gov/uts/signup-login\" ) path = download_umls ( version = UMLS_VERSION , api_key = api_key ) # https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.T.concept_names_and_sources_file_mr/ # noqa header = [ \"CUI\" , \"LAT\" , \"TS\" , \"LUI\" , \"STT\" , \"SUI\" , \"ISPREF\" , \"AUI\" , \"SAUI\" , \"SCUI\" , \"SDUI\" , \"SAB\" , \"TTY\" , \"CODE\" , \"STR\" , \"STRL\" , \"SUPPRESS\" , ] header_to_idx = dict ( zip ( header , range ( len ( header )))) patterns = defaultdict ( list ) languages = config . get ( \"languages\" ) sources = config . get ( \"sources\" ) with zipfile . ZipFile ( path ) as zip_file : with zip_file . open ( \"MRCONSO.RRF\" , mode = \"r\" ) as file : for row in tqdm ( file . readlines (), desc = \"Loading 'MRCONSO.RRF' into a dictionnary\" ): row = row . decode ( \"utf-8\" ) . split ( \"|\" ) if ( languages is None or row [ header_to_idx [ \"LAT\" ]] in languages ) and ( sources is None or row [ header_to_idx [ \"SAB\" ]] in sources ): cui = row [ header_to_idx [ \"CUI\" ]] synonym = row [ header_to_idx [ \"STR\" ]] patterns [ cui ] . append ( synonym ) return patterns","title":"Notes"},{"location":"reference/pipelines/qualifiers/","text":"edsnlp.pipelines.qualifiers","title":"`edsnlp.pipelines.qualifiers`"},{"location":"reference/pipelines/qualifiers/#edsnlppipelinesqualifiers","text":"","title":"edsnlp.pipelines.qualifiers"},{"location":"reference/pipelines/qualifiers/base/","text":"edsnlp.pipelines.qualifiers.base Qualifier Bases: BaseComponent Implements the NegEx algorithm. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool **terms Terms to look for. TYPE: Dict[str, Optional[List[str]]] Source code in edsnlp/pipelines/qualifiers/base.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class Qualifier ( BaseComponent ): \"\"\" Implements the NegEx algorithm. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. explain : bool Whether to keep track of cues for each entity. **terms : Dict[str, Optional[List[str]]] Terms to look for. \"\"\" defaults = dict () def __init__ ( self , nlp : Language , attr : str , on_ents_only : bool , explain : bool , ** terms : Dict [ str , Optional [ List [ str ]]], ): if attr . upper () == \"NORM\" : check_normalizer ( nlp ) self . phrase_matcher = EDSPhraseMatcher ( vocab = nlp . vocab , attr = attr ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . on_ents_only = on_ents_only self . explain = explain def get_defaults ( self , ** kwargs : Dict [ str , Optional [ List [ str ]]] ) -> Dict [ str , List [ str ]]: \"\"\" Merge terms with their defaults. Null keys are replaced with defaults. Returns ------- Dict[str, List[str]] Merged dictionary \"\"\" # Filter out empty keys kwargs = { k : v for k , v in kwargs . items () if v is not None } # Update defaults terms = self . defaults . copy () terms . update ( kwargs ) return terms def get_matches ( self , doc : Doc ) -> List [ Span ]: \"\"\" Extract matches. Parameters ---------- doc : Doc spaCy `Doc` object. Returns ------- List[Span] List of detected spans \"\"\" if self . on_ents_only : sents = set ([ ent . sent for ent in doc . ents ]) match_iterator = map ( lambda sent : self . phrase_matcher ( sent , as_spans = True ), sents ) matches = chain . from_iterable ( match_iterator ) else : matches = self . phrase_matcher ( doc , as_spans = True ) return list ( matches ) def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc ) defaults = dict () class-attribute phrase_matcher = EDSPhraseMatcher ( vocab = nlp . vocab , attr = attr ) instance-attribute on_ents_only = on_ents_only instance-attribute explain = explain instance-attribute __init__ ( nlp , attr , on_ents_only , explain , ** terms ) Source code in edsnlp/pipelines/qualifiers/base.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , nlp : Language , attr : str , on_ents_only : bool , explain : bool , ** terms : Dict [ str , Optional [ List [ str ]]], ): if attr . upper () == \"NORM\" : check_normalizer ( nlp ) self . phrase_matcher = EDSPhraseMatcher ( vocab = nlp . vocab , attr = attr ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . on_ents_only = on_ents_only self . explain = explain get_defaults ( ** kwargs ) Merge terms with their defaults. Null keys are replaced with defaults. RETURNS DESCRIPTION Dict[str, List[str]] Merged dictionary Source code in edsnlp/pipelines/qualifiers/base.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_defaults ( self , ** kwargs : Dict [ str , Optional [ List [ str ]]] ) -> Dict [ str , List [ str ]]: \"\"\" Merge terms with their defaults. Null keys are replaced with defaults. Returns ------- Dict[str, List[str]] Merged dictionary \"\"\" # Filter out empty keys kwargs = { k : v for k , v in kwargs . items () if v is not None } # Update defaults terms = self . defaults . copy () terms . update ( kwargs ) return terms get_matches ( doc ) Extract matches. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION List[Span] List of detected spans Source code in edsnlp/pipelines/qualifiers/base.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_matches ( self , doc : Doc ) -> List [ Span ]: \"\"\" Extract matches. Parameters ---------- doc : Doc spaCy `Doc` object. Returns ------- List[Span] List of detected spans \"\"\" if self . on_ents_only : sents = set ([ ent . sent for ent in doc . ents ]) match_iterator = map ( lambda sent : self . phrase_matcher ( sent , as_spans = True ), sents ) matches = chain . from_iterable ( match_iterator ) else : matches = self . phrase_matcher ( doc , as_spans = True ) return list ( matches ) __call__ ( doc ) Source code in edsnlp/pipelines/qualifiers/base.py 114 115 def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc ) check_normalizer ( nlp ) Source code in edsnlp/pipelines/qualifiers/base.py 12 13 14 15 16 17 18 19 20 21 22 def check_normalizer ( nlp : Language ) -> None : components = { name : component for name , component in nlp . pipeline } normalizer = components . get ( \"normalizer\" ) if normalizer and not normalizer . lowercase : logger . warning ( \"You have chosen the NORM attribute, but disabled lowercasing \" \"in your normalisation pipeline. \" \"This WILL hurt performance : you might want to use the \" \"LOWER attribute instead.\" )","title":"base"},{"location":"reference/pipelines/qualifiers/base/#edsnlppipelinesqualifiersbase","text":"","title":"edsnlp.pipelines.qualifiers.base"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier","text":"Bases: BaseComponent Implements the NegEx algorithm. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool **terms Terms to look for. TYPE: Dict[str, Optional[List[str]]] Source code in edsnlp/pipelines/qualifiers/base.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class Qualifier ( BaseComponent ): \"\"\" Implements the NegEx algorithm. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. explain : bool Whether to keep track of cues for each entity. **terms : Dict[str, Optional[List[str]]] Terms to look for. \"\"\" defaults = dict () def __init__ ( self , nlp : Language , attr : str , on_ents_only : bool , explain : bool , ** terms : Dict [ str , Optional [ List [ str ]]], ): if attr . upper () == \"NORM\" : check_normalizer ( nlp ) self . phrase_matcher = EDSPhraseMatcher ( vocab = nlp . vocab , attr = attr ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . on_ents_only = on_ents_only self . explain = explain def get_defaults ( self , ** kwargs : Dict [ str , Optional [ List [ str ]]] ) -> Dict [ str , List [ str ]]: \"\"\" Merge terms with their defaults. Null keys are replaced with defaults. Returns ------- Dict[str, List[str]] Merged dictionary \"\"\" # Filter out empty keys kwargs = { k : v for k , v in kwargs . items () if v is not None } # Update defaults terms = self . defaults . copy () terms . update ( kwargs ) return terms def get_matches ( self , doc : Doc ) -> List [ Span ]: \"\"\" Extract matches. Parameters ---------- doc : Doc spaCy `Doc` object. Returns ------- List[Span] List of detected spans \"\"\" if self . on_ents_only : sents = set ([ ent . sent for ent in doc . ents ]) match_iterator = map ( lambda sent : self . phrase_matcher ( sent , as_spans = True ), sents ) matches = chain . from_iterable ( match_iterator ) else : matches = self . phrase_matcher ( doc , as_spans = True ) return list ( matches ) def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc )","title":"Qualifier"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.defaults","text":"","title":"defaults"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.phrase_matcher","text":"","title":"phrase_matcher"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.on_ents_only","text":"","title":"on_ents_only"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.explain","text":"","title":"explain"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.__init__","text":"Source code in edsnlp/pipelines/qualifiers/base.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , nlp : Language , attr : str , on_ents_only : bool , explain : bool , ** terms : Dict [ str , Optional [ List [ str ]]], ): if attr . upper () == \"NORM\" : check_normalizer ( nlp ) self . phrase_matcher = EDSPhraseMatcher ( vocab = nlp . vocab , attr = attr ) self . phrase_matcher . build_patterns ( nlp = nlp , terms = terms ) self . on_ents_only = on_ents_only self . explain = explain","title":"__init__()"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.get_defaults","text":"Merge terms with their defaults. Null keys are replaced with defaults. RETURNS DESCRIPTION Dict[str, List[str]] Merged dictionary Source code in edsnlp/pipelines/qualifiers/base.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_defaults ( self , ** kwargs : Dict [ str , Optional [ List [ str ]]] ) -> Dict [ str , List [ str ]]: \"\"\" Merge terms with their defaults. Null keys are replaced with defaults. Returns ------- Dict[str, List[str]] Merged dictionary \"\"\" # Filter out empty keys kwargs = { k : v for k , v in kwargs . items () if v is not None } # Update defaults terms = self . defaults . copy () terms . update ( kwargs ) return terms","title":"get_defaults()"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.get_matches","text":"Extract matches. PARAMETER DESCRIPTION doc spaCy Doc object. TYPE: Doc RETURNS DESCRIPTION List[Span] List of detected spans Source code in edsnlp/pipelines/qualifiers/base.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_matches ( self , doc : Doc ) -> List [ Span ]: \"\"\" Extract matches. Parameters ---------- doc : Doc spaCy `Doc` object. Returns ------- List[Span] List of detected spans \"\"\" if self . on_ents_only : sents = set ([ ent . sent for ent in doc . ents ]) match_iterator = map ( lambda sent : self . phrase_matcher ( sent , as_spans = True ), sents ) matches = chain . from_iterable ( match_iterator ) else : matches = self . phrase_matcher ( doc , as_spans = True ) return list ( matches )","title":"get_matches()"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.__call__","text":"Source code in edsnlp/pipelines/qualifiers/base.py 114 115 def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc )","title":"__call__()"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.check_normalizer","text":"Source code in edsnlp/pipelines/qualifiers/base.py 12 13 14 15 16 17 18 19 20 21 22 def check_normalizer ( nlp : Language ) -> None : components = { name : component for name , component in nlp . pipeline } normalizer = components . get ( \"normalizer\" ) if normalizer and not normalizer . lowercase : logger . warning ( \"You have chosen the NORM attribute, but disabled lowercasing \" \"in your normalisation pipeline. \" \"This WILL hurt performance : you might want to use the \" \"LOWER attribute instead.\" )","title":"check_normalizer()"},{"location":"reference/pipelines/qualifiers/factories/","text":"edsnlp.pipelines.qualifiers.factories","title":"factories"},{"location":"reference/pipelines/qualifiers/factories/#edsnlppipelinesqualifiersfactories","text":"","title":"edsnlp.pipelines.qualifiers.factories"},{"location":"reference/pipelines/qualifiers/family/","text":"edsnlp.pipelines.qualifiers.family","title":"`edsnlp.pipelines.qualifiers.family`"},{"location":"reference/pipelines/qualifiers/family/#edsnlppipelinesqualifiersfamily","text":"","title":"edsnlp.pipelines.qualifiers.family"},{"location":"reference/pipelines/qualifiers/family/factory/","text":"edsnlp.pipelines.qualifiers.family.factory DEFAULT_CONFIG = dict ( family = None , termination = None , attr = 'NORM' , use_sections = False , explain = False , on_ents_only = True ) module-attribute create_component ( nlp , name , family , termination , attr , explain , on_ents_only , use_sections ) Source code in edsnlp/pipelines/qualifiers/family/factory.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @deprecated_factory ( \"family\" , \"eds.family\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.family\" ], ) @Language . factory ( \"eds.family\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.family\" ], ) def create_component ( nlp : Language , name : str , family : Optional [ List [ str ]], termination : Optional [ List [ str ]], attr : str , explain : bool , on_ents_only : bool , use_sections : bool , ): return FamilyContext ( nlp , family = family , termination = termination , attr = attr , explain = explain , on_ents_only = on_ents_only , use_sections = use_sections , )","title":"factory"},{"location":"reference/pipelines/qualifiers/family/factory/#edsnlppipelinesqualifiersfamilyfactory","text":"","title":"edsnlp.pipelines.qualifiers.family.factory"},{"location":"reference/pipelines/qualifiers/family/factory/#edsnlp.pipelines.qualifiers.family.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/qualifiers/family/factory/#edsnlp.pipelines.qualifiers.family.factory.create_component","text":"Source code in edsnlp/pipelines/qualifiers/family/factory.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @deprecated_factory ( \"family\" , \"eds.family\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.family\" ], ) @Language . factory ( \"eds.family\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.family\" ], ) def create_component ( nlp : Language , name : str , family : Optional [ List [ str ]], termination : Optional [ List [ str ]], attr : str , explain : bool , on_ents_only : bool , use_sections : bool , ): return FamilyContext ( nlp , family = family , termination = termination , attr = attr , explain = explain , on_ents_only = on_ents_only , use_sections = use_sections , )","title":"create_component()"},{"location":"reference/pipelines/qualifiers/family/family/","text":"edsnlp.pipelines.qualifiers.family.family FamilyContext Bases: Qualifier Implements a family context detection algorithm. The components looks for terms indicating family references in the text. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language family List of terms indicating family reference. TYPE: Optional[List[str]] terminations List of termination terms, to separate syntagmas. TYPE: Optional[List[str]] attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] explain Whether to keep track of cues for each entity. TYPE: bool use_sections Whether to use annotated sections (namely ant\u00e9c\u00e9dents familiaux ). TYPE: bool, by default Source code in edsnlp/pipelines/qualifiers/family/family.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class FamilyContext ( Qualifier ): \"\"\" Implements a family context detection algorithm. The components looks for terms indicating family references in the text. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. family : Optional[List[str]] List of terms indicating family reference. terminations : Optional[List[str]] List of termination terms, to separate syntagmas. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. explain : bool Whether to keep track of cues for each entity. use_sections : bool, by default `False` Whether to use annotated sections (namely `ant\u00e9c\u00e9dents familiaux`). \"\"\" defaults = dict ( family = family , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , family : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( family = family , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"family\" ): Token . set_extension ( \"family\" , default = False ) if not Token . has_extension ( \"family_\" ): Token . set_extension ( \"family_\" , getter = lambda token : \"FAMILY\" if token . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family\" ): Span . set_extension ( \"family\" , default = False ) if not Span . has_extension ( \"family_\" ): Span . set_extension ( \"family_\" , getter = lambda span : \"FAMILY\" if span . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family_cues\" ): Span . set_extension ( \"family_cues\" , default = []) if not Doc . has_extension ( \"family\" ): Doc . set_extension ( \"family\" , default = []) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to family context. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for context \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"FAMILY\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents familiaux\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"family\" ) cues += sub_sections if not cues : continue family = bool ( cues ) if not family : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . family = True for ent in ents : ent . _ . family = True if self . explain : ent . _ . family_cues += cues if not self . on_ents_only : for token in ent : token . _ . family = True return doc defaults = dict ( family = family , termination = termination ) class-attribute sections = use_sections and 'eds.sections' in nlp . pipe_names or 'sections' in nlp . pipe_names instance-attribute __init__ ( nlp , attr , family , termination , use_sections , explain , on_ents_only ) Source code in edsnlp/pipelines/qualifiers/family/family.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , nlp : Language , attr : str , family : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( family = family , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) set_extensions () Source code in edsnlp/pipelines/qualifiers/family/family.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"family\" ): Token . set_extension ( \"family\" , default = False ) if not Token . has_extension ( \"family_\" ): Token . set_extension ( \"family_\" , getter = lambda token : \"FAMILY\" if token . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family\" ): Span . set_extension ( \"family\" , default = False ) if not Span . has_extension ( \"family_\" ): Span . set_extension ( \"family_\" , getter = lambda span : \"FAMILY\" if span . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family_cues\" ): Span . set_extension ( \"family_cues\" , default = []) if not Doc . has_extension ( \"family\" ): Doc . set_extension ( \"family\" , default = []) process ( doc ) Finds entities related to family context. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/family/family.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to family context. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for context \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"FAMILY\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents familiaux\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"family\" ) cues += sub_sections if not cues : continue family = bool ( cues ) if not family : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . family = True for ent in ents : ent . _ . family = True if self . explain : ent . _ . family_cues += cues if not self . on_ents_only : for token in ent : token . _ . family = True return doc","title":"family"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlppipelinesqualifiersfamilyfamily","text":"","title":"edsnlp.pipelines.qualifiers.family.family"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext","text":"Bases: Qualifier Implements a family context detection algorithm. The components looks for terms indicating family references in the text. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language family List of terms indicating family reference. TYPE: Optional[List[str]] terminations List of termination terms, to separate syntagmas. TYPE: Optional[List[str]] attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] explain Whether to keep track of cues for each entity. TYPE: bool use_sections Whether to use annotated sections (namely ant\u00e9c\u00e9dents familiaux ). TYPE: bool, by default Source code in edsnlp/pipelines/qualifiers/family/family.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class FamilyContext ( Qualifier ): \"\"\" Implements a family context detection algorithm. The components looks for terms indicating family references in the text. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. family : Optional[List[str]] List of terms indicating family reference. terminations : Optional[List[str]] List of termination terms, to separate syntagmas. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. explain : bool Whether to keep track of cues for each entity. use_sections : bool, by default `False` Whether to use annotated sections (namely `ant\u00e9c\u00e9dents familiaux`). \"\"\" defaults = dict ( family = family , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , family : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( family = family , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"family\" ): Token . set_extension ( \"family\" , default = False ) if not Token . has_extension ( \"family_\" ): Token . set_extension ( \"family_\" , getter = lambda token : \"FAMILY\" if token . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family\" ): Span . set_extension ( \"family\" , default = False ) if not Span . has_extension ( \"family_\" ): Span . set_extension ( \"family_\" , getter = lambda span : \"FAMILY\" if span . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family_cues\" ): Span . set_extension ( \"family_cues\" , default = []) if not Doc . has_extension ( \"family\" ): Doc . set_extension ( \"family\" , default = []) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to family context. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for context \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"FAMILY\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents familiaux\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"family\" ) cues += sub_sections if not cues : continue family = bool ( cues ) if not family : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . family = True for ent in ents : ent . _ . family = True if self . explain : ent . _ . family_cues += cues if not self . on_ents_only : for token in ent : token . _ . family = True return doc","title":"FamilyContext"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext.defaults","text":"","title":"defaults"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext.sections","text":"","title":"sections"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext.__init__","text":"Source code in edsnlp/pipelines/qualifiers/family/family.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , nlp : Language , attr : str , family : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( family = family , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" )","title":"__init__()"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext.set_extensions","text":"Source code in edsnlp/pipelines/qualifiers/family/family.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"family\" ): Token . set_extension ( \"family\" , default = False ) if not Token . has_extension ( \"family_\" ): Token . set_extension ( \"family_\" , getter = lambda token : \"FAMILY\" if token . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family\" ): Span . set_extension ( \"family\" , default = False ) if not Span . has_extension ( \"family_\" ): Span . set_extension ( \"family_\" , getter = lambda span : \"FAMILY\" if span . _ . family else \"PATIENT\" , ) if not Span . has_extension ( \"family_cues\" ): Span . set_extension ( \"family_cues\" , default = []) if not Doc . has_extension ( \"family\" ): Doc . set_extension ( \"family\" , default = [])","title":"set_extensions()"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext.process","text":"Finds entities related to family context. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/family/family.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to family context. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for context \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"FAMILY\" ) for section in doc . spans [ \"sections\" ] if section . label_ == \"ant\u00e9c\u00e9dents familiaux\" ] for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) sub_sections , sections = consume_spans ( sections , lambda s : doc [ start ] in s ) if self . on_ents_only and not ents : continue cues = get_spans ( sub_matches , \"family\" ) cues += sub_sections if not cues : continue family = bool ( cues ) if not family : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . family = True for ent in ents : ent . _ . family = True if self . explain : ent . _ . family_cues += cues if not self . on_ents_only : for token in ent : token . _ . family = True return doc","title":"process()"},{"location":"reference/pipelines/qualifiers/family/patterns/","text":"edsnlp.pipelines.qualifiers.family.patterns family : List [ str ] = [ 'a\u00efeul' , 'a\u00efeux' , 'ant\u00e9c\u00e9dent familial' , 'ant\u00e9c\u00e9dents familiaux' , 'arri\u00e8re-grand-m\u00e8re' , 'arri\u00e8re-grand-p\u00e8re' , 'arri\u00e8re-grands-parents' , 'cousin' , 'cousine' , 'cousines' , 'cousins' , 'enfant' , 'enfants' , '\u00e9pouse' , '\u00e9poux' , 'familial' , 'familiale' , 'familiales' , 'familiaux' , 'famille' , 'fianc\u00e9' , 'fianc\u00e9e' , 'fils' , 'fille' , 'filles' , 'fr\u00e8re' , 'fr\u00e8res' , 'grand-m\u00e8re' , 'grand-p\u00e8re' , 'grands-parents' , 'maman' , 'mari' , 'm\u00e8re' , 'oncle' , 'papa' , 'parent' , 'parents' , 'p\u00e8re' , 'soeur' , 's\u0153ur' , 's\u0153urs' , 'soeurs' , 'tante' , 'neveu' , 'neveux' , 'ni\u00e8ce' , 'ni\u00e8ces' ] module-attribute","title":"patterns"},{"location":"reference/pipelines/qualifiers/family/patterns/#edsnlppipelinesqualifiersfamilypatterns","text":"","title":"edsnlp.pipelines.qualifiers.family.patterns"},{"location":"reference/pipelines/qualifiers/family/patterns/#edsnlp.pipelines.qualifiers.family.patterns.family","text":"","title":"family"},{"location":"reference/pipelines/qualifiers/history/","text":"edsnlp.pipelines.qualifiers.history","title":"`edsnlp.pipelines.qualifiers.history`"},{"location":"reference/pipelines/qualifiers/history/#edsnlppipelinesqualifiershistory","text":"","title":"edsnlp.pipelines.qualifiers.history"},{"location":"reference/pipelines/qualifiers/history/factory/","text":"edsnlp.pipelines.qualifiers.history.factory DEFAULT_CONFIG = dict ( attr = 'NORM' , history = patterns . history , termination = termination , use_sections = False , use_dates = False , history_limit = 14 , exclude_birthdate = True , closest_dates_only = True , explain = False , on_ents_only = True ) module-attribute create_component ( nlp , name , history , termination , use_sections , use_dates , history_limit , exclude_birthdate , closest_dates_only , attr , explain , on_ents_only ) Source code in edsnlp/pipelines/qualifiers/history/factory.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @deprecated_factory ( \"antecedents\" , \"eds.history\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.history\" ], ) @deprecated_factory ( \"eds.antecedents\" , \"eds.history\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.history\" ], ) @deprecated_factory ( \"history\" , \"eds.history\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.history\" ], ) @Language . factory ( \"eds.history\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.history\" ], ) def create_component ( nlp : Language , name : str , history : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , use_dates : bool , history_limit : int , exclude_birthdate : bool , closest_dates_only : bool , attr : str , explain : bool , on_ents_only : bool , ): return History ( nlp , attr = attr , history = history , termination = termination , use_sections = use_sections , use_dates = use_dates , history_limit = history_limit , exclude_birthdate = exclude_birthdate , closest_dates_only = closest_dates_only , explain = explain , on_ents_only = on_ents_only , )","title":"factory"},{"location":"reference/pipelines/qualifiers/history/factory/#edsnlppipelinesqualifiershistoryfactory","text":"","title":"edsnlp.pipelines.qualifiers.history.factory"},{"location":"reference/pipelines/qualifiers/history/factory/#edsnlp.pipelines.qualifiers.history.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/qualifiers/history/factory/#edsnlp.pipelines.qualifiers.history.factory.create_component","text":"Source code in edsnlp/pipelines/qualifiers/history/factory.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @deprecated_factory ( \"antecedents\" , \"eds.history\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.history\" ], ) @deprecated_factory ( \"eds.antecedents\" , \"eds.history\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.history\" ], ) @deprecated_factory ( \"history\" , \"eds.history\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.history\" ], ) @Language . factory ( \"eds.history\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.history\" ], ) def create_component ( nlp : Language , name : str , history : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , use_dates : bool , history_limit : int , exclude_birthdate : bool , closest_dates_only : bool , attr : str , explain : bool , on_ents_only : bool , ): return History ( nlp , attr = attr , history = history , termination = termination , use_sections = use_sections , use_dates = use_dates , history_limit = history_limit , exclude_birthdate = exclude_birthdate , closest_dates_only = closest_dates_only , explain = explain , on_ents_only = on_ents_only , )","title":"create_component()"},{"location":"reference/pipelines/qualifiers/history/history/","text":"edsnlp.pipelines.qualifiers.history.history History Bases: Qualifier Implements an history detection algorithm. The components looks for terms indicating history in the text. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language history List of terms indicating medical history reference. TYPE: Optional[List[str]] termination List of syntagme termination terms. TYPE: Optional[List[str]] use_sections Whether to use section pipeline to detect medical history section. TYPE: bool use_dates Whether to use dates pipeline to detect if the event occurs a long time before the document date. TYPE: bool history_limit The number of days after which the event is considered as history. TYPE: int exclude_birthdate Whether to exclude the birth date from history dates. TYPE: bool closest_dates_only Whether to include the closest dates only. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool regex A dictionary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/history/history.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 class History ( Qualifier ): \"\"\" Implements an history detection algorithm. The components looks for terms indicating history in the text. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. history : Optional[List[str]] List of terms indicating medical history reference. termination : Optional[List[str]] List of syntagme termination terms. use_sections : bool Whether to use section pipeline to detect medical history section. use_dates : bool Whether to use dates pipeline to detect if the event occurs a long time before the document date. history_limit : int The number of days after which the event is considered as history. exclude_birthdate : bool Whether to exclude the birth date from history dates. closest_dates_only : bool Whether to include the closest dates only. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. regex : Optional[Dict[str, Union[List[str], str]]] A dictionary of regex patterns. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( history = history , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , history : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , use_dates : bool , history_limit : int , closest_dates_only : bool , exclude_birthdate : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( history = history , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . history_limit = timedelta ( history_limit ) self . exclude_birthdate = exclude_birthdate self . closest_dates_only = closest_dates_only self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . dates = use_dates and ( \"eds.dates\" in nlp . pipe_names or \"dates\" in nlp . pipe_names ) if use_dates : if not self . dates : logger . warning ( \"You have requested that the pipeline use dates \" \"provided by the `dates` pipeline, but it was not set. \" \"Skipping that step.\" ) elif exclude_birthdate : logger . info ( \"You have requested that the pipeline use date \" \"and exclude birth dates. \" \"To make the most of this feature, \" \"make sur you provide the `birth_datetime` \" \"context and `note_datetime` context. \" ) else : logger . info ( \"You have requested that the pipeline use date \" \"To make the most of this feature, \" \"make sure you provide the `note_datetime` \" \"context. \" ) @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"history\" ): Token . set_extension ( \"history\" , default = False ) if not Token . has_extension ( \"antecedents\" ): Token . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Token . has_extension ( \"antecedent\" ): Token . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Token . has_extension ( \"history_\" ): Token . set_extension ( \"history_\" , getter = lambda token : \"ATCD\" if token . _ . history else \"CURRENT\" , ) if not Token . has_extension ( \"antecedents_\" ): Token . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Token . has_extension ( \"antecedent_\" ): Token . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) if not Span . has_extension ( \"history\" ): Span . set_extension ( \"history\" , default = False ) if not Span . has_extension ( \"antecedents\" ): Span . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Span . has_extension ( \"antecedent\" ): Span . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Span . has_extension ( \"history_\" ): Span . set_extension ( \"history_\" , getter = lambda span : \"ATCD\" if span . _ . history else \"CURRENT\" , ) if not Span . has_extension ( \"antecedents_\" ): Span . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Span . has_extension ( \"antecedent_\" ): Span . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) # Store history mentions responsible for the history entity's character if not Span . has_extension ( \"history_cues\" ): Span . set_extension ( \"history_cues\" , default = []) # Store recent mentions responsible for the non-antecedent entity's character if not Span . has_extension ( \"recent_cues\" ): Span . set_extension ( \"recent_cues\" , default = []) if not Span . has_extension ( \"antecedents_cues\" ): Span . set_extension ( \"antecedents_cues\" , getter = deprecated_getter_factory ( \"antecedents_cues\" , \"history_cues\" ), ) if not Span . has_extension ( \"antecedent_cues\" ): Span . set_extension ( \"antecedent_cues\" , getter = deprecated_getter_factory ( \"antecedent_cues\" , \"history_cues\" ), ) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to history. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for history \"\"\" try : note_datetime = pendulum . instance ( doc . _ . note_datetime ) note_datetime = note_datetime . set ( tz = \"Europe/Paris\" ) except ValueError : logger . debug ( \"Note date time must be datetime objects. Skkipping absolute value\" ) note_datetime = None try : birth_datetime = pendulum . instance ( doc . _ . birth_datetime ) birth_datetime = birth_datetime . set ( tz = \"Europe/Paris\" ) except ValueError : logger . debug ( \"Birth date time must be datetime objects. Skkipping exclude birth date\" ) birth_datetime = None matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sub_sections = None sub_recent_dates = None sub_history_dates = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"ATCD\" ) for section in doc . spans [ \"sections\" ] if section . label_ in sections_history ] history_dates = [] recent_dates = [] if self . dates : for date in doc . spans [ \"dates\" ]: if date . label_ == \"relative\" : if date . _ . date . direction . value == \"CURRENT\" : if ( ( date . _ . date . year == 0 and self . history_limit >= timedelta ( 365 ) ) or ( date . _ . date . month == 0 and self . history_limit >= timedelta ( 30 ) ) or ( date . _ . date . week == 0 and self . history_limit >= timedelta ( 7 ) ) or ( date . _ . date . day == 0 ) ): recent_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) elif date . _ . date . direction . value == \"PAST\" : if - date . _ . date . to_datetime () >= self . history_limit : history_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) else : recent_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) elif date . label_ == \"absolute\" and doc . _ . note_datetime : try : absolute_date = date . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = True , tz = \"Europe/Paris\" , default_day = 15 , ) except ValueError as e : absolute_date = None logger . warning ( \"In doc {} , the following date {} raises this error: {} . \" \"Skipping this date.\" , doc . _ . note_id , date . _ . date , e , ) if absolute_date : if note_datetime . diff ( absolute_date ) < self . history_limit : recent_dates . append ( Span ( doc , date . start , date . end , label = \"absolute_date\" ) ) elif not ( self . exclude_birthdate and birth_datetime and absolute_date == birth_datetime ): history_dates . append ( Span ( doc , date . start , date . end , label = \"absolute_date\" ) ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . sections : sub_sections , sections = consume_spans ( sections , lambda s : s . start < end <= s . end , sub_sections ) if self . dates : sub_recent_dates , recent_dates = consume_spans ( recent_dates , lambda s : check_sent_inclusion ( s , start , end ), sub_recent_dates , ) sub_history_dates , history_dates = consume_spans ( history_dates , lambda s : check_sent_inclusion ( s , start , end ), sub_history_dates , ) # Filter dates inside the boundaries only if self . closest_dates_only : close_recent_dates = [] close_history_dates = [] if sub_recent_dates : close_recent_dates = [ recent_date for recent_date in sub_recent_dates if check_inclusion ( recent_date , start , end ) ] if sub_history_dates : close_history_dates = [ history_date for history_date in sub_history_dates if check_inclusion ( history_date , start , end ) ] # If no date inside the boundaries, get the closest if not close_recent_dates and not close_history_dates : min_distance_recent_date = min ( [ abs ( sub_recent_date . start - start ) for sub_recent_date in sub_recent_dates ] ) min_distance_history_date = min ( [ abs ( sub_history_date . start - start ) for sub_history_date in sub_history_dates ] ) if min_distance_recent_date < min_distance_history_date : close_recent_dates = [ min ( sub_recent_dates , key = lambda x : abs ( x . start - start ), ) ] else : close_history_dates = [ min ( sub_history_dates , key = lambda x : abs ( x . start - start ), ) ] elif not close_recent_dates : close_recent_dates = [ min ( sub_recent_dates , key = lambda x : abs ( x . start - start ), ) ] elif sub_history_dates : close_history_dates = [ history_date for history_date in sub_history_dates if check_inclusion ( history_date , start , end ) ] # If no date inside the boundaries, get the closest if not close_history_dates : close_history_dates = [ min ( sub_history_dates , key = lambda x : abs ( x . start - start ), ) ] if self . on_ents_only and not ents : continue history_cues = get_spans ( sub_matches , \"history\" ) recent_cues = [] if self . sections : history_cues . extend ( sub_sections ) if self . dates : history_cues . extend ( close_history_dates if self . closest_dates_only else sub_history_dates ) recent_cues . extend ( close_recent_dates if self . closest_dates_only else sub_recent_dates ) history = bool ( history_cues ) and not bool ( recent_cues ) if not self . on_ents_only : for token in doc [ start : end ]: token . _ . history = history for ent in ents : ent . _ . history = ent . _ . history or history if self . explain : ent . _ . history_cues += history_cues ent . _ . recent_cues += recent_cues if not self . on_ents_only and ent . _ . history : for token in ent : token . _ . history = True return doc defaults = dict ( history = history , termination = termination ) class-attribute history_limit = timedelta ( history_limit ) instance-attribute exclude_birthdate = exclude_birthdate instance-attribute closest_dates_only = closest_dates_only instance-attribute sections = use_sections and 'eds.sections' in nlp . pipe_names or 'sections' in nlp . pipe_names instance-attribute dates = use_dates and 'eds.dates' in nlp . pipe_names or 'dates' in nlp . pipe_names instance-attribute __init__ ( nlp , attr , history , termination , use_sections , use_dates , history_limit , closest_dates_only , exclude_birthdate , explain , on_ents_only ) Source code in edsnlp/pipelines/qualifiers/history/history.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , nlp : Language , attr : str , history : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , use_dates : bool , history_limit : int , closest_dates_only : bool , exclude_birthdate : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( history = history , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . history_limit = timedelta ( history_limit ) self . exclude_birthdate = exclude_birthdate self . closest_dates_only = closest_dates_only self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . dates = use_dates and ( \"eds.dates\" in nlp . pipe_names or \"dates\" in nlp . pipe_names ) if use_dates : if not self . dates : logger . warning ( \"You have requested that the pipeline use dates \" \"provided by the `dates` pipeline, but it was not set. \" \"Skipping that step.\" ) elif exclude_birthdate : logger . info ( \"You have requested that the pipeline use date \" \"and exclude birth dates. \" \"To make the most of this feature, \" \"make sur you provide the `birth_datetime` \" \"context and `note_datetime` context. \" ) else : logger . info ( \"You have requested that the pipeline use date \" \"To make the most of this feature, \" \"make sure you provide the `note_datetime` \" \"context. \" ) set_extensions () Source code in edsnlp/pipelines/qualifiers/history/history.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"history\" ): Token . set_extension ( \"history\" , default = False ) if not Token . has_extension ( \"antecedents\" ): Token . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Token . has_extension ( \"antecedent\" ): Token . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Token . has_extension ( \"history_\" ): Token . set_extension ( \"history_\" , getter = lambda token : \"ATCD\" if token . _ . history else \"CURRENT\" , ) if not Token . has_extension ( \"antecedents_\" ): Token . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Token . has_extension ( \"antecedent_\" ): Token . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) if not Span . has_extension ( \"history\" ): Span . set_extension ( \"history\" , default = False ) if not Span . has_extension ( \"antecedents\" ): Span . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Span . has_extension ( \"antecedent\" ): Span . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Span . has_extension ( \"history_\" ): Span . set_extension ( \"history_\" , getter = lambda span : \"ATCD\" if span . _ . history else \"CURRENT\" , ) if not Span . has_extension ( \"antecedents_\" ): Span . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Span . has_extension ( \"antecedent_\" ): Span . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) # Store history mentions responsible for the history entity's character if not Span . has_extension ( \"history_cues\" ): Span . set_extension ( \"history_cues\" , default = []) # Store recent mentions responsible for the non-antecedent entity's character if not Span . has_extension ( \"recent_cues\" ): Span . set_extension ( \"recent_cues\" , default = []) if not Span . has_extension ( \"antecedents_cues\" ): Span . set_extension ( \"antecedents_cues\" , getter = deprecated_getter_factory ( \"antecedents_cues\" , \"history_cues\" ), ) if not Span . has_extension ( \"antecedent_cues\" ): Span . set_extension ( \"antecedent_cues\" , getter = deprecated_getter_factory ( \"antecedent_cues\" , \"history_cues\" ), ) process ( doc ) Finds entities related to history. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for history Source code in edsnlp/pipelines/qualifiers/history/history.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to history. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for history \"\"\" try : note_datetime = pendulum . instance ( doc . _ . note_datetime ) note_datetime = note_datetime . set ( tz = \"Europe/Paris\" ) except ValueError : logger . debug ( \"Note date time must be datetime objects. Skkipping absolute value\" ) note_datetime = None try : birth_datetime = pendulum . instance ( doc . _ . birth_datetime ) birth_datetime = birth_datetime . set ( tz = \"Europe/Paris\" ) except ValueError : logger . debug ( \"Birth date time must be datetime objects. Skkipping exclude birth date\" ) birth_datetime = None matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sub_sections = None sub_recent_dates = None sub_history_dates = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"ATCD\" ) for section in doc . spans [ \"sections\" ] if section . label_ in sections_history ] history_dates = [] recent_dates = [] if self . dates : for date in doc . spans [ \"dates\" ]: if date . label_ == \"relative\" : if date . _ . date . direction . value == \"CURRENT\" : if ( ( date . _ . date . year == 0 and self . history_limit >= timedelta ( 365 ) ) or ( date . _ . date . month == 0 and self . history_limit >= timedelta ( 30 ) ) or ( date . _ . date . week == 0 and self . history_limit >= timedelta ( 7 ) ) or ( date . _ . date . day == 0 ) ): recent_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) elif date . _ . date . direction . value == \"PAST\" : if - date . _ . date . to_datetime () >= self . history_limit : history_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) else : recent_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) elif date . label_ == \"absolute\" and doc . _ . note_datetime : try : absolute_date = date . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = True , tz = \"Europe/Paris\" , default_day = 15 , ) except ValueError as e : absolute_date = None logger . warning ( \"In doc {} , the following date {} raises this error: {} . \" \"Skipping this date.\" , doc . _ . note_id , date . _ . date , e , ) if absolute_date : if note_datetime . diff ( absolute_date ) < self . history_limit : recent_dates . append ( Span ( doc , date . start , date . end , label = \"absolute_date\" ) ) elif not ( self . exclude_birthdate and birth_datetime and absolute_date == birth_datetime ): history_dates . append ( Span ( doc , date . start , date . end , label = \"absolute_date\" ) ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . sections : sub_sections , sections = consume_spans ( sections , lambda s : s . start < end <= s . end , sub_sections ) if self . dates : sub_recent_dates , recent_dates = consume_spans ( recent_dates , lambda s : check_sent_inclusion ( s , start , end ), sub_recent_dates , ) sub_history_dates , history_dates = consume_spans ( history_dates , lambda s : check_sent_inclusion ( s , start , end ), sub_history_dates , ) # Filter dates inside the boundaries only if self . closest_dates_only : close_recent_dates = [] close_history_dates = [] if sub_recent_dates : close_recent_dates = [ recent_date for recent_date in sub_recent_dates if check_inclusion ( recent_date , start , end ) ] if sub_history_dates : close_history_dates = [ history_date for history_date in sub_history_dates if check_inclusion ( history_date , start , end ) ] # If no date inside the boundaries, get the closest if not close_recent_dates and not close_history_dates : min_distance_recent_date = min ( [ abs ( sub_recent_date . start - start ) for sub_recent_date in sub_recent_dates ] ) min_distance_history_date = min ( [ abs ( sub_history_date . start - start ) for sub_history_date in sub_history_dates ] ) if min_distance_recent_date < min_distance_history_date : close_recent_dates = [ min ( sub_recent_dates , key = lambda x : abs ( x . start - start ), ) ] else : close_history_dates = [ min ( sub_history_dates , key = lambda x : abs ( x . start - start ), ) ] elif not close_recent_dates : close_recent_dates = [ min ( sub_recent_dates , key = lambda x : abs ( x . start - start ), ) ] elif sub_history_dates : close_history_dates = [ history_date for history_date in sub_history_dates if check_inclusion ( history_date , start , end ) ] # If no date inside the boundaries, get the closest if not close_history_dates : close_history_dates = [ min ( sub_history_dates , key = lambda x : abs ( x . start - start ), ) ] if self . on_ents_only and not ents : continue history_cues = get_spans ( sub_matches , \"history\" ) recent_cues = [] if self . sections : history_cues . extend ( sub_sections ) if self . dates : history_cues . extend ( close_history_dates if self . closest_dates_only else sub_history_dates ) recent_cues . extend ( close_recent_dates if self . closest_dates_only else sub_recent_dates ) history = bool ( history_cues ) and not bool ( recent_cues ) if not self . on_ents_only : for token in doc [ start : end ]: token . _ . history = history for ent in ents : ent . _ . history = ent . _ . history or history if self . explain : ent . _ . history_cues += history_cues ent . _ . recent_cues += recent_cues if not self . on_ents_only and ent . _ . history : for token in ent : token . _ . history = True return doc","title":"history"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlppipelinesqualifiershistoryhistory","text":"","title":"edsnlp.pipelines.qualifiers.history.history"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History","text":"Bases: Qualifier Implements an history detection algorithm. The components looks for terms indicating history in the text. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language history List of terms indicating medical history reference. TYPE: Optional[List[str]] termination List of syntagme termination terms. TYPE: Optional[List[str]] use_sections Whether to use section pipeline to detect medical history section. TYPE: bool use_dates Whether to use dates pipeline to detect if the event occurs a long time before the document date. TYPE: bool history_limit The number of days after which the event is considered as history. TYPE: int exclude_birthdate Whether to exclude the birth date from history dates. TYPE: bool closest_dates_only Whether to include the closest dates only. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool regex A dictionary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/history/history.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 class History ( Qualifier ): \"\"\" Implements an history detection algorithm. The components looks for terms indicating history in the text. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. history : Optional[List[str]] List of terms indicating medical history reference. termination : Optional[List[str]] List of syntagme termination terms. use_sections : bool Whether to use section pipeline to detect medical history section. use_dates : bool Whether to use dates pipeline to detect if the event occurs a long time before the document date. history_limit : int The number of days after which the event is considered as history. exclude_birthdate : bool Whether to exclude the birth date from history dates. closest_dates_only : bool Whether to include the closest dates only. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. regex : Optional[Dict[str, Union[List[str], str]]] A dictionary of regex patterns. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( history = history , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , history : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , use_dates : bool , history_limit : int , closest_dates_only : bool , exclude_birthdate : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( history = history , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . history_limit = timedelta ( history_limit ) self . exclude_birthdate = exclude_birthdate self . closest_dates_only = closest_dates_only self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . dates = use_dates and ( \"eds.dates\" in nlp . pipe_names or \"dates\" in nlp . pipe_names ) if use_dates : if not self . dates : logger . warning ( \"You have requested that the pipeline use dates \" \"provided by the `dates` pipeline, but it was not set. \" \"Skipping that step.\" ) elif exclude_birthdate : logger . info ( \"You have requested that the pipeline use date \" \"and exclude birth dates. \" \"To make the most of this feature, \" \"make sur you provide the `birth_datetime` \" \"context and `note_datetime` context. \" ) else : logger . info ( \"You have requested that the pipeline use date \" \"To make the most of this feature, \" \"make sure you provide the `note_datetime` \" \"context. \" ) @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"history\" ): Token . set_extension ( \"history\" , default = False ) if not Token . has_extension ( \"antecedents\" ): Token . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Token . has_extension ( \"antecedent\" ): Token . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Token . has_extension ( \"history_\" ): Token . set_extension ( \"history_\" , getter = lambda token : \"ATCD\" if token . _ . history else \"CURRENT\" , ) if not Token . has_extension ( \"antecedents_\" ): Token . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Token . has_extension ( \"antecedent_\" ): Token . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) if not Span . has_extension ( \"history\" ): Span . set_extension ( \"history\" , default = False ) if not Span . has_extension ( \"antecedents\" ): Span . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Span . has_extension ( \"antecedent\" ): Span . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Span . has_extension ( \"history_\" ): Span . set_extension ( \"history_\" , getter = lambda span : \"ATCD\" if span . _ . history else \"CURRENT\" , ) if not Span . has_extension ( \"antecedents_\" ): Span . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Span . has_extension ( \"antecedent_\" ): Span . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) # Store history mentions responsible for the history entity's character if not Span . has_extension ( \"history_cues\" ): Span . set_extension ( \"history_cues\" , default = []) # Store recent mentions responsible for the non-antecedent entity's character if not Span . has_extension ( \"recent_cues\" ): Span . set_extension ( \"recent_cues\" , default = []) if not Span . has_extension ( \"antecedents_cues\" ): Span . set_extension ( \"antecedents_cues\" , getter = deprecated_getter_factory ( \"antecedents_cues\" , \"history_cues\" ), ) if not Span . has_extension ( \"antecedent_cues\" ): Span . set_extension ( \"antecedent_cues\" , getter = deprecated_getter_factory ( \"antecedent_cues\" , \"history_cues\" ), ) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to history. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for history \"\"\" try : note_datetime = pendulum . instance ( doc . _ . note_datetime ) note_datetime = note_datetime . set ( tz = \"Europe/Paris\" ) except ValueError : logger . debug ( \"Note date time must be datetime objects. Skkipping absolute value\" ) note_datetime = None try : birth_datetime = pendulum . instance ( doc . _ . birth_datetime ) birth_datetime = birth_datetime . set ( tz = \"Europe/Paris\" ) except ValueError : logger . debug ( \"Birth date time must be datetime objects. Skkipping exclude birth date\" ) birth_datetime = None matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sub_sections = None sub_recent_dates = None sub_history_dates = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"ATCD\" ) for section in doc . spans [ \"sections\" ] if section . label_ in sections_history ] history_dates = [] recent_dates = [] if self . dates : for date in doc . spans [ \"dates\" ]: if date . label_ == \"relative\" : if date . _ . date . direction . value == \"CURRENT\" : if ( ( date . _ . date . year == 0 and self . history_limit >= timedelta ( 365 ) ) or ( date . _ . date . month == 0 and self . history_limit >= timedelta ( 30 ) ) or ( date . _ . date . week == 0 and self . history_limit >= timedelta ( 7 ) ) or ( date . _ . date . day == 0 ) ): recent_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) elif date . _ . date . direction . value == \"PAST\" : if - date . _ . date . to_datetime () >= self . history_limit : history_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) else : recent_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) elif date . label_ == \"absolute\" and doc . _ . note_datetime : try : absolute_date = date . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = True , tz = \"Europe/Paris\" , default_day = 15 , ) except ValueError as e : absolute_date = None logger . warning ( \"In doc {} , the following date {} raises this error: {} . \" \"Skipping this date.\" , doc . _ . note_id , date . _ . date , e , ) if absolute_date : if note_datetime . diff ( absolute_date ) < self . history_limit : recent_dates . append ( Span ( doc , date . start , date . end , label = \"absolute_date\" ) ) elif not ( self . exclude_birthdate and birth_datetime and absolute_date == birth_datetime ): history_dates . append ( Span ( doc , date . start , date . end , label = \"absolute_date\" ) ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . sections : sub_sections , sections = consume_spans ( sections , lambda s : s . start < end <= s . end , sub_sections ) if self . dates : sub_recent_dates , recent_dates = consume_spans ( recent_dates , lambda s : check_sent_inclusion ( s , start , end ), sub_recent_dates , ) sub_history_dates , history_dates = consume_spans ( history_dates , lambda s : check_sent_inclusion ( s , start , end ), sub_history_dates , ) # Filter dates inside the boundaries only if self . closest_dates_only : close_recent_dates = [] close_history_dates = [] if sub_recent_dates : close_recent_dates = [ recent_date for recent_date in sub_recent_dates if check_inclusion ( recent_date , start , end ) ] if sub_history_dates : close_history_dates = [ history_date for history_date in sub_history_dates if check_inclusion ( history_date , start , end ) ] # If no date inside the boundaries, get the closest if not close_recent_dates and not close_history_dates : min_distance_recent_date = min ( [ abs ( sub_recent_date . start - start ) for sub_recent_date in sub_recent_dates ] ) min_distance_history_date = min ( [ abs ( sub_history_date . start - start ) for sub_history_date in sub_history_dates ] ) if min_distance_recent_date < min_distance_history_date : close_recent_dates = [ min ( sub_recent_dates , key = lambda x : abs ( x . start - start ), ) ] else : close_history_dates = [ min ( sub_history_dates , key = lambda x : abs ( x . start - start ), ) ] elif not close_recent_dates : close_recent_dates = [ min ( sub_recent_dates , key = lambda x : abs ( x . start - start ), ) ] elif sub_history_dates : close_history_dates = [ history_date for history_date in sub_history_dates if check_inclusion ( history_date , start , end ) ] # If no date inside the boundaries, get the closest if not close_history_dates : close_history_dates = [ min ( sub_history_dates , key = lambda x : abs ( x . start - start ), ) ] if self . on_ents_only and not ents : continue history_cues = get_spans ( sub_matches , \"history\" ) recent_cues = [] if self . sections : history_cues . extend ( sub_sections ) if self . dates : history_cues . extend ( close_history_dates if self . closest_dates_only else sub_history_dates ) recent_cues . extend ( close_recent_dates if self . closest_dates_only else sub_recent_dates ) history = bool ( history_cues ) and not bool ( recent_cues ) if not self . on_ents_only : for token in doc [ start : end ]: token . _ . history = history for ent in ents : ent . _ . history = ent . _ . history or history if self . explain : ent . _ . history_cues += history_cues ent . _ . recent_cues += recent_cues if not self . on_ents_only and ent . _ . history : for token in ent : token . _ . history = True return doc","title":"History"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.defaults","text":"","title":"defaults"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.history_limit","text":"","title":"history_limit"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.exclude_birthdate","text":"","title":"exclude_birthdate"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.closest_dates_only","text":"","title":"closest_dates_only"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.sections","text":"","title":"sections"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.dates","text":"","title":"dates"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.__init__","text":"Source code in edsnlp/pipelines/qualifiers/history/history.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , nlp : Language , attr : str , history : Optional [ List [ str ]], termination : Optional [ List [ str ]], use_sections : bool , use_dates : bool , history_limit : int , closest_dates_only : bool , exclude_birthdate : bool , explain : bool , on_ents_only : bool , ): terms = self . get_defaults ( history = history , termination = termination , ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . set_extensions () self . history_limit = timedelta ( history_limit ) self . exclude_birthdate = exclude_birthdate self . closest_dates_only = closest_dates_only self . sections = use_sections and ( \"eds.sections\" in nlp . pipe_names or \"sections\" in nlp . pipe_names ) if use_sections and not self . sections : logger . warning ( \"You have requested that the pipeline use annotations \" \"provided by the `section` pipeline, but it was not set. \" \"Skipping that step.\" ) self . dates = use_dates and ( \"eds.dates\" in nlp . pipe_names or \"dates\" in nlp . pipe_names ) if use_dates : if not self . dates : logger . warning ( \"You have requested that the pipeline use dates \" \"provided by the `dates` pipeline, but it was not set. \" \"Skipping that step.\" ) elif exclude_birthdate : logger . info ( \"You have requested that the pipeline use date \" \"and exclude birth dates. \" \"To make the most of this feature, \" \"make sur you provide the `birth_datetime` \" \"context and `note_datetime` context. \" ) else : logger . info ( \"You have requested that the pipeline use date \" \"To make the most of this feature, \" \"make sure you provide the `note_datetime` \" \"context. \" )","title":"__init__()"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.set_extensions","text":"Source code in edsnlp/pipelines/qualifiers/history/history.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"history\" ): Token . set_extension ( \"history\" , default = False ) if not Token . has_extension ( \"antecedents\" ): Token . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Token . has_extension ( \"antecedent\" ): Token . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Token . has_extension ( \"history_\" ): Token . set_extension ( \"history_\" , getter = lambda token : \"ATCD\" if token . _ . history else \"CURRENT\" , ) if not Token . has_extension ( \"antecedents_\" ): Token . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Token . has_extension ( \"antecedent_\" ): Token . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) if not Span . has_extension ( \"history\" ): Span . set_extension ( \"history\" , default = False ) if not Span . has_extension ( \"antecedents\" ): Span . set_extension ( \"antecedents\" , getter = deprecated_getter_factory ( \"antecedents\" , \"history\" ), ) if not Span . has_extension ( \"antecedent\" ): Span . set_extension ( \"antecedent\" , getter = deprecated_getter_factory ( \"antecedent\" , \"history\" ), ) if not Span . has_extension ( \"history_\" ): Span . set_extension ( \"history_\" , getter = lambda span : \"ATCD\" if span . _ . history else \"CURRENT\" , ) if not Span . has_extension ( \"antecedents_\" ): Span . set_extension ( \"antecedents_\" , getter = deprecated_getter_factory ( \"antecedents_\" , \"history_\" ), ) if not Span . has_extension ( \"antecedent_\" ): Span . set_extension ( \"antecedent_\" , getter = deprecated_getter_factory ( \"antecedent_\" , \"history_\" ), ) # Store history mentions responsible for the history entity's character if not Span . has_extension ( \"history_cues\" ): Span . set_extension ( \"history_cues\" , default = []) # Store recent mentions responsible for the non-antecedent entity's character if not Span . has_extension ( \"recent_cues\" ): Span . set_extension ( \"recent_cues\" , default = []) if not Span . has_extension ( \"antecedents_cues\" ): Span . set_extension ( \"antecedents_cues\" , getter = deprecated_getter_factory ( \"antecedents_cues\" , \"history_cues\" ), ) if not Span . has_extension ( \"antecedent_cues\" ): Span . set_extension ( \"antecedent_cues\" , getter = deprecated_getter_factory ( \"antecedent_cues\" , \"history_cues\" ), )","title":"set_extensions()"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.process","text":"Finds entities related to history. PARAMETER DESCRIPTION doc spaCy Doc object TYPE: Doc RETURNS DESCRIPTION doc spaCy Doc object, annotated for history Source code in edsnlp/pipelines/qualifiers/history/history.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to history. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for history \"\"\" try : note_datetime = pendulum . instance ( doc . _ . note_datetime ) note_datetime = note_datetime . set ( tz = \"Europe/Paris\" ) except ValueError : logger . debug ( \"Note date time must be datetime objects. Skkipping absolute value\" ) note_datetime = None try : birth_datetime = pendulum . instance ( doc . _ . birth_datetime ) birth_datetime = birth_datetime . set ( tz = \"Europe/Paris\" ) except ValueError : logger . debug ( \"Birth date time must be datetime objects. Skkipping exclude birth date\" ) birth_datetime = None matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None sub_sections = None sub_recent_dates = None sub_history_dates = None sections = [] if self . sections : sections = [ Span ( doc , section . start , section . end , label = \"ATCD\" ) for section in doc . spans [ \"sections\" ] if section . label_ in sections_history ] history_dates = [] recent_dates = [] if self . dates : for date in doc . spans [ \"dates\" ]: if date . label_ == \"relative\" : if date . _ . date . direction . value == \"CURRENT\" : if ( ( date . _ . date . year == 0 and self . history_limit >= timedelta ( 365 ) ) or ( date . _ . date . month == 0 and self . history_limit >= timedelta ( 30 ) ) or ( date . _ . date . week == 0 and self . history_limit >= timedelta ( 7 ) ) or ( date . _ . date . day == 0 ) ): recent_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) elif date . _ . date . direction . value == \"PAST\" : if - date . _ . date . to_datetime () >= self . history_limit : history_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) else : recent_dates . append ( Span ( doc , date . start , date . end , label = \"relative_date\" ) ) elif date . label_ == \"absolute\" and doc . _ . note_datetime : try : absolute_date = date . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = True , tz = \"Europe/Paris\" , default_day = 15 , ) except ValueError as e : absolute_date = None logger . warning ( \"In doc {} , the following date {} raises this error: {} . \" \"Skipping this date.\" , doc . _ . note_id , date . _ . date , e , ) if absolute_date : if note_datetime . diff ( absolute_date ) < self . history_limit : recent_dates . append ( Span ( doc , date . start , date . end , label = \"absolute_date\" ) ) elif not ( self . exclude_birthdate and birth_datetime and absolute_date == birth_datetime ): history_dates . append ( Span ( doc , date . start , date . end , label = \"absolute_date\" ) ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . sections : sub_sections , sections = consume_spans ( sections , lambda s : s . start < end <= s . end , sub_sections ) if self . dates : sub_recent_dates , recent_dates = consume_spans ( recent_dates , lambda s : check_sent_inclusion ( s , start , end ), sub_recent_dates , ) sub_history_dates , history_dates = consume_spans ( history_dates , lambda s : check_sent_inclusion ( s , start , end ), sub_history_dates , ) # Filter dates inside the boundaries only if self . closest_dates_only : close_recent_dates = [] close_history_dates = [] if sub_recent_dates : close_recent_dates = [ recent_date for recent_date in sub_recent_dates if check_inclusion ( recent_date , start , end ) ] if sub_history_dates : close_history_dates = [ history_date for history_date in sub_history_dates if check_inclusion ( history_date , start , end ) ] # If no date inside the boundaries, get the closest if not close_recent_dates and not close_history_dates : min_distance_recent_date = min ( [ abs ( sub_recent_date . start - start ) for sub_recent_date in sub_recent_dates ] ) min_distance_history_date = min ( [ abs ( sub_history_date . start - start ) for sub_history_date in sub_history_dates ] ) if min_distance_recent_date < min_distance_history_date : close_recent_dates = [ min ( sub_recent_dates , key = lambda x : abs ( x . start - start ), ) ] else : close_history_dates = [ min ( sub_history_dates , key = lambda x : abs ( x . start - start ), ) ] elif not close_recent_dates : close_recent_dates = [ min ( sub_recent_dates , key = lambda x : abs ( x . start - start ), ) ] elif sub_history_dates : close_history_dates = [ history_date for history_date in sub_history_dates if check_inclusion ( history_date , start , end ) ] # If no date inside the boundaries, get the closest if not close_history_dates : close_history_dates = [ min ( sub_history_dates , key = lambda x : abs ( x . start - start ), ) ] if self . on_ents_only and not ents : continue history_cues = get_spans ( sub_matches , \"history\" ) recent_cues = [] if self . sections : history_cues . extend ( sub_sections ) if self . dates : history_cues . extend ( close_history_dates if self . closest_dates_only else sub_history_dates ) recent_cues . extend ( close_recent_dates if self . closest_dates_only else sub_recent_dates ) history = bool ( history_cues ) and not bool ( recent_cues ) if not self . on_ents_only : for token in doc [ start : end ]: token . _ . history = history for ent in ents : ent . _ . history = ent . _ . history or history if self . explain : ent . _ . history_cues += history_cues ent . _ . recent_cues += recent_cues if not self . on_ents_only and ent . _ . history : for token in ent : token . _ . history = True return doc","title":"process()"},{"location":"reference/pipelines/qualifiers/history/patterns/","text":"edsnlp.pipelines.qualifiers.history.patterns history = [ 'ant\u00e9c\u00e9dents' , 'atcd' , 'atcds' , 'tacds' , 'ant\u00e9c\u00e9dent' ] module-attribute sections_history = [ 'ant\u00e9c\u00e9dents' , 'ant\u00e9c\u00e9dents familiaux' , 'histoire de la maladie' ] module-attribute","title":"patterns"},{"location":"reference/pipelines/qualifiers/history/patterns/#edsnlppipelinesqualifiershistorypatterns","text":"","title":"edsnlp.pipelines.qualifiers.history.patterns"},{"location":"reference/pipelines/qualifiers/history/patterns/#edsnlp.pipelines.qualifiers.history.patterns.history","text":"","title":"history"},{"location":"reference/pipelines/qualifiers/history/patterns/#edsnlp.pipelines.qualifiers.history.patterns.sections_history","text":"","title":"sections_history"},{"location":"reference/pipelines/qualifiers/hypothesis/","text":"edsnlp.pipelines.qualifiers.hypothesis","title":"`edsnlp.pipelines.qualifiers.hypothesis`"},{"location":"reference/pipelines/qualifiers/hypothesis/#edsnlppipelinesqualifiershypothesis","text":"","title":"edsnlp.pipelines.qualifiers.hypothesis"},{"location":"reference/pipelines/qualifiers/hypothesis/factory/","text":"edsnlp.pipelines.qualifiers.hypothesis.factory DEFAULT_CONFIG = dict ( pseudo = None , preceding = None , following = None , termination = None , verbs_hyp = None , verbs_eds = None , attr = 'NORM' , on_ents_only = True , within_ents = False , explain = False ) module-attribute create_component ( nlp , name , attr , pseudo , preceding , following , termination , verbs_eds , verbs_hyp , on_ents_only , within_ents , explain ) Source code in edsnlp/pipelines/qualifiers/hypothesis/factory.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @deprecated_factory ( \"hypothesis\" , \"eds.hypothesis\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.hypothesis\" ], ) @Language . factory ( \"eds.hypothesis\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.hypothesis\" ], ) def create_component ( nlp : Language , name : str , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs_eds : Optional [ List [ str ]], verbs_hyp : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): return Hypothesis ( nlp = nlp , attr = attr , pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , on_ents_only = on_ents_only , within_ents = within_ents , explain = explain , )","title":"factory"},{"location":"reference/pipelines/qualifiers/hypothesis/factory/#edsnlppipelinesqualifiershypothesisfactory","text":"","title":"edsnlp.pipelines.qualifiers.hypothesis.factory"},{"location":"reference/pipelines/qualifiers/hypothesis/factory/#edsnlp.pipelines.qualifiers.hypothesis.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/qualifiers/hypothesis/factory/#edsnlp.pipelines.qualifiers.hypothesis.factory.create_component","text":"Source code in edsnlp/pipelines/qualifiers/hypothesis/factory.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @deprecated_factory ( \"hypothesis\" , \"eds.hypothesis\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.hypothesis\" ], ) @Language . factory ( \"eds.hypothesis\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.hypothesis\" ], ) def create_component ( nlp : Language , name : str , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs_eds : Optional [ List [ str ]], verbs_hyp : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): return Hypothesis ( nlp = nlp , attr = attr , pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , on_ents_only = on_ents_only , within_ents = within_ents , explain = explain , )","title":"create_component()"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/","text":"edsnlp.pipelines.qualifiers.hypothesis.hypothesis Hypothesis Bases: Qualifier Hypothesis detection with spaCy. The component looks for five kinds of expressions in the text : preceding hypothesis, ie cues that precede a hypothetic expression following hypothesis, ie cues that follow a hypothetic expression pseudo hypothesis : contain a hypothesis cue, but are not hypothesis (eg \"pas de doute\"/\"no doubt\") hypothetic verbs : verbs indicating hypothesis (eg \"douter\") classic verbs conjugated to the conditional, thus indicating hypothesis PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language pseudo List of pseudo hypothesis cues. TYPE: Optional[List[str]] preceding List of preceding hypothesis cues TYPE: Optional[List[str]] following List of following hypothesis cues. TYPE: Optional[List[str]] verbs_hyp List of hypothetic verbs. TYPE: Optional[List[str]] verbs_eds List of mainstream verbs. TYPE: Optional[List[str]] filter_matches Whether to filter out overlapping matches. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 class Hypothesis ( Qualifier ): \"\"\" Hypothesis detection with spaCy. The component looks for five kinds of expressions in the text : - preceding hypothesis, ie cues that precede a hypothetic expression - following hypothesis, ie cues that follow a hypothetic expression - pseudo hypothesis : contain a hypothesis cue, but are not hypothesis (eg \"pas de doute\"/\"no doubt\") - hypothetic verbs : verbs indicating hypothesis (eg \"douter\") - classic verbs conjugated to the conditional, thus indicating hypothesis Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. pseudo : Optional[List[str]] List of pseudo hypothesis cues. preceding : Optional[List[str]] List of preceding hypothesis cues following : Optional[List[str]] List of following hypothesis cues. verbs_hyp : Optional[List[str]] List of hypothetic verbs. verbs_eds : Optional[List[str]] List of mainstream verbs. filter_matches : bool Whether to filter out overlapping matches. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. \"\"\" defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs_eds : Optional [ List [ str ]], verbs_hyp : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) terms [ \"verbs_preceding\" ], terms [ \"verbs_following\" ] = self . load_verbs ( verbs_hyp = terms . pop ( \"verbs_hyp\" ), verbs_eds = terms . pop ( \"verbs_eds\" ), ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"hypothesis\" ): Token . set_extension ( \"hypothesis\" , default = False ) if not Token . has_extension ( \"hypothesis_\" ): Token . set_extension ( \"hypothesis_\" , getter = lambda token : \"HYP\" if token . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis\" ): Span . set_extension ( \"hypothesis\" , default = False ) if not Span . has_extension ( \"hypothesis_\" ): Span . set_extension ( \"hypothesis_\" , getter = lambda span : \"HYP\" if span . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis_cues\" ): Span . set_extension ( \"hypothesis_cues\" , default = []) if not Doc . has_extension ( \"hypothesis\" ): Doc . set_extension ( \"hypothesis\" , default = []) def load_verbs ( self , verbs_hyp : List [ str ], verbs_eds : List [ str ], ) -> List [ str ]: \"\"\" Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. Parameters ---------- verbs_hyp: List of verbs that specifically imply an hypothesis. verbs_eds: List of general verbs. Returns ------- list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. \"\"\" classic_verbs = get_verbs ( verbs_eds ) classic_verbs = classic_verbs . loc [ classic_verbs [ \"mode\" ] == \"Conditionnel\" ] list_classic_verbs = list ( classic_verbs [ \"term\" ] . unique ()) hypo_verbs = get_verbs ( verbs_hyp ) list_hypo_verbs_preceding = list ( hypo_verbs [ \"term\" ] . unique ()) hypo_verbs_following = hypo_verbs . loc [ hypo_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ] list_hypo_verbs_following = list ( hypo_verbs_following [ \"term\" ] . unique ()) return ( list_hypo_verbs_preceding + list_classic_verbs , list_hypo_verbs_following , ) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to hypothesis. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for hypothesis \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_preceding += get_spans ( sub_matches , \"verbs_preceding\" ) sub_following += get_spans ( sub_matches , \"verbs_following\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . hypothesis = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] hypothesis = ent . _ . hypothesis or bool ( cues ) ent . _ . hypothesis = hypothesis if self . explain and hypothesis : ent . _ . hypothesis_cues += cues if not self . on_ents_only and hypothesis : for token in ent : token . _ . hypothesis = True return doc defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp ) class-attribute within_ents = within_ents instance-attribute __init__ ( nlp , attr , pseudo , preceding , following , termination , verbs_eds , verbs_hyp , on_ents_only , within_ents , explain ) Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs_eds : Optional [ List [ str ]], verbs_hyp : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) terms [ \"verbs_preceding\" ], terms [ \"verbs_following\" ] = self . load_verbs ( verbs_hyp = terms . pop ( \"verbs_hyp\" ), verbs_eds = terms . pop ( \"verbs_eds\" ), ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () set_extensions () Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"hypothesis\" ): Token . set_extension ( \"hypothesis\" , default = False ) if not Token . has_extension ( \"hypothesis_\" ): Token . set_extension ( \"hypothesis_\" , getter = lambda token : \"HYP\" if token . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis\" ): Span . set_extension ( \"hypothesis\" , default = False ) if not Span . has_extension ( \"hypothesis_\" ): Span . set_extension ( \"hypothesis_\" , getter = lambda span : \"HYP\" if span . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis_cues\" ): Span . set_extension ( \"hypothesis_cues\" , default = []) if not Doc . has_extension ( \"hypothesis\" ): Doc . set_extension ( \"hypothesis\" , default = []) load_verbs ( verbs_hyp , verbs_eds ) Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. PARAMETER DESCRIPTION verbs_hyp TYPE: List [ str ] verbs_eds TYPE: List [ str ] RETURNS DESCRIPTION list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def load_verbs ( self , verbs_hyp : List [ str ], verbs_eds : List [ str ], ) -> List [ str ]: \"\"\" Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. Parameters ---------- verbs_hyp: List of verbs that specifically imply an hypothesis. verbs_eds: List of general verbs. Returns ------- list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. \"\"\" classic_verbs = get_verbs ( verbs_eds ) classic_verbs = classic_verbs . loc [ classic_verbs [ \"mode\" ] == \"Conditionnel\" ] list_classic_verbs = list ( classic_verbs [ \"term\" ] . unique ()) hypo_verbs = get_verbs ( verbs_hyp ) list_hypo_verbs_preceding = list ( hypo_verbs [ \"term\" ] . unique ()) hypo_verbs_following = hypo_verbs . loc [ hypo_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ] list_hypo_verbs_following = list ( hypo_verbs_following [ \"term\" ] . unique ()) return ( list_hypo_verbs_preceding + list_classic_verbs , list_hypo_verbs_following , ) process ( doc ) Finds entities related to hypothesis. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to hypothesis. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for hypothesis \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_preceding += get_spans ( sub_matches , \"verbs_preceding\" ) sub_following += get_spans ( sub_matches , \"verbs_following\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . hypothesis = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] hypothesis = ent . _ . hypothesis or bool ( cues ) ent . _ . hypothesis = hypothesis if self . explain and hypothesis : ent . _ . hypothesis_cues += cues if not self . on_ents_only and hypothesis : for token in ent : token . _ . hypothesis = True return doc","title":"hypothesis"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlppipelinesqualifiershypothesishypothesis","text":"","title":"edsnlp.pipelines.qualifiers.hypothesis.hypothesis"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis","text":"Bases: Qualifier Hypothesis detection with spaCy. The component looks for five kinds of expressions in the text : preceding hypothesis, ie cues that precede a hypothetic expression following hypothesis, ie cues that follow a hypothetic expression pseudo hypothesis : contain a hypothesis cue, but are not hypothesis (eg \"pas de doute\"/\"no doubt\") hypothetic verbs : verbs indicating hypothesis (eg \"douter\") classic verbs conjugated to the conditional, thus indicating hypothesis PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language pseudo List of pseudo hypothesis cues. TYPE: Optional[List[str]] preceding List of preceding hypothesis cues TYPE: Optional[List[str]] following List of following hypothesis cues. TYPE: Optional[List[str]] verbs_hyp List of hypothetic verbs. TYPE: Optional[List[str]] verbs_eds List of mainstream verbs. TYPE: Optional[List[str]] filter_matches Whether to filter out overlapping matches. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool regex A dictionnary of regex patterns. TYPE: Optional[Dict[str, Union[List[str], str]]] Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 class Hypothesis ( Qualifier ): \"\"\" Hypothesis detection with spaCy. The component looks for five kinds of expressions in the text : - preceding hypothesis, ie cues that precede a hypothetic expression - following hypothesis, ie cues that follow a hypothetic expression - pseudo hypothesis : contain a hypothesis cue, but are not hypothesis (eg \"pas de doute\"/\"no doubt\") - hypothetic verbs : verbs indicating hypothesis (eg \"douter\") - classic verbs conjugated to the conditional, thus indicating hypothesis Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. pseudo : Optional[List[str]] List of pseudo hypothesis cues. preceding : Optional[List[str]] List of preceding hypothesis cues following : Optional[List[str]] List of following hypothesis cues. verbs_hyp : Optional[List[str]] List of hypothetic verbs. verbs_eds : Optional[List[str]] List of mainstream verbs. filter_matches : bool Whether to filter out overlapping matches. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. regex : Optional[Dict[str, Union[List[str], str]]] A dictionnary of regex patterns. \"\"\" defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs_eds : Optional [ List [ str ]], verbs_hyp : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) terms [ \"verbs_preceding\" ], terms [ \"verbs_following\" ] = self . load_verbs ( verbs_hyp = terms . pop ( \"verbs_hyp\" ), verbs_eds = terms . pop ( \"verbs_eds\" ), ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"hypothesis\" ): Token . set_extension ( \"hypothesis\" , default = False ) if not Token . has_extension ( \"hypothesis_\" ): Token . set_extension ( \"hypothesis_\" , getter = lambda token : \"HYP\" if token . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis\" ): Span . set_extension ( \"hypothesis\" , default = False ) if not Span . has_extension ( \"hypothesis_\" ): Span . set_extension ( \"hypothesis_\" , getter = lambda span : \"HYP\" if span . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis_cues\" ): Span . set_extension ( \"hypothesis_cues\" , default = []) if not Doc . has_extension ( \"hypothesis\" ): Doc . set_extension ( \"hypothesis\" , default = []) def load_verbs ( self , verbs_hyp : List [ str ], verbs_eds : List [ str ], ) -> List [ str ]: \"\"\" Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. Parameters ---------- verbs_hyp: List of verbs that specifically imply an hypothesis. verbs_eds: List of general verbs. Returns ------- list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. \"\"\" classic_verbs = get_verbs ( verbs_eds ) classic_verbs = classic_verbs . loc [ classic_verbs [ \"mode\" ] == \"Conditionnel\" ] list_classic_verbs = list ( classic_verbs [ \"term\" ] . unique ()) hypo_verbs = get_verbs ( verbs_hyp ) list_hypo_verbs_preceding = list ( hypo_verbs [ \"term\" ] . unique ()) hypo_verbs_following = hypo_verbs . loc [ hypo_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ] list_hypo_verbs_following = list ( hypo_verbs_following [ \"term\" ] . unique ()) return ( list_hypo_verbs_preceding + list_classic_verbs , list_hypo_verbs_following , ) def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to hypothesis. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for hypothesis \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_preceding += get_spans ( sub_matches , \"verbs_preceding\" ) sub_following += get_spans ( sub_matches , \"verbs_following\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . hypothesis = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] hypothesis = ent . _ . hypothesis or bool ( cues ) ent . _ . hypothesis = hypothesis if self . explain and hypothesis : ent . _ . hypothesis_cues += cues if not self . on_ents_only and hypothesis : for token in ent : token . _ . hypothesis = True return doc","title":"Hypothesis"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.defaults","text":"","title":"defaults"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.within_ents","text":"","title":"within_ents"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.__init__","text":"Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs_eds : Optional [ List [ str ]], verbs_hyp : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs_eds = verbs_eds , verbs_hyp = verbs_hyp , ) terms [ \"verbs_preceding\" ], terms [ \"verbs_following\" ] = self . load_verbs ( verbs_hyp = terms . pop ( \"verbs_hyp\" ), verbs_eds = terms . pop ( \"verbs_eds\" ), ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.set_extensions","text":"Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"hypothesis\" ): Token . set_extension ( \"hypothesis\" , default = False ) if not Token . has_extension ( \"hypothesis_\" ): Token . set_extension ( \"hypothesis_\" , getter = lambda token : \"HYP\" if token . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis\" ): Span . set_extension ( \"hypothesis\" , default = False ) if not Span . has_extension ( \"hypothesis_\" ): Span . set_extension ( \"hypothesis_\" , getter = lambda span : \"HYP\" if span . _ . hypothesis else \"CERT\" , ) if not Span . has_extension ( \"hypothesis_cues\" ): Span . set_extension ( \"hypothesis_cues\" , default = []) if not Doc . has_extension ( \"hypothesis\" ): Doc . set_extension ( \"hypothesis\" , default = [])","title":"set_extensions()"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.load_verbs","text":"Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. PARAMETER DESCRIPTION verbs_hyp TYPE: List [ str ] verbs_eds TYPE: List [ str ] RETURNS DESCRIPTION list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def load_verbs ( self , verbs_hyp : List [ str ], verbs_eds : List [ str ], ) -> List [ str ]: \"\"\" Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses. Parameters ---------- verbs_hyp: List of verbs that specifically imply an hypothesis. verbs_eds: List of general verbs. Returns ------- list of hypothesis verbs conjugated at all tenses and classic verbs conjugated to conditional. \"\"\" classic_verbs = get_verbs ( verbs_eds ) classic_verbs = classic_verbs . loc [ classic_verbs [ \"mode\" ] == \"Conditionnel\" ] list_classic_verbs = list ( classic_verbs [ \"term\" ] . unique ()) hypo_verbs = get_verbs ( verbs_hyp ) list_hypo_verbs_preceding = list ( hypo_verbs [ \"term\" ] . unique ()) hypo_verbs_following = hypo_verbs . loc [ hypo_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ] list_hypo_verbs_following = list ( hypo_verbs_following [ \"term\" ] . unique ()) return ( list_hypo_verbs_preceding + list_classic_verbs , list_hypo_verbs_following , )","title":"load_verbs()"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.process","text":"Finds entities related to hypothesis. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to hypothesis. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for hypothesis \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_preceding += get_spans ( sub_matches , \"verbs_preceding\" ) sub_following += get_spans ( sub_matches , \"verbs_following\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . hypothesis = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] hypothesis = ent . _ . hypothesis or bool ( cues ) ent . _ . hypothesis = hypothesis if self . explain and hypothesis : ent . _ . hypothesis_cues += cues if not self . on_ents_only and hypothesis : for token in ent : token . _ . hypothesis = True return doc","title":"process()"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/","text":"edsnlp.pipelines.qualifiers.hypothesis.patterns pseudo : List [ str ] = [ 'aucun doute' , 'm\u00eame si' , 'pas de condition' , 'pas de doute' , 'sans aucun doute' , 'sans condition' , 'sans risque' ] module-attribute confirmation : List [ str ] = [ 'certain' , 'certaine' , 'certainement' , 'certaines' , 'certains' , 'confirmer' , '\u00e9videmment' , '\u00e9vident' , '\u00e9vidente' , 'montrer que' , 'visiblement' ] module-attribute preceding : List [ str ] = [ '\u00e0 condition' , '\u00e0 la condition que' , '\u00e0 moins que' , 'au cas o\u00f9' , 'conditionnellement' , 'discret' , 'discrets' , 'discr\u00e8te' , 'discr\u00e8tes' , 'en admettant que' , 'en cas' , 'en consid\u00e9rant que' , 'en supposant que' , '\u00e9ventuellement' , 'exploration' , 'faudrait' , 'hypoth\u00e8se' , 'hypoth\u00e8ses' , 'id\u00e9e de' , 'pas confirmer' , 'pas s\u00fbr' , 'pas s\u00fbre' , 'peut correspondre' , 'peut-\u00eatre' , 'peuvent correspondre' , 'possibilit\u00e9' , 'possible' , 'possiblement' , 'potentiel' , 'potentielle' , 'potentiellement' , 'potentielles' , 'potentiels' , 'pr\u00e9disposant \u00e0' , 'probable' , 'probablement' , 'probables' , \"recherche d'\" , 'recherche de' , 'recherche des' , 'risque' , 'selon' , 'si' , \"s'il\" , 'soit' , 'sous condition' , 'sous r\u00e9serve' , 'suspicion' ] module-attribute following : List [ str ] = [ '?' , 'envisageable' , 'envisageables' , 'hypoth\u00e9tique' , 'hypoth\u00e9tiquement' , 'hypoth\u00e9tiques' , 'pas certain' , 'pas certaine' , 'pas clair' , 'pas claire' , 'pas confirm\u00e9' , 'pas confirm\u00e9e' , 'pas confirm\u00e9es' , 'pas confirm\u00e9s' , 'pas \u00e9vident' , 'pas \u00e9vidente' , 'pas s\u00fbr' , 'pas s\u00fbre' , 'possibilit\u00e9' , 'possible' , 'potentiel' , 'potentielle' , 'potentiels' , 'probable' , 'probables' ] module-attribute verbs_hyp : List [ str ] = [ 'douter' , 'envisager' , 'explorer' , 'rechercher' , \"s'apparenter\" , 'sembler' , 'soup\u00e7onner' , 'sugg\u00e9rer' , 'suspecter' ] module-attribute verbs_eds : List [ str ] = [ 'abandonner' , 'abolir' , 'aborder' , 'accepter' , 'accidenter' , 'accompagnemer' , 'accompagner' , 'acoller' , 'acqu\u00e9rir' , 'activer' , 'actualiser' , 'adapter' , 'adh\u00e9rer' , 'adjuver' , 'admettre' , 'administrer' , 'adopter' , 'adresser' , 'aggraver' , 'agir' , 'agr\u00e9er' , 'aider' , 'aimer' , 'alcooliser' , 'alerter' , 'alimenter' , 'aller' , 'allonger' , 'all\u00e9ger' , 'alterner' , 'alt\u00e9rer' , 'amender' , 'amener' , 'am\u00e9liorer' , 'amyotrophier' , 'am\u00e9liorer' , 'analyser' , 'anesth\u00e9sier' , 'animer' , 'annexer' , 'annuler' , 'anonymiser' , 'anticiper' , 'anticoaguler' , 'apercevoir' , 'aplatir' , 'appara\u00eetre' , 'appareiller' , 'appeler' , 'appliquer' , 'apporter' , 'apprendre' , 'appr\u00e9cier' , 'appuyer' , 'argumenter' , 'arquer' , 'arr\u00eater' , 'arriver' , 'arr\u00eater' , 'articuler' , 'aspirer' , 'asseoir' , 'assister' , 'associer' , 'assurer' , 'ass\u00e9cher' , 'attacher' , 'atteindre' , 'attendre' , 'attribuer' , 'augmenter' , 'autonomiser' , 'autoriser' , 'avaler' , 'avancer' , 'avertir' , 'avoir' , 'av\u00e9rer' , 'a\u00e9rer' , 'baisser' , 'ballonner' , 'blesser' , 'bloquer' , 'boire' , 'border' , 'brancher' , 'br\u00fbler' , 'b\u00e9n\u00e9ficier' , 'cadrer' , 'calcifier' , 'calculer' , 'calmer' , 'canaliser' , 'capter' , 'carencer' , 'casser' , 'centrer' , 'cerner' , 'certifier' , 'changer' , 'charger' , 'chevaucher' , 'choisir' , 'chronomoduler' , 'chuter' , 'cicatriser' , 'circoncire' , 'circuler' , 'classer' , 'cod\u00e9iner' , 'coincer' , 'colorer' , 'combler' , 'commander' , 'commencer' , 'communiquer' , 'comparer' , 'compliquer' , 'compl\u00e9ter' , 'comporter' , 'comprendre' , 'comprimer' , 'concerner' , 'conclure' , 'condamner' , 'conditionner' , 'conduire' , 'confiner' , 'confirmer' , 'confronter' , 'congeler' , 'conjoindre' , 'conjuguer' , 'conna\u00eetre' , 'connecter' , 'conseiller' , 'conserver' , 'consid\u00e9rer' , 'consommer' , 'constater' , 'constituer' , 'consulter' , 'contacter' , 'contaminer' , 'contenir' , 'contentionner' , 'continuer' , 'contracter' , 'contrarier' , 'contribuer' , 'contr\u00f4ler' , 'convaincre' , 'convenir' , 'convier' , 'convoquer' , 'copier' , 'correspondre' , 'corriger' , 'corr\u00e9ler' , 'coucher' , 'coupler' , 'couvrir' , 'crapotter' , 'creuser' , 'croire' , 'croiser' , 'cr\u00e9er' , 'cr\u00e9mer' , 'cr\u00e9piter' , 'cumuler' , 'curariser' , 'c\u00e9der' , 'dater' , 'demander' , 'demeurer' , 'destiner' , 'devenir' , 'devoir' , 'diagnostiquer' , 'dialyser' , 'dicter' , 'diffuser' , 'diff\u00e9rencier' , 'diff\u00e9rer' , 'dig\u00e9rer' , 'dilater' , 'diluer' , 'diminuer' , 'diner' , 'dire' , 'diriger' , 'discuter' , 'dispara\u00eetre' , 'disposer' , 'dissocier' , 'diss\u00e9miner' , 'diss\u00e9quer' , 'distendre' , 'distinguer' , 'divorcer' , 'documenter' , 'donner' , 'dorer' , 'doser' , 'doubler' , 'durer' , 'dyaliser' , 'dyspner' , 'd\u00e9buter' , 'd\u00e9caler' , 'd\u00e9celer' , 'd\u00e9cider' , 'd\u00e9clarer' , 'd\u00e9clencher' , 'd\u00e9couvrir' , 'd\u00e9crire' , 'd\u00e9cro\u00eetre' , 'd\u00e9curariser' , 'd\u00e9c\u00e9der' , 'd\u00e9dier' , 'd\u00e9finir' , 'd\u00e9grader' , 'd\u00e9livrer' , 'd\u00e9passer' , 'd\u00e9pendre' , 'd\u00e9placer' , 'd\u00e9polir' , 'd\u00e9poser' , 'd\u00e9river' , 'd\u00e9rouler' , 'd\u00e9sappareiller' , 'd\u00e9signer' , 'd\u00e9sinfecter' , 'd\u00e9sorienter' , 'd\u00e9tecter' , 'd\u00e9terminer' , 'd\u00e9truire' , 'd\u00e9velopper' , 'd\u00e9vouer' , 'd\u00eener' , '\u00e9craser' , 'effacer' , 'effectuer' , 'effondrer' , 'emboliser' , 'emmener' , 'emp\u00eacher' , 'encadrer' , 'encourager' , 'endormir' , 'endurer' , 'enlever' , 'enregistrer' , 'entamer' , 'entendre' , 'entourer' , 'entra\u00eener' , 'entreprendre' , 'entrer' , 'envahir' , 'envisager' , 'envoyer' , 'esp\u00e9rer' , 'essayer' , 'estimer' , '\u00eatre' , 'examiner' , 'excentrer' , 'exciser' , 'exclure' , 'expirer' , 'expliquer' , 'explorer' , 'exposer' , 'exprimer' , 'ext\u00e9rioriser' , 'ex\u00e9cuter' , 'faciliter' , 'faire' , 'fatiguer' , 'favoriser' , 'faxer' , 'fermer' , 'figurer' , 'fixer' , 'focaliser' , 'foncer' , 'former' , 'fournir' , 'fractionner' , 'fragmenter' , 'fuiter' , 'fusionner' , 'garder' , 'graver' , 'guider' , 'g\u00e9rer' , 'g\u00eaner' , 'honorer' , 'hopsitaliser' , 'hospitaliser' , 'hydrater' , 'hyperart\u00e9rialiser' , 'hyperfixer' , 'hypertrophier' , 'h\u00e9siter' , 'identifier' , 'illustrer' , 'immuniser' , 'impacter' , 'implanter' , 'impliquer' , 'importer' , 'imposer' , 'impregner' , 'imprimer' , 'inclure' , 'indifferencier' , 'indiquer' , 'infecter' , 'infertiliser' , 'infirmer' , 'infiltrer' , 'informer' , 'inhaler' , 'initier' , 'injecter' , 'inscrire' , 'insister' , 'installer' , 'interdire' , 'interpr\u00e9ter' , 'interrompre' , 'intervenir' , 'intituler' , 'introduire' , 'int\u00e9ragir' , 'inverser' , 'inviter' , 'ioder' , 'ioniser' , 'irradier' , 'it\u00e9rativer' , 'joindre' , 'juger' , 'justifier' , 'laisser' , 'laminer' , 'lancer' , 'lat\u00e9raliser' , 'laver' , 'lever' , 'lier' , 'ligaturer' , 'limiter' , 'lire' , 'localiser' , 'loger' , 'louper' , 'luire' , 'lutter' , 'lyricer' , 'lyser' , 'maculer' , 'mac\u00e9rer' , 'maintenir' , 'majorer' , 'malaiser' , 'manger' , 'manifester' , 'manipuler' , 'manquer' , 'marcher' , 'marier' , 'marmoner' , 'marquer' , 'masquer' , 'masser' , 'mater' , 'mener' , 'mesurer' , 'meteoriser' , 'mettre' , 'mitiger' , 'modifier' , 'moduler' , 'mod\u00e9rer' , 'monter' , 'montrer' , 'motiver' , 'moucheter' , 'mouler' , 'mourir' , 'multiop\u00e9r\u00e9er' , 'munir' , 'muter' , 'm\u00e9dicaliser' , 'm\u00e9t\u00e9oriser' , 'na\u00eetre' , 'normaliser' , 'noter' , 'nuire' , 'num\u00e9riser' , 'n\u00e9cessiter' , 'n\u00e9gativer' , 'objectiver' , 'observer' , 'obstruer' , 'obtenir' , 'occasionner' , 'occuper' , 'opposer' , 'op\u00e9rer' , 'organiser' , 'orienter' , 'ouvrir' , 'palper' , 'parasiter' , 'para\u00eetre' , 'parcourir' , 'parer' , 'paresth\u00e9sier' , 'parfaire' , 'partager' , 'partir' , 'parvenir' , 'passer' , 'penser' , 'percevoir' , 'perdre' , 'perforer' , 'permettre' , 'persister' , 'personnaliser' , 'peser' , 'pigmenter' , 'piloter' , 'placer' , 'plaindre' , 'planifier' , 'plier' , 'plonger' , 'porter' , 'poser' , 'positionner' , 'poss\u00e9der' , 'poursuivre' , 'pousser' , 'pouvoir' , 'pratiquer' , 'preciser' , 'prendre' , 'prescrire' , 'prier' , 'produire' , 'programmer' , 'prolonger' , 'prononcer' , 'proposer' , 'prouver' , 'provoquer' , 'pr\u00e9ciser' , 'pr\u00e9c\u00e9der' , 'pr\u00e9dominer' , 'pr\u00e9exister' , 'pr\u00e9f\u00e9rer' , 'pr\u00e9lever' , 'pr\u00e9parer' , 'pr\u00e9senter' , 'pr\u00e9server' , 'pr\u00e9venir' , 'pr\u00e9voir' , 'puruler' , 'p\u00e9n\u00e9trer' , 'radiofr\u00e9quencer' , 'ralentir' , 'ramener' , 'rappeler' , 'rapporter' , 'rapprocher' , 'rassurer' , 'rattacher' , 'rattraper' , 'realiser' , 'recenser' , 'recevoir' , 'rechercher' , 'recommander' , 'reconna\u00eetre' , 'reconsulter' , 'recontacter' , 'recontr\u00f4ler' , 'reconvoquer' , 'recouvrir' , 'recueillir' , 'recuperer' , 'redescendre' , 'rediscuter' , 'refaire' , 'refouler' , 'refuser' , 'regarder' , 'rehausser' , 'relancer' , 'relayer' , 'relever' , 'relire' , 'rel\u00e2cher' , 'remanier' , 'remarquer' , 'remercier' , 'remettre' , 'remonter' , 'remplacer' , 'remplir' , 'rencontrer' , 'rendormir' , 'rendre' , 'renfermer' , 'renforcer' , 'renouveler' , 'renseigner' , 'rentrer' , 'reparler' , 'repasser' , 'reporter' , 'reprendre' , 'represcrire' , 'reproduire' , 'reprogrammer' , 'repr\u00e9senter' , 'rep\u00e9rer' , 'requ\u00e9rir' , 'respecter' , 'ressembler' , 'ressentir' , 'rester' , 'restreindre' , 'retarder' , 'retenir' , 'retirer' , 'retrouver' , 'revasculariser' , 'revenir' , 'reverticaliser' , 'revoir' , 'rompre' , 'rouler' , 'r\u00e9adapter' , 'r\u00e9admettre' , 'r\u00e9adresser' , 'r\u00e9aliser' , 'r\u00e9cidiver' , 'r\u00e9cup\u00e9rer' , 'r\u00e9diger' , 'r\u00e9duire' , 'r\u00e9essayer' , 'r\u00e9expliquer' , 'r\u00e9f\u00e9rer' , 'r\u00e9gler' , 'r\u00e9gresser' , 'r\u00e9hausser' , 'r\u00e9op\u00e9rer' , 'r\u00e9partir' , 'r\u00e9pondre' , 'r\u00e9p\u00e9ter' , 'r\u00e9server' , 'r\u00e9sorber' , 'r\u00e9soudre' , 'r\u00e9s\u00e9quer' , 'r\u00e9veiller' , 'r\u00e9v\u00e9ler' , 'r\u00e9\u00e9valuer' , 'r\u00eaver' , 'sacrer' , 'saisir' , 'satisfaire' , 'savoir' , 'scanner' , 'scolariser' , 'sembler' , 'sensibiliser' , 'sentir' , 'serrer' , 'servir' , 'sevrer' , 'signaler' , 'signer' , 'situer' , 'si\u00e9ger' , 'soigner' , 'sommeiller' , 'sonder' , 'sortir' , 'souffler' , 'souhaiter' , 'soulager' , 'soussigner' , 'souvenir' , 'sp\u00e9cialiser' , 'stabiliser' , 'statuer' , 'stenter' , 'stopper' , 'stratifier' , 'subir' , 'substituer' , 'sucrer' , 'sugg\u00e9rer' , 'suivre' , 'supporter' , 'supprimer' , 'surajouter' , 'surmonter' , 'surveiller' , 'survenir' , 'suspecter' , 'suspendre' , 'suturer' , 'synchroniser' , 'syst\u00e9matiser' , 's\u00e9cr\u00e9ter' , 's\u00e9curiser' , 's\u00e9dater' , 's\u00e9journer' , 's\u00e9parer' , 'taire' , 'taper' , 'teinter' , 'tendre' , 'tenir' , 'tenter' , 'terminer' , 'tester' , 'thromboser' , 'tirer' , 'tiroir' , 'tissulaire' , 'titulariser' , 'tol\u00e9rer' , 'tourner' , 'tracer' , 'trach\u00e9otomiser' , 'traduire' , 'traiter' , 'transcrire' , 'transf\u00e9rer' , 'transmettre' , 'transporter' , 'trasnfixer' , 'travailler' , 'tronquer' , 'trouver' , 't\u00e9l\u00e9phoner' , 'ulc\u00e9rer' , 'uriner' , 'utiliser' , 'vacciner' , 'valider' , 'valoir' , 'varier' , 'vasculariser' , 'venir' , 'verifier' , 'vieillir' , 'viser' , 'visualiser' , 'vivre' , 'voir' , 'vouloir' , 'v\u00e9rifier' , '\u00e9baucher' , '\u00e9carter' , '\u00e9chographier' , '\u00e9choguider' , '\u00e9choir' , '\u00e9chouer' , '\u00e9clairer' , '\u00e9craser' , '\u00e9largir' , '\u00e9liminer' , '\u00e9mousser' , '\u00e9paissir' , '\u00e9pargner' , '\u00e9puiser' , '\u00e9purer' , '\u00e9quilibrer' , '\u00e9tablir' , '\u00e9tager' , '\u00e9tendre' , '\u00e9tiqueter' , '\u00e9trangler' , '\u00e9valuer' , '\u00e9viter' , '\u00e9voluer' , '\u00e9voquer' , '\u00eatre' ] module-attribute","title":"patterns"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/#edsnlppipelinesqualifiershypothesispatterns","text":"","title":"edsnlp.pipelines.qualifiers.hypothesis.patterns"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/#edsnlp.pipelines.qualifiers.hypothesis.patterns.pseudo","text":"","title":"pseudo"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/#edsnlp.pipelines.qualifiers.hypothesis.patterns.confirmation","text":"","title":"confirmation"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/#edsnlp.pipelines.qualifiers.hypothesis.patterns.preceding","text":"","title":"preceding"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/#edsnlp.pipelines.qualifiers.hypothesis.patterns.following","text":"","title":"following"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/#edsnlp.pipelines.qualifiers.hypothesis.patterns.verbs_hyp","text":"","title":"verbs_hyp"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/#edsnlp.pipelines.qualifiers.hypothesis.patterns.verbs_eds","text":"","title":"verbs_eds"},{"location":"reference/pipelines/qualifiers/negation/","text":"edsnlp.pipelines.qualifiers.negation","title":"`edsnlp.pipelines.qualifiers.negation`"},{"location":"reference/pipelines/qualifiers/negation/#edsnlppipelinesqualifiersnegation","text":"","title":"edsnlp.pipelines.qualifiers.negation"},{"location":"reference/pipelines/qualifiers/negation/factory/","text":"edsnlp.pipelines.qualifiers.negation.factory DEFAULT_CONFIG = dict ( pseudo = None , preceding = None , following = None , termination = None , verbs = None , attr = 'NORM' , on_ents_only = True , within_ents = False , explain = False ) module-attribute create_component ( nlp , name , attr , pseudo , preceding , following , termination , verbs , on_ents_only , within_ents , explain ) Source code in edsnlp/pipelines/qualifiers/negation/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @deprecated_factory ( \"negation\" , \"eds.negation\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.negation\" ], ) @Language . factory ( \"eds.negation\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.negation\" ], ) def create_component ( nlp : Language , name : str , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): return Negation ( nlp = nlp , attr = attr , pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs = verbs , on_ents_only = on_ents_only , within_ents = within_ents , explain = explain , )","title":"factory"},{"location":"reference/pipelines/qualifiers/negation/factory/#edsnlppipelinesqualifiersnegationfactory","text":"","title":"edsnlp.pipelines.qualifiers.negation.factory"},{"location":"reference/pipelines/qualifiers/negation/factory/#edsnlp.pipelines.qualifiers.negation.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/qualifiers/negation/factory/#edsnlp.pipelines.qualifiers.negation.factory.create_component","text":"Source code in edsnlp/pipelines/qualifiers/negation/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @deprecated_factory ( \"negation\" , \"eds.negation\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.negation\" ], ) @Language . factory ( \"eds.negation\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.negation\" ], ) def create_component ( nlp : Language , name : str , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): return Negation ( nlp = nlp , attr = attr , pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs = verbs , on_ents_only = on_ents_only , within_ents = within_ents , explain = explain , )","title":"create_component()"},{"location":"reference/pipelines/qualifiers/negation/negation/","text":"edsnlp.pipelines.qualifiers.negation.negation Negation Bases: Qualifier Implements the NegEx algorithm. The component looks for five kinds of expressions in the text : preceding negations, ie cues that precede a negated expression following negations, ie cues that follow a negated expression pseudo negations : contain a negation cue, but are not negations (eg \"pas de doute\"/\"no doubt\") negation verbs, ie verbs that indicate a negation terminations, ie words that delimit propositions. The negation spans from the preceding cue to the termination. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language attr spaCy's attribute to use TYPE: str pseudo List of pseudo negation terms. TYPE: Optional[List[str]] preceding List of preceding negation terms TYPE: Optional[List[str]] following List of following negation terms. TYPE: Optional[List[str]] termination List of termination terms. TYPE: Optional[List[str]] verbs List of negation verbs. TYPE: Optional[List[str]] on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/negation/negation.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 class Negation ( Qualifier ): \"\"\" Implements the NegEx algorithm. The component looks for five kinds of expressions in the text : - preceding negations, ie cues that precede a negated expression - following negations, ie cues that follow a negated expression - pseudo negations : contain a negation cue, but are not negations (eg \"pas de doute\"/\"no doubt\") - negation verbs, ie verbs that indicate a negation - terminations, ie words that delimit propositions. The negation spans from the preceding cue to the termination. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. attr : str spaCy's attribute to use pseudo : Optional[List[str]] List of pseudo negation terms. preceding : Optional[List[str]] List of preceding negation terms following : Optional[List[str]] List of following negation terms. termination : Optional[List[str]] List of termination terms. verbs : Optional[List[str]] List of negation verbs. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , verbs = verbs , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs = verbs , ) terms [ \"verbs_preceding\" ], terms [ \"verbs_following\" ] = self . load_verbs ( terms [ \"verbs\" ] ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () @classmethod def set_extensions ( cl ) -> None : if not Token . has_extension ( \"negation\" ): Token . set_extension ( \"negation\" , default = False ) if not Token . has_extension ( \"negated\" ): Token . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Token . has_extension ( \"negation_\" ): Token . set_extension ( \"negation_\" , getter = lambda token : \"NEG\" if token . _ . negation else \"AFF\" , ) if not Token . has_extension ( \"polarity_\" ): Token . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Span . has_extension ( \"negation\" ): Span . set_extension ( \"negation\" , default = False ) if not Span . has_extension ( \"negated\" ): Span . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Span . has_extension ( \"negation_cues\" ): Span . set_extension ( \"negation_cues\" , default = []) if not Span . has_extension ( \"negation_\" ): Span . set_extension ( \"negation_\" , getter = lambda span : \"NEG\" if span . _ . negation else \"AFF\" , ) if not Span . has_extension ( \"polarity_\" ): Span . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Doc . has_extension ( \"negations\" ): Doc . set_extension ( \"negations\" , default = []) def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate negating verbs to specific tenses. Parameters ---------- verbs: list of negating verbs to conjugate Returns ------- list_neg_verbs_preceding: List of conjugated negating verbs preceding entities. list_neg_verbs_following: List of conjugated negating verbs following entities. \"\"\" neg_verbs = get_verbs ( verbs ) neg_verbs_preceding = neg_verbs . loc [ (( neg_verbs [ \"mode\" ] == \"Indicatif\" ) & ( neg_verbs [ \"tense\" ] == \"Pr\u00e9sent\" )) | ( neg_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) | ( neg_verbs [ \"tense\" ] == \"Infinitif Pr\u00e9sent\" ) ] neg_verbs_following = neg_verbs . loc [ neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ] list_neg_verbs_preceding = list ( neg_verbs_preceding [ \"term\" ] . unique ()) list_neg_verbs_following = list ( neg_verbs_following [ \"term\" ] . unique ()) return ( list_neg_verbs_preceding , list_neg_verbs_following ) def annotate_entity ( self , ent : Span , sub_preceding : List [ Span ], sub_following : List [ Span ], ) -> None : \"\"\" Annotate entities using preceding and following negations. Parameters ---------- ent : Span Entity to annotate sub_preceding : List[Span] List of preceding negations cues sub_following : List[Span] List of following negations cues \"\"\" if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] negation = ent . _ . negation or bool ( cues ) ent . _ . negation = negation if self . explain and negation : ent . _ . negation_cues += cues if not self . on_ents_only and negation : for token in ent : token . _ . negation = True def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to negation. Parameters ---------- doc: spaCy `Doc` object Returns ------- doc: spaCy `Doc` object, annotated for negation \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) # Verbs preceding negated content sub_preceding += get_spans ( sub_matches , \"verbs_preceding\" ) # Verbs following negated content sub_following += get_spans ( sub_matches , \"verbs_following\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . negation = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : self . annotate_entity ( ent = ent , sub_preceding = sub_preceding , sub_following = sub_following , ) return doc def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc ) defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , verbs = verbs , termination = termination ) class-attribute within_ents = within_ents instance-attribute __init__ ( nlp , attr , pseudo , preceding , following , termination , verbs , on_ents_only , within_ents , explain ) Source code in edsnlp/pipelines/qualifiers/negation/negation.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs = verbs , ) terms [ \"verbs_preceding\" ], terms [ \"verbs_following\" ] = self . load_verbs ( terms [ \"verbs\" ] ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () set_extensions ( cl ) Source code in edsnlp/pipelines/qualifiers/negation/negation.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 @classmethod def set_extensions ( cl ) -> None : if not Token . has_extension ( \"negation\" ): Token . set_extension ( \"negation\" , default = False ) if not Token . has_extension ( \"negated\" ): Token . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Token . has_extension ( \"negation_\" ): Token . set_extension ( \"negation_\" , getter = lambda token : \"NEG\" if token . _ . negation else \"AFF\" , ) if not Token . has_extension ( \"polarity_\" ): Token . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Span . has_extension ( \"negation\" ): Span . set_extension ( \"negation\" , default = False ) if not Span . has_extension ( \"negated\" ): Span . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Span . has_extension ( \"negation_cues\" ): Span . set_extension ( \"negation_cues\" , default = []) if not Span . has_extension ( \"negation_\" ): Span . set_extension ( \"negation_\" , getter = lambda span : \"NEG\" if span . _ . negation else \"AFF\" , ) if not Span . has_extension ( \"polarity_\" ): Span . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Doc . has_extension ( \"negations\" ): Doc . set_extension ( \"negations\" , default = []) load_verbs ( verbs ) Conjugate negating verbs to specific tenses. PARAMETER DESCRIPTION verbs TYPE: List [ str ] RETURNS DESCRIPTION list_neg_verbs_preceding list_neg_verbs_following Source code in edsnlp/pipelines/qualifiers/negation/negation.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate negating verbs to specific tenses. Parameters ---------- verbs: list of negating verbs to conjugate Returns ------- list_neg_verbs_preceding: List of conjugated negating verbs preceding entities. list_neg_verbs_following: List of conjugated negating verbs following entities. \"\"\" neg_verbs = get_verbs ( verbs ) neg_verbs_preceding = neg_verbs . loc [ (( neg_verbs [ \"mode\" ] == \"Indicatif\" ) & ( neg_verbs [ \"tense\" ] == \"Pr\u00e9sent\" )) | ( neg_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) | ( neg_verbs [ \"tense\" ] == \"Infinitif Pr\u00e9sent\" ) ] neg_verbs_following = neg_verbs . loc [ neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ] list_neg_verbs_preceding = list ( neg_verbs_preceding [ \"term\" ] . unique ()) list_neg_verbs_following = list ( neg_verbs_following [ \"term\" ] . unique ()) return ( list_neg_verbs_preceding , list_neg_verbs_following ) annotate_entity ( ent , sub_preceding , sub_following ) Annotate entities using preceding and following negations. PARAMETER DESCRIPTION ent Entity to annotate TYPE: Span sub_preceding List of preceding negations cues TYPE: List[Span] sub_following List of following negations cues TYPE: List[Span] Source code in edsnlp/pipelines/qualifiers/negation/negation.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def annotate_entity ( self , ent : Span , sub_preceding : List [ Span ], sub_following : List [ Span ], ) -> None : \"\"\" Annotate entities using preceding and following negations. Parameters ---------- ent : Span Entity to annotate sub_preceding : List[Span] List of preceding negations cues sub_following : List[Span] List of following negations cues \"\"\" if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] negation = ent . _ . negation or bool ( cues ) ent . _ . negation = negation if self . explain and negation : ent . _ . negation_cues += cues if not self . on_ents_only and negation : for token in ent : token . _ . negation = True process ( doc ) Finds entities related to negation. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/negation/negation.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to negation. Parameters ---------- doc: spaCy `Doc` object Returns ------- doc: spaCy `Doc` object, annotated for negation \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) # Verbs preceding negated content sub_preceding += get_spans ( sub_matches , \"verbs_preceding\" ) # Verbs following negated content sub_following += get_spans ( sub_matches , \"verbs_following\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . negation = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : self . annotate_entity ( ent = ent , sub_preceding = sub_preceding , sub_following = sub_following , ) return doc __call__ ( doc ) Source code in edsnlp/pipelines/qualifiers/negation/negation.py 280 281 def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc )","title":"negation"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlppipelinesqualifiersnegationnegation","text":"","title":"edsnlp.pipelines.qualifiers.negation.negation"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation","text":"Bases: Qualifier Implements the NegEx algorithm. The component looks for five kinds of expressions in the text : preceding negations, ie cues that precede a negated expression following negations, ie cues that follow a negated expression pseudo negations : contain a negation cue, but are not negations (eg \"pas de doute\"/\"no doubt\") negation verbs, ie verbs that indicate a negation terminations, ie words that delimit propositions. The negation spans from the preceding cue to the termination. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language attr spaCy's attribute to use TYPE: str pseudo List of pseudo negation terms. TYPE: Optional[List[str]] preceding List of preceding negation terms TYPE: Optional[List[str]] following List of following negation terms. TYPE: Optional[List[str]] termination List of termination terms. TYPE: Optional[List[str]] verbs List of negation verbs. TYPE: Optional[List[str]] on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/negation/negation.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 class Negation ( Qualifier ): \"\"\" Implements the NegEx algorithm. The component looks for five kinds of expressions in the text : - preceding negations, ie cues that precede a negated expression - following negations, ie cues that follow a negated expression - pseudo negations : contain a negation cue, but are not negations (eg \"pas de doute\"/\"no doubt\") - negation verbs, ie verbs that indicate a negation - terminations, ie words that delimit propositions. The negation spans from the preceding cue to the termination. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. attr : str spaCy's attribute to use pseudo : Optional[List[str]] List of pseudo negation terms. preceding : Optional[List[str]] List of preceding negation terms following : Optional[List[str]] List of following negation terms. termination : Optional[List[str]] List of termination terms. verbs : Optional[List[str]] List of negation verbs. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( following = following , preceding = preceding , pseudo = pseudo , verbs = verbs , termination = termination , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs = verbs , ) terms [ \"verbs_preceding\" ], terms [ \"verbs_following\" ] = self . load_verbs ( terms [ \"verbs\" ] ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions () @classmethod def set_extensions ( cl ) -> None : if not Token . has_extension ( \"negation\" ): Token . set_extension ( \"negation\" , default = False ) if not Token . has_extension ( \"negated\" ): Token . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Token . has_extension ( \"negation_\" ): Token . set_extension ( \"negation_\" , getter = lambda token : \"NEG\" if token . _ . negation else \"AFF\" , ) if not Token . has_extension ( \"polarity_\" ): Token . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Span . has_extension ( \"negation\" ): Span . set_extension ( \"negation\" , default = False ) if not Span . has_extension ( \"negated\" ): Span . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Span . has_extension ( \"negation_cues\" ): Span . set_extension ( \"negation_cues\" , default = []) if not Span . has_extension ( \"negation_\" ): Span . set_extension ( \"negation_\" , getter = lambda span : \"NEG\" if span . _ . negation else \"AFF\" , ) if not Span . has_extension ( \"polarity_\" ): Span . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Doc . has_extension ( \"negations\" ): Doc . set_extension ( \"negations\" , default = []) def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate negating verbs to specific tenses. Parameters ---------- verbs: list of negating verbs to conjugate Returns ------- list_neg_verbs_preceding: List of conjugated negating verbs preceding entities. list_neg_verbs_following: List of conjugated negating verbs following entities. \"\"\" neg_verbs = get_verbs ( verbs ) neg_verbs_preceding = neg_verbs . loc [ (( neg_verbs [ \"mode\" ] == \"Indicatif\" ) & ( neg_verbs [ \"tense\" ] == \"Pr\u00e9sent\" )) | ( neg_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) | ( neg_verbs [ \"tense\" ] == \"Infinitif Pr\u00e9sent\" ) ] neg_verbs_following = neg_verbs . loc [ neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ] list_neg_verbs_preceding = list ( neg_verbs_preceding [ \"term\" ] . unique ()) list_neg_verbs_following = list ( neg_verbs_following [ \"term\" ] . unique ()) return ( list_neg_verbs_preceding , list_neg_verbs_following ) def annotate_entity ( self , ent : Span , sub_preceding : List [ Span ], sub_following : List [ Span ], ) -> None : \"\"\" Annotate entities using preceding and following negations. Parameters ---------- ent : Span Entity to annotate sub_preceding : List[Span] List of preceding negations cues sub_following : List[Span] List of following negations cues \"\"\" if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] negation = ent . _ . negation or bool ( cues ) ent . _ . negation = negation if self . explain and negation : ent . _ . negation_cues += cues if not self . on_ents_only and negation : for token in ent : token . _ . negation = True def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to negation. Parameters ---------- doc: spaCy `Doc` object Returns ------- doc: spaCy `Doc` object, annotated for negation \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) # Verbs preceding negated content sub_preceding += get_spans ( sub_matches , \"verbs_preceding\" ) # Verbs following negated content sub_following += get_spans ( sub_matches , \"verbs_following\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . negation = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : self . annotate_entity ( ent = ent , sub_preceding = sub_preceding , sub_following = sub_following , ) return doc def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc )","title":"Negation"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.defaults","text":"","title":"defaults"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.within_ents","text":"","title":"within_ents"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.__init__","text":"Source code in edsnlp/pipelines/qualifiers/negation/negation.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], termination : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , termination = termination , verbs = verbs , ) terms [ \"verbs_preceding\" ], terms [ \"verbs_following\" ] = self . load_verbs ( terms [ \"verbs\" ] ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . within_ents = within_ents self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.set_extensions","text":"Source code in edsnlp/pipelines/qualifiers/negation/negation.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 @classmethod def set_extensions ( cl ) -> None : if not Token . has_extension ( \"negation\" ): Token . set_extension ( \"negation\" , default = False ) if not Token . has_extension ( \"negated\" ): Token . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Token . has_extension ( \"negation_\" ): Token . set_extension ( \"negation_\" , getter = lambda token : \"NEG\" if token . _ . negation else \"AFF\" , ) if not Token . has_extension ( \"polarity_\" ): Token . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Span . has_extension ( \"negation\" ): Span . set_extension ( \"negation\" , default = False ) if not Span . has_extension ( \"negated\" ): Span . set_extension ( \"negated\" , getter = deprecated_getter_factory ( \"negated\" , \"negation\" ) ) if not Span . has_extension ( \"negation_cues\" ): Span . set_extension ( \"negation_cues\" , default = []) if not Span . has_extension ( \"negation_\" ): Span . set_extension ( \"negation_\" , getter = lambda span : \"NEG\" if span . _ . negation else \"AFF\" , ) if not Span . has_extension ( \"polarity_\" ): Span . set_extension ( \"polarity_\" , getter = deprecated_getter_factory ( \"polarity_\" , \"negation_\" ), ) if not Doc . has_extension ( \"negations\" ): Doc . set_extension ( \"negations\" , default = [])","title":"set_extensions()"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.load_verbs","text":"Conjugate negating verbs to specific tenses. PARAMETER DESCRIPTION verbs TYPE: List [ str ] RETURNS DESCRIPTION list_neg_verbs_preceding list_neg_verbs_following Source code in edsnlp/pipelines/qualifiers/negation/negation.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate negating verbs to specific tenses. Parameters ---------- verbs: list of negating verbs to conjugate Returns ------- list_neg_verbs_preceding: List of conjugated negating verbs preceding entities. list_neg_verbs_following: List of conjugated negating verbs following entities. \"\"\" neg_verbs = get_verbs ( verbs ) neg_verbs_preceding = neg_verbs . loc [ (( neg_verbs [ \"mode\" ] == \"Indicatif\" ) & ( neg_verbs [ \"tense\" ] == \"Pr\u00e9sent\" )) | ( neg_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) | ( neg_verbs [ \"tense\" ] == \"Infinitif Pr\u00e9sent\" ) ] neg_verbs_following = neg_verbs . loc [ neg_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ] list_neg_verbs_preceding = list ( neg_verbs_preceding [ \"term\" ] . unique ()) list_neg_verbs_following = list ( neg_verbs_following [ \"term\" ] . unique ()) return ( list_neg_verbs_preceding , list_neg_verbs_following )","title":"load_verbs()"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.annotate_entity","text":"Annotate entities using preceding and following negations. PARAMETER DESCRIPTION ent Entity to annotate TYPE: Span sub_preceding List of preceding negations cues TYPE: List[Span] sub_following List of following negations cues TYPE: List[Span] Source code in edsnlp/pipelines/qualifiers/negation/negation.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def annotate_entity ( self , ent : Span , sub_preceding : List [ Span ], sub_following : List [ Span ], ) -> None : \"\"\" Annotate entities using preceding and following negations. Parameters ---------- ent : Span Entity to annotate sub_preceding : List[Span] List of preceding negations cues sub_following : List[Span] List of following negations cues \"\"\" if self . within_ents : cues = [ m for m in sub_preceding if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] negation = ent . _ . negation or bool ( cues ) ent . _ . negation = negation if self . explain and negation : ent . _ . negation_cues += cues if not self . on_ents_only and negation : for token in ent : token . _ . negation = True","title":"annotate_entity()"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.process","text":"Finds entities related to negation. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/negation/negation.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to negation. Parameters ---------- doc: spaCy `Doc` object Returns ------- doc: spaCy `Doc` object, annotated for negation \"\"\" matches = self . get_matches ( doc ) terminations = get_spans ( matches , \"termination\" ) boundaries = self . _boundaries ( doc , terminations ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) # Verbs preceding negated content sub_preceding += get_spans ( sub_matches , \"verbs_preceding\" ) # Verbs following negated content sub_following += get_spans ( sub_matches , \"verbs_following\" ) if not sub_preceding + sub_following : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . negation = any ( m . end <= token . i for m in sub_preceding ) or any ( m . start > token . i for m in sub_following ) for ent in ents : self . annotate_entity ( ent = ent , sub_preceding = sub_preceding , sub_following = sub_following , ) return doc","title":"process()"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.__call__","text":"Source code in edsnlp/pipelines/qualifiers/negation/negation.py 280 281 def __call__ ( self , doc : Doc ) -> Doc : return self . process ( doc )","title":"__call__()"},{"location":"reference/pipelines/qualifiers/negation/patterns/","text":"edsnlp.pipelines.qualifiers.negation.patterns pseudo : List [ str ] = [ 'aucun changement' , 'aucun doute' , 'aucune h\u00e9sitation' , 'aucune diminution' , \"ne permet pas d'\" , 'ne permet pas de' , \"n'exclut pas\" , 'non n\u00e9gligeable' , \"pas d'am\u00e9lioration\" , \"pas d'augmentation\" , \"pas d'autre\" , 'pas de changement' , 'pas de diminution' , 'pas de doute' , 'pas \u00e9cart\u00e9' , 'pas \u00e9cart\u00e9e' , 'pas \u00e9cart\u00e9es' , 'pas exclu' , 'pas exclue' , 'pas exclues' , 'pas exclus' , 'pas immunis\u00e9' , 'pas immunis\u00e9e' , 'pas immunis\u00e9s' , 'pas immunis\u00e9es' , 'sans am\u00e9lioration' , 'sans aucun doute' , 'sans augmentation' , 'sans certitude' , 'sans changement' , 'sans diminution' , 'sans doute' , 'sans \u00eatre certain' ] module-attribute preceding : List [ str ] = [ '\u00e0 la place de' , 'absence' , 'aucun' , 'aucune' , 'aucunes' , 'aucuns' , 'd\u00e9cline' , 'd\u00e9clin\u00e9' , 'd\u00e9pourvu' , 'd\u00e9pourvue' , 'd\u00e9pourvues' , 'd\u00e9pourvus' , 'disparition de' , 'disparition des' , '\u00e9limination' , 'exclusion' , 'excluent' , 'exclut' , 'impossibilit\u00e9 de' , 'immunis\u00e9' , 'immunis\u00e9e' , 'immunis\u00e9s' , 'immunis\u00e9es' , 'incompatible avec' , 'incompatibles avec' , 'infirmation de' , 'infirmation du' , 'jamais' , 'n\u00e9gatif pour' , 'ni' , 'niant' , 'nie' , 'ni\u00e9' , 'non' , 'nullement' , 'pas' , \"permet d'exclure\" , \"peu d'argument en\" , \"peu d'argument pour\" , \"peu d'arguments en\" , \"peu d'arguments pour\" , \"plus d'aspect de\" , 'sans' , 'sympt\u00f4me atypique' ] module-attribute following : List [ str ] = [ ':0' , ': 0' , ':non' , ': non' , 'absent' , 'absente' , 'absentes' , 'absents' , 'd\u00e9pourvu' , 'd\u00e9pourvue' , 'd\u00e9pourvues' , 'd\u00e9pourvus' , 'disparaissent' , 'disparait' , '\u00e9limination' , 'est exclu' , 'est exclue' , 'immunis\u00e9' , 'immunis\u00e9e' , 'immunis\u00e9s' , 'immunis\u00e9es' , 'impossible' , 'improbable' , 'n\u00e9gatif' , 'n\u00e9gatifs' , 'n\u00e9gative' , 'n\u00e9gatives' , 'n\u00e9gligeable' , 'n\u00e9gligeables' , 'ni\u00e9' , 'ni\u00e9e' , 'non' , 'pas n\u00e9cessaire' , 'peu probable' , 'sont exclues' , 'sont exclus' ] module-attribute verbs : List [ str ] = [ '\u00e9carter' , '\u00e9liminer' , 'exclure' , 'infirmer' , 'interdire' , 'nier' , 'r\u00e9futer' , 'rejeter' ] module-attribute","title":"patterns"},{"location":"reference/pipelines/qualifiers/negation/patterns/#edsnlppipelinesqualifiersnegationpatterns","text":"","title":"edsnlp.pipelines.qualifiers.negation.patterns"},{"location":"reference/pipelines/qualifiers/negation/patterns/#edsnlp.pipelines.qualifiers.negation.patterns.pseudo","text":"","title":"pseudo"},{"location":"reference/pipelines/qualifiers/negation/patterns/#edsnlp.pipelines.qualifiers.negation.patterns.preceding","text":"","title":"preceding"},{"location":"reference/pipelines/qualifiers/negation/patterns/#edsnlp.pipelines.qualifiers.negation.patterns.following","text":"","title":"following"},{"location":"reference/pipelines/qualifiers/negation/patterns/#edsnlp.pipelines.qualifiers.negation.patterns.verbs","text":"","title":"verbs"},{"location":"reference/pipelines/qualifiers/reported_speech/","text":"edsnlp.pipelines.qualifiers.reported_speech","title":"`edsnlp.pipelines.qualifiers.reported_speech`"},{"location":"reference/pipelines/qualifiers/reported_speech/#edsnlppipelinesqualifiersreported_speech","text":"","title":"edsnlp.pipelines.qualifiers.reported_speech"},{"location":"reference/pipelines/qualifiers/reported_speech/factory/","text":"edsnlp.pipelines.qualifiers.reported_speech.factory DEFAULT_CONFIG = dict ( pseudo = None , preceding = None , following = None , quotation = None , verbs = None , attr = 'NORM' , on_ents_only = True , within_ents = False , explain = False ) module-attribute create_component ( nlp , name , attr , pseudo , preceding , following , quotation , verbs , on_ents_only , within_ents , explain ) Source code in edsnlp/pipelines/qualifiers/reported_speech/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @deprecated_factory ( \"rspeech\" , \"eds.reported_speech\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.reported_speech\" ], ) @deprecated_factory ( \"reported_speech\" , \"eds.reported_speech\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.reported_speech\" ], ) @Language . factory ( \"eds.reported_speech\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.reported_speech\" ], ) def create_component ( nlp : Language , name : str , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], quotation : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): return ReportedSpeech ( nlp = nlp , attr = attr , pseudo = pseudo , preceding = preceding , following = following , quotation = quotation , verbs = verbs , on_ents_only = on_ents_only , within_ents = within_ents , explain = explain , )","title":"factory"},{"location":"reference/pipelines/qualifiers/reported_speech/factory/#edsnlppipelinesqualifiersreported_speechfactory","text":"","title":"edsnlp.pipelines.qualifiers.reported_speech.factory"},{"location":"reference/pipelines/qualifiers/reported_speech/factory/#edsnlp.pipelines.qualifiers.reported_speech.factory.DEFAULT_CONFIG","text":"","title":"DEFAULT_CONFIG"},{"location":"reference/pipelines/qualifiers/reported_speech/factory/#edsnlp.pipelines.qualifiers.reported_speech.factory.create_component","text":"Source code in edsnlp/pipelines/qualifiers/reported_speech/factory.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @deprecated_factory ( \"rspeech\" , \"eds.reported_speech\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.reported_speech\" ], ) @deprecated_factory ( \"reported_speech\" , \"eds.reported_speech\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.reported_speech\" ], ) @Language . factory ( \"eds.reported_speech\" , default_config = DEFAULT_CONFIG , assigns = [ \"span._.reported_speech\" ], ) def create_component ( nlp : Language , name : str , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], quotation : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): return ReportedSpeech ( nlp = nlp , attr = attr , pseudo = pseudo , preceding = preceding , following = following , quotation = quotation , verbs = verbs , on_ents_only = on_ents_only , within_ents = within_ents , explain = explain , )","title":"create_component()"},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/","text":"edsnlp.pipelines.qualifiers.reported_speech.patterns verbs : List [ str ] = [ 'affirmer' , 'ajouter' , 'assurer' , 'confirmer' , 'demander' , 'dire' , 'd\u00e9clarer' , 'd\u00e9crire' , 'd\u00e9crire' , 'd\u00e9montrer' , 'expliquer' , 'faire remarquer' , 'indiquer' , 'informer' , 'insinuer' , 'insister' , 'jurer' , 'nier' , 'nier' , 'noter' , 'objecter' , 'observer' , 'parler' , 'promettre' , 'pr\u00e9ciser' , 'pr\u00e9tendre' , 'pr\u00e9venir' , 'raconter' , 'rappeler' , 'rapporter' , 'reconna\u00eetre' , 'r\u00e9futer' , 'r\u00e9pliquer' , 'r\u00e9pondre' , 'r\u00e9p\u00e9ter' , 'r\u00e9v\u00e9ler' , 'se plaindre' , 'souhaiter' , 'souligner' , 'supplier' , 'verbaliser' , 'vouloir' , 'vouloir' ] module-attribute following : List [ str ] = [ \"d'apr\u00e8s le patient\" , \"d'apr\u00e8s la patiente\" ] module-attribute preceding : List [ str ] = [ 'pas de critique de' , 'crainte de' , 'menace de' , 'insiste sur le fait que' , \"d'apr\u00e8s le patient\" , \"d'apr\u00e8s la patiente\" , 'peur de' ] module-attribute quotation : str = '( \\\\ \".+ \\\\ \")|( \\\\ \u00ab.+ \\\\ \u00bb)' module-attribute","title":"patterns"},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/#edsnlppipelinesqualifiersreported_speechpatterns","text":"","title":"edsnlp.pipelines.qualifiers.reported_speech.patterns"},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/#edsnlp.pipelines.qualifiers.reported_speech.patterns.verbs","text":"","title":"verbs"},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/#edsnlp.pipelines.qualifiers.reported_speech.patterns.following","text":"","title":"following"},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/#edsnlp.pipelines.qualifiers.reported_speech.patterns.preceding","text":"","title":"preceding"},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/#edsnlp.pipelines.qualifiers.reported_speech.patterns.quotation","text":"","title":"quotation"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/","text":"edsnlp.pipelines.qualifiers.reported_speech.reported_speech ReportedSpeech Bases: Qualifier Implements a reported speech detection algorithm. The components looks for terms indicating patient statements, and quotations to detect patient speech. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language quotation String gathering all quotation cues. TYPE: str verbs List of reported speech verbs. TYPE: List[str] following List of terms following a reported speech. TYPE: List[str] preceding List of terms preceding a reported speech. TYPE: List[str] filter_matches Whether to filter out overlapping matches. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class ReportedSpeech ( Qualifier ): \"\"\" Implements a reported speech detection algorithm. The components looks for terms indicating patient statements, and quotations to detect patient speech. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. quotation : str String gathering all quotation cues. verbs : List[str] List of reported speech verbs. following : List[str] List of terms following a reported speech. preceding : List[str] List of terms preceding a reported speech. filter_matches : bool Whether to filter out overlapping matches. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( following = following , preceding = preceding , verbs = verbs , quotation = quotation , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], quotation : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , quotation = quotation , verbs = verbs , ) terms [ \"verbs\" ] = self . load_verbs ( terms [ \"verbs\" ]) quotation = terms . pop ( \"quotation\" ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . regex_matcher = RegexMatcher ( attr = attr ) self . regex_matcher . build_patterns ( dict ( quotation = quotation )) self . within_ents = within_ents self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"reported_speech\" ): Token . set_extension ( \"reported_speech\" , default = False ) if not Token . has_extension ( \"reported_speech_\" ): Token . set_extension ( \"reported_speech_\" , getter = lambda token : \"REPORTED\" if token . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech\" ): Span . set_extension ( \"reported_speech\" , default = False ) if not Span . has_extension ( \"reported_speech_\" ): Span . set_extension ( \"reported_speech_\" , getter = lambda span : \"REPORTED\" if span . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech_cues\" ): Span . set_extension ( \"reported_speech_cues\" , default = []) if not Doc . has_extension ( \"rspeechs\" ): Doc . set_extension ( \"rspeechs\" , default = []) def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate reporting verbs to specific tenses (trhid person) Parameters ---------- verbs: list of reporting verbs to conjugate Returns ------- list_rep_verbs: List of reporting verbs conjugated to specific tenses. \"\"\" rep_verbs = get_verbs ( verbs ) rep_verbs = rep_verbs . loc [ ( ( rep_verbs [ \"mode\" ] == \"Indicatif\" ) & ( rep_verbs [ \"tense\" ] == \"Pr\u00e9sent\" ) & ( rep_verbs [ \"person\" ] . isin ([ \"3s\" , \"3p\" ])) ) | ( rep_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( rep_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_rep_verbs = list ( rep_verbs [ \"term\" ] . unique ()) return list_rep_verbs def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to reported speech. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for negation \"\"\" matches = self . get_matches ( doc ) matches += list ( self . regex_matcher ( doc , as_spans = True )) boundaries = self . _boundaries ( doc ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) sub_quotation = get_spans ( sub_matches , \"quotation\" ) if not sub_preceding + sub_following + sub_verbs + sub_quotation : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . reported_speech = ( any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) or any ( (( m . start < token . i ) & ( m . end > token . i + 1 )) for m in sub_quotation ) ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] cues += [ m for m in sub_quotation if ( m . start < ent . start ) & ( m . end > ent . end ) ] reported_speech = ent . _ . reported_speech or bool ( cues ) ent . _ . reported_speech = reported_speech if self . explain : ent . _ . reported_speech_cues += cues if not self . on_ents_only and reported_speech : for token in ent : token . _ . reported_speech = True return doc defaults = dict ( following = following , preceding = preceding , verbs = verbs , quotation = quotation ) class-attribute regex_matcher = RegexMatcher ( attr = attr ) instance-attribute within_ents = within_ents instance-attribute __init__ ( nlp , attr , pseudo , preceding , following , quotation , verbs , on_ents_only , within_ents , explain ) Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], quotation : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , quotation = quotation , verbs = verbs , ) terms [ \"verbs\" ] = self . load_verbs ( terms [ \"verbs\" ]) quotation = terms . pop ( \"quotation\" ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . regex_matcher = RegexMatcher ( attr = attr ) self . regex_matcher . build_patterns ( dict ( quotation = quotation )) self . within_ents = within_ents self . set_extensions () set_extensions () Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"reported_speech\" ): Token . set_extension ( \"reported_speech\" , default = False ) if not Token . has_extension ( \"reported_speech_\" ): Token . set_extension ( \"reported_speech_\" , getter = lambda token : \"REPORTED\" if token . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech\" ): Span . set_extension ( \"reported_speech\" , default = False ) if not Span . has_extension ( \"reported_speech_\" ): Span . set_extension ( \"reported_speech_\" , getter = lambda span : \"REPORTED\" if span . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech_cues\" ): Span . set_extension ( \"reported_speech_cues\" , default = []) if not Doc . has_extension ( \"rspeechs\" ): Doc . set_extension ( \"rspeechs\" , default = []) load_verbs ( verbs ) Conjugate reporting verbs to specific tenses (trhid person) PARAMETER DESCRIPTION verbs TYPE: List [ str ] RETURNS DESCRIPTION list_rep_verbs Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate reporting verbs to specific tenses (trhid person) Parameters ---------- verbs: list of reporting verbs to conjugate Returns ------- list_rep_verbs: List of reporting verbs conjugated to specific tenses. \"\"\" rep_verbs = get_verbs ( verbs ) rep_verbs = rep_verbs . loc [ ( ( rep_verbs [ \"mode\" ] == \"Indicatif\" ) & ( rep_verbs [ \"tense\" ] == \"Pr\u00e9sent\" ) & ( rep_verbs [ \"person\" ] . isin ([ \"3s\" , \"3p\" ])) ) | ( rep_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( rep_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_rep_verbs = list ( rep_verbs [ \"term\" ] . unique ()) return list_rep_verbs process ( doc ) Finds entities related to reported speech. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to reported speech. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for negation \"\"\" matches = self . get_matches ( doc ) matches += list ( self . regex_matcher ( doc , as_spans = True )) boundaries = self . _boundaries ( doc ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) sub_quotation = get_spans ( sub_matches , \"quotation\" ) if not sub_preceding + sub_following + sub_verbs + sub_quotation : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . reported_speech = ( any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) or any ( (( m . start < token . i ) & ( m . end > token . i + 1 )) for m in sub_quotation ) ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] cues += [ m for m in sub_quotation if ( m . start < ent . start ) & ( m . end > ent . end ) ] reported_speech = ent . _ . reported_speech or bool ( cues ) ent . _ . reported_speech = reported_speech if self . explain : ent . _ . reported_speech_cues += cues if not self . on_ents_only and reported_speech : for token in ent : token . _ . reported_speech = True return doc","title":"reported_speech"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlppipelinesqualifiersreported_speechreported_speech","text":"","title":"edsnlp.pipelines.qualifiers.reported_speech.reported_speech"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech","text":"Bases: Qualifier Implements a reported speech detection algorithm. The components looks for terms indicating patient statements, and quotations to detect patient speech. PARAMETER DESCRIPTION nlp spaCy nlp pipeline to use for matching. TYPE: Language quotation String gathering all quotation cues. TYPE: str verbs List of reported speech verbs. TYPE: List[str] following List of terms following a reported speech. TYPE: List[str] preceding List of terms preceding a reported speech. TYPE: List[str] filter_matches Whether to filter out overlapping matches. TYPE: bool attr spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. TYPE: str on_ents_only Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. TYPE: bool within_ents Whether to consider cues within entities. TYPE: bool explain Whether to keep track of cues for each entity. TYPE: bool Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class ReportedSpeech ( Qualifier ): \"\"\" Implements a reported speech detection algorithm. The components looks for terms indicating patient statements, and quotations to detect patient speech. Parameters ---------- nlp : Language spaCy nlp pipeline to use for matching. quotation : str String gathering all quotation cues. verbs : List[str] List of reported speech verbs. following : List[str] List of terms following a reported speech. preceding : List[str] List of terms preceding a reported speech. filter_matches : bool Whether to filter out overlapping matches. attr : str spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex. on_ents_only : bool Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks. within_ents : bool Whether to consider cues within entities. explain : bool Whether to keep track of cues for each entity. \"\"\" defaults = dict ( following = following , preceding = preceding , verbs = verbs , quotation = quotation , ) def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], quotation : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , quotation = quotation , verbs = verbs , ) terms [ \"verbs\" ] = self . load_verbs ( terms [ \"verbs\" ]) quotation = terms . pop ( \"quotation\" ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . regex_matcher = RegexMatcher ( attr = attr ) self . regex_matcher . build_patterns ( dict ( quotation = quotation )) self . within_ents = within_ents self . set_extensions () @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"reported_speech\" ): Token . set_extension ( \"reported_speech\" , default = False ) if not Token . has_extension ( \"reported_speech_\" ): Token . set_extension ( \"reported_speech_\" , getter = lambda token : \"REPORTED\" if token . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech\" ): Span . set_extension ( \"reported_speech\" , default = False ) if not Span . has_extension ( \"reported_speech_\" ): Span . set_extension ( \"reported_speech_\" , getter = lambda span : \"REPORTED\" if span . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech_cues\" ): Span . set_extension ( \"reported_speech_cues\" , default = []) if not Doc . has_extension ( \"rspeechs\" ): Doc . set_extension ( \"rspeechs\" , default = []) def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate reporting verbs to specific tenses (trhid person) Parameters ---------- verbs: list of reporting verbs to conjugate Returns ------- list_rep_verbs: List of reporting verbs conjugated to specific tenses. \"\"\" rep_verbs = get_verbs ( verbs ) rep_verbs = rep_verbs . loc [ ( ( rep_verbs [ \"mode\" ] == \"Indicatif\" ) & ( rep_verbs [ \"tense\" ] == \"Pr\u00e9sent\" ) & ( rep_verbs [ \"person\" ] . isin ([ \"3s\" , \"3p\" ])) ) | ( rep_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( rep_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_rep_verbs = list ( rep_verbs [ \"term\" ] . unique ()) return list_rep_verbs def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to reported speech. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for negation \"\"\" matches = self . get_matches ( doc ) matches += list ( self . regex_matcher ( doc , as_spans = True )) boundaries = self . _boundaries ( doc ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) sub_quotation = get_spans ( sub_matches , \"quotation\" ) if not sub_preceding + sub_following + sub_verbs + sub_quotation : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . reported_speech = ( any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) or any ( (( m . start < token . i ) & ( m . end > token . i + 1 )) for m in sub_quotation ) ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] cues += [ m for m in sub_quotation if ( m . start < ent . start ) & ( m . end > ent . end ) ] reported_speech = ent . _ . reported_speech or bool ( cues ) ent . _ . reported_speech = reported_speech if self . explain : ent . _ . reported_speech_cues += cues if not self . on_ents_only and reported_speech : for token in ent : token . _ . reported_speech = True return doc","title":"ReportedSpeech"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.defaults","text":"","title":"defaults"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.regex_matcher","text":"","title":"regex_matcher"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.within_ents","text":"","title":"within_ents"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.__init__","text":"Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , nlp : Language , attr : str , pseudo : Optional [ List [ str ]], preceding : Optional [ List [ str ]], following : Optional [ List [ str ]], quotation : Optional [ List [ str ]], verbs : Optional [ List [ str ]], on_ents_only : bool , within_ents : bool , explain : bool , ): terms = self . get_defaults ( pseudo = pseudo , preceding = preceding , following = following , quotation = quotation , verbs = verbs , ) terms [ \"verbs\" ] = self . load_verbs ( terms [ \"verbs\" ]) quotation = terms . pop ( \"quotation\" ) super () . __init__ ( nlp = nlp , attr = attr , on_ents_only = on_ents_only , explain = explain , ** terms , ) self . regex_matcher = RegexMatcher ( attr = attr ) self . regex_matcher . build_patterns ( dict ( quotation = quotation )) self . within_ents = within_ents self . set_extensions ()","title":"__init__()"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.set_extensions","text":"Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @classmethod def set_extensions ( cls ) -> None : if not Token . has_extension ( \"reported_speech\" ): Token . set_extension ( \"reported_speech\" , default = False ) if not Token . has_extension ( \"reported_speech_\" ): Token . set_extension ( \"reported_speech_\" , getter = lambda token : \"REPORTED\" if token . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech\" ): Span . set_extension ( \"reported_speech\" , default = False ) if not Span . has_extension ( \"reported_speech_\" ): Span . set_extension ( \"reported_speech_\" , getter = lambda span : \"REPORTED\" if span . _ . reported_speech else \"DIRECT\" , ) if not Span . has_extension ( \"reported_speech_cues\" ): Span . set_extension ( \"reported_speech_cues\" , default = []) if not Doc . has_extension ( \"rspeechs\" ): Doc . set_extension ( \"rspeechs\" , default = [])","title":"set_extensions()"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.load_verbs","text":"Conjugate reporting verbs to specific tenses (trhid person) PARAMETER DESCRIPTION verbs TYPE: List [ str ] RETURNS DESCRIPTION list_rep_verbs Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def load_verbs ( self , verbs : List [ str ]) -> List [ str ]: \"\"\" Conjugate reporting verbs to specific tenses (trhid person) Parameters ---------- verbs: list of reporting verbs to conjugate Returns ------- list_rep_verbs: List of reporting verbs conjugated to specific tenses. \"\"\" rep_verbs = get_verbs ( verbs ) rep_verbs = rep_verbs . loc [ ( ( rep_verbs [ \"mode\" ] == \"Indicatif\" ) & ( rep_verbs [ \"tense\" ] == \"Pr\u00e9sent\" ) & ( rep_verbs [ \"person\" ] . isin ([ \"3s\" , \"3p\" ])) ) | ( rep_verbs [ \"tense\" ] == \"Participe Pr\u00e9sent\" ) | ( rep_verbs [ \"tense\" ] == \"Participe Pass\u00e9\" ) ] list_rep_verbs = list ( rep_verbs [ \"term\" ] . unique ()) return list_rep_verbs","title":"load_verbs()"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.process","text":"Finds entities related to reported speech. PARAMETER DESCRIPTION doc TYPE: Doc RETURNS DESCRIPTION doc Source code in edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def process ( self , doc : Doc ) -> Doc : \"\"\" Finds entities related to reported speech. Parameters ---------- doc: spaCy Doc object Returns ------- doc: spaCy Doc object, annotated for negation \"\"\" matches = self . get_matches ( doc ) matches += list ( self . regex_matcher ( doc , as_spans = True )) boundaries = self . _boundaries ( doc ) entities = list ( doc . ents ) + list ( doc . spans . get ( \"discarded\" , [])) ents = None # Removes duplicate matches and pseudo-expressions in one statement matches = filter_spans ( matches , label_to_remove = \"pseudo\" ) for start , end in boundaries : ents , entities = consume_spans ( entities , filter = lambda s : check_inclusion ( s , start , end ), second_chance = ents , ) sub_matches , matches = consume_spans ( matches , lambda s : start <= s . start < end ) if self . on_ents_only and not ents : continue sub_preceding = get_spans ( sub_matches , \"preceding\" ) sub_following = get_spans ( sub_matches , \"following\" ) sub_verbs = get_spans ( sub_matches , \"verbs\" ) sub_quotation = get_spans ( sub_matches , \"quotation\" ) if not sub_preceding + sub_following + sub_verbs + sub_quotation : continue if not self . on_ents_only : for token in doc [ start : end ]: token . _ . reported_speech = ( any ( m . end <= token . i for m in sub_preceding + sub_verbs ) or any ( m . start > token . i for m in sub_following ) or any ( (( m . start < token . i ) & ( m . end > token . i + 1 )) for m in sub_quotation ) ) for ent in ents : if self . within_ents : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . end ] cues += [ m for m in sub_following if m . start >= ent . start ] else : cues = [ m for m in sub_preceding + sub_verbs if m . end <= ent . start ] cues += [ m for m in sub_following if m . start >= ent . end ] cues += [ m for m in sub_quotation if ( m . start < ent . start ) & ( m . end > ent . end ) ] reported_speech = ent . _ . reported_speech or bool ( cues ) ent . _ . reported_speech = reported_speech if self . explain : ent . _ . reported_speech_cues += cues if not self . on_ents_only and reported_speech : for token in ent : token . _ . reported_speech = True return doc","title":"process()"},{"location":"reference/pipelines/trainable/","text":"edsnlp.pipelines.trainable","title":"`edsnlp.pipelines.trainable`"},{"location":"reference/pipelines/trainable/#edsnlppipelinestrainable","text":"","title":"edsnlp.pipelines.trainable"},{"location":"reference/pipelines/trainable/nested_ner/","text":"edsnlp.pipelines.trainable.nested_ner msg = Printer () module-attribute NUM_INITIALIZATION_EXAMPLES = 1000 module-attribute nested_ner_default_config = ' \\n [model] \\n @architectures = \"eds.stack_crf_ner_model.v1\" \\n mode = \"joint\" \\n\\n [model.tok2vec] \\n @architectures = \"spacy.Tok2Vec.v1\" \\n\\n [model.tok2vec.embed] \\n @architectures = \"spacy.MultiHashEmbed.v1\" \\n width = 96 \\n rows = [5000, 2000, 1000, 1000] \\n attrs = [\"ORTH\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"] \\n include_static_vectors = false \\n\\n [model.tok2vec.encode] \\n @architectures = \"spacy.MaxoutWindowEncoder.v1\" \\n width = $ {model.tok2vec.embed.width} \\n window_size = 1 \\n maxout_pieces = 3 \\n depth = 4 \\n\\n [scorer] \\n @scorers = \"eds.nested_ner_scorer.v1\" \\n ' module-attribute NESTED_NER_DEFAULTS = Config () . from_str ( nested_ner_default_config ) module-attribute np_ops = NumpyOps () module-attribute TrainableNer Bases: TrainablePipe Source code in edsnlp/pipelines/trainable/nested_ner.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 class TrainableNer ( TrainablePipe ): def __init__ ( self , vocab : Vocab , model : Model , name : str = \"nested_ner\" , ent_labels : Iterable [ str ] = (), spans_labels : Mapping [ str , Iterable [ str ]] = None , scorer : Optional [ Callable ] = None , ) -> None : \"\"\" Initialize a general named entity recognizer (with or without nested or overlapping entities). Parameters ---------- vocab: Vocab Spacy vocabulary model: Model The model to extract the spans name: str Name of the component ent_labels: Iterable[str] list of labels to filter entities for in `doc.ents` spans_labels: Mapping[str, Iterable[str]] Mapping from span group names to list of labels to look for entities and assign the predicted entities scorer: Optional[Callable] Method to call to score predictions \"\"\" super () . __init__ ( vocab , model , name ) self . cfg [ \"ent_labels\" ]: Optional [ Tuple [ str ]] = ( tuple ( ent_labels ) if ent_labels is not None else None ) self . cfg [ \"spans_labels\" ]: Optional [ Dict [ str , Tuple [ str ]]] = ( { k : tuple ( labels ) for k , labels in spans_labels . items ()} if spans_labels is not None else None ) self . cfg [ \"labels\" ] = tuple ( sorted ( set ( ( list ( ent_labels ) if ent_labels is not None else []) + [ label for group in ( spans_labels or {}) . values () for label in group ] ) ) ) self . scorer = scorer @property def labels ( self ) -> Tuple [ str ]: \"\"\"Return the labels currently added to the component.\"\"\" return self . cfg [ \"labels\" ] @property def spans_labels ( self ) -> Dict [ str , Tuple [ str ]]: \"\"\"Return the span group to labels filters mapping\"\"\" return self . cfg [ \"spans_labels\" ] @property def ent_labels ( self ): \"\"\"Return the doc.ents labels filters\"\"\" return self . cfg [ \"ent_labels\" ] def add_label ( self , label : str ) -> int : \"\"\"Add a new label to the pipe.\"\"\" raise Exception ( \"Cannot add a new label to the pipe\" ) def predict ( self , docs : List [ Doc ]) -> Dict [ str , Ints2d ]: \"\"\" Apply the pipeline's model to a batch of docs, without modifying them. Parameters ---------- docs: List[Doc] Returns ------- Int2d The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor that contain the spans' prediction for all the batch \"\"\" return self . model . predict (( docs , None , True ))[ 1 ] def set_annotations ( self , docs : List [ Doc ], predictions : Dict [ str , Ints2d ], ** kwargs ) -> None : \"\"\" Modify a batch of `Doc` objects, using predicted spans. Parameters ---------- docs: List[Doc] The documents to update predictions: Spans predictions, as returned by the model's predict method \"\"\" docs = list ( docs ) new_doc_spans : List [ List [ Span ]] = [[] for _ in docs ] for doc_idx , label_idx , begin , end in np_ops . asarray ( predictions . get ( \"spans\" )): label = self . labels [ label_idx ] new_doc_spans [ doc_idx ] . append ( Span ( docs [ doc_idx ], begin , end , label )) for doc , new_spans in zip ( docs , new_doc_spans ): # Only add a span to `doc.ents` if its label is in `self.ents_labels` doc . ents = filter_spans ( [ s for s in new_spans if s . label_ in self . ent_labels ] ) # Only add a span to `doc.spans[name]` if its label is in the matching # `self.spans_labels[name]` list for name , group_labels in self . spans_labels . items (): doc . spans [ name ] = [ s for s in new_spans if s . label_ in group_labels ] def update ( self , examples : Iterable [ Example ], * , drop : float = 0.0 , set_annotations : bool = False , sgd : Optional [ Optimizer ] = None , losses : Optional [ Dict [ str , float ]] = None , ) -> Dict [ str , float ]: \"\"\" Learn from a batch of documents and gold-standard information, updating the pipe's model. Delegates to begin_update and get_loss. Unlike standard TrainablePipe components, the discrete ops (best selection of tags) is performed by the model directly (`begin_update` returns the loss and the predictions) Parameters ---------- examples: Iterable[Example] drop: float = 0.0 set_annotations: bool Whether to update the document with predicted spans sgd: Optional[Optimizer] Optimizer losses: Optional[Dict[str, float]] Dict of loss, updated in place Returns ------- Dict[str, float] Updated losses dict \"\"\" if losses is None : losses = {} losses . setdefault ( self . name , 0.0 ) set_dropout_rate ( self . model , drop ) examples = list ( examples ) # run the model docs = [ eg . predicted for eg in examples ] gold = self . examples_to_truth ( examples ) ( loss , predictions ), backprop = self . model . begin_update ( ( docs , gold , set_annotations ) ) loss , gradient = self . get_loss ( examples , loss ) backprop ( gradient ) if sgd is not None : self . model . finish_update ( sgd ) if set_annotations : self . set_annotations ( docs , predictions ) losses [ self . name ] = loss return loss def get_loss ( self , examples : Iterable [ Example ], loss ) -> Tuple [ float , float ]: \"\"\"Find the loss and gradient of loss for the batch of documents and their predicted scores.\"\"\" return float ( loss . item ()), self . model . ops . xp . array ([ 1 ]) def initialize ( self , get_examples : Callable [[], Iterable [ Example ]], * , nlp : Language = None , labels : Optional [ List [ str ]] = None , ): \"\"\" Initialize the pipe for training, using a representative set of data examples. 1. If no ent_labels are provided, we scrap them from the ents of the set of examples. 2. If no span labels are provided, we scrap them from the spans of the set of examples, and filter these labels with the ents_labels. Parameters ---------- get_examples: Callable[[], Iterable[Example]] Method to sample some examples nlp: spacy.Language Unused spacy model labels Unused list of labels \"\"\" sub_batch = list ( islice ( get_examples (), NUM_INITIALIZATION_EXAMPLES )) if self . ent_labels is None or self . spans_labels is None : ent_labels_before = self . ent_labels if self . ent_labels is None : self . cfg [ \"ent_labels\" ] = tuple ( sorted ( { span . label_ for doc in sub_batch for span in doc . reference . ents } ) ) if self . spans_labels is None : spans_labels = defaultdict ( lambda : set ()) for doc in sub_batch : for name , group in doc . reference . spans . items (): for span in group : if ( ent_labels_before is None or span . label_ in ent_labels_before ): spans_labels [ name ] . add ( span . label_ ) self . cfg [ \"spans_labels\" ] = { name : tuple ( sorted ( group )) for name , group in spans_labels . items () } self . cfg [ \"labels\" ] = tuple ( sorted ( set ( list ( self . ent_labels ) + [ label for group in self . spans_labels . values () for label in group ] ) ) ) doc_sample = [ eg . reference for eg in sub_batch ] spans_sample = self . examples_to_truth ( sub_batch ) if spans_sample is None : raise ValueError ( \"Call begin_training with relevant entities \" \"and relations annotated in \" \"at least a few reference examples!\" ) self . model . attrs [ \"set_n_labels\" ]( len ( self . labels )) self . model . initialize ( X = doc_sample , Y = spans_sample ) def examples_to_truth ( self , examples : List [ Example ]) -> Ints2d : \"\"\" Converts the spans of the examples into a list of (doc_idx, label_idx, begin, end) tuple as a tensor, that will be fed to the model with the `begin_update` method. Parameters ---------- examples: List[Example] Returns ------- Ints2d \"\"\" label_vocab = { self . vocab . strings [ l ]: i for i , l in enumerate ( self . labels )} spans = set () for eg_idx , eg in enumerate ( examples ): for span in ( * eg . reference . ents , * ( span for name in ( self . spans_labels if self . spans_labels is not None else eg . reference . spans ) for span in eg . reference . spans . get ( name , ()) ), ): label_idx = label_vocab . get ( span . label ) if label_idx is None : continue spans . add (( eg_idx , label_idx , span . start , span . end )) truths = self . model . ops . asarray ( list ( spans )) return truths scorer = scorer instance-attribute __init__ ( vocab , model , name = 'nested_ner' , ent_labels = (), spans_labels = None , scorer = None ) Initialize a general named entity recognizer (with or without nested or overlapping entities). PARAMETER DESCRIPTION vocab Spacy vocabulary TYPE: Vocab model The model to extract the spans TYPE: Model name Name of the component TYPE: str DEFAULT: 'nested_ner' ent_labels list of labels to filter entities for in doc.ents TYPE: Iterable [ str ] DEFAULT: () spans_labels Mapping from span group names to list of labels to look for entities and assign the predicted entities TYPE: Mapping [ str , Iterable [ str ]] DEFAULT: None scorer Method to call to score predictions TYPE: Optional [ Callable ] DEFAULT: None Source code in edsnlp/pipelines/trainable/nested_ner.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def __init__ ( self , vocab : Vocab , model : Model , name : str = \"nested_ner\" , ent_labels : Iterable [ str ] = (), spans_labels : Mapping [ str , Iterable [ str ]] = None , scorer : Optional [ Callable ] = None , ) -> None : \"\"\" Initialize a general named entity recognizer (with or without nested or overlapping entities). Parameters ---------- vocab: Vocab Spacy vocabulary model: Model The model to extract the spans name: str Name of the component ent_labels: Iterable[str] list of labels to filter entities for in `doc.ents` spans_labels: Mapping[str, Iterable[str]] Mapping from span group names to list of labels to look for entities and assign the predicted entities scorer: Optional[Callable] Method to call to score predictions \"\"\" super () . __init__ ( vocab , model , name ) self . cfg [ \"ent_labels\" ]: Optional [ Tuple [ str ]] = ( tuple ( ent_labels ) if ent_labels is not None else None ) self . cfg [ \"spans_labels\" ]: Optional [ Dict [ str , Tuple [ str ]]] = ( { k : tuple ( labels ) for k , labels in spans_labels . items ()} if spans_labels is not None else None ) self . cfg [ \"labels\" ] = tuple ( sorted ( set ( ( list ( ent_labels ) if ent_labels is not None else []) + [ label for group in ( spans_labels or {}) . values () for label in group ] ) ) ) self . scorer = scorer labels () Return the labels currently added to the component. Source code in edsnlp/pipelines/trainable/nested_ner.py 207 208 209 210 @property def labels ( self ) -> Tuple [ str ]: \"\"\"Return the labels currently added to the component.\"\"\" return self . cfg [ \"labels\" ] spans_labels () Return the span group to labels filters mapping Source code in edsnlp/pipelines/trainable/nested_ner.py 212 213 214 215 @property def spans_labels ( self ) -> Dict [ str , Tuple [ str ]]: \"\"\"Return the span group to labels filters mapping\"\"\" return self . cfg [ \"spans_labels\" ] ent_labels () Return the doc.ents labels filters Source code in edsnlp/pipelines/trainable/nested_ner.py 217 218 219 220 @property def ent_labels ( self ): \"\"\"Return the doc.ents labels filters\"\"\" return self . cfg [ \"ent_labels\" ] add_label ( label ) Add a new label to the pipe. Source code in edsnlp/pipelines/trainable/nested_ner.py 222 223 224 def add_label ( self , label : str ) -> int : \"\"\"Add a new label to the pipe.\"\"\" raise Exception ( \"Cannot add a new label to the pipe\" ) predict ( docs ) Apply the pipeline's model to a batch of docs, without modifying them. PARAMETER DESCRIPTION docs TYPE: List [ Doc ] RETURNS DESCRIPTION Int2d The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor that contain the spans' prediction for all the batch Source code in edsnlp/pipelines/trainable/nested_ner.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def predict ( self , docs : List [ Doc ]) -> Dict [ str , Ints2d ]: \"\"\" Apply the pipeline's model to a batch of docs, without modifying them. Parameters ---------- docs: List[Doc] Returns ------- Int2d The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor that contain the spans' prediction for all the batch \"\"\" return self . model . predict (( docs , None , True ))[ 1 ] set_annotations ( docs , predictions , ** kwargs ) Modify a batch of Doc objects, using predicted spans. PARAMETER DESCRIPTION docs The documents to update TYPE: List [ Doc ] predictions Spans predictions, as returned by the model's predict method TYPE: Dict [ str , Ints2d ] Source code in edsnlp/pipelines/trainable/nested_ner.py 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def set_annotations ( self , docs : List [ Doc ], predictions : Dict [ str , Ints2d ], ** kwargs ) -> None : \"\"\" Modify a batch of `Doc` objects, using predicted spans. Parameters ---------- docs: List[Doc] The documents to update predictions: Spans predictions, as returned by the model's predict method \"\"\" docs = list ( docs ) new_doc_spans : List [ List [ Span ]] = [[] for _ in docs ] for doc_idx , label_idx , begin , end in np_ops . asarray ( predictions . get ( \"spans\" )): label = self . labels [ label_idx ] new_doc_spans [ doc_idx ] . append ( Span ( docs [ doc_idx ], begin , end , label )) for doc , new_spans in zip ( docs , new_doc_spans ): # Only add a span to `doc.ents` if its label is in `self.ents_labels` doc . ents = filter_spans ( [ s for s in new_spans if s . label_ in self . ent_labels ] ) # Only add a span to `doc.spans[name]` if its label is in the matching # `self.spans_labels[name]` list for name , group_labels in self . spans_labels . items (): doc . spans [ name ] = [ s for s in new_spans if s . label_ in group_labels ] update ( examples , * , drop = 0.0 , set_annotations = False , sgd = None , losses = None ) Learn from a batch of documents and gold-standard information, updating the pipe's model. Delegates to begin_update and get_loss. Unlike standard TrainablePipe components, the discrete ops (best selection of tags) is performed by the model directly ( begin_update returns the loss and the predictions) PARAMETER DESCRIPTION examples TYPE: Iterable [ Example ] drop TYPE: float DEFAULT: 0.0 set_annotations: bool Whether to update the document with predicted spans sgd: Optional[Optimizer] Optimizer losses: Optional[Dict[str, float]] Dict of loss, updated in place RETURNS DESCRIPTION Dict[str, float] Updated losses dict Source code in edsnlp/pipelines/trainable/nested_ner.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def update ( self , examples : Iterable [ Example ], * , drop : float = 0.0 , set_annotations : bool = False , sgd : Optional [ Optimizer ] = None , losses : Optional [ Dict [ str , float ]] = None , ) -> Dict [ str , float ]: \"\"\" Learn from a batch of documents and gold-standard information, updating the pipe's model. Delegates to begin_update and get_loss. Unlike standard TrainablePipe components, the discrete ops (best selection of tags) is performed by the model directly (`begin_update` returns the loss and the predictions) Parameters ---------- examples: Iterable[Example] drop: float = 0.0 set_annotations: bool Whether to update the document with predicted spans sgd: Optional[Optimizer] Optimizer losses: Optional[Dict[str, float]] Dict of loss, updated in place Returns ------- Dict[str, float] Updated losses dict \"\"\" if losses is None : losses = {} losses . setdefault ( self . name , 0.0 ) set_dropout_rate ( self . model , drop ) examples = list ( examples ) # run the model docs = [ eg . predicted for eg in examples ] gold = self . examples_to_truth ( examples ) ( loss , predictions ), backprop = self . model . begin_update ( ( docs , gold , set_annotations ) ) loss , gradient = self . get_loss ( examples , loss ) backprop ( gradient ) if sgd is not None : self . model . finish_update ( sgd ) if set_annotations : self . set_annotations ( docs , predictions ) losses [ self . name ] = loss return loss get_loss ( examples , loss ) Find the loss and gradient of loss for the batch of documents and their predicted scores. Source code in edsnlp/pipelines/trainable/nested_ner.py 330 331 332 333 def get_loss ( self , examples : Iterable [ Example ], loss ) -> Tuple [ float , float ]: \"\"\"Find the loss and gradient of loss for the batch of documents and their predicted scores.\"\"\" return float ( loss . item ()), self . model . ops . xp . array ([ 1 ]) initialize ( get_examples , * , nlp = None , labels = None ) Initialize the pipe for training, using a representative set of data examples. If no ent_labels are provided, we scrap them from the ents of the set of examples. If no span labels are provided, we scrap them from the spans of the set of examples, and filter these labels with the ents_labels. PARAMETER DESCRIPTION get_examples Method to sample some examples TYPE: Callable [[], Iterable [ Example ]] nlp Unused spacy model TYPE: Language DEFAULT: None labels Unused list of labels TYPE: Optional [ List [ str ]] DEFAULT: None Source code in edsnlp/pipelines/trainable/nested_ner.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def initialize ( self , get_examples : Callable [[], Iterable [ Example ]], * , nlp : Language = None , labels : Optional [ List [ str ]] = None , ): \"\"\" Initialize the pipe for training, using a representative set of data examples. 1. If no ent_labels are provided, we scrap them from the ents of the set of examples. 2. If no span labels are provided, we scrap them from the spans of the set of examples, and filter these labels with the ents_labels. Parameters ---------- get_examples: Callable[[], Iterable[Example]] Method to sample some examples nlp: spacy.Language Unused spacy model labels Unused list of labels \"\"\" sub_batch = list ( islice ( get_examples (), NUM_INITIALIZATION_EXAMPLES )) if self . ent_labels is None or self . spans_labels is None : ent_labels_before = self . ent_labels if self . ent_labels is None : self . cfg [ \"ent_labels\" ] = tuple ( sorted ( { span . label_ for doc in sub_batch for span in doc . reference . ents } ) ) if self . spans_labels is None : spans_labels = defaultdict ( lambda : set ()) for doc in sub_batch : for name , group in doc . reference . spans . items (): for span in group : if ( ent_labels_before is None or span . label_ in ent_labels_before ): spans_labels [ name ] . add ( span . label_ ) self . cfg [ \"spans_labels\" ] = { name : tuple ( sorted ( group )) for name , group in spans_labels . items () } self . cfg [ \"labels\" ] = tuple ( sorted ( set ( list ( self . ent_labels ) + [ label for group in self . spans_labels . values () for label in group ] ) ) ) doc_sample = [ eg . reference for eg in sub_batch ] spans_sample = self . examples_to_truth ( sub_batch ) if spans_sample is None : raise ValueError ( \"Call begin_training with relevant entities \" \"and relations annotated in \" \"at least a few reference examples!\" ) self . model . attrs [ \"set_n_labels\" ]( len ( self . labels )) self . model . initialize ( X = doc_sample , Y = spans_sample ) examples_to_truth ( examples ) Converts the spans of the examples into a list of (doc_idx, label_idx, begin, end) tuple as a tensor, that will be fed to the model with the begin_update method. PARAMETER DESCRIPTION examples TYPE: List [ Example ] RETURNS DESCRIPTION Ints2d Source code in edsnlp/pipelines/trainable/nested_ner.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 def examples_to_truth ( self , examples : List [ Example ]) -> Ints2d : \"\"\" Converts the spans of the examples into a list of (doc_idx, label_idx, begin, end) tuple as a tensor, that will be fed to the model with the `begin_update` method. Parameters ---------- examples: List[Example] Returns ------- Ints2d \"\"\" label_vocab = { self . vocab . strings [ l ]: i for i , l in enumerate ( self . labels )} spans = set () for eg_idx , eg in enumerate ( examples ): for span in ( * eg . reference . ents , * ( span for name in ( self . spans_labels if self . spans_labels is not None else eg . reference . spans ) for span in eg . reference . spans . get ( name , ()) ), ): label_idx = label_vocab . get ( span . label ) if label_idx is None : continue spans . add (( eg_idx , label_idx , span . start , span . end )) truths = self . model . ops . asarray ( list ( spans )) return truths create_component ( nlp , name , model , ent_labels = None , spans_labels = None , scorer = None ) Construct a TrainableQualifier component. Source code in edsnlp/pipelines/trainable/nested_ner.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 @Language . factory ( \"nested_ner\" , default_config = NESTED_NER_DEFAULTS , requires = [ \"doc.ents\" , \"doc.spans\" ], assigns = [ \"doc.ents\" , \"doc.spans\" ], default_score_weights = { \"ents_f\" : 1.0 , \"ents_p\" : 0.0 , \"ents_r\" : 0.0 , }, ) def create_component ( nlp : Language , name : str , model : Model , ent_labels = None , spans_labels = None , scorer = None , ): \"\"\"Construct a TrainableQualifier component.\"\"\" return TrainableNer ( vocab = nlp . vocab , model = model , name = name , ent_labels = ent_labels , spans_labels = spans_labels , scorer = scorer , ) nested_ner_scorer ( examples , ** cfg ) Scores the extracted entities that may be overlapping or nested by looking in doc.ents , and doc.spans . PARAMETER DESCRIPTION examples TYPE: Iterable [ Example ] cfg labels: Iterable[str] labels to take into account spans_labels: Iterable[str] span group names to look into for entities RETURNS DESCRIPTION Dict[str, float] Source code in edsnlp/pipelines/trainable/nested_ner.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def nested_ner_scorer ( examples : Iterable [ Example ], ** cfg ): \"\"\" Scores the extracted entities that may be overlapping or nested by looking in `doc.ents`, and `doc.spans`. Parameters ---------- examples: Iterable[Example] cfg: Dict[str] - labels: Iterable[str] labels to take into account - spans_labels: Iterable[str] span group names to look into for entities Returns ------- Dict[str, float] \"\"\" labels = set ( cfg [ \"labels\" ]) if \"labels\" in cfg is not None else None spans_labels = cfg [ \"spans_labels\" ] pred_spans = set () gold_spans = set () for eg_idx , eg in enumerate ( examples ): for span in ( * eg . predicted . ents , * ( span for name in ( spans_labels if spans_labels is not None else eg . reference . spans ) for span in eg . predicted . spans . get ( name , ()) ), ): if labels is None or span . label_ in labels : pred_spans . add (( eg_idx , span . start , span . end , span . label_ )) for span in ( * eg . reference . ents , * ( span for name in ( spans_labels if spans_labels is not None else eg . reference . spans ) for span in eg . reference . spans . get ( name , ()) ), ): if labels is None or span . label_ in labels : gold_spans . add (( eg_idx , span . start , span . end , span . label_ )) tp = len ( pred_spans & gold_spans ) return { \"ents_p\" : tp / len ( pred_spans ) if pred_spans else float ( tp == len ( pred_spans )), \"ents_r\" : tp / len ( gold_spans ) if gold_spans else float ( tp == len ( gold_spans )), \"ents_f\" : 2 * tp / ( len ( pred_spans ) + len ( gold_spans )) if pred_spans or gold_spans else float ( len ( pred_spans ) == len ( gold_spans )), } make_nested_ner_scorer () Source code in edsnlp/pipelines/trainable/nested_ner.py 145 146 147 @spacy . registry . scorers ( \"eds.nested_ner_scorer.v1\" ) def make_nested_ner_scorer (): return nested_ner_scorer","title":"nested_ner"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlppipelinestrainablenested_ner","text":"","title":"edsnlp.pipelines.trainable.nested_ner"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.msg","text":"","title":"msg"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.NUM_INITIALIZATION_EXAMPLES","text":"","title":"NUM_INITIALIZATION_EXAMPLES"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.nested_ner_default_config","text":"","title":"nested_ner_default_config"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.NESTED_NER_DEFAULTS","text":"","title":"NESTED_NER_DEFAULTS"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.np_ops","text":"","title":"np_ops"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer","text":"Bases: TrainablePipe Source code in edsnlp/pipelines/trainable/nested_ner.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 class TrainableNer ( TrainablePipe ): def __init__ ( self , vocab : Vocab , model : Model , name : str = \"nested_ner\" , ent_labels : Iterable [ str ] = (), spans_labels : Mapping [ str , Iterable [ str ]] = None , scorer : Optional [ Callable ] = None , ) -> None : \"\"\" Initialize a general named entity recognizer (with or without nested or overlapping entities). Parameters ---------- vocab: Vocab Spacy vocabulary model: Model The model to extract the spans name: str Name of the component ent_labels: Iterable[str] list of labels to filter entities for in `doc.ents` spans_labels: Mapping[str, Iterable[str]] Mapping from span group names to list of labels to look for entities and assign the predicted entities scorer: Optional[Callable] Method to call to score predictions \"\"\" super () . __init__ ( vocab , model , name ) self . cfg [ \"ent_labels\" ]: Optional [ Tuple [ str ]] = ( tuple ( ent_labels ) if ent_labels is not None else None ) self . cfg [ \"spans_labels\" ]: Optional [ Dict [ str , Tuple [ str ]]] = ( { k : tuple ( labels ) for k , labels in spans_labels . items ()} if spans_labels is not None else None ) self . cfg [ \"labels\" ] = tuple ( sorted ( set ( ( list ( ent_labels ) if ent_labels is not None else []) + [ label for group in ( spans_labels or {}) . values () for label in group ] ) ) ) self . scorer = scorer @property def labels ( self ) -> Tuple [ str ]: \"\"\"Return the labels currently added to the component.\"\"\" return self . cfg [ \"labels\" ] @property def spans_labels ( self ) -> Dict [ str , Tuple [ str ]]: \"\"\"Return the span group to labels filters mapping\"\"\" return self . cfg [ \"spans_labels\" ] @property def ent_labels ( self ): \"\"\"Return the doc.ents labels filters\"\"\" return self . cfg [ \"ent_labels\" ] def add_label ( self , label : str ) -> int : \"\"\"Add a new label to the pipe.\"\"\" raise Exception ( \"Cannot add a new label to the pipe\" ) def predict ( self , docs : List [ Doc ]) -> Dict [ str , Ints2d ]: \"\"\" Apply the pipeline's model to a batch of docs, without modifying them. Parameters ---------- docs: List[Doc] Returns ------- Int2d The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor that contain the spans' prediction for all the batch \"\"\" return self . model . predict (( docs , None , True ))[ 1 ] def set_annotations ( self , docs : List [ Doc ], predictions : Dict [ str , Ints2d ], ** kwargs ) -> None : \"\"\" Modify a batch of `Doc` objects, using predicted spans. Parameters ---------- docs: List[Doc] The documents to update predictions: Spans predictions, as returned by the model's predict method \"\"\" docs = list ( docs ) new_doc_spans : List [ List [ Span ]] = [[] for _ in docs ] for doc_idx , label_idx , begin , end in np_ops . asarray ( predictions . get ( \"spans\" )): label = self . labels [ label_idx ] new_doc_spans [ doc_idx ] . append ( Span ( docs [ doc_idx ], begin , end , label )) for doc , new_spans in zip ( docs , new_doc_spans ): # Only add a span to `doc.ents` if its label is in `self.ents_labels` doc . ents = filter_spans ( [ s for s in new_spans if s . label_ in self . ent_labels ] ) # Only add a span to `doc.spans[name]` if its label is in the matching # `self.spans_labels[name]` list for name , group_labels in self . spans_labels . items (): doc . spans [ name ] = [ s for s in new_spans if s . label_ in group_labels ] def update ( self , examples : Iterable [ Example ], * , drop : float = 0.0 , set_annotations : bool = False , sgd : Optional [ Optimizer ] = None , losses : Optional [ Dict [ str , float ]] = None , ) -> Dict [ str , float ]: \"\"\" Learn from a batch of documents and gold-standard information, updating the pipe's model. Delegates to begin_update and get_loss. Unlike standard TrainablePipe components, the discrete ops (best selection of tags) is performed by the model directly (`begin_update` returns the loss and the predictions) Parameters ---------- examples: Iterable[Example] drop: float = 0.0 set_annotations: bool Whether to update the document with predicted spans sgd: Optional[Optimizer] Optimizer losses: Optional[Dict[str, float]] Dict of loss, updated in place Returns ------- Dict[str, float] Updated losses dict \"\"\" if losses is None : losses = {} losses . setdefault ( self . name , 0.0 ) set_dropout_rate ( self . model , drop ) examples = list ( examples ) # run the model docs = [ eg . predicted for eg in examples ] gold = self . examples_to_truth ( examples ) ( loss , predictions ), backprop = self . model . begin_update ( ( docs , gold , set_annotations ) ) loss , gradient = self . get_loss ( examples , loss ) backprop ( gradient ) if sgd is not None : self . model . finish_update ( sgd ) if set_annotations : self . set_annotations ( docs , predictions ) losses [ self . name ] = loss return loss def get_loss ( self , examples : Iterable [ Example ], loss ) -> Tuple [ float , float ]: \"\"\"Find the loss and gradient of loss for the batch of documents and their predicted scores.\"\"\" return float ( loss . item ()), self . model . ops . xp . array ([ 1 ]) def initialize ( self , get_examples : Callable [[], Iterable [ Example ]], * , nlp : Language = None , labels : Optional [ List [ str ]] = None , ): \"\"\" Initialize the pipe for training, using a representative set of data examples. 1. If no ent_labels are provided, we scrap them from the ents of the set of examples. 2. If no span labels are provided, we scrap them from the spans of the set of examples, and filter these labels with the ents_labels. Parameters ---------- get_examples: Callable[[], Iterable[Example]] Method to sample some examples nlp: spacy.Language Unused spacy model labels Unused list of labels \"\"\" sub_batch = list ( islice ( get_examples (), NUM_INITIALIZATION_EXAMPLES )) if self . ent_labels is None or self . spans_labels is None : ent_labels_before = self . ent_labels if self . ent_labels is None : self . cfg [ \"ent_labels\" ] = tuple ( sorted ( { span . label_ for doc in sub_batch for span in doc . reference . ents } ) ) if self . spans_labels is None : spans_labels = defaultdict ( lambda : set ()) for doc in sub_batch : for name , group in doc . reference . spans . items (): for span in group : if ( ent_labels_before is None or span . label_ in ent_labels_before ): spans_labels [ name ] . add ( span . label_ ) self . cfg [ \"spans_labels\" ] = { name : tuple ( sorted ( group )) for name , group in spans_labels . items () } self . cfg [ \"labels\" ] = tuple ( sorted ( set ( list ( self . ent_labels ) + [ label for group in self . spans_labels . values () for label in group ] ) ) ) doc_sample = [ eg . reference for eg in sub_batch ] spans_sample = self . examples_to_truth ( sub_batch ) if spans_sample is None : raise ValueError ( \"Call begin_training with relevant entities \" \"and relations annotated in \" \"at least a few reference examples!\" ) self . model . attrs [ \"set_n_labels\" ]( len ( self . labels )) self . model . initialize ( X = doc_sample , Y = spans_sample ) def examples_to_truth ( self , examples : List [ Example ]) -> Ints2d : \"\"\" Converts the spans of the examples into a list of (doc_idx, label_idx, begin, end) tuple as a tensor, that will be fed to the model with the `begin_update` method. Parameters ---------- examples: List[Example] Returns ------- Ints2d \"\"\" label_vocab = { self . vocab . strings [ l ]: i for i , l in enumerate ( self . labels )} spans = set () for eg_idx , eg in enumerate ( examples ): for span in ( * eg . reference . ents , * ( span for name in ( self . spans_labels if self . spans_labels is not None else eg . reference . spans ) for span in eg . reference . spans . get ( name , ()) ), ): label_idx = label_vocab . get ( span . label ) if label_idx is None : continue spans . add (( eg_idx , label_idx , span . start , span . end )) truths = self . model . ops . asarray ( list ( spans )) return truths","title":"TrainableNer"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.scorer","text":"","title":"scorer"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.__init__","text":"Initialize a general named entity recognizer (with or without nested or overlapping entities). PARAMETER DESCRIPTION vocab Spacy vocabulary TYPE: Vocab model The model to extract the spans TYPE: Model name Name of the component TYPE: str DEFAULT: 'nested_ner' ent_labels list of labels to filter entities for in doc.ents TYPE: Iterable [ str ] DEFAULT: () spans_labels Mapping from span group names to list of labels to look for entities and assign the predicted entities TYPE: Mapping [ str , Iterable [ str ]] DEFAULT: None scorer Method to call to score predictions TYPE: Optional [ Callable ] DEFAULT: None Source code in edsnlp/pipelines/trainable/nested_ner.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def __init__ ( self , vocab : Vocab , model : Model , name : str = \"nested_ner\" , ent_labels : Iterable [ str ] = (), spans_labels : Mapping [ str , Iterable [ str ]] = None , scorer : Optional [ Callable ] = None , ) -> None : \"\"\" Initialize a general named entity recognizer (with or without nested or overlapping entities). Parameters ---------- vocab: Vocab Spacy vocabulary model: Model The model to extract the spans name: str Name of the component ent_labels: Iterable[str] list of labels to filter entities for in `doc.ents` spans_labels: Mapping[str, Iterable[str]] Mapping from span group names to list of labels to look for entities and assign the predicted entities scorer: Optional[Callable] Method to call to score predictions \"\"\" super () . __init__ ( vocab , model , name ) self . cfg [ \"ent_labels\" ]: Optional [ Tuple [ str ]] = ( tuple ( ent_labels ) if ent_labels is not None else None ) self . cfg [ \"spans_labels\" ]: Optional [ Dict [ str , Tuple [ str ]]] = ( { k : tuple ( labels ) for k , labels in spans_labels . items ()} if spans_labels is not None else None ) self . cfg [ \"labels\" ] = tuple ( sorted ( set ( ( list ( ent_labels ) if ent_labels is not None else []) + [ label for group in ( spans_labels or {}) . values () for label in group ] ) ) ) self . scorer = scorer","title":"__init__()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.labels","text":"Return the labels currently added to the component. Source code in edsnlp/pipelines/trainable/nested_ner.py 207 208 209 210 @property def labels ( self ) -> Tuple [ str ]: \"\"\"Return the labels currently added to the component.\"\"\" return self . cfg [ \"labels\" ]","title":"labels()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.spans_labels","text":"Return the span group to labels filters mapping Source code in edsnlp/pipelines/trainable/nested_ner.py 212 213 214 215 @property def spans_labels ( self ) -> Dict [ str , Tuple [ str ]]: \"\"\"Return the span group to labels filters mapping\"\"\" return self . cfg [ \"spans_labels\" ]","title":"spans_labels()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.ent_labels","text":"Return the doc.ents labels filters Source code in edsnlp/pipelines/trainable/nested_ner.py 217 218 219 220 @property def ent_labels ( self ): \"\"\"Return the doc.ents labels filters\"\"\" return self . cfg [ \"ent_labels\" ]","title":"ent_labels()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.add_label","text":"Add a new label to the pipe. Source code in edsnlp/pipelines/trainable/nested_ner.py 222 223 224 def add_label ( self , label : str ) -> int : \"\"\"Add a new label to the pipe.\"\"\" raise Exception ( \"Cannot add a new label to the pipe\" )","title":"add_label()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.predict","text":"Apply the pipeline's model to a batch of docs, without modifying them. PARAMETER DESCRIPTION docs TYPE: List [ Doc ] RETURNS DESCRIPTION Int2d The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor that contain the spans' prediction for all the batch Source code in edsnlp/pipelines/trainable/nested_ner.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def predict ( self , docs : List [ Doc ]) -> Dict [ str , Ints2d ]: \"\"\" Apply the pipeline's model to a batch of docs, without modifying them. Parameters ---------- docs: List[Doc] Returns ------- Int2d The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor that contain the spans' prediction for all the batch \"\"\" return self . model . predict (( docs , None , True ))[ 1 ]","title":"predict()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.set_annotations","text":"Modify a batch of Doc objects, using predicted spans. PARAMETER DESCRIPTION docs The documents to update TYPE: List [ Doc ] predictions Spans predictions, as returned by the model's predict method TYPE: Dict [ str , Ints2d ] Source code in edsnlp/pipelines/trainable/nested_ner.py 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def set_annotations ( self , docs : List [ Doc ], predictions : Dict [ str , Ints2d ], ** kwargs ) -> None : \"\"\" Modify a batch of `Doc` objects, using predicted spans. Parameters ---------- docs: List[Doc] The documents to update predictions: Spans predictions, as returned by the model's predict method \"\"\" docs = list ( docs ) new_doc_spans : List [ List [ Span ]] = [[] for _ in docs ] for doc_idx , label_idx , begin , end in np_ops . asarray ( predictions . get ( \"spans\" )): label = self . labels [ label_idx ] new_doc_spans [ doc_idx ] . append ( Span ( docs [ doc_idx ], begin , end , label )) for doc , new_spans in zip ( docs , new_doc_spans ): # Only add a span to `doc.ents` if its label is in `self.ents_labels` doc . ents = filter_spans ( [ s for s in new_spans if s . label_ in self . ent_labels ] ) # Only add a span to `doc.spans[name]` if its label is in the matching # `self.spans_labels[name]` list for name , group_labels in self . spans_labels . items (): doc . spans [ name ] = [ s for s in new_spans if s . label_ in group_labels ]","title":"set_annotations()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.update","text":"Learn from a batch of documents and gold-standard information, updating the pipe's model. Delegates to begin_update and get_loss. Unlike standard TrainablePipe components, the discrete ops (best selection of tags) is performed by the model directly ( begin_update returns the loss and the predictions) PARAMETER DESCRIPTION examples TYPE: Iterable [ Example ] drop TYPE: float DEFAULT: 0.0 set_annotations: bool Whether to update the document with predicted spans sgd: Optional[Optimizer] Optimizer losses: Optional[Dict[str, float]] Dict of loss, updated in place RETURNS DESCRIPTION Dict[str, float] Updated losses dict Source code in edsnlp/pipelines/trainable/nested_ner.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def update ( self , examples : Iterable [ Example ], * , drop : float = 0.0 , set_annotations : bool = False , sgd : Optional [ Optimizer ] = None , losses : Optional [ Dict [ str , float ]] = None , ) -> Dict [ str , float ]: \"\"\" Learn from a batch of documents and gold-standard information, updating the pipe's model. Delegates to begin_update and get_loss. Unlike standard TrainablePipe components, the discrete ops (best selection of tags) is performed by the model directly (`begin_update` returns the loss and the predictions) Parameters ---------- examples: Iterable[Example] drop: float = 0.0 set_annotations: bool Whether to update the document with predicted spans sgd: Optional[Optimizer] Optimizer losses: Optional[Dict[str, float]] Dict of loss, updated in place Returns ------- Dict[str, float] Updated losses dict \"\"\" if losses is None : losses = {} losses . setdefault ( self . name , 0.0 ) set_dropout_rate ( self . model , drop ) examples = list ( examples ) # run the model docs = [ eg . predicted for eg in examples ] gold = self . examples_to_truth ( examples ) ( loss , predictions ), backprop = self . model . begin_update ( ( docs , gold , set_annotations ) ) loss , gradient = self . get_loss ( examples , loss ) backprop ( gradient ) if sgd is not None : self . model . finish_update ( sgd ) if set_annotations : self . set_annotations ( docs , predictions ) losses [ self . name ] = loss return loss","title":"update()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.get_loss","text":"Find the loss and gradient of loss for the batch of documents and their predicted scores. Source code in edsnlp/pipelines/trainable/nested_ner.py 330 331 332 333 def get_loss ( self , examples : Iterable [ Example ], loss ) -> Tuple [ float , float ]: \"\"\"Find the loss and gradient of loss for the batch of documents and their predicted scores.\"\"\" return float ( loss . item ()), self . model . ops . xp . array ([ 1 ])","title":"get_loss()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.initialize","text":"Initialize the pipe for training, using a representative set of data examples. If no ent_labels are provided, we scrap them from the ents of the set of examples. If no span labels are provided, we scrap them from the spans of the set of examples, and filter these labels with the ents_labels. PARAMETER DESCRIPTION get_examples Method to sample some examples TYPE: Callable [[], Iterable [ Example ]] nlp Unused spacy model TYPE: Language DEFAULT: None labels Unused list of labels TYPE: Optional [ List [ str ]] DEFAULT: None Source code in edsnlp/pipelines/trainable/nested_ner.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 def initialize ( self , get_examples : Callable [[], Iterable [ Example ]], * , nlp : Language = None , labels : Optional [ List [ str ]] = None , ): \"\"\" Initialize the pipe for training, using a representative set of data examples. 1. If no ent_labels are provided, we scrap them from the ents of the set of examples. 2. If no span labels are provided, we scrap them from the spans of the set of examples, and filter these labels with the ents_labels. Parameters ---------- get_examples: Callable[[], Iterable[Example]] Method to sample some examples nlp: spacy.Language Unused spacy model labels Unused list of labels \"\"\" sub_batch = list ( islice ( get_examples (), NUM_INITIALIZATION_EXAMPLES )) if self . ent_labels is None or self . spans_labels is None : ent_labels_before = self . ent_labels if self . ent_labels is None : self . cfg [ \"ent_labels\" ] = tuple ( sorted ( { span . label_ for doc in sub_batch for span in doc . reference . ents } ) ) if self . spans_labels is None : spans_labels = defaultdict ( lambda : set ()) for doc in sub_batch : for name , group in doc . reference . spans . items (): for span in group : if ( ent_labels_before is None or span . label_ in ent_labels_before ): spans_labels [ name ] . add ( span . label_ ) self . cfg [ \"spans_labels\" ] = { name : tuple ( sorted ( group )) for name , group in spans_labels . items () } self . cfg [ \"labels\" ] = tuple ( sorted ( set ( list ( self . ent_labels ) + [ label for group in self . spans_labels . values () for label in group ] ) ) ) doc_sample = [ eg . reference for eg in sub_batch ] spans_sample = self . examples_to_truth ( sub_batch ) if spans_sample is None : raise ValueError ( \"Call begin_training with relevant entities \" \"and relations annotated in \" \"at least a few reference examples!\" ) self . model . attrs [ \"set_n_labels\" ]( len ( self . labels )) self . model . initialize ( X = doc_sample , Y = spans_sample )","title":"initialize()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.examples_to_truth","text":"Converts the spans of the examples into a list of (doc_idx, label_idx, begin, end) tuple as a tensor, that will be fed to the model with the begin_update method. PARAMETER DESCRIPTION examples TYPE: List [ Example ] RETURNS DESCRIPTION Ints2d Source code in edsnlp/pipelines/trainable/nested_ner.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 def examples_to_truth ( self , examples : List [ Example ]) -> Ints2d : \"\"\" Converts the spans of the examples into a list of (doc_idx, label_idx, begin, end) tuple as a tensor, that will be fed to the model with the `begin_update` method. Parameters ---------- examples: List[Example] Returns ------- Ints2d \"\"\" label_vocab = { self . vocab . strings [ l ]: i for i , l in enumerate ( self . labels )} spans = set () for eg_idx , eg in enumerate ( examples ): for span in ( * eg . reference . ents , * ( span for name in ( self . spans_labels if self . spans_labels is not None else eg . reference . spans ) for span in eg . reference . spans . get ( name , ()) ), ): label_idx = label_vocab . get ( span . label ) if label_idx is None : continue spans . add (( eg_idx , label_idx , span . start , span . end )) truths = self . model . ops . asarray ( list ( spans )) return truths","title":"examples_to_truth()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.create_component","text":"Construct a TrainableQualifier component. Source code in edsnlp/pipelines/trainable/nested_ner.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 @Language . factory ( \"nested_ner\" , default_config = NESTED_NER_DEFAULTS , requires = [ \"doc.ents\" , \"doc.spans\" ], assigns = [ \"doc.ents\" , \"doc.spans\" ], default_score_weights = { \"ents_f\" : 1.0 , \"ents_p\" : 0.0 , \"ents_r\" : 0.0 , }, ) def create_component ( nlp : Language , name : str , model : Model , ent_labels = None , spans_labels = None , scorer = None , ): \"\"\"Construct a TrainableQualifier component.\"\"\" return TrainableNer ( vocab = nlp . vocab , model = model , name = name , ent_labels = ent_labels , spans_labels = spans_labels , scorer = scorer , )","title":"create_component()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.nested_ner_scorer","text":"Scores the extracted entities that may be overlapping or nested by looking in doc.ents , and doc.spans . PARAMETER DESCRIPTION examples TYPE: Iterable [ Example ] cfg labels: Iterable[str] labels to take into account spans_labels: Iterable[str] span group names to look into for entities RETURNS DESCRIPTION Dict[str, float] Source code in edsnlp/pipelines/trainable/nested_ner.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def nested_ner_scorer ( examples : Iterable [ Example ], ** cfg ): \"\"\" Scores the extracted entities that may be overlapping or nested by looking in `doc.ents`, and `doc.spans`. Parameters ---------- examples: Iterable[Example] cfg: Dict[str] - labels: Iterable[str] labels to take into account - spans_labels: Iterable[str] span group names to look into for entities Returns ------- Dict[str, float] \"\"\" labels = set ( cfg [ \"labels\" ]) if \"labels\" in cfg is not None else None spans_labels = cfg [ \"spans_labels\" ] pred_spans = set () gold_spans = set () for eg_idx , eg in enumerate ( examples ): for span in ( * eg . predicted . ents , * ( span for name in ( spans_labels if spans_labels is not None else eg . reference . spans ) for span in eg . predicted . spans . get ( name , ()) ), ): if labels is None or span . label_ in labels : pred_spans . add (( eg_idx , span . start , span . end , span . label_ )) for span in ( * eg . reference . ents , * ( span for name in ( spans_labels if spans_labels is not None else eg . reference . spans ) for span in eg . reference . spans . get ( name , ()) ), ): if labels is None or span . label_ in labels : gold_spans . add (( eg_idx , span . start , span . end , span . label_ )) tp = len ( pred_spans & gold_spans ) return { \"ents_p\" : tp / len ( pred_spans ) if pred_spans else float ( tp == len ( pred_spans )), \"ents_r\" : tp / len ( gold_spans ) if gold_spans else float ( tp == len ( gold_spans )), \"ents_f\" : 2 * tp / ( len ( pred_spans ) + len ( gold_spans )) if pred_spans or gold_spans else float ( len ( pred_spans ) == len ( gold_spans )), }","title":"nested_ner_scorer()"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.make_nested_ner_scorer","text":"Source code in edsnlp/pipelines/trainable/nested_ner.py 145 146 147 @spacy . registry . scorers ( \"eds.nested_ner_scorer.v1\" ) def make_nested_ner_scorer (): return nested_ner_scorer","title":"make_nested_ner_scorer()"},{"location":"reference/processing/","text":"edsnlp.processing","title":"`edsnlp.processing`"},{"location":"reference/processing/#edsnlpprocessing","text":"","title":"edsnlp.processing"},{"location":"reference/processing/distributed/","text":"edsnlp.processing.distributed pyspark_type_finder ( obj ) Returns (when possible) the PySpark type of any python object Source code in edsnlp/processing/distributed.py 24 25 26 27 28 29 30 31 32 33 def pyspark_type_finder ( obj ): \"\"\" Returns (when possible) the PySpark type of any python object \"\"\" try : inferred_type = T . _infer_type ( obj ) logger . info ( f \"Inferred type is { repr ( inferred_type ) } \" ) return inferred_type except TypeError : raise TypeError ( \"Cannot infer type for this object.\" ) module_checker ( func , * args , ** kwargs ) Source code in edsnlp/processing/distributed.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @decorator def module_checker ( func : Callable , * args , ** kwargs , ) -> Any : args = list ( args ) note = args . pop ( 0 ) module = get_module ( note ) if module == DataFrameModules . PYSPARK : return func ( note , * args , ** kwargs ) elif module == DataFrameModules . KOALAS : import databricks.koalas # noqa F401 note_spark = note . to_spark () note_nlp_spark = func ( note_spark , * args , ** kwargs ) return note_nlp_spark . to_koalas () pipe ( note , nlp , context = [], additional_spans = 'discarded' , extensions = {}) Function to apply a spaCy pipe to a pyspark or koalas DataFrame note PARAMETER DESCRIPTION note A Pyspark or Koalas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the eds.dates pipeline component populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: {} RETURNS DESCRIPTION DataFrame A pyspark DataFrame with one line per extraction Source code in edsnlp/processing/distributed.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @module_checker def pipe ( note : DataFrames , nlp : Language , context : List [ str ] = [], additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : Dict [ str , T . DataType ] = {}, ) -> DataFrame : \"\"\" Function to apply a spaCy pipe to a pyspark or koalas DataFrame note Parameters ---------- note : DataFrame A Pyspark or Koalas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `eds.dates` pipeline component populates `doc.spans['dates']` extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. Returns ------- DataFrame A pyspark DataFrame with one line per extraction \"\"\" if context : check_spacy_version_for_context () spark = SparkSession . builder . enableHiveSupport () . getOrCreate () sc = spark . sparkContext if not nlp . has_pipe ( \"eds.context\" ): nlp . add_pipe ( \"eds.context\" , first = True , config = dict ( context = context )) nlp_bc = sc . broadcast ( nlp ) def _udf_factory ( additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : Dict [ str , T . DataType ] = dict (), ): schema = T . ArrayType ( T . StructType ( [ T . StructField ( \"lexical_variant\" , T . StringType (), False ), T . StructField ( \"label\" , T . StringType (), False ), T . StructField ( \"span_type\" , T . StringType (), True ), T . StructField ( \"start\" , T . IntegerType (), False ), T . StructField ( \"end\" , T . IntegerType (), False ), * [ T . StructField ( slugify ( extension_name ), extension_type , True ) for extension_name , extension_type in extensions . items () ], ] ) ) def f ( text , * context_values , additional_spans = additional_spans , extensions = extensions , ): if text is None : return [] nlp = nlp_bc . value for _ , pipe in nlp . pipeline : if isinstance ( pipe , BaseComponent ): pipe . set_extensions () doc = nlp . make_doc ( text ) for context_name , context_value in zip ( context , context_values ): doc . _ . set ( context_name , context_value ) doc = nlp ( doc ) ents = [] for ent in doc . ents : parsed_extensions = [ rgetattr ( ent . _ , extension ) for extension in extensions . keys () ] ents . append ( ( ent . text , ent . label_ , \"ents\" , ent . start_char , ent . end_char , * parsed_extensions , ) ) if additional_spans is None : return ents if type ( additional_spans ) == str : additional_spans = [ additional_spans ] for spans_name in additional_spans : for ent in doc . spans . get ( spans_name , []): parsed_extensions = [ rgetattr ( ent . _ , extension ) for extension in extensions . keys () ] ents . append ( ( ent . text , ent . label_ , spans_name , ent . start_char , ent . end_char , * parsed_extensions , ) ) return ents f_udf = F . udf ( partial ( f , additional_spans = additional_spans , extensions = extensions , ), schema , ) return f_udf matcher = _udf_factory ( additional_spans = additional_spans , extensions = extensions , ) n_needed_partitions = max ( note . count () // 2000 , 1 ) # Batch sizes of 2000 note_nlp = note . repartition ( n_needed_partitions ) . withColumn ( \"matches\" , matcher ( F . col ( \"note_text\" ), * [ F . col ( c ) for c in context ]) ) note_nlp = note_nlp . withColumn ( \"matches\" , F . explode ( note_nlp . matches )) note_nlp = note_nlp . select ( \"note_id\" , \"matches.*\" ) return note_nlp custom_pipe ( note , nlp , results_extractor , dtypes , context = []) Function to apply a spaCy pipe to a pyspark or koalas DataFrame note, a generic callback function that converts a spaCy Doc object into a list of dictionaries. PARAMETER DESCRIPTION note A Pyspark or Koalas DataFrame with a note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language results_extractor Arbitrary function that takes extract serialisable results from the computed spaCy Doc object. The output of the function must be a list of dictionaries containing the extracted spans or entities. There is no requirement for all entities to provide every dictionary key. TYPE: Callable[[Doc], List[Dict[str, Any]]] dtypes Dictionary containing all expected keys from the results_extractor function, along with their types. TYPE: Dict[str, T.DataType] context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] RETURNS DESCRIPTION DataFrame A pyspark DataFrame with one line per extraction Source code in edsnlp/processing/distributed.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @module_checker def custom_pipe ( note : DataFrames , nlp : Language , results_extractor : Callable [[ Doc ], List [ Dict [ str , Any ]]], dtypes : Dict [ str , T . DataType ], context : List [ str ] = [], ) -> DataFrame : \"\"\" Function to apply a spaCy pipe to a pyspark or koalas DataFrame note, a generic callback function that converts a spaCy `Doc` object into a list of dictionaries. Parameters ---------- note : DataFrame A Pyspark or Koalas DataFrame with a `note_text` column nlp : Language A spaCy pipe results_extractor : Callable[[Doc], List[Dict[str, Any]]] Arbitrary function that takes extract serialisable results from the computed spaCy `Doc` object. The output of the function must be a list of dictionaries containing the extracted spans or entities. There is no requirement for all entities to provide every dictionary key. dtypes : Dict[str, T.DataType] Dictionary containing all expected keys from the `results_extractor` function, along with their types. context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. Returns ------- DataFrame A pyspark DataFrame with one line per extraction \"\"\" if context : check_spacy_version_for_context () if ( \"note_id\" not in context ) and ( \"note_id\" in dtypes . keys ()): context . append ( \"note_id\" ) spark = SparkSession . builder . enableHiveSupport () . getOrCreate () sc = spark . sparkContext if not nlp . has_pipe ( \"eds.context\" ): nlp . add_pipe ( \"eds.context\" , first = True , config = dict ( context = context )) nlp_bc = sc . broadcast ( nlp ) schema = T . ArrayType ( T . StructType ([ T . StructField ( key , dtype ) for key , dtype in dtypes . items ()]) ) @F . udf ( schema ) def udf ( text , * context_values , ): if text is None : return [] nlp_ = nlp_bc . value for _ , pipe in nlp . pipeline : if isinstance ( pipe , BaseComponent ): pipe . set_extensions () doc = nlp_ . make_doc ( text ) for context_name , context_value in zip ( context , context_values ): doc . _ . set ( context_name , context_value ) doc = nlp_ ( doc ) results = [] for res in results_extractor ( doc ): results . append ([ res . get ( key ) for key in dtypes ]) return results note_nlp = note . withColumn ( \"matches\" , udf ( F . col ( \"note_text\" ), * [ F . col ( c ) for c in context ]) ) note_nlp = note_nlp . withColumn ( \"matches\" , F . explode ( note_nlp . matches )) if ( \"note_id\" not in dtypes . keys ()) and ( \"note_id\" in note_nlp . columns ): note_nlp = note_nlp . select ( \"note_id\" , \"matches.*\" ) else : note_nlp = note_nlp . select ( \"matches.*\" ) return note_nlp","title":"distributed"},{"location":"reference/processing/distributed/#edsnlpprocessingdistributed","text":"","title":"edsnlp.processing.distributed"},{"location":"reference/processing/distributed/#edsnlp.processing.distributed.pyspark_type_finder","text":"Returns (when possible) the PySpark type of any python object Source code in edsnlp/processing/distributed.py 24 25 26 27 28 29 30 31 32 33 def pyspark_type_finder ( obj ): \"\"\" Returns (when possible) the PySpark type of any python object \"\"\" try : inferred_type = T . _infer_type ( obj ) logger . info ( f \"Inferred type is { repr ( inferred_type ) } \" ) return inferred_type except TypeError : raise TypeError ( \"Cannot infer type for this object.\" )","title":"pyspark_type_finder()"},{"location":"reference/processing/distributed/#edsnlp.processing.distributed.module_checker","text":"Source code in edsnlp/processing/distributed.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @decorator def module_checker ( func : Callable , * args , ** kwargs , ) -> Any : args = list ( args ) note = args . pop ( 0 ) module = get_module ( note ) if module == DataFrameModules . PYSPARK : return func ( note , * args , ** kwargs ) elif module == DataFrameModules . KOALAS : import databricks.koalas # noqa F401 note_spark = note . to_spark () note_nlp_spark = func ( note_spark , * args , ** kwargs ) return note_nlp_spark . to_koalas ()","title":"module_checker()"},{"location":"reference/processing/distributed/#edsnlp.processing.distributed.pipe","text":"Function to apply a spaCy pipe to a pyspark or koalas DataFrame note PARAMETER DESCRIPTION note A Pyspark or Koalas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the eds.dates pipeline component populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: 'discarded' extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: {} RETURNS DESCRIPTION DataFrame A pyspark DataFrame with one line per extraction Source code in edsnlp/processing/distributed.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 @module_checker def pipe ( note : DataFrames , nlp : Language , context : List [ str ] = [], additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : Dict [ str , T . DataType ] = {}, ) -> DataFrame : \"\"\" Function to apply a spaCy pipe to a pyspark or koalas DataFrame note Parameters ---------- note : DataFrame A Pyspark or Koalas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `eds.dates` pipeline component populates `doc.spans['dates']` extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. Returns ------- DataFrame A pyspark DataFrame with one line per extraction \"\"\" if context : check_spacy_version_for_context () spark = SparkSession . builder . enableHiveSupport () . getOrCreate () sc = spark . sparkContext if not nlp . has_pipe ( \"eds.context\" ): nlp . add_pipe ( \"eds.context\" , first = True , config = dict ( context = context )) nlp_bc = sc . broadcast ( nlp ) def _udf_factory ( additional_spans : Union [ List [ str ], str ] = \"discarded\" , extensions : Dict [ str , T . DataType ] = dict (), ): schema = T . ArrayType ( T . StructType ( [ T . StructField ( \"lexical_variant\" , T . StringType (), False ), T . StructField ( \"label\" , T . StringType (), False ), T . StructField ( \"span_type\" , T . StringType (), True ), T . StructField ( \"start\" , T . IntegerType (), False ), T . StructField ( \"end\" , T . IntegerType (), False ), * [ T . StructField ( slugify ( extension_name ), extension_type , True ) for extension_name , extension_type in extensions . items () ], ] ) ) def f ( text , * context_values , additional_spans = additional_spans , extensions = extensions , ): if text is None : return [] nlp = nlp_bc . value for _ , pipe in nlp . pipeline : if isinstance ( pipe , BaseComponent ): pipe . set_extensions () doc = nlp . make_doc ( text ) for context_name , context_value in zip ( context , context_values ): doc . _ . set ( context_name , context_value ) doc = nlp ( doc ) ents = [] for ent in doc . ents : parsed_extensions = [ rgetattr ( ent . _ , extension ) for extension in extensions . keys () ] ents . append ( ( ent . text , ent . label_ , \"ents\" , ent . start_char , ent . end_char , * parsed_extensions , ) ) if additional_spans is None : return ents if type ( additional_spans ) == str : additional_spans = [ additional_spans ] for spans_name in additional_spans : for ent in doc . spans . get ( spans_name , []): parsed_extensions = [ rgetattr ( ent . _ , extension ) for extension in extensions . keys () ] ents . append ( ( ent . text , ent . label_ , spans_name , ent . start_char , ent . end_char , * parsed_extensions , ) ) return ents f_udf = F . udf ( partial ( f , additional_spans = additional_spans , extensions = extensions , ), schema , ) return f_udf matcher = _udf_factory ( additional_spans = additional_spans , extensions = extensions , ) n_needed_partitions = max ( note . count () // 2000 , 1 ) # Batch sizes of 2000 note_nlp = note . repartition ( n_needed_partitions ) . withColumn ( \"matches\" , matcher ( F . col ( \"note_text\" ), * [ F . col ( c ) for c in context ]) ) note_nlp = note_nlp . withColumn ( \"matches\" , F . explode ( note_nlp . matches )) note_nlp = note_nlp . select ( \"note_id\" , \"matches.*\" ) return note_nlp","title":"pipe()"},{"location":"reference/processing/distributed/#edsnlp.processing.distributed.custom_pipe","text":"Function to apply a spaCy pipe to a pyspark or koalas DataFrame note, a generic callback function that converts a spaCy Doc object into a list of dictionaries. PARAMETER DESCRIPTION note A Pyspark or Koalas DataFrame with a note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language results_extractor Arbitrary function that takes extract serialisable results from the computed spaCy Doc object. The output of the function must be a list of dictionaries containing the extracted spans or entities. There is no requirement for all entities to provide every dictionary key. TYPE: Callable[[Doc], List[Dict[str, Any]]] dtypes Dictionary containing all expected keys from the results_extractor function, along with their types. TYPE: Dict[str, T.DataType] context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] RETURNS DESCRIPTION DataFrame A pyspark DataFrame with one line per extraction Source code in edsnlp/processing/distributed.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @module_checker def custom_pipe ( note : DataFrames , nlp : Language , results_extractor : Callable [[ Doc ], List [ Dict [ str , Any ]]], dtypes : Dict [ str , T . DataType ], context : List [ str ] = [], ) -> DataFrame : \"\"\" Function to apply a spaCy pipe to a pyspark or koalas DataFrame note, a generic callback function that converts a spaCy `Doc` object into a list of dictionaries. Parameters ---------- note : DataFrame A Pyspark or Koalas DataFrame with a `note_text` column nlp : Language A spaCy pipe results_extractor : Callable[[Doc], List[Dict[str, Any]]] Arbitrary function that takes extract serialisable results from the computed spaCy `Doc` object. The output of the function must be a list of dictionaries containing the extracted spans or entities. There is no requirement for all entities to provide every dictionary key. dtypes : Dict[str, T.DataType] Dictionary containing all expected keys from the `results_extractor` function, along with their types. context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. Returns ------- DataFrame A pyspark DataFrame with one line per extraction \"\"\" if context : check_spacy_version_for_context () if ( \"note_id\" not in context ) and ( \"note_id\" in dtypes . keys ()): context . append ( \"note_id\" ) spark = SparkSession . builder . enableHiveSupport () . getOrCreate () sc = spark . sparkContext if not nlp . has_pipe ( \"eds.context\" ): nlp . add_pipe ( \"eds.context\" , first = True , config = dict ( context = context )) nlp_bc = sc . broadcast ( nlp ) schema = T . ArrayType ( T . StructType ([ T . StructField ( key , dtype ) for key , dtype in dtypes . items ()]) ) @F . udf ( schema ) def udf ( text , * context_values , ): if text is None : return [] nlp_ = nlp_bc . value for _ , pipe in nlp . pipeline : if isinstance ( pipe , BaseComponent ): pipe . set_extensions () doc = nlp_ . make_doc ( text ) for context_name , context_value in zip ( context , context_values ): doc . _ . set ( context_name , context_value ) doc = nlp_ ( doc ) results = [] for res in results_extractor ( doc ): results . append ([ res . get ( key ) for key in dtypes ]) return results note_nlp = note . withColumn ( \"matches\" , udf ( F . col ( \"note_text\" ), * [ F . col ( c ) for c in context ]) ) note_nlp = note_nlp . withColumn ( \"matches\" , F . explode ( note_nlp . matches )) if ( \"note_id\" not in dtypes . keys ()) and ( \"note_id\" in note_nlp . columns ): note_nlp = note_nlp . select ( \"note_id\" , \"matches.*\" ) else : note_nlp = note_nlp . select ( \"matches.*\" ) return note_nlp","title":"custom_pipe()"},{"location":"reference/processing/helpers/","text":"edsnlp.processing.helpers DataFrames = None module-attribute spec = importlib . util . find_spec ( module . value ) module-attribute DataFrameModules Bases: Enum Source code in edsnlp/processing/helpers.py 10 11 12 13 class DataFrameModules ( Enum ): PANDAS = \"pandas\" PYSPARK = \"pyspark.sql\" KOALAS = \"databricks.koalas\" PANDAS = 'pandas' class-attribute PYSPARK = 'pyspark.sql' class-attribute KOALAS = 'databricks.koalas' class-attribute get_module ( df ) Source code in edsnlp/processing/helpers.py 27 28 29 30 def get_module ( df : DataFrames ): for module in list ( DataFrameModules ): if df . __class__ . __module__ . startswith ( module . value ): return module check_spacy_version_for_context () Source code in edsnlp/processing/helpers.py 33 34 35 36 37 38 39 40 41 42 def check_spacy_version_for_context (): # pragma: no cover import spacy spacy_version = getattr ( spacy , \"__version__\" ) if LooseVersion ( spacy_version ) < LooseVersion ( \"3.2\" ): raise VersionConflict ( \"You provided a `context` argument, which only work with spacy>=3.2. \\n \" f \"However, we found SpaCy version { spacy_version } . \\n \" , \"Please upgrade SpaCy ;)\" , ) rgetattr ( obj , attr , * args ) Get attribute recursively PARAMETER DESCRIPTION obj An object TYPE: Any attr The name of the attribute to get. Can contain dots. TYPE: str Source code in edsnlp/processing/helpers.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def rgetattr ( obj : Any , attr : str , * args : List [ Any ]) -> Any : \"\"\" Get attribute recursively Parameters ---------- obj : Any An object attr : str The name of the attribute to get. Can contain dots. \"\"\" def _getattr ( obj , attr ): return None if obj is None else getattr ( obj , attr , * args ) return functools . reduce ( _getattr , [ obj ] + attr . split ( \".\" )) slugify ( chained_attr ) Slugify a chained attribute name PARAMETER DESCRIPTION chained_attr The string to slugify (replace dots by _) TYPE: str RETURNS DESCRIPTION str The slugified string Source code in edsnlp/processing/helpers.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def slugify ( chained_attr : str ) -> str : \"\"\" Slugify a chained attribute name Parameters ---------- chained_attr : str The string to slugify (replace dots by _) Returns ------- str The slugified string \"\"\" return chained_attr . replace ( \".\" , \"_\" )","title":"helpers"},{"location":"reference/processing/helpers/#edsnlpprocessinghelpers","text":"","title":"edsnlp.processing.helpers"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.DataFrames","text":"","title":"DataFrames"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.spec","text":"","title":"spec"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.DataFrameModules","text":"Bases: Enum Source code in edsnlp/processing/helpers.py 10 11 12 13 class DataFrameModules ( Enum ): PANDAS = \"pandas\" PYSPARK = \"pyspark.sql\" KOALAS = \"databricks.koalas\"","title":"DataFrameModules"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.DataFrameModules.PANDAS","text":"","title":"PANDAS"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.DataFrameModules.PYSPARK","text":"","title":"PYSPARK"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.DataFrameModules.KOALAS","text":"","title":"KOALAS"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.get_module","text":"Source code in edsnlp/processing/helpers.py 27 28 29 30 def get_module ( df : DataFrames ): for module in list ( DataFrameModules ): if df . __class__ . __module__ . startswith ( module . value ): return module","title":"get_module()"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.check_spacy_version_for_context","text":"Source code in edsnlp/processing/helpers.py 33 34 35 36 37 38 39 40 41 42 def check_spacy_version_for_context (): # pragma: no cover import spacy spacy_version = getattr ( spacy , \"__version__\" ) if LooseVersion ( spacy_version ) < LooseVersion ( \"3.2\" ): raise VersionConflict ( \"You provided a `context` argument, which only work with spacy>=3.2. \\n \" f \"However, we found SpaCy version { spacy_version } . \\n \" , \"Please upgrade SpaCy ;)\" , )","title":"check_spacy_version_for_context()"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.rgetattr","text":"Get attribute recursively PARAMETER DESCRIPTION obj An object TYPE: Any attr The name of the attribute to get. Can contain dots. TYPE: str Source code in edsnlp/processing/helpers.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def rgetattr ( obj : Any , attr : str , * args : List [ Any ]) -> Any : \"\"\" Get attribute recursively Parameters ---------- obj : Any An object attr : str The name of the attribute to get. Can contain dots. \"\"\" def _getattr ( obj , attr ): return None if obj is None else getattr ( obj , attr , * args ) return functools . reduce ( _getattr , [ obj ] + attr . split ( \".\" ))","title":"rgetattr()"},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.slugify","text":"Slugify a chained attribute name PARAMETER DESCRIPTION chained_attr The string to slugify (replace dots by _) TYPE: str RETURNS DESCRIPTION str The slugified string Source code in edsnlp/processing/helpers.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def slugify ( chained_attr : str ) -> str : \"\"\" Slugify a chained attribute name Parameters ---------- chained_attr : str The string to slugify (replace dots by _) Returns ------- str The slugified string \"\"\" return chained_attr . replace ( \".\" , \"_\" )","title":"slugify()"},{"location":"reference/processing/parallel/","text":"edsnlp.processing.parallel nlp = spacy . blank ( 'eds' ) module-attribute pipe ( note , nlp , context = [], additional_spans = [], extensions = [], results_extractor = None , chunksize = 100 , n_jobs =- 2 , progress_bar = True , ** pipe_kwargs ) Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] results_extractor Arbitrary function that takes extract serialisable results from the computed spaCy Doc object. The output of the function must be a list of dictionaries containing the extracted spans or entities. TYPE: Optional[Callable[[Doc], List[Dict[str, Any]]]] DEFAULT: None additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default [] (empty list) DEFAULT: [] extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] chunksize Batch size used to split tasks TYPE: int DEFAULT: 100 n_jobs Max number of parallel jobs. The default value uses the maximum number of available cores. TYPE: int DEFAULT: -2 progress_bar Whether to display a progress bar or not TYPE: bool DEFAULT: True **pipe_kwargs Arguments exposed in processing.pipe_generator are also available here DEFAULT: {} RETURNS DESCRIPTION DataFrame A pandas DataFrame with one line per extraction Source code in edsnlp/processing/parallel.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def pipe ( note : pd . DataFrame , nlp : Language , context : List [ str ] = [], additional_spans : Union [ List [ str ], str ] = [], extensions : ExtensionSchema = [], results_extractor : Optional [ Callable [[ Doc ], List [ Dict [ str , Any ]]]] = None , chunksize : int = 100 , n_jobs : int = - 2 , progress_bar : bool = True , ** pipe_kwargs , ): \"\"\" Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. results_extractor : Optional[Callable[[Doc], List[Dict[str, Any]]]] Arbitrary function that takes extract serialisable results from the computed spaCy `Doc` object. The output of the function must be a list of dictionaries containing the extracted spans or entities. additional_spans : Union[List[str], str], by default [] (empty list) A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. chunksize: int, by default 100 Batch size used to split tasks n_jobs: int, by default -2 Max number of parallel jobs. The default value uses the maximum number of available cores. progress_bar: bool, by default True Whether to display a progress bar or not **pipe_kwargs: Arguments exposed in `processing.pipe_generator` are also available here Returns ------- DataFrame A pandas DataFrame with one line per extraction \"\"\" if context : check_spacy_version_for_context () # Setting the nlp variable _define_nlp ( nlp ) verbose = 10 if progress_bar else 0 executor = Parallel ( n_jobs , backend = \"multiprocessing\" , prefer = \"processes\" , verbose = verbose ) executor . warn ( f \"Used nlp components: { nlp . component_names } \" ) pipe_kwargs [ \"additional_spans\" ] = additional_spans pipe_kwargs [ \"extensions\" ] = extensions pipe_kwargs [ \"results_extractor\" ] = results_extractor pipe_kwargs [ \"context\" ] = context if verbose : executor . warn ( f \" { int ( len ( note ) / chunksize ) } tasks to complete\" ) do = delayed ( _process_chunk ) tasks = ( do ( chunk , ** pipe_kwargs ) for chunk in _chunker ( note , len ( note ), chunksize = chunksize ) ) result = executor ( tasks ) out = _flatten ( result ) return pd . DataFrame ( out )","title":"parallel"},{"location":"reference/processing/parallel/#edsnlpprocessingparallel","text":"","title":"edsnlp.processing.parallel"},{"location":"reference/processing/parallel/#edsnlp.processing.parallel.nlp","text":"","title":"nlp"},{"location":"reference/processing/parallel/#edsnlp.processing.parallel.pipe","text":"Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] results_extractor Arbitrary function that takes extract serialisable results from the computed spaCy Doc object. The output of the function must be a list of dictionaries containing the extracted spans or entities. TYPE: Optional[Callable[[Doc], List[Dict[str, Any]]]] DEFAULT: None additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default [] (empty list) DEFAULT: [] extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] chunksize Batch size used to split tasks TYPE: int DEFAULT: 100 n_jobs Max number of parallel jobs. The default value uses the maximum number of available cores. TYPE: int DEFAULT: -2 progress_bar Whether to display a progress bar or not TYPE: bool DEFAULT: True **pipe_kwargs Arguments exposed in processing.pipe_generator are also available here DEFAULT: {} RETURNS DESCRIPTION DataFrame A pandas DataFrame with one line per extraction Source code in edsnlp/processing/parallel.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def pipe ( note : pd . DataFrame , nlp : Language , context : List [ str ] = [], additional_spans : Union [ List [ str ], str ] = [], extensions : ExtensionSchema = [], results_extractor : Optional [ Callable [[ Doc ], List [ Dict [ str , Any ]]]] = None , chunksize : int = 100 , n_jobs : int = - 2 , progress_bar : bool = True , ** pipe_kwargs , ): \"\"\" Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. results_extractor : Optional[Callable[[Doc], List[Dict[str, Any]]]] Arbitrary function that takes extract serialisable results from the computed spaCy `Doc` object. The output of the function must be a list of dictionaries containing the extracted spans or entities. additional_spans : Union[List[str], str], by default [] (empty list) A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. chunksize: int, by default 100 Batch size used to split tasks n_jobs: int, by default -2 Max number of parallel jobs. The default value uses the maximum number of available cores. progress_bar: bool, by default True Whether to display a progress bar or not **pipe_kwargs: Arguments exposed in `processing.pipe_generator` are also available here Returns ------- DataFrame A pandas DataFrame with one line per extraction \"\"\" if context : check_spacy_version_for_context () # Setting the nlp variable _define_nlp ( nlp ) verbose = 10 if progress_bar else 0 executor = Parallel ( n_jobs , backend = \"multiprocessing\" , prefer = \"processes\" , verbose = verbose ) executor . warn ( f \"Used nlp components: { nlp . component_names } \" ) pipe_kwargs [ \"additional_spans\" ] = additional_spans pipe_kwargs [ \"extensions\" ] = extensions pipe_kwargs [ \"results_extractor\" ] = results_extractor pipe_kwargs [ \"context\" ] = context if verbose : executor . warn ( f \" { int ( len ( note ) / chunksize ) } tasks to complete\" ) do = delayed ( _process_chunk ) tasks = ( do ( chunk , ** pipe_kwargs ) for chunk in _chunker ( note , len ( note ), chunksize = chunksize ) ) result = executor ( tasks ) out = _flatten ( result ) return pd . DataFrame ( out )","title":"pipe()"},{"location":"reference/processing/simple/","text":"edsnlp.processing.simple nlp = spacy . blank ( 'eds' ) module-attribute ExtensionSchema = Union [ str , List [ str ], Dict [ str , Any ]] module-attribute pipe ( note , nlp , context = [], results_extractor = None , additional_spans = [], extensions = [], batch_size = 1000 , progress_bar = True ) Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version. PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: [] extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] batch_size Batch size used by spaCy's pipe TYPE: int, by default 1000 DEFAULT: 1000 progress_bar Whether to display a progress bar or not TYPE: bool DEFAULT: True RETURNS DESCRIPTION DataFrame A pandas DataFrame with one line per extraction Source code in edsnlp/processing/simple.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def pipe ( note : pd . DataFrame , nlp : Language , context : List [ str ] = [], results_extractor : Optional [ Callable [[ Doc ], List [ Dict [ str , Any ]]]] = None , additional_spans : Union [ List [ str ], str ] = [], extensions : Union [ List [ str ], str ] = [], batch_size : int = 1000 , progress_bar : bool = True , ): \"\"\" Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version. Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. batch_size : int, by default 1000 Batch size used by spaCy's pipe progress_bar: bool, by default True Whether to display a progress bar or not Returns ------- DataFrame A pandas DataFrame with one line per extraction \"\"\" return pd . DataFrame ( _flatten ( _pipe_generator ( note = note , nlp = nlp , context = context , results_extractor = results_extractor , additional_spans = additional_spans , extensions = extensions , batch_size = batch_size , progress_bar = progress_bar , ) ) )","title":"simple"},{"location":"reference/processing/simple/#edsnlpprocessingsimple","text":"","title":"edsnlp.processing.simple"},{"location":"reference/processing/simple/#edsnlp.processing.simple.nlp","text":"","title":"nlp"},{"location":"reference/processing/simple/#edsnlp.processing.simple.ExtensionSchema","text":"","title":"ExtensionSchema"},{"location":"reference/processing/simple/#edsnlp.processing.simple.pipe","text":"Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version. PARAMETER DESCRIPTION note A pandas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: [] extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] batch_size Batch size used by spaCy's pipe TYPE: int, by default 1000 DEFAULT: 1000 progress_bar Whether to display a progress bar or not TYPE: bool DEFAULT: True RETURNS DESCRIPTION DataFrame A pandas DataFrame with one line per extraction Source code in edsnlp/processing/simple.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def pipe ( note : pd . DataFrame , nlp : Language , context : List [ str ] = [], results_extractor : Optional [ Callable [[ Doc ], List [ Dict [ str , Any ]]]] = None , additional_spans : Union [ List [ str ], str ] = [], extensions : Union [ List [ str ], str ] = [], batch_size : int = 1000 , progress_bar : bool = True , ): \"\"\" Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version. Parameters ---------- note : DataFrame A pandas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. batch_size : int, by default 1000 Batch size used by spaCy's pipe progress_bar: bool, by default True Whether to display a progress bar or not Returns ------- DataFrame A pandas DataFrame with one line per extraction \"\"\" return pd . DataFrame ( _flatten ( _pipe_generator ( note = note , nlp = nlp , context = context , results_extractor = results_extractor , additional_spans = additional_spans , extensions = extensions , batch_size = batch_size , progress_bar = progress_bar , ) ) )","title":"pipe()"},{"location":"reference/processing/utils/","text":"edsnlp.processing.utils dummy_extractor ( doc ) Source code in edsnlp/processing/utils.py 6 7 8 9 10 def dummy_extractor ( doc : Doc ) -> List [ Dict [ str , Any ]]: return [ dict ( snippet = ent . text , length = len ( ent . text ), note_datetime = doc . _ . note_datetime ) for ent in doc . ents ]","title":"utils"},{"location":"reference/processing/utils/#edsnlpprocessingutils","text":"","title":"edsnlp.processing.utils"},{"location":"reference/processing/utils/#edsnlp.processing.utils.dummy_extractor","text":"Source code in edsnlp/processing/utils.py 6 7 8 9 10 def dummy_extractor ( doc : Doc ) -> List [ Dict [ str , Any ]]: return [ dict ( snippet = ent . text , length = len ( ent . text ), note_datetime = doc . _ . note_datetime ) for ent in doc . ents ]","title":"dummy_extractor()"},{"location":"reference/processing/wrapper/","text":"edsnlp.processing.wrapper pipe ( note , nlp , n_jobs =- 2 , context = [], results_extractor = None , additional_spans = [], extensions = [], ** kwargs ) Function to apply a spaCy pipe to a pandas or pyspark DataFrame PARAMETER DESCRIPTION note A pandas/pyspark/koalas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] n_jobs Only used when providing a Pandas DataFrame n_jobs=1 corresponds to simple_pipe n_jobs>1 corresponds to parallel_pipe with n_jobs parallel workers n_jobs=-1 corresponds to parallel_pipe with maximum number of workers n_jobs=-2 corresponds to parallel_pipe with maximum number of workers -1 TYPE: int, by default -2 DEFAULT: -2 additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: [] extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] kwargs Additional parameters depending on the how argument. TYPE: Dict[str, Any] RETURNS DESCRIPTION DataFrame A DataFrame with one line per extraction Source code in edsnlp/processing/wrapper.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def pipe ( note : DataFrames , nlp : Language , n_jobs : int = - 2 , context : List [ str ] = [], results_extractor : Optional [ Callable [[ Doc ], List [ Dict [ str , Any ]]]] = None , additional_spans : Union [ List [ str ], str ] = [], extensions : ExtensionSchema = [], ** kwargs : Dict [ str , Any ], ) -> DataFrames : \"\"\" Function to apply a spaCy pipe to a pandas or pyspark DataFrame Parameters ---------- note : DataFrame A pandas/pyspark/koalas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. n_jobs : int, by default -2 Only used when providing a Pandas DataFrame - `n_jobs=1` corresponds to `simple_pipe` - `n_jobs>1` corresponds to `parallel_pipe` with `n_jobs` parallel workers - `n_jobs=-1` corresponds to `parallel_pipe` with maximum number of workers - `n_jobs=-2` corresponds to `parallel_pipe` with maximum number of workers -1 additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. kwargs : Dict[str, Any] Additional parameters depending on the `how` argument. Returns ------- DataFrame A DataFrame with one line per extraction \"\"\" module = get_module ( note ) if module == DataFrameModules . PANDAS : kwargs . pop ( \"dtypes\" , None ) if n_jobs == 1 : return simple_pipe ( note = note , nlp = nlp , context = context , results_extractor = results_extractor , additional_spans = additional_spans , extensions = extensions , ** kwargs , ) else : return parallel_pipe ( note = note , nlp = nlp , context = context , results_extractor = results_extractor , additional_spans = additional_spans , extensions = extensions , n_jobs = n_jobs , ** kwargs , ) if type ( extensions ) != dict : if extensions : raise ValueError ( \"\"\" When using Spark or Koalas, you should provide extension names along with the extension type (as a dictionnary): `d[extension_name] = extension_type` \"\"\" # noqa W291 ) else : extensions = {} from .distributed import custom_pipe from .distributed import pipe as distributed_pipe if results_extractor is None : return distributed_pipe ( note = note , nlp = nlp , context = context , additional_spans = additional_spans , extensions = extensions , ** kwargs , ) else : dtypes = kwargs . pop ( \"dtypes\" ) return custom_pipe ( note = note , nlp = nlp , context = context , results_extractor = results_extractor , dtypes = dtypes , ** kwargs , )","title":"wrapper"},{"location":"reference/processing/wrapper/#edsnlpprocessingwrapper","text":"","title":"edsnlp.processing.wrapper"},{"location":"reference/processing/wrapper/#edsnlp.processing.wrapper.pipe","text":"Function to apply a spaCy pipe to a pandas or pyspark DataFrame PARAMETER DESCRIPTION note A pandas/pyspark/koalas DataFrame with a note_id and note_text column TYPE: DataFrame nlp A spaCy pipe TYPE: Language context A list of column to add to the generated SpaCy document as an extension. For instance, if context=[\"note_datetime\"], the corresponding value found in the note_datetime column will be stored in doc._.note_datetime , which can be useful e.g. for the dates` pipeline. TYPE: List[str] DEFAULT: [] n_jobs Only used when providing a Pandas DataFrame n_jobs=1 corresponds to simple_pipe n_jobs>1 corresponds to parallel_pipe with n_jobs parallel workers n_jobs=-1 corresponds to parallel_pipe with maximum number of workers n_jobs=-2 corresponds to parallel_pipe with maximum number of workers -1 TYPE: int, by default -2 DEFAULT: -2 additional_spans A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as doc.spans[spangroup_name] and can be generated by some pipes. For instance, the date pipe populates doc.spans['dates'] TYPE: Union[List[str], str], by default \"discarded\" DEFAULT: [] extensions Spans extensions to add to the extracted results: For instance, if extensions=[\"score_name\"] , the extracted result will include, for each entity, ent._.score_name . TYPE: List[Tuple[str, T.DataType]], by default [] DEFAULT: [] kwargs Additional parameters depending on the how argument. TYPE: Dict[str, Any] RETURNS DESCRIPTION DataFrame A DataFrame with one line per extraction Source code in edsnlp/processing/wrapper.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def pipe ( note : DataFrames , nlp : Language , n_jobs : int = - 2 , context : List [ str ] = [], results_extractor : Optional [ Callable [[ Doc ], List [ Dict [ str , Any ]]]] = None , additional_spans : Union [ List [ str ], str ] = [], extensions : ExtensionSchema = [], ** kwargs : Dict [ str , Any ], ) -> DataFrames : \"\"\" Function to apply a spaCy pipe to a pandas or pyspark DataFrame Parameters ---------- note : DataFrame A pandas/pyspark/koalas DataFrame with a `note_id` and `note_text` column nlp : Language A spaCy pipe context : List[str] A list of column to add to the generated SpaCy document as an extension. For instance, if `context=[\"note_datetime\"], the corresponding value found in the `note_datetime` column will be stored in `doc._.note_datetime`, which can be useful e.g. for the `dates` pipeline. n_jobs : int, by default -2 Only used when providing a Pandas DataFrame - `n_jobs=1` corresponds to `simple_pipe` - `n_jobs>1` corresponds to `parallel_pipe` with `n_jobs` parallel workers - `n_jobs=-1` corresponds to `parallel_pipe` with maximum number of workers - `n_jobs=-2` corresponds to `parallel_pipe` with maximum number of workers -1 additional_spans : Union[List[str], str], by default \"discarded\" A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as `doc.spans[spangroup_name]` and can be generated by some pipes. For instance, the `date` pipe populates doc.spans['dates'] extensions : List[Tuple[str, T.DataType]], by default [] Spans extensions to add to the extracted results: For instance, if `extensions=[\"score_name\"]`, the extracted result will include, for each entity, `ent._.score_name`. kwargs : Dict[str, Any] Additional parameters depending on the `how` argument. Returns ------- DataFrame A DataFrame with one line per extraction \"\"\" module = get_module ( note ) if module == DataFrameModules . PANDAS : kwargs . pop ( \"dtypes\" , None ) if n_jobs == 1 : return simple_pipe ( note = note , nlp = nlp , context = context , results_extractor = results_extractor , additional_spans = additional_spans , extensions = extensions , ** kwargs , ) else : return parallel_pipe ( note = note , nlp = nlp , context = context , results_extractor = results_extractor , additional_spans = additional_spans , extensions = extensions , n_jobs = n_jobs , ** kwargs , ) if type ( extensions ) != dict : if extensions : raise ValueError ( \"\"\" When using Spark or Koalas, you should provide extension names along with the extension type (as a dictionnary): `d[extension_name] = extension_type` \"\"\" # noqa W291 ) else : extensions = {} from .distributed import custom_pipe from .distributed import pipe as distributed_pipe if results_extractor is None : return distributed_pipe ( note = note , nlp = nlp , context = context , additional_spans = additional_spans , extensions = extensions , ** kwargs , ) else : dtypes = kwargs . pop ( \"dtypes\" ) return custom_pipe ( note = note , nlp = nlp , context = context , results_extractor = results_extractor , dtypes = dtypes , ** kwargs , )","title":"pipe()"},{"location":"reference/utils/","text":"edsnlp.utils","title":"`edsnlp.utils`"},{"location":"reference/utils/#edsnlputils","text":"","title":"edsnlp.utils"},{"location":"reference/utils/blocs/","text":"edsnlp.utils.blocs Utility that extracts code blocs and runs them. Largely inspired by https://github.com/koaning/mktestdocs BLOCK_PATTERN = re . compile ( '((?P<skip><!-- no-check -->) \\\\ s+)?(?P<indent> *)```(?P<title>.*?) \\\\ n(?P<code>.+?)```' , flags = re . DOTALL ) module-attribute OUTPUT_PATTERN = '# Out: ' module-attribute check_outputs ( code ) Looks for output patterns, and modifies the bloc: The preceding line becomes v = expr The output line becomes an assert statement PARAMETER DESCRIPTION code Code block TYPE: str RETURNS DESCRIPTION str Modified code bloc with assert statements Source code in edsnlp/utils/blocs.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def check_outputs ( code : str ) -> str : \"\"\" Looks for output patterns, and modifies the bloc: 1. The preceding line becomes `#!python v = expr` 2. The output line becomes an `#!python assert` statement Parameters ---------- code : str Code block Returns ------- str Modified code bloc with assert statements \"\"\" lines : List [ str ] = code . split ( \" \\n \" ) code = [] skip = False if len ( lines ) < 2 : return code for expression , output in zip ( lines [: - 1 ], lines [ 1 :]): if skip : skip = not skip continue if output . startswith ( OUTPUT_PATTERN ): expression = f \"v = { expression } \" output = output [ len ( OUTPUT_PATTERN ) :] . replace ( '\"' , r \" \\\" \" ) output = f 'assert repr(v) == \" { output } \" or str(v) == \" { output } \"' code . append ( expression ) code . append ( output ) skip = True else : code . append ( expression ) if not skip : code . append ( output ) return \" \\n \" . join ( code ) remove_indentation ( code , indent ) Remove indentation from a code bloc. PARAMETER DESCRIPTION code Code bloc TYPE: str indent Level of indentation TYPE: int RETURNS DESCRIPTION str Modified code bloc Source code in edsnlp/utils/blocs.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def remove_indentation ( code : str , indent : int ) -> str : \"\"\" Remove indentation from a code bloc. Parameters ---------- code : str Code bloc indent : int Level of indentation Returns ------- str Modified code bloc \"\"\" if not indent : return code lines = [] for line in code . split ( \" \\n \" ): lines . append ( line [ indent :]) return \" \\n \" . join ( lines ) grab_code_blocks ( docstring , lang = 'python' ) Given a docstring, grab all the markdown codeblocks found in docstring. PARAMETER DESCRIPTION docstring Full text. TYPE: str lang Language to execute, by default \"python\" TYPE: str, optional DEFAULT: 'python' RETURNS DESCRIPTION List[str] Extracted code blocks Source code in edsnlp/utils/blocs.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def grab_code_blocks ( docstring : str , lang = \"python\" ) -> List [ str ]: \"\"\" Given a docstring, grab all the markdown codeblocks found in docstring. Parameters ---------- docstring : str Full text. lang : str, optional Language to execute, by default \"python\" Returns ------- List[str] Extracted code blocks \"\"\" codeblocks = [] for match in BLOCK_PATTERN . finditer ( docstring ): d = match . groupdict () if d [ \"skip\" ]: continue if lang in d [ \"title\" ]: code = remove_indentation ( d [ \"code\" ], len ( d [ \"indent\" ])) code = check_outputs ( code ) codeblocks . append ( code ) return codeblocks printer ( code ) Prints a code bloc with lines for easier debugging. PARAMETER DESCRIPTION code Code bloc. TYPE: str Source code in edsnlp/utils/blocs.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def printer ( code : str ) -> None : \"\"\" Prints a code bloc with lines for easier debugging. Parameters ---------- code : str Code bloc. \"\"\" lines = [] for i , line in enumerate ( code . split ( \" \\n \" )): lines . append ( f \" { i + 1 : 03 } { line } \" ) print ( \" \\n \" . join ( lines )) check_docstring ( obj , lang = '' ) Given a function, test the contents of the docstring. Source code in edsnlp/utils/blocs.py 148 149 150 151 152 153 154 155 156 157 158 def check_docstring ( obj , lang = \"\" ): \"\"\" Given a function, test the contents of the docstring. \"\"\" for b in grab_code_blocks ( obj . __doc__ , lang = lang ): try : exec ( b , { \"__MODULE__\" : \"__main__\" }) except Exception : print ( f \"Error Encountered in ` { obj . __name__ } `. Caused by: \\n \" ) printer ( b ) raise check_raw_string ( raw , lang = 'python' ) Given a raw string, test the contents. Source code in edsnlp/utils/blocs.py 161 162 163 164 165 166 167 168 169 170 def check_raw_string ( raw , lang = \"python\" ): \"\"\" Given a raw string, test the contents. \"\"\" for b in grab_code_blocks ( raw , lang = lang ): try : exec ( b , { \"__MODULE__\" : \"__main__\" }) except Exception : printer ( b ) raise check_raw_file_full ( raw , lang = 'python' ) Source code in edsnlp/utils/blocs.py 173 174 175 176 177 178 179 def check_raw_file_full ( raw , lang = \"python\" ): all_code = \" \\n \" . join ( grab_code_blocks ( raw , lang = lang )) try : exec ( all_code , { \"__MODULE__\" : \"__main__\" }) except Exception : printer ( all_code ) raise check_md_file ( path , memory = False ) Given a markdown file, parse the contents for Python code blocs and check that each independant bloc does not cause an error. PARAMETER DESCRIPTION path Path to the markdown file to execute. TYPE: Path memory Whether to keep results from one bloc to the next, by default False TYPE: bool, optional DEFAULT: False Source code in edsnlp/utils/blocs.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def check_md_file ( path : Path , memory : bool = False ) -> None : \"\"\" Given a markdown file, parse the contents for Python code blocs and check that each independant bloc does not cause an error. Parameters ---------- path : Path Path to the markdown file to execute. memory : bool, optional Whether to keep results from one bloc to the next, by default `#!python False` \"\"\" text = Path ( path ) . read_text () if memory : check_raw_file_full ( text , lang = \"python\" ) else : check_raw_string ( text , lang = \"python\" )","title":"blocs"},{"location":"reference/utils/blocs/#edsnlputilsblocs","text":"Utility that extracts code blocs and runs them. Largely inspired by https://github.com/koaning/mktestdocs","title":"edsnlp.utils.blocs"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.BLOCK_PATTERN","text":"","title":"BLOCK_PATTERN"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.OUTPUT_PATTERN","text":"","title":"OUTPUT_PATTERN"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_outputs","text":"Looks for output patterns, and modifies the bloc: The preceding line becomes v = expr The output line becomes an assert statement PARAMETER DESCRIPTION code Code block TYPE: str RETURNS DESCRIPTION str Modified code bloc with assert statements Source code in edsnlp/utils/blocs.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def check_outputs ( code : str ) -> str : \"\"\" Looks for output patterns, and modifies the bloc: 1. The preceding line becomes `#!python v = expr` 2. The output line becomes an `#!python assert` statement Parameters ---------- code : str Code block Returns ------- str Modified code bloc with assert statements \"\"\" lines : List [ str ] = code . split ( \" \\n \" ) code = [] skip = False if len ( lines ) < 2 : return code for expression , output in zip ( lines [: - 1 ], lines [ 1 :]): if skip : skip = not skip continue if output . startswith ( OUTPUT_PATTERN ): expression = f \"v = { expression } \" output = output [ len ( OUTPUT_PATTERN ) :] . replace ( '\"' , r \" \\\" \" ) output = f 'assert repr(v) == \" { output } \" or str(v) == \" { output } \"' code . append ( expression ) code . append ( output ) skip = True else : code . append ( expression ) if not skip : code . append ( output ) return \" \\n \" . join ( code )","title":"check_outputs()"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.remove_indentation","text":"Remove indentation from a code bloc. PARAMETER DESCRIPTION code Code bloc TYPE: str indent Level of indentation TYPE: int RETURNS DESCRIPTION str Modified code bloc Source code in edsnlp/utils/blocs.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def remove_indentation ( code : str , indent : int ) -> str : \"\"\" Remove indentation from a code bloc. Parameters ---------- code : str Code bloc indent : int Level of indentation Returns ------- str Modified code bloc \"\"\" if not indent : return code lines = [] for line in code . split ( \" \\n \" ): lines . append ( line [ indent :]) return \" \\n \" . join ( lines )","title":"remove_indentation()"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.grab_code_blocks","text":"Given a docstring, grab all the markdown codeblocks found in docstring. PARAMETER DESCRIPTION docstring Full text. TYPE: str lang Language to execute, by default \"python\" TYPE: str, optional DEFAULT: 'python' RETURNS DESCRIPTION List[str] Extracted code blocks Source code in edsnlp/utils/blocs.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def grab_code_blocks ( docstring : str , lang = \"python\" ) -> List [ str ]: \"\"\" Given a docstring, grab all the markdown codeblocks found in docstring. Parameters ---------- docstring : str Full text. lang : str, optional Language to execute, by default \"python\" Returns ------- List[str] Extracted code blocks \"\"\" codeblocks = [] for match in BLOCK_PATTERN . finditer ( docstring ): d = match . groupdict () if d [ \"skip\" ]: continue if lang in d [ \"title\" ]: code = remove_indentation ( d [ \"code\" ], len ( d [ \"indent\" ])) code = check_outputs ( code ) codeblocks . append ( code ) return codeblocks","title":"grab_code_blocks()"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.printer","text":"Prints a code bloc with lines for easier debugging. PARAMETER DESCRIPTION code Code bloc. TYPE: str Source code in edsnlp/utils/blocs.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def printer ( code : str ) -> None : \"\"\" Prints a code bloc with lines for easier debugging. Parameters ---------- code : str Code bloc. \"\"\" lines = [] for i , line in enumerate ( code . split ( \" \\n \" )): lines . append ( f \" { i + 1 : 03 } { line } \" ) print ( \" \\n \" . join ( lines ))","title":"printer()"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_docstring","text":"Given a function, test the contents of the docstring. Source code in edsnlp/utils/blocs.py 148 149 150 151 152 153 154 155 156 157 158 def check_docstring ( obj , lang = \"\" ): \"\"\" Given a function, test the contents of the docstring. \"\"\" for b in grab_code_blocks ( obj . __doc__ , lang = lang ): try : exec ( b , { \"__MODULE__\" : \"__main__\" }) except Exception : print ( f \"Error Encountered in ` { obj . __name__ } `. Caused by: \\n \" ) printer ( b ) raise","title":"check_docstring()"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_raw_string","text":"Given a raw string, test the contents. Source code in edsnlp/utils/blocs.py 161 162 163 164 165 166 167 168 169 170 def check_raw_string ( raw , lang = \"python\" ): \"\"\" Given a raw string, test the contents. \"\"\" for b in grab_code_blocks ( raw , lang = lang ): try : exec ( b , { \"__MODULE__\" : \"__main__\" }) except Exception : printer ( b ) raise","title":"check_raw_string()"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_raw_file_full","text":"Source code in edsnlp/utils/blocs.py 173 174 175 176 177 178 179 def check_raw_file_full ( raw , lang = \"python\" ): all_code = \" \\n \" . join ( grab_code_blocks ( raw , lang = lang )) try : exec ( all_code , { \"__MODULE__\" : \"__main__\" }) except Exception : printer ( all_code ) raise","title":"check_raw_file_full()"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_md_file","text":"Given a markdown file, parse the contents for Python code blocs and check that each independant bloc does not cause an error. PARAMETER DESCRIPTION path Path to the markdown file to execute. TYPE: Path memory Whether to keep results from one bloc to the next, by default False TYPE: bool, optional DEFAULT: False Source code in edsnlp/utils/blocs.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def check_md_file ( path : Path , memory : bool = False ) -> None : \"\"\" Given a markdown file, parse the contents for Python code blocs and check that each independant bloc does not cause an error. Parameters ---------- path : Path Path to the markdown file to execute. memory : bool, optional Whether to keep results from one bloc to the next, by default `#!python False` \"\"\" text = Path ( path ) . read_text () if memory : check_raw_file_full ( text , lang = \"python\" ) else : check_raw_string ( text , lang = \"python\" )","title":"check_md_file()"},{"location":"reference/utils/colors/","text":"edsnlp.utils.colors CATEGORY20 = [ '#1f77b4' , '#aec7e8' , '#ff7f0e' , '#ffbb78' , '#2ca02c' , '#98df8a' , '#d62728' , '#ff9896' , '#9467bd' , '#c5b0d5' , '#8c564b' , '#c49c94' , '#e377c2' , '#f7b6d2' , '#7f7f7f' , '#c7c7c7' , '#bcbd22' , '#dbdb8d' , '#17becf' , '#9edae5' ] module-attribute create_colors ( labels ) Assign a colour for each label, using category20 palette. The method loops over the colour palette in case there are too many labels. PARAMETER DESCRIPTION labels List of labels to colorise in displacy. TYPE: List[str] RETURNS DESCRIPTION Dict[str, str] A displacy-compatible colour assignment. Source code in edsnlp/utils/colors.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def create_colors ( labels : List [ str ]) -> Dict [ str , str ]: \"\"\" Assign a colour for each label, using category20 palette. The method loops over the colour palette in case there are too many labels. Parameters ---------- labels : List[str] List of labels to colorise in displacy. Returns ------- Dict[str, str] A displacy-compatible colour assignment. \"\"\" colors = { label : cat for label , cat in zip ( labels , cycle ( CATEGORY20 ))} return colors","title":"colors"},{"location":"reference/utils/colors/#edsnlputilscolors","text":"","title":"edsnlp.utils.colors"},{"location":"reference/utils/colors/#edsnlp.utils.colors.CATEGORY20","text":"","title":"CATEGORY20"},{"location":"reference/utils/colors/#edsnlp.utils.colors.create_colors","text":"Assign a colour for each label, using category20 palette. The method loops over the colour palette in case there are too many labels. PARAMETER DESCRIPTION labels List of labels to colorise in displacy. TYPE: List[str] RETURNS DESCRIPTION Dict[str, str] A displacy-compatible colour assignment. Source code in edsnlp/utils/colors.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def create_colors ( labels : List [ str ]) -> Dict [ str , str ]: \"\"\" Assign a colour for each label, using category20 palette. The method loops over the colour palette in case there are too many labels. Parameters ---------- labels : List[str] List of labels to colorise in displacy. Returns ------- Dict[str, str] A displacy-compatible colour assignment. \"\"\" colors = { label : cat for label , cat in zip ( labels , cycle ( CATEGORY20 ))} return colors","title":"create_colors()"},{"location":"reference/utils/deprecation/","text":"edsnlp.utils.deprecation deprecated_extension ( name , new_name ) Source code in edsnlp/utils/deprecation.py 9 10 11 12 13 14 15 16 def deprecated_extension ( name : str , new_name : str ) -> None : msg = ( f 'The extension \" { name } \" is deprecated and will be ' \"removed in a future version. \" f 'Please use \" { new_name } \" instead.' ) logger . warning ( msg ) deprecated_getter_factory ( name , new_name ) Source code in edsnlp/utils/deprecation.py 19 20 21 22 23 24 25 26 27 28 29 def deprecated_getter_factory ( name : str , new_name : str ) -> Callable : def getter ( toklike : Union [ Token , Span , Doc ]) -> Any : n = f \" { type ( toklike ) . __name__ } ._. { name } \" nn = f \" { type ( toklike ) . __name__ } ._. { new_name } \" deprecated_extension ( n , nn ) return getattr ( toklike . _ , new_name ) return getter deprecation ( name , new_name = None ) Source code in edsnlp/utils/deprecation.py 32 33 34 35 36 37 38 39 40 41 42 def deprecation ( name : str , new_name : Optional [ str ] = None ): new_name = new_name or f \"eds. { name } \" msg = ( f 'Calling \" { name } \" directly is deprecated and ' \"will be removed in a future version. \" f 'Please use \" { new_name } \" instead.' ) logger . warning ( msg ) deprecated_factory ( name , new_name = None , default_config = None , func = None , ** kwargs ) Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning. PARAMETER DESCRIPTION name The deprecated name for the pipeline TYPE: str new_name The new name for the pipeline, which should be used, by default None TYPE: Optional[str], optional DEFAULT: None default_config The configuration that should be passed to Language.factory, by default None TYPE: Optional[Dict[str, Any]], optional DEFAULT: None func The function to decorate, by default None TYPE: Optional[Callable], optional DEFAULT: None RETURNS DESCRIPTION Callable Source code in edsnlp/utils/deprecation.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def deprecated_factory ( name : str , new_name : Optional [ str ] = None , default_config : Optional [ Dict [ str , Any ]] = None , func : Optional [ Callable ] = None , ** kwargs , ) -> Callable : \"\"\" Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning. Parameters ---------- name : str The deprecated name for the pipeline new_name : Optional[str], optional The new name for the pipeline, which should be used, by default None default_config : Optional[Dict[str, Any]], optional The configuration that should be passed to Language.factory, by default None func : Optional[Callable], optional The function to decorate, by default None Returns ------- Callable \"\"\" if default_config is None : default_config = dict () wrapper = Language . factory ( name , default_config = default_config , ** kwargs ) def wrap ( factory ): # Define decorator # We use micheles' decorator package to keep the same signature # See https://github.com/micheles/decorator/ @decorator def decorate ( f , * args , ** kwargs , ): deprecation ( name , new_name ) return f ( * args , ** kwargs , ) decorated = decorate ( factory ) wrapper ( decorated ) return factory if func is not None : return wrap ( func ) return wrap","title":"deprecation"},{"location":"reference/utils/deprecation/#edsnlputilsdeprecation","text":"","title":"edsnlp.utils.deprecation"},{"location":"reference/utils/deprecation/#edsnlp.utils.deprecation.deprecated_extension","text":"Source code in edsnlp/utils/deprecation.py 9 10 11 12 13 14 15 16 def deprecated_extension ( name : str , new_name : str ) -> None : msg = ( f 'The extension \" { name } \" is deprecated and will be ' \"removed in a future version. \" f 'Please use \" { new_name } \" instead.' ) logger . warning ( msg )","title":"deprecated_extension()"},{"location":"reference/utils/deprecation/#edsnlp.utils.deprecation.deprecated_getter_factory","text":"Source code in edsnlp/utils/deprecation.py 19 20 21 22 23 24 25 26 27 28 29 def deprecated_getter_factory ( name : str , new_name : str ) -> Callable : def getter ( toklike : Union [ Token , Span , Doc ]) -> Any : n = f \" { type ( toklike ) . __name__ } ._. { name } \" nn = f \" { type ( toklike ) . __name__ } ._. { new_name } \" deprecated_extension ( n , nn ) return getattr ( toklike . _ , new_name ) return getter","title":"deprecated_getter_factory()"},{"location":"reference/utils/deprecation/#edsnlp.utils.deprecation.deprecation","text":"Source code in edsnlp/utils/deprecation.py 32 33 34 35 36 37 38 39 40 41 42 def deprecation ( name : str , new_name : Optional [ str ] = None ): new_name = new_name or f \"eds. { name } \" msg = ( f 'Calling \" { name } \" directly is deprecated and ' \"will be removed in a future version. \" f 'Please use \" { new_name } \" instead.' ) logger . warning ( msg )","title":"deprecation()"},{"location":"reference/utils/deprecation/#edsnlp.utils.deprecation.deprecated_factory","text":"Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning. PARAMETER DESCRIPTION name The deprecated name for the pipeline TYPE: str new_name The new name for the pipeline, which should be used, by default None TYPE: Optional[str], optional DEFAULT: None default_config The configuration that should be passed to Language.factory, by default None TYPE: Optional[Dict[str, Any]], optional DEFAULT: None func The function to decorate, by default None TYPE: Optional[Callable], optional DEFAULT: None RETURNS DESCRIPTION Callable Source code in edsnlp/utils/deprecation.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def deprecated_factory ( name : str , new_name : Optional [ str ] = None , default_config : Optional [ Dict [ str , Any ]] = None , func : Optional [ Callable ] = None , ** kwargs , ) -> Callable : \"\"\" Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning. Parameters ---------- name : str The deprecated name for the pipeline new_name : Optional[str], optional The new name for the pipeline, which should be used, by default None default_config : Optional[Dict[str, Any]], optional The configuration that should be passed to Language.factory, by default None func : Optional[Callable], optional The function to decorate, by default None Returns ------- Callable \"\"\" if default_config is None : default_config = dict () wrapper = Language . factory ( name , default_config = default_config , ** kwargs ) def wrap ( factory ): # Define decorator # We use micheles' decorator package to keep the same signature # See https://github.com/micheles/decorator/ @decorator def decorate ( f , * args , ** kwargs , ): deprecation ( name , new_name ) return f ( * args , ** kwargs , ) decorated = decorate ( factory ) wrapper ( decorated ) return factory if func is not None : return wrap ( func ) return wrap","title":"deprecated_factory()"},{"location":"reference/utils/examples/","text":"edsnlp.utils.examples entity_pattern = re . compile ( '(<ent[^<>]*>[^<>]+</ent>)' , flags = re . DOTALL ) module-attribute text_pattern = re . compile ( '<ent.*>(.+)</ent>' , flags = re . DOTALL ) module-attribute modifiers_pattern = re . compile ( '<ent \\\\ s?(.*)>.+</ent>' , flags = re . DOTALL ) module-attribute single_modifiers_pattern = regex . compile ( \"(?P<key>[^ \\\\ s]+?)=((?P<value>{.*?})|(?P<value>[^ \\\\ s']+)|'(?P<value>.+?)')\" , flags = regex . DOTALL ) module-attribute Match Bases: BaseModel Source code in edsnlp/utils/examples.py 9 10 11 12 13 class Match ( BaseModel ): start_char : int end_char : int text : str modifiers : str start_char : int = None class-attribute end_char : int = None class-attribute text : str = None class-attribute modifiers : str = None class-attribute Modifier Bases: BaseModel Source code in edsnlp/utils/examples.py 16 17 18 19 20 21 22 23 24 25 26 27 class Modifier ( BaseModel ): key : str value : Union [ int , float , bool , str , Dict [ str , Any ]] @validator ( \"value\" ) def optional_dict_parsing ( cls , v ): if isinstance ( v , str ): try : return json . loads ( v . replace ( \"'\" , '\"' )) except json . JSONDecodeError : return v return v key : str = None class-attribute value : Union [ int , float , bool , str , Dict [ str , Any ]] = None class-attribute optional_dict_parsing ( v ) Source code in edsnlp/utils/examples.py 20 21 22 23 24 25 26 27 @validator ( \"value\" ) def optional_dict_parsing ( cls , v ): if isinstance ( v , str ): try : return json . loads ( v . replace ( \"'\" , '\"' )) except json . JSONDecodeError : return v return v Entity Bases: BaseModel Source code in edsnlp/utils/examples.py 30 31 32 33 class Entity ( BaseModel ): start_char : int end_char : int modifiers : List [ Modifier ] start_char : int = None class-attribute end_char : int = None class-attribute modifiers : List [ Modifier ] = None class-attribute find_matches ( example ) Finds entities within the example. PARAMETER DESCRIPTION example Example to process. TYPE: str RETURNS DESCRIPTION List[re.Match] List of matches for entities. Source code in edsnlp/utils/examples.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def find_matches ( example : str ) -> List [ re . Match ]: \"\"\" Finds entities within the example. Parameters ---------- example : str Example to process. Returns ------- List[re.Match] List of matches for entities. \"\"\" return list ( entity_pattern . finditer ( example )) parse_match ( match ) Parse a regex match representing an entity. PARAMETER DESCRIPTION match Match for an entity. TYPE: re.Match RETURNS DESCRIPTION Match Usable representation for the entity match. Source code in edsnlp/utils/examples.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def parse_match ( match : re . Match ) -> Match : \"\"\" Parse a regex match representing an entity. Parameters ---------- match : re.Match Match for an entity. Returns ------- Match Usable representation for the entity match. \"\"\" lexical_variant = match . group () start_char = match . start () end_char = match . end () text = text_pattern . findall ( lexical_variant )[ 0 ] modifiers = modifiers_pattern . findall ( lexical_variant )[ 0 ] m = Match ( start_char = start_char , end_char = end_char , text = text , modifiers = modifiers ) return m parse_example ( example ) Parses an example : finds examples and removes the tags. PARAMETER DESCRIPTION example Example to process. TYPE: str RETURNS DESCRIPTION Tuple[str, List[Entity]] Cleaned text and extracted entities. Source code in edsnlp/utils/examples.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def parse_example ( example : str ) -> Tuple [ str , List [ Entity ]]: \"\"\" Parses an example : finds examples and removes the tags. Parameters ---------- example : str Example to process. Returns ------- Tuple[str, List[Entity]] Cleaned text and extracted entities. \"\"\" matches = [ parse_match ( match ) for match in find_matches ( example = example )] text = \"\" entities = [] cursor = 0 for match in matches : text += example [ cursor : match . start_char ] start_char = len ( text ) text += match . text end_char = len ( text ) cursor = match . end_char entity = Entity ( start_char = start_char , end_char = end_char , modifiers = [ Modifier . parse_obj ( m . groupdict ()) for m in single_modifiers_pattern . finditer ( match . modifiers ) ], ) entities . append ( entity ) text += example [ cursor :] return text , entities","title":"examples"},{"location":"reference/utils/examples/#edsnlputilsexamples","text":"","title":"edsnlp.utils.examples"},{"location":"reference/utils/examples/#edsnlp.utils.examples.entity_pattern","text":"","title":"entity_pattern"},{"location":"reference/utils/examples/#edsnlp.utils.examples.text_pattern","text":"","title":"text_pattern"},{"location":"reference/utils/examples/#edsnlp.utils.examples.modifiers_pattern","text":"","title":"modifiers_pattern"},{"location":"reference/utils/examples/#edsnlp.utils.examples.single_modifiers_pattern","text":"","title":"single_modifiers_pattern"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Match","text":"Bases: BaseModel Source code in edsnlp/utils/examples.py 9 10 11 12 13 class Match ( BaseModel ): start_char : int end_char : int text : str modifiers : str","title":"Match"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Match.start_char","text":"","title":"start_char"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Match.end_char","text":"","title":"end_char"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Match.text","text":"","title":"text"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Match.modifiers","text":"","title":"modifiers"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Modifier","text":"Bases: BaseModel Source code in edsnlp/utils/examples.py 16 17 18 19 20 21 22 23 24 25 26 27 class Modifier ( BaseModel ): key : str value : Union [ int , float , bool , str , Dict [ str , Any ]] @validator ( \"value\" ) def optional_dict_parsing ( cls , v ): if isinstance ( v , str ): try : return json . loads ( v . replace ( \"'\" , '\"' )) except json . JSONDecodeError : return v return v","title":"Modifier"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Modifier.key","text":"","title":"key"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Modifier.value","text":"","title":"value"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Modifier.optional_dict_parsing","text":"Source code in edsnlp/utils/examples.py 20 21 22 23 24 25 26 27 @validator ( \"value\" ) def optional_dict_parsing ( cls , v ): if isinstance ( v , str ): try : return json . loads ( v . replace ( \"'\" , '\"' )) except json . JSONDecodeError : return v return v","title":"optional_dict_parsing()"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Entity","text":"Bases: BaseModel Source code in edsnlp/utils/examples.py 30 31 32 33 class Entity ( BaseModel ): start_char : int end_char : int modifiers : List [ Modifier ]","title":"Entity"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Entity.start_char","text":"","title":"start_char"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Entity.end_char","text":"","title":"end_char"},{"location":"reference/utils/examples/#edsnlp.utils.examples.Entity.modifiers","text":"","title":"modifiers"},{"location":"reference/utils/examples/#edsnlp.utils.examples.find_matches","text":"Finds entities within the example. PARAMETER DESCRIPTION example Example to process. TYPE: str RETURNS DESCRIPTION List[re.Match] List of matches for entities. Source code in edsnlp/utils/examples.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def find_matches ( example : str ) -> List [ re . Match ]: \"\"\" Finds entities within the example. Parameters ---------- example : str Example to process. Returns ------- List[re.Match] List of matches for entities. \"\"\" return list ( entity_pattern . finditer ( example ))","title":"find_matches()"},{"location":"reference/utils/examples/#edsnlp.utils.examples.parse_match","text":"Parse a regex match representing an entity. PARAMETER DESCRIPTION match Match for an entity. TYPE: re.Match RETURNS DESCRIPTION Match Usable representation for the entity match. Source code in edsnlp/utils/examples.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def parse_match ( match : re . Match ) -> Match : \"\"\" Parse a regex match representing an entity. Parameters ---------- match : re.Match Match for an entity. Returns ------- Match Usable representation for the entity match. \"\"\" lexical_variant = match . group () start_char = match . start () end_char = match . end () text = text_pattern . findall ( lexical_variant )[ 0 ] modifiers = modifiers_pattern . findall ( lexical_variant )[ 0 ] m = Match ( start_char = start_char , end_char = end_char , text = text , modifiers = modifiers ) return m","title":"parse_match()"},{"location":"reference/utils/examples/#edsnlp.utils.examples.parse_example","text":"Parses an example : finds examples and removes the tags. PARAMETER DESCRIPTION example Example to process. TYPE: str RETURNS DESCRIPTION Tuple[str, List[Entity]] Cleaned text and extracted entities. Source code in edsnlp/utils/examples.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def parse_example ( example : str ) -> Tuple [ str , List [ Entity ]]: \"\"\" Parses an example : finds examples and removes the tags. Parameters ---------- example : str Example to process. Returns ------- Tuple[str, List[Entity]] Cleaned text and extracted entities. \"\"\" matches = [ parse_match ( match ) for match in find_matches ( example = example )] text = \"\" entities = [] cursor = 0 for match in matches : text += example [ cursor : match . start_char ] start_char = len ( text ) text += match . text end_char = len ( text ) cursor = match . end_char entity = Entity ( start_char = start_char , end_char = end_char , modifiers = [ Modifier . parse_obj ( m . groupdict ()) for m in single_modifiers_pattern . finditer ( match . modifiers ) ], ) entities . append ( entity ) text += example [ cursor :] return text , entities","title":"parse_example()"},{"location":"reference/utils/filter/","text":"edsnlp.utils.filter default_sort_key ( span ) Returns the sort key for filtering spans. PARAMETER DESCRIPTION span Span to sort. TYPE: Span RETURNS DESCRIPTION key Sort key. TYPE: Tuple(int, int) Source code in edsnlp/utils/filter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def default_sort_key ( span : Span ) -> Tuple [ int , int ]: \"\"\" Returns the sort key for filtering spans. Parameters ---------- span : Span Span to sort. Returns ------- key : Tuple(int, int) Sort key. \"\"\" if isinstance ( span , tuple ): span = span [ 0 ] return span . end - span . start , - span . start start_sort_key ( span ) Returns the sort key for filtering spans by start order. PARAMETER DESCRIPTION span Span to sort. TYPE: Span RETURNS DESCRIPTION key Sort key. TYPE: Tuple(int, int) Source code in edsnlp/utils/filter.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def start_sort_key ( span : Union [ Span , Tuple [ Span , Any ]]) -> Tuple [ int , int ]: \"\"\" Returns the sort key for filtering spans by start order. Parameters ---------- span : Span Span to sort. Returns ------- key : Tuple(int, int) Sort key. \"\"\" if isinstance ( span , tuple ): span = span [ 0 ] return span . start filter_spans ( spans , label_to_remove = None , return_discarded = False , sort_key = default_sort_key ) Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones. Can also accept a label_to_remove argument, useful for filtering out pseudo cues. If set, results can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues. It can handle an iterable of tuples instead of an iterable of Span s. The primary use-case is the use with the RegexMatcher 's capacity to return the span's groupdict . The spaCy documentation states : Filter a sequence of spans and remove duplicates or overlaps. Useful for creating named entities (where one token can only be part of one entity) or when merging spans with Retokenizer.merge . When spans overlap, the (first) longest span is preferred over shorter spans. Filtering out spans If the label_to_remove argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove. The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede and follow a marked entity. Hence we need to keep every example. PARAMETER DESCRIPTION spans Spans to filter. TYPE: Iterable[Union[\"Span\", Tuple[\"Span\", Any]]] return_discarded Whether to return discarded spans. TYPE: bool DEFAULT: False label_to_remove Label to remove. If set, results can contain overlapping spans. TYPE: str, optional DEFAULT: None sort_key Key to sorting spans before applying overlap conflict resolution. A span with a higher key will have precedence over another span. By default, the largest, leftmost spans are selected first. TYPE: Callable[Span, Any], optional DEFAULT: default_sort_key RETURNS DESCRIPTION results Filtered spans TYPE: List[Union[Span, Tuple[Span, Any]]] discarded Discarded spans TYPE: List[Union[Span, Tuple[Span, Any]]], optional Source code in edsnlp/utils/filter.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def filter_spans ( spans : Iterable [ Union [ \"Span\" , Tuple [ \"Span\" , Any ]]], label_to_remove : Optional [ str ] = None , return_discarded : bool = False , sort_key : Callable [[ Span ], Any ] = default_sort_key , ) -> Union [ List [ Union [ Span , Tuple [ Span , Any ]]], Tuple [ List [ Union [ Span , Tuple [ Span , Any ]]], List [ Union [ Span , Tuple [ Span , Any ]]]], ]: \"\"\" Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones. Can also accept a `label_to_remove` argument, useful for filtering out pseudo cues. If set, `results` can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues. It can handle an iterable of tuples instead of an iterable of `Span`s. The primary use-case is the use with the `RegexMatcher`'s capacity to return the span's `groupdict`. !!! note \"\" The **spaCy documentation states**: > Filter a sequence of spans and remove duplicates or overlaps. > Useful for creating named entities (where one token can only > be part of one entity) or when merging spans with > `Retokenizer.merge`. When spans overlap, the (first) > longest span is preferred over shorter spans. !!! danger \"Filtering out spans\" If the `label_to_remove` argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove. The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede **and** follow a marked entity. Hence we need to keep every example. Parameters ---------- spans : Iterable[Union[\"Span\", Tuple[\"Span\", Any]]] Spans to filter. return_discarded : bool Whether to return discarded spans. label_to_remove : str, optional Label to remove. If set, results can contain overlapping spans. sort_key : Callable[Span, Any], optional Key to sorting spans before applying overlap conflict resolution. A span with a higher key will have precedence over another span. By default, the largest, leftmost spans are selected first. Returns ------- results : List[Union[Span, Tuple[Span, Any]]] Filtered spans discarded : List[Union[Span, Tuple[Span, Any]]], optional Discarded spans \"\"\" sorted_spans = sorted ( spans , key = sort_key , reverse = True ) result = [] discarded = [] seen_tokens = set () for span in sorted_spans : s = span if isinstance ( span , Span ) else span [ 0 ] # Check for end - 1 here because boundaries are inclusive token_range = set ( range ( s . start , s . end )) if token_range . isdisjoint ( seen_tokens ): if label_to_remove is None or s . label_ != label_to_remove : result . append ( span ) if label_to_remove is None or s . label_ == label_to_remove : seen_tokens . update ( token_range ) elif label_to_remove is None or s . label_ != label_to_remove : discarded . append ( span ) result = sorted ( result , key = start_sort_key ) discarded = sorted ( discarded , key = start_sort_key ) if return_discarded : return result , discarded return result consume_spans ( spans , filter , second_chance = None ) Consume a list of span, according to a filter. Warning This method makes the hard hypothesis that: Spans are sorted. Spans are consumed in sequence and only once. The second item is problematic for the way we treat long entities, hence the second_chance parameter, which lets entities be seen more than once. PARAMETER DESCRIPTION spans List of spans to filter TYPE: List of spans filter Filtering function. Should return True when the item is to be included. TYPE: Callable second_chance Optional list of spans to include again (useful for long entities), by default None TYPE: List of spans, optional DEFAULT: None RETURNS DESCRIPTION matches List of spans consumed by the filter. TYPE: List of spans remainder List of remaining spans in the original spans parameter. TYPE: List of spans Source code in edsnlp/utils/filter.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def consume_spans ( spans : List [ Span ], filter : Callable , second_chance : Optional [ List [ Span ]] = None , ) -> Tuple [ List [ Span ], List [ Span ]]: \"\"\" Consume a list of span, according to a filter. !!! warning This method makes the hard hypothesis that: 1. Spans are sorted. 2. Spans are consumed in sequence and only once. The second item is problematic for the way we treat long entities, hence the `second_chance` parameter, which lets entities be seen more than once. Parameters ---------- spans : List of spans List of spans to filter filter : Callable Filtering function. Should return True when the item is to be included. second_chance : List of spans, optional Optional list of spans to include again (useful for long entities), by default None Returns ------- matches : List of spans List of spans consumed by the filter. remainder : List of spans List of remaining spans in the original `spans` parameter. \"\"\" if not second_chance : second_chance = [] else : second_chance = [ m for m in second_chance if filter ( m )] if not spans : return second_chance , [] for i , span in enumerate ( spans ): if not filter ( span ): break else : i += 1 matches = spans [: i ] remainder = spans [ i :] matches . extend ( second_chance ) return matches , remainder get_spans ( spans , label ) Extracts spans with a given label. Prefer using hash label for performance reasons. PARAMETER DESCRIPTION spans List of spans to filter. TYPE: List[Span] label Label to filter on. TYPE: Union[int, str] RETURNS DESCRIPTION List[Span] Filtered spans. Source code in edsnlp/utils/filter.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def get_spans ( spans : List [ Span ], label : Union [ int , str ]) -> List [ Span ]: \"\"\" Extracts spans with a given label. Prefer using hash label for performance reasons. Parameters ---------- spans : List[Span] List of spans to filter. label : Union[int, str] Label to filter on. Returns ------- List[Span] Filtered spans. \"\"\" if isinstance ( label , int ): return [ span for span in spans if span . label == label ] else : return [ span for span in spans if span . label_ == label ]","title":"filter"},{"location":"reference/utils/filter/#edsnlputilsfilter","text":"","title":"edsnlp.utils.filter"},{"location":"reference/utils/filter/#edsnlp.utils.filter.default_sort_key","text":"Returns the sort key for filtering spans. PARAMETER DESCRIPTION span Span to sort. TYPE: Span RETURNS DESCRIPTION key Sort key. TYPE: Tuple(int, int) Source code in edsnlp/utils/filter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def default_sort_key ( span : Span ) -> Tuple [ int , int ]: \"\"\" Returns the sort key for filtering spans. Parameters ---------- span : Span Span to sort. Returns ------- key : Tuple(int, int) Sort key. \"\"\" if isinstance ( span , tuple ): span = span [ 0 ] return span . end - span . start , - span . start","title":"default_sort_key()"},{"location":"reference/utils/filter/#edsnlp.utils.filter.start_sort_key","text":"Returns the sort key for filtering spans by start order. PARAMETER DESCRIPTION span Span to sort. TYPE: Span RETURNS DESCRIPTION key Sort key. TYPE: Tuple(int, int) Source code in edsnlp/utils/filter.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def start_sort_key ( span : Union [ Span , Tuple [ Span , Any ]]) -> Tuple [ int , int ]: \"\"\" Returns the sort key for filtering spans by start order. Parameters ---------- span : Span Span to sort. Returns ------- key : Tuple(int, int) Sort key. \"\"\" if isinstance ( span , tuple ): span = span [ 0 ] return span . start","title":"start_sort_key()"},{"location":"reference/utils/filter/#edsnlp.utils.filter.filter_spans","text":"Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones. Can also accept a label_to_remove argument, useful for filtering out pseudo cues. If set, results can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues. It can handle an iterable of tuples instead of an iterable of Span s. The primary use-case is the use with the RegexMatcher 's capacity to return the span's groupdict . The spaCy documentation states : Filter a sequence of spans and remove duplicates or overlaps. Useful for creating named entities (where one token can only be part of one entity) or when merging spans with Retokenizer.merge . When spans overlap, the (first) longest span is preferred over shorter spans. Filtering out spans If the label_to_remove argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove. The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede and follow a marked entity. Hence we need to keep every example. PARAMETER DESCRIPTION spans Spans to filter. TYPE: Iterable[Union[\"Span\", Tuple[\"Span\", Any]]] return_discarded Whether to return discarded spans. TYPE: bool DEFAULT: False label_to_remove Label to remove. If set, results can contain overlapping spans. TYPE: str, optional DEFAULT: None sort_key Key to sorting spans before applying overlap conflict resolution. A span with a higher key will have precedence over another span. By default, the largest, leftmost spans are selected first. TYPE: Callable[Span, Any], optional DEFAULT: default_sort_key RETURNS DESCRIPTION results Filtered spans TYPE: List[Union[Span, Tuple[Span, Any]]] discarded Discarded spans TYPE: List[Union[Span, Tuple[Span, Any]]], optional Source code in edsnlp/utils/filter.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def filter_spans ( spans : Iterable [ Union [ \"Span\" , Tuple [ \"Span\" , Any ]]], label_to_remove : Optional [ str ] = None , return_discarded : bool = False , sort_key : Callable [[ Span ], Any ] = default_sort_key , ) -> Union [ List [ Union [ Span , Tuple [ Span , Any ]]], Tuple [ List [ Union [ Span , Tuple [ Span , Any ]]], List [ Union [ Span , Tuple [ Span , Any ]]]], ]: \"\"\" Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones. Can also accept a `label_to_remove` argument, useful for filtering out pseudo cues. If set, `results` can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues. It can handle an iterable of tuples instead of an iterable of `Span`s. The primary use-case is the use with the `RegexMatcher`'s capacity to return the span's `groupdict`. !!! note \"\" The **spaCy documentation states**: > Filter a sequence of spans and remove duplicates or overlaps. > Useful for creating named entities (where one token can only > be part of one entity) or when merging spans with > `Retokenizer.merge`. When spans overlap, the (first) > longest span is preferred over shorter spans. !!! danger \"Filtering out spans\" If the `label_to_remove` argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove. The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede **and** follow a marked entity. Hence we need to keep every example. Parameters ---------- spans : Iterable[Union[\"Span\", Tuple[\"Span\", Any]]] Spans to filter. return_discarded : bool Whether to return discarded spans. label_to_remove : str, optional Label to remove. If set, results can contain overlapping spans. sort_key : Callable[Span, Any], optional Key to sorting spans before applying overlap conflict resolution. A span with a higher key will have precedence over another span. By default, the largest, leftmost spans are selected first. Returns ------- results : List[Union[Span, Tuple[Span, Any]]] Filtered spans discarded : List[Union[Span, Tuple[Span, Any]]], optional Discarded spans \"\"\" sorted_spans = sorted ( spans , key = sort_key , reverse = True ) result = [] discarded = [] seen_tokens = set () for span in sorted_spans : s = span if isinstance ( span , Span ) else span [ 0 ] # Check for end - 1 here because boundaries are inclusive token_range = set ( range ( s . start , s . end )) if token_range . isdisjoint ( seen_tokens ): if label_to_remove is None or s . label_ != label_to_remove : result . append ( span ) if label_to_remove is None or s . label_ == label_to_remove : seen_tokens . update ( token_range ) elif label_to_remove is None or s . label_ != label_to_remove : discarded . append ( span ) result = sorted ( result , key = start_sort_key ) discarded = sorted ( discarded , key = start_sort_key ) if return_discarded : return result , discarded return result","title":"filter_spans()"},{"location":"reference/utils/filter/#edsnlp.utils.filter.consume_spans","text":"Consume a list of span, according to a filter. Warning This method makes the hard hypothesis that: Spans are sorted. Spans are consumed in sequence and only once. The second item is problematic for the way we treat long entities, hence the second_chance parameter, which lets entities be seen more than once. PARAMETER DESCRIPTION spans List of spans to filter TYPE: List of spans filter Filtering function. Should return True when the item is to be included. TYPE: Callable second_chance Optional list of spans to include again (useful for long entities), by default None TYPE: List of spans, optional DEFAULT: None RETURNS DESCRIPTION matches List of spans consumed by the filter. TYPE: List of spans remainder List of remaining spans in the original spans parameter. TYPE: List of spans Source code in edsnlp/utils/filter.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def consume_spans ( spans : List [ Span ], filter : Callable , second_chance : Optional [ List [ Span ]] = None , ) -> Tuple [ List [ Span ], List [ Span ]]: \"\"\" Consume a list of span, according to a filter. !!! warning This method makes the hard hypothesis that: 1. Spans are sorted. 2. Spans are consumed in sequence and only once. The second item is problematic for the way we treat long entities, hence the `second_chance` parameter, which lets entities be seen more than once. Parameters ---------- spans : List of spans List of spans to filter filter : Callable Filtering function. Should return True when the item is to be included. second_chance : List of spans, optional Optional list of spans to include again (useful for long entities), by default None Returns ------- matches : List of spans List of spans consumed by the filter. remainder : List of spans List of remaining spans in the original `spans` parameter. \"\"\" if not second_chance : second_chance = [] else : second_chance = [ m for m in second_chance if filter ( m )] if not spans : return second_chance , [] for i , span in enumerate ( spans ): if not filter ( span ): break else : i += 1 matches = spans [: i ] remainder = spans [ i :] matches . extend ( second_chance ) return matches , remainder","title":"consume_spans()"},{"location":"reference/utils/filter/#edsnlp.utils.filter.get_spans","text":"Extracts spans with a given label. Prefer using hash label for performance reasons. PARAMETER DESCRIPTION spans List of spans to filter. TYPE: List[Span] label Label to filter on. TYPE: Union[int, str] RETURNS DESCRIPTION List[Span] Filtered spans. Source code in edsnlp/utils/filter.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def get_spans ( spans : List [ Span ], label : Union [ int , str ]) -> List [ Span ]: \"\"\" Extracts spans with a given label. Prefer using hash label for performance reasons. Parameters ---------- spans : List[Span] List of spans to filter. label : Union[int, str] Label to filter on. Returns ------- List[Span] Filtered spans. \"\"\" if isinstance ( label , int ): return [ span for span in spans if span . label == label ] else : return [ span for span in spans if span . label_ == label ]","title":"get_spans()"},{"location":"reference/utils/inclusion/","text":"edsnlp.utils.inclusion check_inclusion ( span , start , end ) Checks whether the span overlaps the boundaries. PARAMETER DESCRIPTION span Span to check. TYPE: Span start Start of the boundary TYPE: int end End of the boundary TYPE: int RETURNS DESCRIPTION bool Whether the span overlaps the boundaries. Source code in edsnlp/utils/inclusion.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def check_inclusion ( span : Span , start : int , end : int ) -> bool : \"\"\" Checks whether the span overlaps the boundaries. Parameters ---------- span : Span Span to check. start : int Start of the boundary end : int End of the boundary Returns ------- bool Whether the span overlaps the boundaries. \"\"\" if span . start >= end or span . end <= start : return False return True check_sent_inclusion ( span , start , end ) Checks whether the span overlaps the boundaries. PARAMETER DESCRIPTION span Span to check. TYPE: Span start Start of the boundary TYPE: int end End of the boundary TYPE: int RETURNS DESCRIPTION bool Whether the span overlaps the boundaries. Source code in edsnlp/utils/inclusion.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def check_sent_inclusion ( span : Span , start : int , end : int ) -> bool : \"\"\" Checks whether the span overlaps the boundaries. Parameters ---------- span : Span Span to check. start : int Start of the boundary end : int End of the boundary Returns ------- bool Whether the span overlaps the boundaries. \"\"\" if span . sent . start >= end or span . sent . end <= start : return False return True","title":"inclusion"},{"location":"reference/utils/inclusion/#edsnlputilsinclusion","text":"","title":"edsnlp.utils.inclusion"},{"location":"reference/utils/inclusion/#edsnlp.utils.inclusion.check_inclusion","text":"Checks whether the span overlaps the boundaries. PARAMETER DESCRIPTION span Span to check. TYPE: Span start Start of the boundary TYPE: int end End of the boundary TYPE: int RETURNS DESCRIPTION bool Whether the span overlaps the boundaries. Source code in edsnlp/utils/inclusion.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def check_inclusion ( span : Span , start : int , end : int ) -> bool : \"\"\" Checks whether the span overlaps the boundaries. Parameters ---------- span : Span Span to check. start : int Start of the boundary end : int End of the boundary Returns ------- bool Whether the span overlaps the boundaries. \"\"\" if span . start >= end or span . end <= start : return False return True","title":"check_inclusion()"},{"location":"reference/utils/inclusion/#edsnlp.utils.inclusion.check_sent_inclusion","text":"Checks whether the span overlaps the boundaries. PARAMETER DESCRIPTION span Span to check. TYPE: Span start Start of the boundary TYPE: int end End of the boundary TYPE: int RETURNS DESCRIPTION bool Whether the span overlaps the boundaries. Source code in edsnlp/utils/inclusion.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def check_sent_inclusion ( span : Span , start : int , end : int ) -> bool : \"\"\" Checks whether the span overlaps the boundaries. Parameters ---------- span : Span Span to check. start : int Start of the boundary end : int End of the boundary Returns ------- bool Whether the span overlaps the boundaries. \"\"\" if span . sent . start >= end or span . sent . end <= start : return False return True","title":"check_sent_inclusion()"},{"location":"reference/utils/lists/","text":"edsnlp.utils.lists flatten ( my_list ) Flatten (if necessary) a list of sublists PARAMETER DESCRIPTION my_list A list of items, each items in turn can be a list TYPE: List RETURNS DESCRIPTION List A flatten list Source code in edsnlp/utils/lists.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def flatten ( my_list : List ): \"\"\" Flatten (if necessary) a list of sublists Parameters ---------- my_list : List A list of items, each items in turn can be a list Returns ------- List A flatten list \"\"\" if not my_list : return my_list my_list = [ item if isinstance ( item , list ) else [ item ] for item in my_list ] return [ item for sublist in my_list for item in sublist ]","title":"lists"},{"location":"reference/utils/lists/#edsnlputilslists","text":"","title":"edsnlp.utils.lists"},{"location":"reference/utils/lists/#edsnlp.utils.lists.flatten","text":"Flatten (if necessary) a list of sublists PARAMETER DESCRIPTION my_list A list of items, each items in turn can be a list TYPE: List RETURNS DESCRIPTION List A flatten list Source code in edsnlp/utils/lists.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def flatten ( my_list : List ): \"\"\" Flatten (if necessary) a list of sublists Parameters ---------- my_list : List A list of items, each items in turn can be a list Returns ------- List A flatten list \"\"\" if not my_list : return my_list my_list = [ item if isinstance ( item , list ) else [ item ] for item in my_list ] return [ item for sublist in my_list for item in sublist ]","title":"flatten()"},{"location":"reference/utils/merge_configs/","text":"edsnlp.utils.merge_configs merge_configs ( config , * updates , remove_extra = False ) Deep merge two configs. Source code in edsnlp/utils/merge_configs.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def merge_configs ( config : Union [ Dict [ str , Any ], Config ], * updates : Union [ Dict [ str , Any ], Config ], remove_extra : bool = False , ) -> Union [ Dict [ str , Any ], Config ]: \"\"\"Deep merge two configs.\"\"\" def deep_set ( current , path , val ): path = path . split ( \".\" ) for part in path [: - 1 ]: current = current [ part ] current [ path [ - 1 ]] = val def rec ( old , new ): if remove_extra : # Filter out values in the original config that are not in defaults keys = list ( new . keys ()) for key in keys : if key not in old : del new [ key ] for key , new_val in list ( new . items ()): if \".\" in key : deep_set ( old , key , new_val ) continue if key not in old : old [ key ] = new_val continue old_val = old [ key ] if isinstance ( old_val , dict ) and isinstance ( new_val , dict ): old_promise = next (( k for k in old_val if k . startswith ( \"@\" )), None ) new_promise = next (( k for k in new_val if k . startswith ( \"@\" )), None ) if ( new_promise is not None and old_promise != new_promise or old_val . get ( old_promise ) != new_val . get ( new_promise ) ): old [ key ] = new_val else : rec ( old [ key ], new_val ) else : old [ key ] = new_val return old config = deepcopy ( config ) for u in updates : u = deepcopy ( u ) rec ( config , u ) return Config ( config )","title":"merge_configs"},{"location":"reference/utils/merge_configs/#edsnlputilsmerge_configs","text":"","title":"edsnlp.utils.merge_configs"},{"location":"reference/utils/merge_configs/#edsnlp.utils.merge_configs.merge_configs","text":"Deep merge two configs. Source code in edsnlp/utils/merge_configs.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def merge_configs ( config : Union [ Dict [ str , Any ], Config ], * updates : Union [ Dict [ str , Any ], Config ], remove_extra : bool = False , ) -> Union [ Dict [ str , Any ], Config ]: \"\"\"Deep merge two configs.\"\"\" def deep_set ( current , path , val ): path = path . split ( \".\" ) for part in path [: - 1 ]: current = current [ part ] current [ path [ - 1 ]] = val def rec ( old , new ): if remove_extra : # Filter out values in the original config that are not in defaults keys = list ( new . keys ()) for key in keys : if key not in old : del new [ key ] for key , new_val in list ( new . items ()): if \".\" in key : deep_set ( old , key , new_val ) continue if key not in old : old [ key ] = new_val continue old_val = old [ key ] if isinstance ( old_val , dict ) and isinstance ( new_val , dict ): old_promise = next (( k for k in old_val if k . startswith ( \"@\" )), None ) new_promise = next (( k for k in new_val if k . startswith ( \"@\" )), None ) if ( new_promise is not None and old_promise != new_promise or old_val . get ( old_promise ) != new_val . get ( new_promise ) ): old [ key ] = new_val else : rec ( old [ key ], new_val ) else : old [ key ] = new_val return old config = deepcopy ( config ) for u in updates : u = deepcopy ( u ) rec ( config , u ) return Config ( config )","title":"merge_configs()"},{"location":"reference/utils/regex/","text":"edsnlp.utils.regex make_pattern ( patterns , with_breaks = False , name = None ) Create OR pattern from a list of patterns. PARAMETER DESCRIPTION patterns List of patterns to merge. TYPE: List[str] with_breaks Whether to add breaks ( \\b ) on each side, by default False TYPE: bool, optional DEFAULT: False name Name of the group, using regex ?P<> directive. TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION str Merged pattern. Source code in edsnlp/utils/regex.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def make_pattern ( patterns : List [ str ], with_breaks : bool = False , name : Optional [ str ] = None , ) -> str : r \"\"\" Create OR pattern from a list of patterns. Parameters ---------- patterns : List[str] List of patterns to merge. with_breaks : bool, optional Whether to add breaks (`\\b`) on each side, by default False name: str, optional Name of the group, using regex `?P<>` directive. Returns ------- str Merged pattern. \"\"\" if name : prefix = f \"(?P< { name } >\" else : prefix = \"(\" # Sorting by length might be more efficient patterns . sort ( key = len , reverse = True ) pattern = prefix + \"|\" . join ( patterns ) + \")\" if with_breaks : pattern = r \"\\b\" + pattern + r \"\\b\" return pattern compile_regex ( reg , flags ) This function tries to compile reg using the re module, and fallbacks to the regex module that is more permissive. PARAMETER DESCRIPTION reg TYPE: str RETURNS DESCRIPTION Union[re.Pattern, regex.Pattern] Source code in edsnlp/utils/regex.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def compile_regex ( reg : str , flags : re . RegexFlag ): \"\"\" This function tries to compile `reg` using the `re` module, and fallbacks to the `regex` module that is more permissive. Parameters ---------- reg: str Returns ------- Union[re.Pattern, regex.Pattern] \"\"\" try : return re . compile ( reg , flags = flags ) except re . error : try : return regex . compile ( reg , flags = flags ) except regex . error : raise Exception ( \"Could not compile: {} \" . format ( repr ( reg )))","title":"regex"},{"location":"reference/utils/regex/#edsnlputilsregex","text":"","title":"edsnlp.utils.regex"},{"location":"reference/utils/regex/#edsnlp.utils.regex.make_pattern","text":"Create OR pattern from a list of patterns. PARAMETER DESCRIPTION patterns List of patterns to merge. TYPE: List[str] with_breaks Whether to add breaks ( \\b ) on each side, by default False TYPE: bool, optional DEFAULT: False name Name of the group, using regex ?P<> directive. TYPE: Optional [ str ] DEFAULT: None RETURNS DESCRIPTION str Merged pattern. Source code in edsnlp/utils/regex.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def make_pattern ( patterns : List [ str ], with_breaks : bool = False , name : Optional [ str ] = None , ) -> str : r \"\"\" Create OR pattern from a list of patterns. Parameters ---------- patterns : List[str] List of patterns to merge. with_breaks : bool, optional Whether to add breaks (`\\b`) on each side, by default False name: str, optional Name of the group, using regex `?P<>` directive. Returns ------- str Merged pattern. \"\"\" if name : prefix = f \"(?P< { name } >\" else : prefix = \"(\" # Sorting by length might be more efficient patterns . sort ( key = len , reverse = True ) pattern = prefix + \"|\" . join ( patterns ) + \")\" if with_breaks : pattern = r \"\\b\" + pattern + r \"\\b\" return pattern","title":"make_pattern()"},{"location":"reference/utils/regex/#edsnlp.utils.regex.compile_regex","text":"This function tries to compile reg using the re module, and fallbacks to the regex module that is more permissive. PARAMETER DESCRIPTION reg TYPE: str RETURNS DESCRIPTION Union[re.Pattern, regex.Pattern] Source code in edsnlp/utils/regex.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def compile_regex ( reg : str , flags : re . RegexFlag ): \"\"\" This function tries to compile `reg` using the `re` module, and fallbacks to the `regex` module that is more permissive. Parameters ---------- reg: str Returns ------- Union[re.Pattern, regex.Pattern] \"\"\" try : return re . compile ( reg , flags = flags ) except re . error : try : return regex . compile ( reg , flags = flags ) except regex . error : raise Exception ( \"Could not compile: {} \" . format ( repr ( reg )))","title":"compile_regex()"},{"location":"reference/utils/resources/","text":"edsnlp.utils.resources get_verbs ( verbs = None , check_contains = True ) Extract verbs from the resources, as a pandas dataframe. PARAMETER DESCRIPTION verbs List of verbs to keep. Returns all verbs by default. TYPE: List[str], optional DEFAULT: None check_contains Whether to check that no verb is missing if a list of verbs was provided. By default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION pd.DataFrame DataFrame containing conjugated verbs. Source code in edsnlp/utils/resources.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def get_verbs ( verbs : Optional [ List [ str ]] = None , check_contains : bool = True ) -> pd . DataFrame : \"\"\" Extract verbs from the resources, as a pandas dataframe. Parameters ---------- verbs : List[str], optional List of verbs to keep. Returns all verbs by default. check_contains : bool, optional Whether to check that no verb is missing if a list of verbs was provided. By default True Returns ------- pd.DataFrame DataFrame containing conjugated verbs. \"\"\" conjugated_verbs = pd . read_csv ( BASE_DIR / \"resources\" / \"verbs.csv.gz\" ) if not verbs : return conjugated_verbs verbs = set ( verbs ) selected_verbs = conjugated_verbs [ conjugated_verbs . verb . isin ( verbs )] if check_contains : assert len ( verbs ) == selected_verbs . verb . nunique (), \"Some verbs are missing !\" return selected_verbs get_adicap_dict () RETURNS DESCRIPTION Dict Source code in edsnlp/utils/resources.py 46 47 48 49 50 51 52 53 54 55 56 57 @lru_cache () def get_adicap_dict (): \"\"\" Returns ------- Dict \"\"\" with gzip . open ( BASE_DIR / \"resources\" / \"adicap.json.gz\" , \"r\" ) as fin : decode_dict = json . loads ( fin . read () . decode ( \"utf-8\" )) return decode_dict","title":"resources"},{"location":"reference/utils/resources/#edsnlputilsresources","text":"","title":"edsnlp.utils.resources"},{"location":"reference/utils/resources/#edsnlp.utils.resources.get_verbs","text":"Extract verbs from the resources, as a pandas dataframe. PARAMETER DESCRIPTION verbs List of verbs to keep. Returns all verbs by default. TYPE: List[str], optional DEFAULT: None check_contains Whether to check that no verb is missing if a list of verbs was provided. By default True TYPE: bool, optional DEFAULT: True RETURNS DESCRIPTION pd.DataFrame DataFrame containing conjugated verbs. Source code in edsnlp/utils/resources.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def get_verbs ( verbs : Optional [ List [ str ]] = None , check_contains : bool = True ) -> pd . DataFrame : \"\"\" Extract verbs from the resources, as a pandas dataframe. Parameters ---------- verbs : List[str], optional List of verbs to keep. Returns all verbs by default. check_contains : bool, optional Whether to check that no verb is missing if a list of verbs was provided. By default True Returns ------- pd.DataFrame DataFrame containing conjugated verbs. \"\"\" conjugated_verbs = pd . read_csv ( BASE_DIR / \"resources\" / \"verbs.csv.gz\" ) if not verbs : return conjugated_verbs verbs = set ( verbs ) selected_verbs = conjugated_verbs [ conjugated_verbs . verb . isin ( verbs )] if check_contains : assert len ( verbs ) == selected_verbs . verb . nunique (), \"Some verbs are missing !\" return selected_verbs","title":"get_verbs()"},{"location":"reference/utils/resources/#edsnlp.utils.resources.get_adicap_dict","text":"RETURNS DESCRIPTION Dict Source code in edsnlp/utils/resources.py 46 47 48 49 50 51 52 53 54 55 56 57 @lru_cache () def get_adicap_dict (): \"\"\" Returns ------- Dict \"\"\" with gzip . open ( BASE_DIR / \"resources\" / \"adicap.json.gz\" , \"r\" ) as fin : decode_dict = json . loads ( fin . read () . decode ( \"utf-8\" )) return decode_dict","title":"get_adicap_dict()"},{"location":"reference/utils/training/","text":"edsnlp.utils.training __all__ = [ 'Config' , 'train' , 'DEFAULT_TRAIN_CONFIG' ] module-attribute DEFAULT_TRAIN_CONFIG = Config () . from_str ( ' \\n [system] \\n gpu_allocator = null \\n seed = 0 \\n\\n [paths] \\n train = null \\n dev = null \\n raw = null \\n init_tok2vec = null \\n vectors = null \\n\\n [corpora] \\n [corpora.train] \\n @readers = \"spacy.Corpus.v1\" \\n path = $ {paths.train} \\n max_length = 0 \\n gold_preproc = false \\n limit = 0 \\n augmenter = null \\n\\n [corpora.dev] \\n @readers = \"spacy.Corpus.v1\" \\n path = $ {paths.dev} \\n max_length = 0 \\n gold_preproc = false \\n limit = 0 \\n augmenter = null \\n\\n [training] \\n train_corpus = \"corpora.train\" \\n dev_corpus = \"corpora.dev\" \\n seed = $ {system.seed} \\n gpu_allocator = $ {system.gpu_allocator} \\n dropout = 0.1 \\n accumulate_gradient = 1 \\n patience = 10000 \\n max_epochs = 0 \\n max_steps = 20000 \\n eval_frequency = 200 \\n frozen_components = [] \\n before_to_disk = null \\n\\n [training.batcher] \\n @batchers = \"spacy.batch_by_words.v1\" \\n discard_oversize = false \\n tolerance = 0.2 \\n get_length = null \\n\\n [training.batcher.size] \\n @schedules = \"compounding.v1\" \\n start = 100 \\n stop = 1000 \\n compound = 1.001 \\n t = 0.0 \\n\\n [training.optimizer] \\n @optimizers = \"Adam.v1\" \\n beta1 = 0.9 \\n beta2 = 0.999 \\n L2_is_weight_decay = true \\n L2 = 0.01 \\n grad_clip = 1.0 \\n use_averages = false \\n eps = 0.00000001 \\n learn_rate = 0.001 \\n\\n [initialize] \\n vectors = $ {paths.vectors} \\n init_tok2vec = $ {paths.init_tok2vec} \\n vocab_data = null \\n lookups = null \\n before_init = null \\n after_init = null \\n ' , interpolate = False ) module-attribute DataFormat Bases: str , Enum Source code in edsnlp/utils/training.py 106 107 108 109 class DataFormat ( str , Enum ): brat = \"brat\" standoff = \"brat\" spacy = \"spacy\" brat = 'brat' class-attribute standoff = 'brat' class-attribute spacy = 'spacy' class-attribute make_spacy_corpus_config ( train_data , dev_data , data_format = None , nlp = None , seed = 0 , reader = 'spacy.Corpus.v1' ) Helper to create a spacy's corpus config from training and dev data by loading the documents accordingly and exporting the documents using spacy's DocBin. PARAMETER DESCRIPTION train_data The training data. Can be: - a list of spacy.Doc - a path to a given dataset TYPE: Union [ str , List [ Doc ]] dev_data The development data. Can be: - a list of spacy.Doc - a path to a given dataset - the number of documents to take from the training data - the fraction of documents to take from the training data TYPE: Union [ str , List [ Doc ], int , float ] data_format Optional data format to determine how we should load the documents from the disk TYPE: Union [ Optional [ DataFormat ], str ] DEFAULT: None nlp Optional spacy model to load documents from non-spacy formats (like brat) TYPE: Optional [ spacy . Language ] DEFAULT: None seed The seed if we need to shuffle the data when splitting the dataset TYPE: int DEFAULT: 0 reader Which spacy reader to use when loading the data TYPE: str DEFAULT: 'spacy.Corpus.v1' RETURNS DESCRIPTION Config Source code in edsnlp/utils/training.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def make_spacy_corpus_config ( train_data : Union [ str , List [ Doc ]], dev_data : Union [ str , List [ Doc ], int , float ], data_format : Union [ Optional [ DataFormat ], str ] = None , nlp : Optional [ spacy . Language ] = None , seed : int = 0 , reader : str = \"spacy.Corpus.v1\" , ): \"\"\" Helper to create a spacy's corpus config from training and dev data by loading the documents accordingly and exporting the documents using spacy's DocBin. Parameters ---------- train_data: Union[str, List[Doc]] The training data. Can be: - a list of spacy.Doc - a path to a given dataset dev_data: Union[str, List[Doc], int, float] The development data. Can be: - a list of spacy.Doc - a path to a given dataset - the number of documents to take from the training data - the fraction of documents to take from the training data data_format: Optional[DataFormat] Optional data format to determine how we should load the documents from the disk nlp: Optional[spacy.Language] Optional spacy model to load documents from non-spacy formats (like brat) seed: int The seed if we need to shuffle the data when splitting the dataset reader: str Which spacy reader to use when loading the data Returns ------- Config \"\"\" fix_random_seed ( seed ) train_docs = dev_docs = None if data_format is None : if isinstance ( train_data , list ): assert all ( isinstance ( doc , Doc ) for doc in train_data ) train_docs = train_data elif isinstance ( train_data , ( str , Path )) and train_data . endswith ( \".spacy\" ): data_format = DataFormat . spacy else : raise Exception () if data_format == DataFormat . brat : train_docs = list ( BratConnector ( train_data ) . brat2docs ( nlp )) elif data_format == DataFormat . spacy : if isinstance ( dev_data , ( float , int )): train_docs = DocBin () . from_disk ( train_data ) elif train_docs is None : raise Exception () if isinstance ( dev_data , ( float , int )): if isinstance ( dev_data , float ): n_dev = int ( len ( train_docs )) * dev_data else : n_dev = dev_data shuffle ( train_docs ) dev_docs = train_docs [: n_dev ] train_docs = train_docs [ n_dev :] elif data_format == DataFormat . brat : dev_docs = list ( BratConnector ( dev_data ) . brat2docs ( nlp )) elif data_format == DataFormat . spacy : pass elif data_format is None : if isinstance ( dev_data , list ): assert all ( isinstance ( doc , Doc ) for doc in dev_data ) dev_docs = dev_data else : raise Exception () else : raise Exception () if data_format != \"spacy\" or isinstance ( dev_data , ( float , int )): tmp_path = Path ( tempfile . mkdtemp ()) train_path = tmp_path / \"train.spacy\" dev_path = tmp_path / \"dev.spacy\" DocBin ( docs = train_docs ) . to_disk ( train_path ) DocBin ( docs = dev_docs ) . to_disk ( dev_path ) else : train_path = train_data dev_path = dev_data return Config () . from_str ( f \"\"\" [corpora] [corpora.train] @readers = { reader } path = { train_path } max_length = 0 gold_preproc = false limit = 0 augmenter = null [corpora.dev] @readers = { reader } path = { dev_path } max_length = 0 gold_preproc = false limit = 0 augmenter = null \"\"\" ) train ( nlp , output_path , config , use_gpu =- 1 ) Training help to learn weight of trainable components in a pipeline. This function has been adapted from https://github.com/explosion/spaCy/blob/397197e/spacy/cli/train.py#L18 PARAMETER DESCRIPTION nlp Spacy model to train TYPE: spacy . Language output_path Path to save the model TYPE: Union [ Path , str ] config Optional config overrides TYPE: Union [ Config , dict ] use_gpu Which gpu to use for training (-1 means CPU) TYPE: int DEFAULT: -1 Source code in edsnlp/utils/training.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def train ( nlp : spacy . Language , output_path : Union [ Path , str ], config : Union [ Config , dict ], use_gpu : int = - 1 , ): \"\"\" Training help to learn weight of trainable components in a pipeline. This function has been adapted from https://github.com/explosion/spaCy/blob/397197e/spacy/cli/train.py#L18 Parameters ---------- nlp: spacy.Language Spacy model to train output_path: Union[Path,str] Path to save the model config: Union[Config,dict] Optional config overrides use_gpu: bool Which gpu to use for training (-1 means CPU) \"\"\" if \"components\" in config : raise ValueError ( \"Cannot update components config after the model has been \" \"instantiated.\" ) output_path = Path ( output_path ) nlp . config = merge_configs ( nlp . config , DEFAULT_TRAIN_CONFIG , config , remove_extra = False ) config = nlp . config . interpolate () nlp . config = config if \"seed\" not in config [ \"training\" ]: raise ValueError ( Errors . E1015 . format ( value = \"[training] seed\" )) if \"gpu_allocator\" not in config [ \"training\" ]: raise ValueError ( Errors . E1015 . format ( value = \"[training] gpu_allocator\" )) if config [ \"training\" ][ \"seed\" ] is not None : fix_random_seed ( config [ \"training\" ][ \"seed\" ]) allocator = config [ \"training\" ][ \"gpu_allocator\" ] if use_gpu >= 0 and allocator : set_gpu_allocator ( allocator ) # Use nlp config here before it's resolved to functions sourced = get_sourced_components ( config ) # ----------------------------- # # Resolve functions and classes # # ----------------------------- # # Resolve all training-relevant sections using the filled nlp config T = registry . resolve ( config [ \"training\" ], schema = ConfigSchemaTraining ) dot_names = [ T [ \"train_corpus\" ], T [ \"dev_corpus\" ]] if not isinstance ( T [ \"train_corpus\" ], str ): raise ConfigValidationError ( desc = Errors . E897 . format ( field = \"training.train_corpus\" , type = type ( T [ \"train_corpus\" ]) ) ) if not isinstance ( T [ \"dev_corpus\" ], str ): raise ConfigValidationError ( desc = Errors . E897 . format ( field = \"training.dev_corpus\" , type = type ( T [ \"dev_corpus\" ]) ) ) train_corpus , dev_corpus = resolve_dot_names ( config , dot_names ) optimizer = T [ \"optimizer\" ] # Components that shouldn't be updated during training frozen_components = T [ \"frozen_components\" ] # Sourced components that require resume_training resume_components = [ p for p in sourced if p not in frozen_components ] logger . info ( f \"Pipeline: { nlp . pipe_names } \" ) if resume_components : with nlp . select_pipes ( enable = resume_components ): logger . info ( f \"Resuming training for: { resume_components } \" ) nlp . resume_training ( sgd = optimizer ) # Make sure that listeners are defined before initializing further nlp . _link_components () with nlp . select_pipes ( disable = [ * frozen_components , * resume_components ]): if T [ \"max_epochs\" ] == - 1 : sample_size = 100 logger . debug ( f \"Due to streamed train corpus, using only first { sample_size } \" f \"examples for initialization. If necessary, provide all labels \" f \"in [initialize]. More info: https://spacy.io/api/cli#init_labels\" ) nlp . initialize ( lambda : islice ( train_corpus ( nlp ), sample_size ), sgd = optimizer ) else : nlp . initialize ( lambda : train_corpus ( nlp ), sgd = optimizer ) logger . info ( f \"Initialized pipeline components: { nlp . pipe_names } \" ) # Detect components with listeners that are not frozen consistently for name , proc in nlp . pipeline : for listener in getattr ( proc , \"listening_components\" , [] ): # e.g. tok2vec/transformer # Don't warn about components not in the pipeline if listener not in nlp . pipe_names : continue if listener in frozen_components and name not in frozen_components : logger . warning ( Warnings . W087 . format ( name = name , listener = listener )) # We always check this regardless, in case user freezes tok2vec if listener not in frozen_components and name in frozen_components : if name not in T [ \"annotating_components\" ]: logger . warning ( Warnings . W086 . format ( name = name , listener = listener )) os . makedirs ( output_path , exist_ok = True ) train_loop ( nlp , output_path )","title":"training"},{"location":"reference/utils/training/#edsnlputilstraining","text":"","title":"edsnlp.utils.training"},{"location":"reference/utils/training/#edsnlp.utils.training.__all__","text":"","title":"__all__"},{"location":"reference/utils/training/#edsnlp.utils.training.DEFAULT_TRAIN_CONFIG","text":"","title":"DEFAULT_TRAIN_CONFIG"},{"location":"reference/utils/training/#edsnlp.utils.training.DataFormat","text":"Bases: str , Enum Source code in edsnlp/utils/training.py 106 107 108 109 class DataFormat ( str , Enum ): brat = \"brat\" standoff = \"brat\" spacy = \"spacy\"","title":"DataFormat"},{"location":"reference/utils/training/#edsnlp.utils.training.DataFormat.brat","text":"","title":"brat"},{"location":"reference/utils/training/#edsnlp.utils.training.DataFormat.standoff","text":"","title":"standoff"},{"location":"reference/utils/training/#edsnlp.utils.training.DataFormat.spacy","text":"","title":"spacy"},{"location":"reference/utils/training/#edsnlp.utils.training.make_spacy_corpus_config","text":"Helper to create a spacy's corpus config from training and dev data by loading the documents accordingly and exporting the documents using spacy's DocBin. PARAMETER DESCRIPTION train_data The training data. Can be: - a list of spacy.Doc - a path to a given dataset TYPE: Union [ str , List [ Doc ]] dev_data The development data. Can be: - a list of spacy.Doc - a path to a given dataset - the number of documents to take from the training data - the fraction of documents to take from the training data TYPE: Union [ str , List [ Doc ], int , float ] data_format Optional data format to determine how we should load the documents from the disk TYPE: Union [ Optional [ DataFormat ], str ] DEFAULT: None nlp Optional spacy model to load documents from non-spacy formats (like brat) TYPE: Optional [ spacy . Language ] DEFAULT: None seed The seed if we need to shuffle the data when splitting the dataset TYPE: int DEFAULT: 0 reader Which spacy reader to use when loading the data TYPE: str DEFAULT: 'spacy.Corpus.v1' RETURNS DESCRIPTION Config Source code in edsnlp/utils/training.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def make_spacy_corpus_config ( train_data : Union [ str , List [ Doc ]], dev_data : Union [ str , List [ Doc ], int , float ], data_format : Union [ Optional [ DataFormat ], str ] = None , nlp : Optional [ spacy . Language ] = None , seed : int = 0 , reader : str = \"spacy.Corpus.v1\" , ): \"\"\" Helper to create a spacy's corpus config from training and dev data by loading the documents accordingly and exporting the documents using spacy's DocBin. Parameters ---------- train_data: Union[str, List[Doc]] The training data. Can be: - a list of spacy.Doc - a path to a given dataset dev_data: Union[str, List[Doc], int, float] The development data. Can be: - a list of spacy.Doc - a path to a given dataset - the number of documents to take from the training data - the fraction of documents to take from the training data data_format: Optional[DataFormat] Optional data format to determine how we should load the documents from the disk nlp: Optional[spacy.Language] Optional spacy model to load documents from non-spacy formats (like brat) seed: int The seed if we need to shuffle the data when splitting the dataset reader: str Which spacy reader to use when loading the data Returns ------- Config \"\"\" fix_random_seed ( seed ) train_docs = dev_docs = None if data_format is None : if isinstance ( train_data , list ): assert all ( isinstance ( doc , Doc ) for doc in train_data ) train_docs = train_data elif isinstance ( train_data , ( str , Path )) and train_data . endswith ( \".spacy\" ): data_format = DataFormat . spacy else : raise Exception () if data_format == DataFormat . brat : train_docs = list ( BratConnector ( train_data ) . brat2docs ( nlp )) elif data_format == DataFormat . spacy : if isinstance ( dev_data , ( float , int )): train_docs = DocBin () . from_disk ( train_data ) elif train_docs is None : raise Exception () if isinstance ( dev_data , ( float , int )): if isinstance ( dev_data , float ): n_dev = int ( len ( train_docs )) * dev_data else : n_dev = dev_data shuffle ( train_docs ) dev_docs = train_docs [: n_dev ] train_docs = train_docs [ n_dev :] elif data_format == DataFormat . brat : dev_docs = list ( BratConnector ( dev_data ) . brat2docs ( nlp )) elif data_format == DataFormat . spacy : pass elif data_format is None : if isinstance ( dev_data , list ): assert all ( isinstance ( doc , Doc ) for doc in dev_data ) dev_docs = dev_data else : raise Exception () else : raise Exception () if data_format != \"spacy\" or isinstance ( dev_data , ( float , int )): tmp_path = Path ( tempfile . mkdtemp ()) train_path = tmp_path / \"train.spacy\" dev_path = tmp_path / \"dev.spacy\" DocBin ( docs = train_docs ) . to_disk ( train_path ) DocBin ( docs = dev_docs ) . to_disk ( dev_path ) else : train_path = train_data dev_path = dev_data return Config () . from_str ( f \"\"\" [corpora] [corpora.train] @readers = { reader } path = { train_path } max_length = 0 gold_preproc = false limit = 0 augmenter = null [corpora.dev] @readers = { reader } path = { dev_path } max_length = 0 gold_preproc = false limit = 0 augmenter = null \"\"\" )","title":"make_spacy_corpus_config()"},{"location":"reference/utils/training/#edsnlp.utils.training.train","text":"Training help to learn weight of trainable components in a pipeline. This function has been adapted from https://github.com/explosion/spaCy/blob/397197e/spacy/cli/train.py#L18 PARAMETER DESCRIPTION nlp Spacy model to train TYPE: spacy . Language output_path Path to save the model TYPE: Union [ Path , str ] config Optional config overrides TYPE: Union [ Config , dict ] use_gpu Which gpu to use for training (-1 means CPU) TYPE: int DEFAULT: -1 Source code in edsnlp/utils/training.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def train ( nlp : spacy . Language , output_path : Union [ Path , str ], config : Union [ Config , dict ], use_gpu : int = - 1 , ): \"\"\" Training help to learn weight of trainable components in a pipeline. This function has been adapted from https://github.com/explosion/spaCy/blob/397197e/spacy/cli/train.py#L18 Parameters ---------- nlp: spacy.Language Spacy model to train output_path: Union[Path,str] Path to save the model config: Union[Config,dict] Optional config overrides use_gpu: bool Which gpu to use for training (-1 means CPU) \"\"\" if \"components\" in config : raise ValueError ( \"Cannot update components config after the model has been \" \"instantiated.\" ) output_path = Path ( output_path ) nlp . config = merge_configs ( nlp . config , DEFAULT_TRAIN_CONFIG , config , remove_extra = False ) config = nlp . config . interpolate () nlp . config = config if \"seed\" not in config [ \"training\" ]: raise ValueError ( Errors . E1015 . format ( value = \"[training] seed\" )) if \"gpu_allocator\" not in config [ \"training\" ]: raise ValueError ( Errors . E1015 . format ( value = \"[training] gpu_allocator\" )) if config [ \"training\" ][ \"seed\" ] is not None : fix_random_seed ( config [ \"training\" ][ \"seed\" ]) allocator = config [ \"training\" ][ \"gpu_allocator\" ] if use_gpu >= 0 and allocator : set_gpu_allocator ( allocator ) # Use nlp config here before it's resolved to functions sourced = get_sourced_components ( config ) # ----------------------------- # # Resolve functions and classes # # ----------------------------- # # Resolve all training-relevant sections using the filled nlp config T = registry . resolve ( config [ \"training\" ], schema = ConfigSchemaTraining ) dot_names = [ T [ \"train_corpus\" ], T [ \"dev_corpus\" ]] if not isinstance ( T [ \"train_corpus\" ], str ): raise ConfigValidationError ( desc = Errors . E897 . format ( field = \"training.train_corpus\" , type = type ( T [ \"train_corpus\" ]) ) ) if not isinstance ( T [ \"dev_corpus\" ], str ): raise ConfigValidationError ( desc = Errors . E897 . format ( field = \"training.dev_corpus\" , type = type ( T [ \"dev_corpus\" ]) ) ) train_corpus , dev_corpus = resolve_dot_names ( config , dot_names ) optimizer = T [ \"optimizer\" ] # Components that shouldn't be updated during training frozen_components = T [ \"frozen_components\" ] # Sourced components that require resume_training resume_components = [ p for p in sourced if p not in frozen_components ] logger . info ( f \"Pipeline: { nlp . pipe_names } \" ) if resume_components : with nlp . select_pipes ( enable = resume_components ): logger . info ( f \"Resuming training for: { resume_components } \" ) nlp . resume_training ( sgd = optimizer ) # Make sure that listeners are defined before initializing further nlp . _link_components () with nlp . select_pipes ( disable = [ * frozen_components , * resume_components ]): if T [ \"max_epochs\" ] == - 1 : sample_size = 100 logger . debug ( f \"Due to streamed train corpus, using only first { sample_size } \" f \"examples for initialization. If necessary, provide all labels \" f \"in [initialize]. More info: https://spacy.io/api/cli#init_labels\" ) nlp . initialize ( lambda : islice ( train_corpus ( nlp ), sample_size ), sgd = optimizer ) else : nlp . initialize ( lambda : train_corpus ( nlp ), sgd = optimizer ) logger . info ( f \"Initialized pipeline components: { nlp . pipe_names } \" ) # Detect components with listeners that are not frozen consistently for name , proc in nlp . pipeline : for listener in getattr ( proc , \"listening_components\" , [] ): # e.g. tok2vec/transformer # Don't warn about components not in the pipeline if listener not in nlp . pipe_names : continue if listener in frozen_components and name not in frozen_components : logger . warning ( Warnings . W087 . format ( name = name , listener = listener )) # We always check this regardless, in case user freezes tok2vec if listener not in frozen_components and name in frozen_components : if name not in T [ \"annotating_components\" ]: logger . warning ( Warnings . W086 . format ( name = name , listener = listener )) os . makedirs ( output_path , exist_ok = True ) train_loop ( nlp , output_path )","title":"train()"},{"location":"tutorials/","text":"Tutorials We provide step-by-step guides to get you started. We cover the following use-cases: Matching a terminology : you're looking for a concept within a corpus of texts. Qualifying entities : you want to make sure that the concept you've extracted are not invalidated by linguistic modulation. Detecting dates , which could serve as the basis for an event ordering algorithm. Processing multiple texts : to improve the inference speed of your pipeline ! Detecting Hospitalisation Reason : you want to look spans that mention the reason of hospitalisation or tag entities as the reason. Detecting false endlines : classify each endline and add the attribute excluded to the these tokens. Rationale In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents. Now, consider the following example: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort. Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort. To sum up, a typical medical NLP project consists in: Editing a terminology \"Matching\" this terminology on a corpus, ie extract phrases that belong to that terminology \"Qualifying\" entities to avoid false positives Once the pipeline is ready, we need to deploy it efficiently.","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"We provide step-by-step guides to get you started. We cover the following use-cases: Matching a terminology : you're looking for a concept within a corpus of texts. Qualifying entities : you want to make sure that the concept you've extracted are not invalidated by linguistic modulation. Detecting dates , which could serve as the basis for an event ordering algorithm. Processing multiple texts : to improve the inference speed of your pipeline ! Detecting Hospitalisation Reason : you want to look spans that mention the reason of hospitalisation or tag entities as the reason. Detecting false endlines : classify each endline and add the attribute excluded to the these tokens.","title":"Tutorials"},{"location":"tutorials/#rationale","text":"In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents. Now, consider the following example: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort. Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort. To sum up, a typical medical NLP project consists in: Editing a terminology \"Matching\" this terminology on a corpus, ie extract phrases that belong to that terminology \"Qualifying\" entities to avoid false positives Once the pipeline is ready, we need to deploy it efficiently.","title":"Rationale"},{"location":"tutorials/detecting-dates/","text":"Detecting dates We now know how to match a terminology and qualify detected entities, which covers most use cases for a typical medical NLP project. In this tutorial, we'll see how to use EDS-NLP to detect and normalise date mentions using eds.dates . This can have many applications, for dating medical events in particular. The eds.consultation_dates component, for instance, combines the date detection capabilities with a few simple patterns to detect the date of the consultation, when mentioned in clinical reports. Dates in clinical notes Consider the following example: French English Le patient est admis le 21 janvier pour une douleur dans le cou. Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans. The patient is admitted on January 21st for a neck pain. He complains about chronique pain that started three years ago. Clinical notes contain many different types of dates. To name a few examples: Type Description Examples Absolute Explicit date 2022-03-03 Partial Date missing the day, month or year le 3 janvier/on January 3rd , en 2021/in 2021 Relative Relative dates hier/yesterday , le mois dernier/last month Duration Durations pendant trois mois/for three months Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. Extracting dates The followings snippet adds the eds.date component to the pipeline: import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.dates\" ) # (1) text = ( \"Le patient est admis le 21 janvier pour une douleur dans le cou. \\n \" \"Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\" ) # Detecting dates becomes trivial doc = nlp ( text ) # Likewise, accessing detected dates is hassle-free dates = doc . spans [ \"dates\" ] # (2) The date detection component is declared with eds.dates Dates are saved in the doc . spans [ \"dates\" ] key After this, accessing dates and there normalisation becomes trivial: # \u2191 Omitted code above \u2191 dates # (1) # Out: [21 janvier, il y a trois ans] dates is a list of spaCy Span objects. Normalisation We can review each date and get its normalisation: date.text date._.date 21 janvier { \"day\" : 21 , \"month\" : 1 } il y a trois ans { \"direction\" : \"past\" , \"year\" : 3 } Dates detected by the pipeline component are parsed into a dictionary-like object. It includes every information that is actually contained in the text. To get a more usable representation, you may call the to_datetime() method. If there's enough information, the date will be represented in a datetime.datetime or datetime.timedelta object. If some information is missing, It will return None . Alternatively for this case, you can optionally set to True the parameter infer_from_context and you may also give a value for note_datetime . Date normalisation Since dates can be missing some information (eg en ao\u00fbt ), we refrain from outputting a datetime object in that case. Doing so would amount to guessing, and we made the choice of letting you decide how you want to handle missing dates. What next? The eds.dates pipeline component's role is merely to detect and normalise dates. It is the user's responsibility to use this information in a downstream application. For instance, you could use this pipeline to date medical entities. Let's do that. A medical event tagger Our pipeline will detect entities and events separately, and we will post-process the output Doc object to determine whether a given entity can be linked to a date. import spacy from datetime import datetime nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.dates\" ) config = dict ( regex = dict ( admission = [ \"admissions?\" , \"admise?\" , \"prise? en charge\" ]), attr = \"LOWER\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) text = ( \"Le patient est admis le 12 avril pour une douleur \" \"survenue il y a trois jours. \" \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re. \" \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\" ) doc = nlp ( text ) At this point, the document is ready to be post-processed: its ents and spans [ \"dates\" ] are populated: # \u2191 Omitted code above \u2191 doc . ents # Out: (admis, pris en charge) doc . spans [ \"dates\" ] # Out: [12 avril, il y a trois jours, l'ann\u00e9e derni\u00e8re, mai 1995] note_datetime = datetime ( year = 1999 , month = 8 , day = 27 ) for i , date in enumerate ( doc . spans [ \"dates\" ]): print ( i , \" - \" , date , \" - \" , date . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = False , tz = None ), ) # Out: 0 - 12 avril - None # Out: 1 - il y a trois jours - 1999-08-24 00:00:00 # Out: 2 - l'ann\u00e9e derni\u00e8re - 1998-08-27 00:00:00 # Out: 3 - mai 1995 - None for i , date in enumerate ( doc . spans [ \"dates\" ]): print ( i , \" - \" , date , \" - \" , date . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = True , tz = None , default_day = 15 , ), ) # Out: 0 - 12 avril - 1999-04-12T00:00:00 # Out: 1 - il y a trois jours - 1999-08-24 00:00:00 # Out: 2 - l'ann\u00e9e derni\u00e8re - 1998-08-27 00:00:00 # Out: 3 - mai 1995 - 1995-05-15T00:00:00 As a first heuristic, let's consider that an entity can be linked to a date if the two are in the same sentence. In the case where multiple dates are present, we'll select the closest one. utils.py from spacy.tokens import Span from typing import List , Optional def candidate_dates ( ent : Span ) -> List [ Span ]: \"\"\"Return every dates in the same sentence as the entity\"\"\" return [ date for date in ent . doc . spans [ \"dates\" ] if date . sent == ent . sent ] def get_event_date ( ent : Span ) -> Optional [ Span ]: \"\"\"Link an entity to the closest date in the sentence, if any\"\"\" dates = candidate_dates ( ent ) # (1) if not dates : return dates = sorted ( dates , key = lambda d : min ( abs ( d . start - ent . end ), abs ( ent . start - d . end )), ) return dates [ 0 ] # (2) Get all dates present in the same sentence. Sort the dates, and keep the first item. We can apply this simple function: import spacy from utils import get_event_date nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.dates\" ) config = dict ( regex = dict ( admission = [ \"admissions?\" , \"admise?\" , \"prise? en charge\" ]), attr = \"LOWER\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) text = ( \"Le patient est admis le 12 avril pour une douleur \" \"survenue il y a trois jours. \" \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\" ) doc = nlp ( text ) for ent in doc . ents : date = get_event_date ( ent ) print ( f \" { ent . text : <20 }{ date . text : <20 }{ date . _ . date . to_datetime () } \" ) # Out: admis 12 avril 2020 2020-04-12T00:00:00+02:00 # Out: pris en charge l'ann\u00e9e derni\u00e8re -1 year Which will output: ent get_event_date(ent) get_event_date(ent)._.date.to_datetime() admis 12 avril 2020-04-12T00:00:00+02:00 pris en charge l'ann\u00e9e derni\u00e8re -1 year","title":"Detecting dates"},{"location":"tutorials/detecting-dates/#detecting-dates","text":"We now know how to match a terminology and qualify detected entities, which covers most use cases for a typical medical NLP project. In this tutorial, we'll see how to use EDS-NLP to detect and normalise date mentions using eds.dates . This can have many applications, for dating medical events in particular. The eds.consultation_dates component, for instance, combines the date detection capabilities with a few simple patterns to detect the date of the consultation, when mentioned in clinical reports.","title":"Detecting dates"},{"location":"tutorials/detecting-dates/#dates-in-clinical-notes","text":"Consider the following example: French English Le patient est admis le 21 janvier pour une douleur dans le cou. Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans. The patient is admitted on January 21st for a neck pain. He complains about chronique pain that started three years ago. Clinical notes contain many different types of dates. To name a few examples: Type Description Examples Absolute Explicit date 2022-03-03 Partial Date missing the day, month or year le 3 janvier/on January 3rd , en 2021/in 2021 Relative Relative dates hier/yesterday , le mois dernier/last month Duration Durations pendant trois mois/for three months Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.","title":"Dates in clinical notes"},{"location":"tutorials/detecting-dates/#extracting-dates","text":"The followings snippet adds the eds.date component to the pipeline: import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.dates\" ) # (1) text = ( \"Le patient est admis le 21 janvier pour une douleur dans le cou. \\n \" \"Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\" ) # Detecting dates becomes trivial doc = nlp ( text ) # Likewise, accessing detected dates is hassle-free dates = doc . spans [ \"dates\" ] # (2) The date detection component is declared with eds.dates Dates are saved in the doc . spans [ \"dates\" ] key After this, accessing dates and there normalisation becomes trivial: # \u2191 Omitted code above \u2191 dates # (1) # Out: [21 janvier, il y a trois ans] dates is a list of spaCy Span objects.","title":"Extracting dates"},{"location":"tutorials/detecting-dates/#normalisation","text":"We can review each date and get its normalisation: date.text date._.date 21 janvier { \"day\" : 21 , \"month\" : 1 } il y a trois ans { \"direction\" : \"past\" , \"year\" : 3 } Dates detected by the pipeline component are parsed into a dictionary-like object. It includes every information that is actually contained in the text. To get a more usable representation, you may call the to_datetime() method. If there's enough information, the date will be represented in a datetime.datetime or datetime.timedelta object. If some information is missing, It will return None . Alternatively for this case, you can optionally set to True the parameter infer_from_context and you may also give a value for note_datetime . Date normalisation Since dates can be missing some information (eg en ao\u00fbt ), we refrain from outputting a datetime object in that case. Doing so would amount to guessing, and we made the choice of letting you decide how you want to handle missing dates.","title":"Normalisation"},{"location":"tutorials/detecting-dates/#what-next","text":"The eds.dates pipeline component's role is merely to detect and normalise dates. It is the user's responsibility to use this information in a downstream application. For instance, you could use this pipeline to date medical entities. Let's do that.","title":"What next?"},{"location":"tutorials/detecting-dates/#a-medical-event-tagger","text":"Our pipeline will detect entities and events separately, and we will post-process the output Doc object to determine whether a given entity can be linked to a date. import spacy from datetime import datetime nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.dates\" ) config = dict ( regex = dict ( admission = [ \"admissions?\" , \"admise?\" , \"prise? en charge\" ]), attr = \"LOWER\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) text = ( \"Le patient est admis le 12 avril pour une douleur \" \"survenue il y a trois jours. \" \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re. \" \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\" ) doc = nlp ( text ) At this point, the document is ready to be post-processed: its ents and spans [ \"dates\" ] are populated: # \u2191 Omitted code above \u2191 doc . ents # Out: (admis, pris en charge) doc . spans [ \"dates\" ] # Out: [12 avril, il y a trois jours, l'ann\u00e9e derni\u00e8re, mai 1995] note_datetime = datetime ( year = 1999 , month = 8 , day = 27 ) for i , date in enumerate ( doc . spans [ \"dates\" ]): print ( i , \" - \" , date , \" - \" , date . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = False , tz = None ), ) # Out: 0 - 12 avril - None # Out: 1 - il y a trois jours - 1999-08-24 00:00:00 # Out: 2 - l'ann\u00e9e derni\u00e8re - 1998-08-27 00:00:00 # Out: 3 - mai 1995 - None for i , date in enumerate ( doc . spans [ \"dates\" ]): print ( i , \" - \" , date , \" - \" , date . _ . date . to_datetime ( note_datetime = note_datetime , infer_from_context = True , tz = None , default_day = 15 , ), ) # Out: 0 - 12 avril - 1999-04-12T00:00:00 # Out: 1 - il y a trois jours - 1999-08-24 00:00:00 # Out: 2 - l'ann\u00e9e derni\u00e8re - 1998-08-27 00:00:00 # Out: 3 - mai 1995 - 1995-05-15T00:00:00 As a first heuristic, let's consider that an entity can be linked to a date if the two are in the same sentence. In the case where multiple dates are present, we'll select the closest one. utils.py from spacy.tokens import Span from typing import List , Optional def candidate_dates ( ent : Span ) -> List [ Span ]: \"\"\"Return every dates in the same sentence as the entity\"\"\" return [ date for date in ent . doc . spans [ \"dates\" ] if date . sent == ent . sent ] def get_event_date ( ent : Span ) -> Optional [ Span ]: \"\"\"Link an entity to the closest date in the sentence, if any\"\"\" dates = candidate_dates ( ent ) # (1) if not dates : return dates = sorted ( dates , key = lambda d : min ( abs ( d . start - ent . end ), abs ( ent . start - d . end )), ) return dates [ 0 ] # (2) Get all dates present in the same sentence. Sort the dates, and keep the first item. We can apply this simple function: import spacy from utils import get_event_date nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.dates\" ) config = dict ( regex = dict ( admission = [ \"admissions?\" , \"admise?\" , \"prise? en charge\" ]), attr = \"LOWER\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) text = ( \"Le patient est admis le 12 avril pour une douleur \" \"survenue il y a trois jours. \" \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\" ) doc = nlp ( text ) for ent in doc . ents : date = get_event_date ( ent ) print ( f \" { ent . text : <20 }{ date . text : <20 }{ date . _ . date . to_datetime () } \" ) # Out: admis 12 avril 2020 2020-04-12T00:00:00+02:00 # Out: pris en charge l'ann\u00e9e derni\u00e8re -1 year Which will output: ent get_event_date(ent) get_event_date(ent)._.date.to_datetime() admis 12 avril 2020-04-12T00:00:00+02:00 pris en charge l'ann\u00e9e derni\u00e8re -1 year","title":"A medical event tagger"},{"location":"tutorials/endlines/","text":"Detecting end-of-lines A common problem in medical corpus is that the character \\n does not necessarily correspond to a real new line as in other domains. For example, it is common to find texts like: Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. Inserted new line characters This issue is especially impactful for clinical notes that have been extracted from PDF documents. In that case, the new line character could be deliberately inserted by the doctor, or more likely added to respect the layout during the edition of the PDF. The aim of this tutorial is to train a unsupervised model to detect this false endlines and to use it for inference. The implemented model is based on the work of Zweigenbaum et al 1 . Training the model Let's train the model using an example corpus of three documents: import spacy from edsnlp.pipelines.core.endlines import EndLinesModel nlp = spacy . blank ( \"fr\" ) text1 = \"\"\"Le patient est arriv\u00e9 hier soir. Il est accompagn\u00e9 par son fils ANTECEDENTS Il a fait une TS en 2010; Fumeur, il est arr\u00eat\u00e9 il a 5 mois Chirurgie de coeur en 2011 CONCLUSION Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. DIAGNOSTIC : Antecedents Familiaux: - 1. P\u00e8re avec diab\u00e8te \"\"\" text2 = \"\"\"J'aime le \\n fromage... \\n \"\"\" text3 = ( \"/n\" \"Intervention(s) - acte(s) r\u00e9alis\u00e9(s) :/n\" \"Parathyro\u00efdectomie \u00e9lective le [DATE]\" ) texts = [ text1 , text2 , text3 , ] corpus = nlp . pipe ( texts ) # Fit the model endlines = EndLinesModel ( nlp = nlp ) # (1) df = endlines . fit_and_predict ( corpus ) # (2) # Save model PATH = \"/tmp/path_to_model\" endlines . save ( PATH ) Initialize the EndLinesModel object and then fit (and predict) in the training corpus. The corpus should be an iterable of spacy documents. Use a trained model for inference import spacy nlp = spacy . blank ( \"fr\" ) PATH = \"/path_to_model\" nlp . add_pipe ( \"eds.endlines\" , config = dict ( model_path = PATH )) # (1) nlp . add_pipe ( \"eds.sentences\" ) # (1) docs = list ( nlp . pipe ([ text1 , text2 , text3 ])) doc = docs [ 1 ] doc # Out: J'aime le # Out: fromage... list ( doc . sents )[ 0 ] # Out: J'aime le # Out: fromage... You should specify the path to the trained model here. All fake new line are excluded by setting their tag to 'EXCLUDED' and all true new lines' tag are set to 'ENDLINE'. Declared extensions It lets downstream matchers skip excluded tokens (see normalisation ) for more detail. Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters) , 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7 . \u21a9","title":"Detecting end-of-lines"},{"location":"tutorials/endlines/#detecting-end-of-lines","text":"A common problem in medical corpus is that the character \\n does not necessarily correspond to a real new line as in other domains. For example, it is common to find texts like: Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. Inserted new line characters This issue is especially impactful for clinical notes that have been extracted from PDF documents. In that case, the new line character could be deliberately inserted by the doctor, or more likely added to respect the layout during the edition of the PDF. The aim of this tutorial is to train a unsupervised model to detect this false endlines and to use it for inference. The implemented model is based on the work of Zweigenbaum et al 1 .","title":"Detecting end-of-lines"},{"location":"tutorials/endlines/#training-the-model","text":"Let's train the model using an example corpus of three documents: import spacy from edsnlp.pipelines.core.endlines import EndLinesModel nlp = spacy . blank ( \"fr\" ) text1 = \"\"\"Le patient est arriv\u00e9 hier soir. Il est accompagn\u00e9 par son fils ANTECEDENTS Il a fait une TS en 2010; Fumeur, il est arr\u00eat\u00e9 il a 5 mois Chirurgie de coeur en 2011 CONCLUSION Il doit prendre le medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin dans 1 mois. DIAGNOSTIC : Antecedents Familiaux: - 1. P\u00e8re avec diab\u00e8te \"\"\" text2 = \"\"\"J'aime le \\n fromage... \\n \"\"\" text3 = ( \"/n\" \"Intervention(s) - acte(s) r\u00e9alis\u00e9(s) :/n\" \"Parathyro\u00efdectomie \u00e9lective le [DATE]\" ) texts = [ text1 , text2 , text3 , ] corpus = nlp . pipe ( texts ) # Fit the model endlines = EndLinesModel ( nlp = nlp ) # (1) df = endlines . fit_and_predict ( corpus ) # (2) # Save model PATH = \"/tmp/path_to_model\" endlines . save ( PATH ) Initialize the EndLinesModel object and then fit (and predict) in the training corpus. The corpus should be an iterable of spacy documents.","title":"Training the model"},{"location":"tutorials/endlines/#use-a-trained-model-for-inference","text":"import spacy nlp = spacy . blank ( \"fr\" ) PATH = \"/path_to_model\" nlp . add_pipe ( \"eds.endlines\" , config = dict ( model_path = PATH )) # (1) nlp . add_pipe ( \"eds.sentences\" ) # (1) docs = list ( nlp . pipe ([ text1 , text2 , text3 ])) doc = docs [ 1 ] doc # Out: J'aime le # Out: fromage... list ( doc . sents )[ 0 ] # Out: J'aime le # Out: fromage... You should specify the path to the trained model here. All fake new line are excluded by setting their tag to 'EXCLUDED' and all true new lines' tag are set to 'ENDLINE'.","title":"Use a trained model for inference"},{"location":"tutorials/endlines/#declared-extensions","text":"It lets downstream matchers skip excluded tokens (see normalisation ) for more detail. Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters) , 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7 . \u21a9","title":"Declared extensions"},{"location":"tutorials/matching-a-terminology/","text":"Matching a terminology Matching a terminology is perhaps the most basic application of a medical NLP pipeline. In this tutorial, we will cover : Matching a terminology using spaCy's matchers, as well as RegExps Matching on a specific attribute You should consider reading the matcher's specific documentation for a description. Comparison to spaCy's matcher spaCy's Matcher and PhraseMatcher use a very efficient algorithm that compare a hashed representation token by token. They are not components by themselves, but can underpin rule-based pipelines. EDS-NLP's RegexMatcher lets the user match entire expressions using regular expressions. To achieve this, the matcher has to get to the text representation, match on it, and get back to spaCy's abstraction. The EDSPhraseMatcher lets EDS-NLP reuse spaCy's efficient algorithm, while adding the ability to skip pollution tokens (see the normalisation documentation for detail) A simple use case : finding COVID19 Let's try to find mentions of COVID19 and references to patients within a clinical note. import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], respiratoire = [ \"asthmatique\" , \"respiratoire\" ], ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms )) doc = nlp ( text ) doc . ents # Out: (asthmatique,) Let's unpack what happened: We defined a dictionary of terms to look for, in the form {'label': list of terms} . We declared a spaCy pipeline, and add the eds.matcher component. We applied the pipeline to the texts... ... and explored the extracted entities. This example showcases a limitation of our term dictionary : the phrases COVID19 and difficult\u00e9s respiratoires were not detected by the pipeline. To increase recall, we could just add every possible variation : terms = dict( - covid=[\"coronavirus\", \"covid19\"], + covid=[\"coronavirus\", \"covid19\", \"COVID19\"], - respiratoire=[\"asthmatique\", \"respiratoire\"], + respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"], ) But what if we come across Coronavirus ? Surely we can do better! Matching on normalised text We can modify the matcher's configuration to match on other attributes instead of the verbatim input. You can refer to spaCy's list of available token attributes . Let's focus on two: The LOWER attribute, which lets you match on a lowercased version of the text. The NORM attribute, which adds some basic normalisation (eg \u0153 to oe ). EDS-NLP provides a eds.normalizer component that extends the level of cleaning on the NORM attribute. The LOWER attribute Matching on the lowercased version is extremely easy: import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], respiratoire = [ \"asthmatique\" , \"respiratoire\" , \"respiratoires\" ], ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , attr = \"LOWER\" , # (1) ), ) doc = nlp ( text ) doc . ents # Out: (COVID19, respiratoires, asthmatique) The matcher's attr parameter defines the attribute that the matcher will use. It is set to \"TEXT\" by default (ie verbatim text). This code is complete, and should run as is. Using the normalisation component EDS-NLP provides its own normalisation component, which modifies the NORM attribute in place. It handles: removal of accentuated characters; normalisation of quotes and apostrophes; lowercasing, which enabled by default in spaCy \u2013 EDS-NLP lets you disable it; removal of pollution. Pollution in clinical texts EDS-NLP is meant to be deployed on clinical reports extracted from hospitals information systems. As such, it is often riddled with extraction issues or administrative artifacts that \"pollute\" the report. As a core principle, EDS-NLP never modifies the input text , and nlp ( text ) . text == text is always true . However, we can tag some tokens as pollution elements, and avoid using them for matching the terminology. You can activate it like any other component. import spacy text = ( \"Motif de prise en charge : probable pneumopathie a ===== COVID19, \" # (1) \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" , \"pneumopathie \u00e0 covid19\" ], # (2) respiratoire = [ \"asthmatique\" , \"respiratoire\" , \"respiratoires\" ], ) nlp = spacy . blank ( \"fr\" ) # Add the normalisation component nlp . add_pipe ( \"eds.normalizer\" ) # (3) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , attr = \"NORM\" , # (4) ignore_excluded = True , # (5) ), ) doc = nlp ( text ) doc . ents # Out: (pneumopathie a ===== COVID19, respiratoires, asthmatique) We've modified the example to include a simple pollution. We've added pneumopathie \u00e0 covid19 to the list of synonyms detected by the pipeline. Note that in the synonym we provide, we kept the accentuated \u00e0 , whereas the example displays an unaccentuated a . The component can be configured. See the specific documentation for detail. The normalisation lives in the NORM attribute We can tell the matcher to ignore excluded tokens (tokens tagged as pollution by the normalisation component). This is not an obligation. Using the normalisation component, you can match on a normalised version of the text, as well as skip pollution tokens during the matching process . Using term matching with the normalisation If you use the term matcher with the normalisation, bear in mind that the examples go through the pipeline . That's how the matcher was able to recover pneumopathie a ===== COVID19 despite the fact that we used an accentuated \u00e0 in the terminology. The term matcher matches the input text to the provided terminology, using the selected attribute in both cases. The NORM attribute that corresponds to \u00e0 and a is the same: a . Preliminary conclusion We have matched all mentions! However, we had to spell out the singular and plural form of respiratoire ... And what if we wanted to detect covid 19 , or covid-19 ? Of course, we could write out every imaginable possibility, but this will quickly become tedious. Using regular expressions Let us redefine the pipeline once again, this time using regular expressions: import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , # (1) terms = terms , # (2) attr = \"LOWER\" , # (3) ), ) doc = nlp ( text ) doc . ents # Out: (COVID19, respiratoires, asthmatique) We can now match using regular expressions. We can mix and match patterns! Here we keep looking for patients using spaCy's term matching. RegExp matching is not limited to the verbatim text! You can choose to use one of spaCy's native attribute, ignore excluded tokens, etc. This code is complete, and should run as is. Using regular expressions can help define richer patterns using more compact queries. Visualising matched entities EDS-NLP is part of the spaCy ecosystem, which means we can benefit from spaCy helper functions. For instance, spaCy's visualiser displacy can let us visualise the matched entities: # \u2191 Omitted code above \u2191 from spacy import displacy colors = { \"covid\" : \"orange\" , \"respiratoire\" : \"steelblue\" , } options = { \"colors\" : colors , } displacy . render ( doc , style = \"ent\" , options = options ) If you run this within a notebook, you should get: Motif de prise en charge : probable pneumopathie a COVID19 covid , sans difficult\u00e9s respiratoires respiratoire Le p\u00e8re du patient est asthmatique respiratoire .","title":"Matching a terminology"},{"location":"tutorials/matching-a-terminology/#matching-a-terminology","text":"Matching a terminology is perhaps the most basic application of a medical NLP pipeline. In this tutorial, we will cover : Matching a terminology using spaCy's matchers, as well as RegExps Matching on a specific attribute You should consider reading the matcher's specific documentation for a description. Comparison to spaCy's matcher spaCy's Matcher and PhraseMatcher use a very efficient algorithm that compare a hashed representation token by token. They are not components by themselves, but can underpin rule-based pipelines. EDS-NLP's RegexMatcher lets the user match entire expressions using regular expressions. To achieve this, the matcher has to get to the text representation, match on it, and get back to spaCy's abstraction. The EDSPhraseMatcher lets EDS-NLP reuse spaCy's efficient algorithm, while adding the ability to skip pollution tokens (see the normalisation documentation for detail)","title":"Matching a terminology"},{"location":"tutorials/matching-a-terminology/#a-simple-use-case-finding-covid19","text":"Let's try to find mentions of COVID19 and references to patients within a clinical note. import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], respiratoire = [ \"asthmatique\" , \"respiratoire\" ], ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms )) doc = nlp ( text ) doc . ents # Out: (asthmatique,) Let's unpack what happened: We defined a dictionary of terms to look for, in the form {'label': list of terms} . We declared a spaCy pipeline, and add the eds.matcher component. We applied the pipeline to the texts... ... and explored the extracted entities. This example showcases a limitation of our term dictionary : the phrases COVID19 and difficult\u00e9s respiratoires were not detected by the pipeline. To increase recall, we could just add every possible variation : terms = dict( - covid=[\"coronavirus\", \"covid19\"], + covid=[\"coronavirus\", \"covid19\", \"COVID19\"], - respiratoire=[\"asthmatique\", \"respiratoire\"], + respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"], ) But what if we come across Coronavirus ? Surely we can do better!","title":"A simple use case : finding COVID19"},{"location":"tutorials/matching-a-terminology/#matching-on-normalised-text","text":"We can modify the matcher's configuration to match on other attributes instead of the verbatim input. You can refer to spaCy's list of available token attributes . Let's focus on two: The LOWER attribute, which lets you match on a lowercased version of the text. The NORM attribute, which adds some basic normalisation (eg \u0153 to oe ). EDS-NLP provides a eds.normalizer component that extends the level of cleaning on the NORM attribute.","title":"Matching on normalised text"},{"location":"tutorials/matching-a-terminology/#the-lower-attribute","text":"Matching on the lowercased version is extremely easy: import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" ], respiratoire = [ \"asthmatique\" , \"respiratoire\" , \"respiratoires\" ], ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , attr = \"LOWER\" , # (1) ), ) doc = nlp ( text ) doc . ents # Out: (COVID19, respiratoires, asthmatique) The matcher's attr parameter defines the attribute that the matcher will use. It is set to \"TEXT\" by default (ie verbatim text). This code is complete, and should run as is.","title":"The LOWER attribute"},{"location":"tutorials/matching-a-terminology/#using-the-normalisation-component","text":"EDS-NLP provides its own normalisation component, which modifies the NORM attribute in place. It handles: removal of accentuated characters; normalisation of quotes and apostrophes; lowercasing, which enabled by default in spaCy \u2013 EDS-NLP lets you disable it; removal of pollution. Pollution in clinical texts EDS-NLP is meant to be deployed on clinical reports extracted from hospitals information systems. As such, it is often riddled with extraction issues or administrative artifacts that \"pollute\" the report. As a core principle, EDS-NLP never modifies the input text , and nlp ( text ) . text == text is always true . However, we can tag some tokens as pollution elements, and avoid using them for matching the terminology. You can activate it like any other component. import spacy text = ( \"Motif de prise en charge : probable pneumopathie a ===== COVID19, \" # (1) \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) terms = dict ( covid = [ \"coronavirus\" , \"covid19\" , \"pneumopathie \u00e0 covid19\" ], # (2) respiratoire = [ \"asthmatique\" , \"respiratoire\" , \"respiratoires\" ], ) nlp = spacy . blank ( \"fr\" ) # Add the normalisation component nlp . add_pipe ( \"eds.normalizer\" ) # (3) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = terms , attr = \"NORM\" , # (4) ignore_excluded = True , # (5) ), ) doc = nlp ( text ) doc . ents # Out: (pneumopathie a ===== COVID19, respiratoires, asthmatique) We've modified the example to include a simple pollution. We've added pneumopathie \u00e0 covid19 to the list of synonyms detected by the pipeline. Note that in the synonym we provide, we kept the accentuated \u00e0 , whereas the example displays an unaccentuated a . The component can be configured. See the specific documentation for detail. The normalisation lives in the NORM attribute We can tell the matcher to ignore excluded tokens (tokens tagged as pollution by the normalisation component). This is not an obligation. Using the normalisation component, you can match on a normalised version of the text, as well as skip pollution tokens during the matching process . Using term matching with the normalisation If you use the term matcher with the normalisation, bear in mind that the examples go through the pipeline . That's how the matcher was able to recover pneumopathie a ===== COVID19 despite the fact that we used an accentuated \u00e0 in the terminology. The term matcher matches the input text to the provided terminology, using the selected attribute in both cases. The NORM attribute that corresponds to \u00e0 and a is the same: a .","title":"Using the normalisation component"},{"location":"tutorials/matching-a-terminology/#preliminary-conclusion","text":"We have matched all mentions! However, we had to spell out the singular and plural form of respiratoire ... And what if we wanted to detect covid 19 , or covid-19 ? Of course, we could write out every imaginable possibility, but this will quickly become tedious.","title":"Preliminary conclusion"},{"location":"tutorials/matching-a-terminology/#using-regular-expressions","text":"Let us redefine the pipeline once again, this time using regular expressions: import spacy text = ( \"Motif de prise en charge : probable pneumopathie a COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , # (1) terms = terms , # (2) attr = \"LOWER\" , # (3) ), ) doc = nlp ( text ) doc . ents # Out: (COVID19, respiratoires, asthmatique) We can now match using regular expressions. We can mix and match patterns! Here we keep looking for patients using spaCy's term matching. RegExp matching is not limited to the verbatim text! You can choose to use one of spaCy's native attribute, ignore excluded tokens, etc. This code is complete, and should run as is. Using regular expressions can help define richer patterns using more compact queries.","title":"Using regular expressions"},{"location":"tutorials/matching-a-terminology/#visualising-matched-entities","text":"EDS-NLP is part of the spaCy ecosystem, which means we can benefit from spaCy helper functions. For instance, spaCy's visualiser displacy can let us visualise the matched entities: # \u2191 Omitted code above \u2191 from spacy import displacy colors = { \"covid\" : \"orange\" , \"respiratoire\" : \"steelblue\" , } options = { \"colors\" : colors , } displacy . render ( doc , style = \"ent\" , options = options ) If you run this within a notebook, you should get: Motif de prise en charge : probable pneumopathie a COVID19 covid , sans difficult\u00e9s respiratoires respiratoire Le p\u00e8re du patient est asthmatique respiratoire .","title":"Visualising matched entities"},{"location":"tutorials/multiple-texts/","text":"Processing multiple texts In the previous tutorials, we've seen how to apply a spaCy NLP pipeline to a single text. Once the pipeline is tested and ready to be applied on an entire corpus, we'll want to deploy it efficiently. In this tutorial, we'll cover a few best practices and some caveats to avoid. Then, we'll explore methods that EDS-NLP provides to use a spaCy pipeline directly on a pandas or Spark DataFrame. These can drastically increase throughput. Consider this simple pipeline: Pipeline definition: pipeline.py import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) config = dict ( terms = dict ( patient = [ \"patient\" , \"malade\" ]), attr = \"NORM\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) # Add qualifiers nlp . add_pipe ( \"eds.negation\" ) nlp . add_pipe ( \"eds.hypothesis\" ) nlp . add_pipe ( \"eds.family\" ) # Add date detection nlp . add_pipe ( \"eds.dates\" ) Let's deploy it on a large number of documents. What about a for loop? Suppose we have a corpus of text: text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) corpus = [ text ] * 10000 # (1) This is admittedly ugly. But you get the idea, we have a corpus of 10 000 documents we want to process... You could just apply the pipeline document by document. A naive approach # \u2191 Omitted code above \u2191 docs = [ nlp ( text ) for text in corpus ] It turns out spaCy has a powerful parallelisation engine for an efficient processing of multiple texts. So the first step for writing more efficient spaCy code is to use nlp.pipe when processing multiple texts: - docs = [nlp(text) for text in corpus] + docs = list(nlp.pipe(corpus)) The nlp.pipe method takes an iterable as input, and outputs a generator of Doc object. Under the hood, texts are processed in batches, which is often much more efficient. Batch processing and EDS-NLP For now, EDS-NLP does not natively parallelise its components, so the gain from using nlp.pipe will not be that significant. Nevertheless, it's good practice to avoid using for loops when possible. Moreover, you will benefit from the batched tokenisation step. The way EDS-NLP is used may depend on how many documents you are working with. Once working with tens of thousands of them, parallelising the processing can be really efficient (up to 20x faster), but will require a (tiny) bit more work. Here are shown 4 ways to analyse texts depending on your needs A wrapper is available to simply switch between those use cases. Processing a pandas DataFrame Processing text within a pandas DataFrame is a very common use case. In many applications, you'll select a corpus of documents over a distributed cluster, load it in memory and process all texts. The OMOP CDM In every tutorial that mentions distributing EDS-NLP over a corpus of documents, we will expect the data to be organised using a flavour of the OMOP Common Data Model . The OMOP CDM defines two tables of interest to us: the note table contains the clinical notes the note_nlp table holds the results of a NLP pipeline applied to the note table. To make sure we can follow along, we propose three recipes for getting the DataFrame: using a dummy dataset like before, loading a CSV or by loading a Spark DataFrame into memory. Dummy example Loading data from a CSV Loading data from a Spark DataFrame import pandas as pd text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) corpus = [ text ] * 1000 data = pd . DataFrame ( dict ( note_text = corpus )) data [ \"note_id\" ] = range ( len ( data )) import pandas as pd data = pd . read_csv ( \"note.csv\" ) from pyspark.sql.session import SparkSession spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT * FROM note\" ) df = df . select ( \"note_id\" , \"note_text\" ) data = df . limit ( 1000 ) . toPandas () # (1) We limit the size of the DataFrame to make sure we do not overwhelm our machine. We'll see in what follows how we can efficiently deploy our pipeline on the data object. \"By hand\" We can deploy the pipeline using nlp.pipe directly, but we'll need some work to format the results in a usable way. Let's see how this might go, before using EDS-NLP's helper function to avoid the boilerplate code. processing.py from spacy.tokens import Doc from typing import Any , Dict , List def get_entities ( doc : Doc ) -> List [ Dict [ str , Any ]]: \"\"\"Return a list of dict representation for the entities\"\"\" entities = [] for ent in doc . ents : d = dict ( start = ent . start_char , end = ent . end_char , label = ent . label_ , lexical_variant = ent . text , negation = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , key = \"ents\" , ) entities . append ( d ) for date in doc . spans . get ( \"dates\" , []): d = dict ( start = date . start_char , end = date . end_char , label = date . _ . date , lexical_variant = date . text , key = \"dates\" , ) entities . append ( d ) return entities # \u2191 Omitted code above \u2191 from processing import get_entities import pandas as pd data [ \"doc\" ] = list ( nlp . pipe ( data . note_text )) # (1) data [ \"entities\" ] = data . doc . apply ( get_entities ) # (2) # \"Explode\" the dataframe data = data [[ \"note_id\" , \"entities\" ]] . explode ( \"entities\" ) data = data . dropna () data = data . reset_index ( drop = True ) data = data [[ \"note_id\" ]] . join ( pd . json_normalize ( data . entities )) We use spaCy's efficient nlp.pipe method This part is far from optimal, since it uses apply... But the computationally heavy part is in the previous line, since get_entities merely reads pre-computed values from the document. The result on the first note: note_id start end label lexical_variant negation hypothesis family key 0 0 7 patient Patient 0 0 0 ents 0 114 121 patient patient 0 0 1 ents 0 17 34 2021-09-25 25 septembre 2021 nan nan nan dates Using EDS-NLP's helper functions Let's see how we can efficiently deploy our pipeline using EDS-NLP's utility methods. They share the same arguments: Argument Description Default note A DataFrame, with two required columns, note_id and note_text Required nlp The pipeline object Required context A list of column names to add context to the generate Doc [] additional_spans Keys in doc.spans to include besides doc.ents [] extensions Custom extensions to use [] results_extractor An arbitrary callback function that turns a Doc into a list of dictionaries None (use extensions) Adding context You might want to store some context information contained in the note DataFrame as an extension in the generated Doc object. For instance, you may use the eds.dates pipeline in coordination with the note_datetime field to normalise a relative date (eg Le patient est venu il y a trois jours/The patient came three days ago ). In this case, you can use the context parameter and provide a list of column names you want to add: note_nlp = single_pipe ( data , nlp , context = [ \"note_datetime\" ], additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], ) In this example, the note_datetime field becomes available as doc._.note_datetime . Depending on your pipeline, you may want to extract other extensions. To do so, simply provide those extension names (without the leading underscore) to the extensions argument. This should cover most use-cases . In case you need more fine-grained control over how you want to process the results of your pipeline, you can provide an arbitrary results_extractor function. Said function is expected to take a spaCy Doc object as input, and return a list of dictionaries that will be used to construct the note_nlp table. For instance, the get_entities function defined earlier could be distributed directly: # \u2191 Omitted code above \u2191 from edsnlp.processing.simple import pipe as single_pipe from processing import get_entities note_nlp = single_pipe ( data , nlp , results_extractor = get_entities , ) A few caveats on using an arbitrary function Should you use multiprocessing, your arbitrary function needs to be serialisable as a pickle object in order to be distributed. That implies a few limitations on the way your function can be defined. Namely, your function needs to be discoverable (see the pickle documentation on the subject ). When deploying it should be defined such a way that can be accessed by the worker processes. For that reason, arbitrary functions can only be distributed via Spark/Koalas if their source code is advertised to the Spark workers . To that end, you should define your custom function in a pip-installed Python package. Single process EDS-NLP provides a single_pipe helper function that avoids the hassle we just went through in the previous section. Using it is trivial: # \u2191 Omitted code above \u2191 from edsnlp.processing.simple import pipe as single_pipe note_nlp = single_pipe ( data , nlp , additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], ) In just two Python statements, we get the exact same result as before! Multiple processes Depending on the size of your corpus, and if you have CPU cores to spare, you may want to distribute the computation. Again, EDS-NLP makes it extremely easy for you, through the parallel_pipe helper: # \u2191 Omitted code above \u2191 from edsnlp.processing.parallel import pipe as parallel_pipe note_nlp = parallel_pipe ( data , nlp , additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], n_jobs =- 2 , # (1) ) The n_jobs parameter controls the number of workers that you deploy in parallel. Negative inputs means \"all cores minus abs ( n_jobs + 1 ) \" Using a large number of workers and memory use In spaCy, even a rule-based pipeline is a memory intensive object. Be wary of using too many workers, lest you get a memory error. Depending on your machine, you should get a significant speed boost (we got 20x acceleration on a shared cluster using 62 cores). Deploying EDS-NLP on Spark/Koalas Should you need to deploy spaCy on a distributed DataFrame such as a Spark or a Koalas DataFrame, EDS-NLP has you covered. The procedure for those two types of DataFrame is virtually the same. Under the hood, EDS-NLP automatically deals with the necessary conversions. Suppose you have a Spark DataFrame: Using a dummy example Loading a pre-existing table Using a Koalas DataFrame from pyspark.sql.session import SparkSession from pyspark.sql import types as T spark = SparkSession . builder . getOrCreate () schema = T . StructType ( [ T . StructField ( \"note_id\" , T . IntegerType ()), T . StructField ( \"note_text\" , T . StringType ()), ] ) text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) data = [( i , text ) for i in range ( 1000 )] df = spark . createDataFrame ( data = data , schema = schema ) from pyspark.sql.session import SparkSession spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT * FROM note\" ) df = df . select ( \"note_id\" , \"note_text\" ) from pyspark.sql.session import SparkSession import databricks.koalas spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT note_id, note_text FROM note\" ) . to_koalas () Declaring types There is a minor twist, though: Spark (or Koalas) needs to know in advance the type of each extension you want to save. Thus, if you need additional extensions to be saved, you'll have to provide a dictionary to the extensions argument instead of a list of strings. This dictionary will have the name of the extension as keys and its PySpark type as value. Accepted types are the ones present in pyspark.sql.types . EDS-NLP provides a helper function, pyspark_type_finder , is available to get the correct type for most Python objects. You just need to provide an example of the type you wish to collect: int_type = pyspark_type_finder ( 1 ) # Out: IntegerType() Be careful when providing the example Do not blindly provide the first entity matched by your pipeline : it might be ill-suited. For instance, the Span._.date makes sense for a date span, but will be None if you use an entity... Deploying the pipeline Once again, using the helper is trivial: Spark Koalas # \u2191 Omitted code above \u2191 from edsnlp.processing.distributed import pipe as distributed_pipe note_nlp = distributed_pipe ( df , nlp , additional_spans = [ \"dates\" ], extensions = { \"date.year\" : int_type , \"date.month\" : int_type , \"date.day\" : int_type }, ) # Check that the pipeline was correctly distributed: note_nlp . show ( 5 ) # \u2191 Omitted code above \u2191 from edsnlp.processing.distributed import pipe as distributed_pipe note_nlp = distributed_pipe ( df , nlp , additional_spans = [ \"dates\" ], extensions = { \"date.year\" : int_type , \"date.month\" : int_type , \"date.day\" : int_type }, ) # Check that the pipeline was correctly distributed: note_nlp . head () Using Spark or Koalas, you can deploy EDS-NLP pipelines on tens of millions of documents with ease! One function to rule them all EDS-NLP provides a wrapper to simplify deployment even further: # \u2191 Omitted code above \u2191 from edsnlp.processing import pipe ### Small pandas DataFrame note_nlp = pipe ( note = df . limit ( 1000 ) . toPandas (), nlp = nlp , n_jobs = 1 , additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], ) ### Larger pandas DataFrame note_nlp = pipe ( note = df . limit ( 10000 ) . toPandas (), nlp = nlp , n_jobs =- 2 , additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], ) ### Huge Spark or Koalas DataFrame note_nlp = pipe ( note = df , nlp = nlp , how = \"spark\" , additional_spans = [ \"dates\" ], extensions = { \"date.year\" : int_type , \"date.month\" : int_type , \"date.day\" : int_type }, )","title":"Processing multiple texts"},{"location":"tutorials/multiple-texts/#processing-multiple-texts","text":"In the previous tutorials, we've seen how to apply a spaCy NLP pipeline to a single text. Once the pipeline is tested and ready to be applied on an entire corpus, we'll want to deploy it efficiently. In this tutorial, we'll cover a few best practices and some caveats to avoid. Then, we'll explore methods that EDS-NLP provides to use a spaCy pipeline directly on a pandas or Spark DataFrame. These can drastically increase throughput. Consider this simple pipeline: Pipeline definition: pipeline.py import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.normalizer\" ) config = dict ( terms = dict ( patient = [ \"patient\" , \"malade\" ]), attr = \"NORM\" , ) nlp . add_pipe ( \"eds.matcher\" , config = config ) # Add qualifiers nlp . add_pipe ( \"eds.negation\" ) nlp . add_pipe ( \"eds.hypothesis\" ) nlp . add_pipe ( \"eds.family\" ) # Add date detection nlp . add_pipe ( \"eds.dates\" ) Let's deploy it on a large number of documents.","title":"Processing multiple texts"},{"location":"tutorials/multiple-texts/#what-about-a-for-loop","text":"Suppose we have a corpus of text: text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) corpus = [ text ] * 10000 # (1) This is admittedly ugly. But you get the idea, we have a corpus of 10 000 documents we want to process... You could just apply the pipeline document by document. A naive approach # \u2191 Omitted code above \u2191 docs = [ nlp ( text ) for text in corpus ] It turns out spaCy has a powerful parallelisation engine for an efficient processing of multiple texts. So the first step for writing more efficient spaCy code is to use nlp.pipe when processing multiple texts: - docs = [nlp(text) for text in corpus] + docs = list(nlp.pipe(corpus)) The nlp.pipe method takes an iterable as input, and outputs a generator of Doc object. Under the hood, texts are processed in batches, which is often much more efficient. Batch processing and EDS-NLP For now, EDS-NLP does not natively parallelise its components, so the gain from using nlp.pipe will not be that significant. Nevertheless, it's good practice to avoid using for loops when possible. Moreover, you will benefit from the batched tokenisation step. The way EDS-NLP is used may depend on how many documents you are working with. Once working with tens of thousands of them, parallelising the processing can be really efficient (up to 20x faster), but will require a (tiny) bit more work. Here are shown 4 ways to analyse texts depending on your needs A wrapper is available to simply switch between those use cases.","title":"What about a for loop?"},{"location":"tutorials/multiple-texts/#processing-a-pandas-dataframe","text":"Processing text within a pandas DataFrame is a very common use case. In many applications, you'll select a corpus of documents over a distributed cluster, load it in memory and process all texts. The OMOP CDM In every tutorial that mentions distributing EDS-NLP over a corpus of documents, we will expect the data to be organised using a flavour of the OMOP Common Data Model . The OMOP CDM defines two tables of interest to us: the note table contains the clinical notes the note_nlp table holds the results of a NLP pipeline applied to the note table. To make sure we can follow along, we propose three recipes for getting the DataFrame: using a dummy dataset like before, loading a CSV or by loading a Spark DataFrame into memory. Dummy example Loading data from a CSV Loading data from a Spark DataFrame import pandas as pd text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) corpus = [ text ] * 1000 data = pd . DataFrame ( dict ( note_text = corpus )) data [ \"note_id\" ] = range ( len ( data )) import pandas as pd data = pd . read_csv ( \"note.csv\" ) from pyspark.sql.session import SparkSession spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT * FROM note\" ) df = df . select ( \"note_id\" , \"note_text\" ) data = df . limit ( 1000 ) . toPandas () # (1) We limit the size of the DataFrame to make sure we do not overwhelm our machine. We'll see in what follows how we can efficiently deploy our pipeline on the data object.","title":"Processing a pandas DataFrame"},{"location":"tutorials/multiple-texts/#by-hand","text":"We can deploy the pipeline using nlp.pipe directly, but we'll need some work to format the results in a usable way. Let's see how this might go, before using EDS-NLP's helper function to avoid the boilerplate code. processing.py from spacy.tokens import Doc from typing import Any , Dict , List def get_entities ( doc : Doc ) -> List [ Dict [ str , Any ]]: \"\"\"Return a list of dict representation for the entities\"\"\" entities = [] for ent in doc . ents : d = dict ( start = ent . start_char , end = ent . end_char , label = ent . label_ , lexical_variant = ent . text , negation = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , key = \"ents\" , ) entities . append ( d ) for date in doc . spans . get ( \"dates\" , []): d = dict ( start = date . start_char , end = date . end_char , label = date . _ . date , lexical_variant = date . text , key = \"dates\" , ) entities . append ( d ) return entities # \u2191 Omitted code above \u2191 from processing import get_entities import pandas as pd data [ \"doc\" ] = list ( nlp . pipe ( data . note_text )) # (1) data [ \"entities\" ] = data . doc . apply ( get_entities ) # (2) # \"Explode\" the dataframe data = data [[ \"note_id\" , \"entities\" ]] . explode ( \"entities\" ) data = data . dropna () data = data . reset_index ( drop = True ) data = data [[ \"note_id\" ]] . join ( pd . json_normalize ( data . entities )) We use spaCy's efficient nlp.pipe method This part is far from optimal, since it uses apply... But the computationally heavy part is in the previous line, since get_entities merely reads pre-computed values from the document. The result on the first note: note_id start end label lexical_variant negation hypothesis family key 0 0 7 patient Patient 0 0 0 ents 0 114 121 patient patient 0 0 1 ents 0 17 34 2021-09-25 25 septembre 2021 nan nan nan dates","title":"\"By hand\""},{"location":"tutorials/multiple-texts/#using-eds-nlps-helper-functions","text":"Let's see how we can efficiently deploy our pipeline using EDS-NLP's utility methods. They share the same arguments: Argument Description Default note A DataFrame, with two required columns, note_id and note_text Required nlp The pipeline object Required context A list of column names to add context to the generate Doc [] additional_spans Keys in doc.spans to include besides doc.ents [] extensions Custom extensions to use [] results_extractor An arbitrary callback function that turns a Doc into a list of dictionaries None (use extensions) Adding context You might want to store some context information contained in the note DataFrame as an extension in the generated Doc object. For instance, you may use the eds.dates pipeline in coordination with the note_datetime field to normalise a relative date (eg Le patient est venu il y a trois jours/The patient came three days ago ). In this case, you can use the context parameter and provide a list of column names you want to add: note_nlp = single_pipe ( data , nlp , context = [ \"note_datetime\" ], additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], ) In this example, the note_datetime field becomes available as doc._.note_datetime . Depending on your pipeline, you may want to extract other extensions. To do so, simply provide those extension names (without the leading underscore) to the extensions argument. This should cover most use-cases . In case you need more fine-grained control over how you want to process the results of your pipeline, you can provide an arbitrary results_extractor function. Said function is expected to take a spaCy Doc object as input, and return a list of dictionaries that will be used to construct the note_nlp table. For instance, the get_entities function defined earlier could be distributed directly: # \u2191 Omitted code above \u2191 from edsnlp.processing.simple import pipe as single_pipe from processing import get_entities note_nlp = single_pipe ( data , nlp , results_extractor = get_entities , ) A few caveats on using an arbitrary function Should you use multiprocessing, your arbitrary function needs to be serialisable as a pickle object in order to be distributed. That implies a few limitations on the way your function can be defined. Namely, your function needs to be discoverable (see the pickle documentation on the subject ). When deploying it should be defined such a way that can be accessed by the worker processes. For that reason, arbitrary functions can only be distributed via Spark/Koalas if their source code is advertised to the Spark workers . To that end, you should define your custom function in a pip-installed Python package.","title":"Using EDS-NLP's helper functions"},{"location":"tutorials/multiple-texts/#single-process","text":"EDS-NLP provides a single_pipe helper function that avoids the hassle we just went through in the previous section. Using it is trivial: # \u2191 Omitted code above \u2191 from edsnlp.processing.simple import pipe as single_pipe note_nlp = single_pipe ( data , nlp , additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], ) In just two Python statements, we get the exact same result as before!","title":"Single process"},{"location":"tutorials/multiple-texts/#multiple-processes","text":"Depending on the size of your corpus, and if you have CPU cores to spare, you may want to distribute the computation. Again, EDS-NLP makes it extremely easy for you, through the parallel_pipe helper: # \u2191 Omitted code above \u2191 from edsnlp.processing.parallel import pipe as parallel_pipe note_nlp = parallel_pipe ( data , nlp , additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], n_jobs =- 2 , # (1) ) The n_jobs parameter controls the number of workers that you deploy in parallel. Negative inputs means \"all cores minus abs ( n_jobs + 1 ) \" Using a large number of workers and memory use In spaCy, even a rule-based pipeline is a memory intensive object. Be wary of using too many workers, lest you get a memory error. Depending on your machine, you should get a significant speed boost (we got 20x acceleration on a shared cluster using 62 cores).","title":"Multiple processes"},{"location":"tutorials/multiple-texts/#deploying-eds-nlp-on-sparkkoalas","text":"Should you need to deploy spaCy on a distributed DataFrame such as a Spark or a Koalas DataFrame, EDS-NLP has you covered. The procedure for those two types of DataFrame is virtually the same. Under the hood, EDS-NLP automatically deals with the necessary conversions. Suppose you have a Spark DataFrame: Using a dummy example Loading a pre-existing table Using a Koalas DataFrame from pyspark.sql.session import SparkSession from pyspark.sql import types as T spark = SparkSession . builder . getOrCreate () schema = T . StructType ( [ T . StructField ( \"note_id\" , T . IntegerType ()), T . StructField ( \"note_text\" , T . StringType ()), ] ) text = ( \"Patient admis le 25 septembre 2021 pour suspicion de Covid. \\n \" \"Pas de cas de coronavirus dans ce service. \\n \" \"Le p\u00e8re du patient est atteint du covid.\" ) data = [( i , text ) for i in range ( 1000 )] df = spark . createDataFrame ( data = data , schema = schema ) from pyspark.sql.session import SparkSession spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT * FROM note\" ) df = df . select ( \"note_id\" , \"note_text\" ) from pyspark.sql.session import SparkSession import databricks.koalas spark = SparkSession . builder . getOrCreate () df = spark . sql ( \"SELECT note_id, note_text FROM note\" ) . to_koalas ()","title":"Deploying EDS-NLP on Spark/Koalas"},{"location":"tutorials/multiple-texts/#declaring-types","text":"There is a minor twist, though: Spark (or Koalas) needs to know in advance the type of each extension you want to save. Thus, if you need additional extensions to be saved, you'll have to provide a dictionary to the extensions argument instead of a list of strings. This dictionary will have the name of the extension as keys and its PySpark type as value. Accepted types are the ones present in pyspark.sql.types . EDS-NLP provides a helper function, pyspark_type_finder , is available to get the correct type for most Python objects. You just need to provide an example of the type you wish to collect: int_type = pyspark_type_finder ( 1 ) # Out: IntegerType() Be careful when providing the example Do not blindly provide the first entity matched by your pipeline : it might be ill-suited. For instance, the Span._.date makes sense for a date span, but will be None if you use an entity...","title":"Declaring types"},{"location":"tutorials/multiple-texts/#deploying-the-pipeline","text":"Once again, using the helper is trivial: Spark Koalas # \u2191 Omitted code above \u2191 from edsnlp.processing.distributed import pipe as distributed_pipe note_nlp = distributed_pipe ( df , nlp , additional_spans = [ \"dates\" ], extensions = { \"date.year\" : int_type , \"date.month\" : int_type , \"date.day\" : int_type }, ) # Check that the pipeline was correctly distributed: note_nlp . show ( 5 ) # \u2191 Omitted code above \u2191 from edsnlp.processing.distributed import pipe as distributed_pipe note_nlp = distributed_pipe ( df , nlp , additional_spans = [ \"dates\" ], extensions = { \"date.year\" : int_type , \"date.month\" : int_type , \"date.day\" : int_type }, ) # Check that the pipeline was correctly distributed: note_nlp . head () Using Spark or Koalas, you can deploy EDS-NLP pipelines on tens of millions of documents with ease!","title":"Deploying the pipeline"},{"location":"tutorials/multiple-texts/#one-function-to-rule-them-all","text":"EDS-NLP provides a wrapper to simplify deployment even further: # \u2191 Omitted code above \u2191 from edsnlp.processing import pipe ### Small pandas DataFrame note_nlp = pipe ( note = df . limit ( 1000 ) . toPandas (), nlp = nlp , n_jobs = 1 , additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], ) ### Larger pandas DataFrame note_nlp = pipe ( note = df . limit ( 10000 ) . toPandas (), nlp = nlp , n_jobs =- 2 , additional_spans = [ \"dates\" ], extensions = [ \"date.day\" , \"date.month\" , \"date.year\" ], ) ### Huge Spark or Koalas DataFrame note_nlp = pipe ( note = df , nlp = nlp , how = \"spark\" , additional_spans = [ \"dates\" ], extensions = { \"date.year\" : int_type , \"date.month\" : int_type , \"date.day\" : int_type }, )","title":"One function to rule them all"},{"location":"tutorials/qualifying-entities/","text":"Qualifying entities In the previous tutorial, we saw how to match a terminology on a text. Using the doc . ents attribute, we can check whether a document mentions a concept of interest to build a cohort or describe patients. The issue However, consider the classical example where we look for the diabetes concept: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. None of these expressions should be used to build a cohort: the detected entity is either negated, speculative, or does not concern the patient themself. That's why we need to qualify the matched entities . Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library. The solution We can use EDS-NLP's qualifier pipelines to achieve that. Let's add specific components to our pipeline to detect these three modalities. Adding qualifiers Adding qualifier pipelines is straightforward: import spacy text = ( \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , terms = terms , attr = \"LOWER\" , ), ) nlp . add_pipe ( \"eds.sentences\" ) # (1) nlp . add_pipe ( \"eds.negation\" ) # Negation component nlp . add_pipe ( \"eds.hypothesis\" ) # Speculation pipeline nlp . add_pipe ( \"eds.family\" ) # Family context detection Qualifiers pipelines need sentence boundaries to be set (see the specific documentation for detail). This code is complete, and should run as is. Reading the results Let's output the results as a pandas DataFrame for better readability: import spacy import pandas as pd text = ( \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , terms = terms , attr = \"LOWER\" , ), ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.negation\" ) # Negation component nlp . add_pipe ( \"eds.hypothesis\" ) # Speculation pipeline nlp . add_pipe ( \"eds.family\" ) # Family context detection doc = nlp ( text ) # Extraction as a pandas DataFrame entities = [] for ent in doc . ents : d = dict ( lexical_variant = ent . text , label = ent . label_ , negation = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , ) entities . append ( d ) df = pd . DataFrame . from_records ( entities ) This code is complete, and should run as is. We get the following result: lexical_variant label negation hypothesis family COVID19 covid False True False respiratoires respiratoire True False False asthmatique respiratoire False False True Conclusion The qualifier pipelines limits the number of false positives by detecting linguistic modulations such as negations or speculations. Go to the full documentation for a complete presentation of the different pipelines, their configuration options and validation performance. Recall the qualifier pipeline proposed by EDS-NLP: Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based history detection","title":"Qualifying entities"},{"location":"tutorials/qualifying-entities/#qualifying-entities","text":"In the previous tutorial, we saw how to match a terminology on a text. Using the doc . ents attribute, we can check whether a document mentions a concept of interest to build a cohort or describe patients.","title":"Qualifying entities"},{"location":"tutorials/qualifying-entities/#the-issue","text":"However, consider the classical example where we look for the diabetes concept: French English Le patient n'est pas diab\u00e9tique. Le patient est peut-\u00eatre diab\u00e9tique. Le p\u00e8re du patient est diab\u00e9tique. The patient is not diabetic. The patient could be diabetic. The patient's father is diabetic. None of these expressions should be used to build a cohort: the detected entity is either negated, speculative, or does not concern the patient themself. That's why we need to qualify the matched entities . Warning We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.","title":"The issue"},{"location":"tutorials/qualifying-entities/#the-solution","text":"We can use EDS-NLP's qualifier pipelines to achieve that. Let's add specific components to our pipeline to detect these three modalities.","title":"The solution"},{"location":"tutorials/qualifying-entities/#adding-qualifiers","text":"Adding qualifier pipelines is straightforward: import spacy text = ( \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , terms = terms , attr = \"LOWER\" , ), ) nlp . add_pipe ( \"eds.sentences\" ) # (1) nlp . add_pipe ( \"eds.negation\" ) # Negation component nlp . add_pipe ( \"eds.hypothesis\" ) # Speculation pipeline nlp . add_pipe ( \"eds.family\" ) # Family context detection Qualifiers pipelines need sentence boundaries to be set (see the specific documentation for detail). This code is complete, and should run as is.","title":"Adding qualifiers"},{"location":"tutorials/qualifying-entities/#reading-the-results","text":"Let's output the results as a pandas DataFrame for better readability: import spacy import pandas as pd text = ( \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \" \"sans difficult\u00e9s respiratoires \\n \" \"Le p\u00e8re du patient est asthmatique.\" ) regex = dict ( covid = r \"(coronavirus|covid[-\\s]?19)\" , respiratoire = r \"respiratoires?\" , ) terms = dict ( respiratoire = \"asthmatique\" ) nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( regex = regex , terms = terms , attr = \"LOWER\" , ), ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.negation\" ) # Negation component nlp . add_pipe ( \"eds.hypothesis\" ) # Speculation pipeline nlp . add_pipe ( \"eds.family\" ) # Family context detection doc = nlp ( text ) # Extraction as a pandas DataFrame entities = [] for ent in doc . ents : d = dict ( lexical_variant = ent . text , label = ent . label_ , negation = ent . _ . negation , hypothesis = ent . _ . hypothesis , family = ent . _ . family , ) entities . append ( d ) df = pd . DataFrame . from_records ( entities ) This code is complete, and should run as is. We get the following result: lexical_variant label negation hypothesis family COVID19 covid False True False respiratoires respiratoire True False False asthmatique respiratoire False False True","title":"Reading the results"},{"location":"tutorials/qualifying-entities/#conclusion","text":"The qualifier pipelines limits the number of false positives by detecting linguistic modulations such as negations or speculations. Go to the full documentation for a complete presentation of the different pipelines, their configuration options and validation performance. Recall the qualifier pipeline proposed by EDS-NLP: Pipeline Description eds.negation Rule-based negation detection eds.family Rule-based family context detection eds.hypothesis Rule-based speculation detection eds.reported_speech Rule-based reported speech detection eds.history Rule-based history detection","title":"Conclusion"},{"location":"tutorials/reason/","text":"Detecting Reason of Hospitalisation In this tutorial we will use the pipeline eds.reason to : Identify spans that corresponds to the reason of hospitalisation Check if there are named entities overlapping with my span of 'reason of hospitalisation' Check for all named entities if they are tagged is_reason import spacy text = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018 MOTIF D'HOSPITALISATION Monsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. ANT\u00c9C\u00c9DENTS Ant\u00e9c\u00e9dents m\u00e9dicaux : Premier \u00e9pisode d'asthme en mai 2018.\"\"\" nlp = spacy . blank ( \"fr\" ) # Extraction d'entit\u00e9s nomm\u00e9es nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( respiratoire = [ \"asthmatique\" , \"asthme\" , \"toux\" , ] ) ), ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) nlp . add_pipe ( \"eds.reason\" , config = dict ( use_sections = True )) doc = nlp ( text ) The pipeline reason will add a key of spans called reasons . We check the first item in this list. # \u2191 Omitted code above \u2191 reason = doc . spans [ \"reasons\" ][ 0 ] reason # Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. Naturally, all spans included the reasons key have the attribute reason . _ . is_reason == True . # \u2191 Omitted code above \u2191 reason . _ . is_reason # Out: True # \u2191 Omitted code above \u2191 entities = reason . _ . ents_reason # (1) for e in entities : print ( \"Entity:\" , e . text , \"-- Label:\" , e . label_ , \"-- is_reason:\" , e . _ . is_reason , ) # Out: Entity: asthme -- Label: respiratoire -- is_reason: True We check if the span include named entities, their labels and the attribute is_reason We can verify that named entities that do not overlap with the spans of reason, have their attribute reason . _ . is_reason == False : for e in doc . ents : print ( e . start , e , e . _ . is_reason ) # Out: 42 asthme True # Out: 54 asthme False","title":"Detecting Reason of Hospitalisation"},{"location":"tutorials/reason/#detecting-reason-of-hospitalisation","text":"In this tutorial we will use the pipeline eds.reason to : Identify spans that corresponds to the reason of hospitalisation Check if there are named entities overlapping with my span of 'reason of hospitalisation' Check for all named entities if they are tagged is_reason import spacy text = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018 MOTIF D'HOSPITALISATION Monsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. ANT\u00c9C\u00c9DENTS Ant\u00e9c\u00e9dents m\u00e9dicaux : Premier \u00e9pisode d'asthme en mai 2018.\"\"\" nlp = spacy . blank ( \"fr\" ) # Extraction d'entit\u00e9s nomm\u00e9es nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( respiratoire = [ \"asthmatique\" , \"asthme\" , \"toux\" , ] ) ), ) nlp . add_pipe ( \"eds.normalizer\" ) nlp . add_pipe ( \"eds.sections\" ) nlp . add_pipe ( \"eds.reason\" , config = dict ( use_sections = True )) doc = nlp ( text ) The pipeline reason will add a key of spans called reasons . We check the first item in this list. # \u2191 Omitted code above \u2191 reason = doc . spans [ \"reasons\" ][ 0 ] reason # Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme. Naturally, all spans included the reasons key have the attribute reason . _ . is_reason == True . # \u2191 Omitted code above \u2191 reason . _ . is_reason # Out: True # \u2191 Omitted code above \u2191 entities = reason . _ . ents_reason # (1) for e in entities : print ( \"Entity:\" , e . text , \"-- Label:\" , e . label_ , \"-- is_reason:\" , e . _ . is_reason , ) # Out: Entity: asthme -- Label: respiratoire -- is_reason: True We check if the span include named entities, their labels and the attribute is_reason We can verify that named entities that do not overlap with the spans of reason, have their attribute reason . _ . is_reason == False : for e in doc . ents : print ( e . start , e , e . _ . is_reason ) # Out: 42 asthme True # Out: 54 asthme False","title":"Detecting Reason of Hospitalisation"},{"location":"tutorials/spacy101/","text":"spaCy 101 EDS-NLP is a spaCy library. To use it, you will need to familiarise yourself with some key spaCy concepts. Skip if you're familiar with spaCy This page is intended as a crash course for the very basic spaCy concepts that are needed to use EDS-NLP. If you've already used spaCy, you should probably skip to the next page. In a nutshell, spaCy offers three things: a convenient abstraction with a language-dependent, rule-based, deterministic and non-destructive tokenizer a rich set of rule-based and trainable components a configuration and training system We will focus on the first item. Be sure to check out spaCy's crash course page for more information on the possibilities offered by the library. Resources The spaCy documentation is one of the great strengths of the library. In particular, you should check out the \"Advanced NLP with spaCy\" course , which provides a more in-depth presentation. spaCy in action Consider the following minimal example: import spacy # (1) # Initialise a spaCy pipeline nlp = spacy . blank ( \"fr\" ) # (2) text = \"Michel est un penseur lat\u00e9ral.\" # (3) # Apply the pipeline doc = nlp ( text ) # (4) doc . text # Out: 'Michel est un penseur lat\u00e9ral.' Import spaCy... Load a pipeline. In spaCy, the nlp object handles the entire processing. Define a text you want to process. Apply the pipeline and get a spaCy Doc object. We just created a spaCy pipeline and applied it to a sample text. It's that simple. Note that we use spaCy's \"blank\" NLP pipeline here. It actually carries a lot of information, and defines spaCy's language-dependent, rule-based tokenizer. Non-destructive processing In EDS-NLP, just like spaCy, non-destructiveness is a core principle. Your detected entities will always be linked to the original text . In other words, nlp ( text ) . text == text is always true. The Doc abstraction The doc object carries the result of the entire processing. It's the most important abstraction in spaCy, and holds a token-based representation of the text along with the results of every pipeline components. It also keeps track of the input text in a non-destructive manner, meaning that doc . text == text is always true. # \u2191 Omitted code above \u2191 # Text processing in spaCy is non-destructive doc . text == text # (1) # You can access a specific token token = doc [ 2 ] # (2) # And create a Span using slices span = doc [: 3 ] # (3) # Entities are tracked in the ents attribute doc . ents # (4) # Out: () This feature is a core principle in spaCy. It will always be true in EDS-NLP. token is a Token object referencing the third token span is a Span object referencing the first three tokens. We have not declared any entity recognizer in our pipeline, hence this attribute is empty. Adding pipeline components You can add pipeline components with the nlp . add_pipe method. Let's add two simple components to our pipeline. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # (1) nlp . add_pipe ( \"eds.dates\" ) # (2) text = \"Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.\" doc = nlp ( text ) Like the name suggests, this pipeline is declared by EDS-NLP. eds.sentences is a rule-based sentence boundary prediction. See its documentation for detail. Like the name suggests, this pipeline is declared by EDS-NLP. eds.dates is a date extraction and normalisation component. See its documentation for detail. The doc object just became more interesting! # \u2191 Omitted code above \u2191 # We can split the document into sentences list ( doc . sents ) # (1) # Out: [Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.] # And look for dates doc . spans [ \"dates\" ] # (2) # Out: [5 mai 2005] span = doc . spans [ \"dates\" ][ 0 ] # (3) span . _ . date . to_datetime () # (4) # Out: DateTime(2005, 5, 5, 0, 0, 0, tzinfo=Timezone('Europe/Paris')) In this example, there is only one sentence... The eds.dates adds a key to the doc.spans attribute span is a spaCy Span object. In spaCy, you can declare custom extensions that live in the _ attribute. Here, the eds.dates pipeline uses a Span._.date extension to persist the normalised date. We use the to_datetime() method to get an object that is usable by Python. Conclusion This page is just a glimpse of a few possibilities offered by spaCy. To get a sense of what spaCy can help you achieve, we strongly recommend you visit their documentation and take the time to follow the spaCy course . Moreover, be sure to check out spaCy's own crash course , which is an excellent read. It goes into more detail on what's possible with the library.","title":"spaCy 101"},{"location":"tutorials/spacy101/#spacy-101","text":"EDS-NLP is a spaCy library. To use it, you will need to familiarise yourself with some key spaCy concepts. Skip if you're familiar with spaCy This page is intended as a crash course for the very basic spaCy concepts that are needed to use EDS-NLP. If you've already used spaCy, you should probably skip to the next page. In a nutshell, spaCy offers three things: a convenient abstraction with a language-dependent, rule-based, deterministic and non-destructive tokenizer a rich set of rule-based and trainable components a configuration and training system We will focus on the first item. Be sure to check out spaCy's crash course page for more information on the possibilities offered by the library.","title":"spaCy 101"},{"location":"tutorials/spacy101/#resources","text":"The spaCy documentation is one of the great strengths of the library. In particular, you should check out the \"Advanced NLP with spaCy\" course , which provides a more in-depth presentation.","title":"Resources"},{"location":"tutorials/spacy101/#spacy-in-action","text":"Consider the following minimal example: import spacy # (1) # Initialise a spaCy pipeline nlp = spacy . blank ( \"fr\" ) # (2) text = \"Michel est un penseur lat\u00e9ral.\" # (3) # Apply the pipeline doc = nlp ( text ) # (4) doc . text # Out: 'Michel est un penseur lat\u00e9ral.' Import spaCy... Load a pipeline. In spaCy, the nlp object handles the entire processing. Define a text you want to process. Apply the pipeline and get a spaCy Doc object. We just created a spaCy pipeline and applied it to a sample text. It's that simple. Note that we use spaCy's \"blank\" NLP pipeline here. It actually carries a lot of information, and defines spaCy's language-dependent, rule-based tokenizer. Non-destructive processing In EDS-NLP, just like spaCy, non-destructiveness is a core principle. Your detected entities will always be linked to the original text . In other words, nlp ( text ) . text == text is always true.","title":"spaCy in action"},{"location":"tutorials/spacy101/#the-doc-abstraction","text":"The doc object carries the result of the entire processing. It's the most important abstraction in spaCy, and holds a token-based representation of the text along with the results of every pipeline components. It also keeps track of the input text in a non-destructive manner, meaning that doc . text == text is always true. # \u2191 Omitted code above \u2191 # Text processing in spaCy is non-destructive doc . text == text # (1) # You can access a specific token token = doc [ 2 ] # (2) # And create a Span using slices span = doc [: 3 ] # (3) # Entities are tracked in the ents attribute doc . ents # (4) # Out: () This feature is a core principle in spaCy. It will always be true in EDS-NLP. token is a Token object referencing the third token span is a Span object referencing the first three tokens. We have not declared any entity recognizer in our pipeline, hence this attribute is empty.","title":"The Doc abstraction"},{"location":"tutorials/spacy101/#adding-pipeline-components","text":"You can add pipeline components with the nlp . add_pipe method. Let's add two simple components to our pipeline. import spacy nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) # (1) nlp . add_pipe ( \"eds.dates\" ) # (2) text = \"Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.\" doc = nlp ( text ) Like the name suggests, this pipeline is declared by EDS-NLP. eds.sentences is a rule-based sentence boundary prediction. See its documentation for detail. Like the name suggests, this pipeline is declared by EDS-NLP. eds.dates is a date extraction and normalisation component. See its documentation for detail. The doc object just became more interesting! # \u2191 Omitted code above \u2191 # We can split the document into sentences list ( doc . sents ) # (1) # Out: [Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.] # And look for dates doc . spans [ \"dates\" ] # (2) # Out: [5 mai 2005] span = doc . spans [ \"dates\" ][ 0 ] # (3) span . _ . date . to_datetime () # (4) # Out: DateTime(2005, 5, 5, 0, 0, 0, tzinfo=Timezone('Europe/Paris')) In this example, there is only one sentence... The eds.dates adds a key to the doc.spans attribute span is a spaCy Span object. In spaCy, you can declare custom extensions that live in the _ attribute. Here, the eds.dates pipeline uses a Span._.date extension to persist the normalised date. We use the to_datetime() method to get an object that is usable by Python.","title":"Adding pipeline components"},{"location":"tutorials/spacy101/#conclusion","text":"This page is just a glimpse of a few possibilities offered by spaCy. To get a sense of what spaCy can help you achieve, we strongly recommend you visit their documentation and take the time to follow the spaCy course . Moreover, be sure to check out spaCy's own crash course , which is an excellent read. It goes into more detail on what's possible with the library.","title":"Conclusion"},{"location":"utilities/","text":"Utilities EDS-NLP provides a few utilities to deploy pipelines, process RegExps, etc.","title":"Utilities"},{"location":"utilities/#utilities","text":"EDS-NLP provides a few utilities to deploy pipelines, process RegExps, etc.","title":"Utilities"},{"location":"utilities/evaluation/","text":"Pipeline evaluation","title":"Pipeline evaluation"},{"location":"utilities/evaluation/#pipeline-evaluation","text":"","title":"Pipeline evaluation"},{"location":"utilities/matchers/","text":"Matchers We implemented three pattern matchers that are fit to clinical documents: the EDSPhraseMatcher the RegexMatcher the SimstringMatcher However, note that for most use-cases, you should instead use the eds.matcher pipeline that wraps these classes to annotate documents. EDSPhraseMatcher The EDSPhraseMatcher lets you efficiently match large terminology lists, by comparing tokenx against a given attribute. This matcher differs from the spacy.PhraseMatcher in that it allows to skip pollution tokens. To make it efficient, we have reimplemented the matching algorithm in Cython, like the original spacy.PhraseMatcher . You can use it as described in the code below. import spacy from edsnlp.matchers.phrase import EDSPhraseMatcher nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.normalizer\" ) doc = nlp ( \"On ne rel\u00e8ve pas de signe du Corona =============== virus.\" ) matcher = EDSPhraseMatcher ( nlp . vocab , attr = \"NORM\" ) matcher . build_patterns ( nlp , { \"covid\" : [ \"corona virus\" , \"coronavirus\" , \"covid\" ], \"diabete\" : [ \"diabete\" , \"diabetique\" ], }, ) list ( matcher ( doc , as_spans = True ))[ 0 ] . text # Out: Corona =============== virus RegexMatcher The RegexMatcher performs full-text regex matching. It is especially useful to handle spelling variations like mammo-?graphies? . Like the EDSPhraseMatcher , this class allows to skip pollution tokens. Note that this class is significantly slower than the EDSPhraseMatcher : if you can, try enumerating lexical variations of the target phrases and feed them to the PhraseMatcher instead. You can use it as described in the code below. import spacy from edsnlp.matchers.regex import RegexMatcher nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.normalizer\" ) doc = nlp ( \"On ne rel\u00e8ve pas de signe du Corona =============== virus.\" ) matcher = RegexMatcher ( attr = \"NORM\" , ignore_excluded = True ) matcher . build_patterns ( { \"covid\" : [ \"corona[ ]*virus\" , \"covid\" ], \"diabete\" : [ \"diabete\" , \"diabetique\" ], }, ) list ( matcher ( doc , as_spans = True ))[ 0 ] . text # Out: Corona =============== virus SimstringMatcher The SimstringMatcher performs fuzzy term matching by comparing spans of text with a similarity metric. It is especially useful to handle spelling variations like paracetomol (instead of paracetamol ). The simstring algorithm compares two strings by enumerating their char trigrams and measuring the overlap between the two sets. In the previous example: - paracetomol becomes ##p #pa par ara rac ace cet eto tom omo mol ol# l## - paracetamol becomes ##p #pa par ara rac ace cet eta tam amo mol ol# l## and the Dice (or F1) similarity between the two sets is 0.75. Like the EDSPhraseMatcher , this class allows to skip pollution tokens. Just like the RegexMatcher , this class is significantly slower than the EDSPhraseMatcher : if you can, try enumerating lexical variations of the target phrases and feed them to the PhraseMatcher instead. You can use it as described in the code below. import spacy from edsnlp.matchers.simstring import SimstringMatcher nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.normalizer\" ) doc = nlp ( \"On ne rel\u00e8ve pas de signe du corona-virus. Historique d'un hepatocellulaire carcinome.\" ) matcher = SimstringMatcher ( nlp . vocab , attr = \"NORM\" , ignore_excluded = True , measure = \"dice\" , threshold = 0.75 , windows = 5 , ) matcher . build_patterns ( nlp , { \"covid\" : [ \"coronavirus\" , \"covid\" ], \"carcinome\" : [ \"carcinome hepatocellulaire\" ], }, ) list ( matcher ( doc , as_spans = True ))[ 0 ] . text # Out: corona-virus list ( matcher ( doc , as_spans = True ))[ 1 ] . text # Out: hepatocellulaire carcinome","title":"Matchers"},{"location":"utilities/matchers/#matchers","text":"We implemented three pattern matchers that are fit to clinical documents: the EDSPhraseMatcher the RegexMatcher the SimstringMatcher However, note that for most use-cases, you should instead use the eds.matcher pipeline that wraps these classes to annotate documents.","title":"Matchers"},{"location":"utilities/matchers/#edsphrasematcher","text":"The EDSPhraseMatcher lets you efficiently match large terminology lists, by comparing tokenx against a given attribute. This matcher differs from the spacy.PhraseMatcher in that it allows to skip pollution tokens. To make it efficient, we have reimplemented the matching algorithm in Cython, like the original spacy.PhraseMatcher . You can use it as described in the code below. import spacy from edsnlp.matchers.phrase import EDSPhraseMatcher nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.normalizer\" ) doc = nlp ( \"On ne rel\u00e8ve pas de signe du Corona =============== virus.\" ) matcher = EDSPhraseMatcher ( nlp . vocab , attr = \"NORM\" ) matcher . build_patterns ( nlp , { \"covid\" : [ \"corona virus\" , \"coronavirus\" , \"covid\" ], \"diabete\" : [ \"diabete\" , \"diabetique\" ], }, ) list ( matcher ( doc , as_spans = True ))[ 0 ] . text # Out: Corona =============== virus","title":"EDSPhraseMatcher"},{"location":"utilities/matchers/#regexmatcher","text":"The RegexMatcher performs full-text regex matching. It is especially useful to handle spelling variations like mammo-?graphies? . Like the EDSPhraseMatcher , this class allows to skip pollution tokens. Note that this class is significantly slower than the EDSPhraseMatcher : if you can, try enumerating lexical variations of the target phrases and feed them to the PhraseMatcher instead. You can use it as described in the code below. import spacy from edsnlp.matchers.regex import RegexMatcher nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.normalizer\" ) doc = nlp ( \"On ne rel\u00e8ve pas de signe du Corona =============== virus.\" ) matcher = RegexMatcher ( attr = \"NORM\" , ignore_excluded = True ) matcher . build_patterns ( { \"covid\" : [ \"corona[ ]*virus\" , \"covid\" ], \"diabete\" : [ \"diabete\" , \"diabetique\" ], }, ) list ( matcher ( doc , as_spans = True ))[ 0 ] . text # Out: Corona =============== virus","title":"RegexMatcher"},{"location":"utilities/matchers/#simstringmatcher","text":"The SimstringMatcher performs fuzzy term matching by comparing spans of text with a similarity metric. It is especially useful to handle spelling variations like paracetomol (instead of paracetamol ). The simstring algorithm compares two strings by enumerating their char trigrams and measuring the overlap between the two sets. In the previous example: - paracetomol becomes ##p #pa par ara rac ace cet eto tom omo mol ol# l## - paracetamol becomes ##p #pa par ara rac ace cet eta tam amo mol ol# l## and the Dice (or F1) similarity between the two sets is 0.75. Like the EDSPhraseMatcher , this class allows to skip pollution tokens. Just like the RegexMatcher , this class is significantly slower than the EDSPhraseMatcher : if you can, try enumerating lexical variations of the target phrases and feed them to the PhraseMatcher instead. You can use it as described in the code below. import spacy from edsnlp.matchers.simstring import SimstringMatcher nlp = spacy . blank ( \"eds\" ) nlp . add_pipe ( \"eds.normalizer\" ) doc = nlp ( \"On ne rel\u00e8ve pas de signe du corona-virus. Historique d'un hepatocellulaire carcinome.\" ) matcher = SimstringMatcher ( nlp . vocab , attr = \"NORM\" , ignore_excluded = True , measure = \"dice\" , threshold = 0.75 , windows = 5 , ) matcher . build_patterns ( nlp , { \"covid\" : [ \"coronavirus\" , \"covid\" ], \"carcinome\" : [ \"carcinome hepatocellulaire\" ], }, ) list ( matcher ( doc , as_spans = True ))[ 0 ] . text # Out: corona-virus list ( matcher ( doc , as_spans = True ))[ 1 ] . text # Out: hepatocellulaire carcinome","title":"SimstringMatcher"},{"location":"utilities/regex/","text":"Work with RegExp","title":"Work with RegExp"},{"location":"utilities/regex/#work-with-regexp","text":"","title":"Work with RegExp"},{"location":"utilities/connectors/","text":"Overview of connectors EDS-NLP provides a series of connectors apt to convert back and forth from different formats into spaCy representation. We provide the following connectors: BRAT OMOP","title":"Overview of connectors"},{"location":"utilities/connectors/#overview-of-connectors","text":"EDS-NLP provides a series of connectors apt to convert back and forth from different formats into spaCy representation. We provide the following connectors: BRAT OMOP","title":"Overview of connectors"},{"location":"utilities/connectors/brat/","text":"BRAT Connector BRAT is currently the only supported in-text annotation editor at EDS. BRAT annotations are in the standoff format . Consider the following document: Le patient est admis pour une pneumopathie au coronavirus. On lui prescrit du parac\u00e9tamol. It could be annotated as follows : T1 Patient 4 11 patient T2 Disease 31 58 pneumopathie au coronavirus T3 Drug 79 90 parac\u00e9tamol The point of the BRAT connector is to go from the standoff annotation format to an annotated spaCy document : import spacy from edsnlp.connectors.brat import BratConnector # Instantiate the connector brat = BratConnector ( \"path/to/brat\" ) # Instantiate the spacy pipeline nlp = spacy . blank ( \"fr\" ) # Convert all BRAT files to a list of documents docs = brat . brat2docs ( nlp ) doc = docs [ 0 ] doc . ents # Out: [patient, pneumopathie au coronavirus, parac\u00e9tamol] doc . ents [ 0 ] . label_ # Out: Patient The connector can also go the other way around, enabling pre-annotations and an ersatz of active learning.","title":"BRAT Connector"},{"location":"utilities/connectors/brat/#brat-connector","text":"BRAT is currently the only supported in-text annotation editor at EDS. BRAT annotations are in the standoff format . Consider the following document: Le patient est admis pour une pneumopathie au coronavirus. On lui prescrit du parac\u00e9tamol. It could be annotated as follows : T1 Patient 4 11 patient T2 Disease 31 58 pneumopathie au coronavirus T3 Drug 79 90 parac\u00e9tamol The point of the BRAT connector is to go from the standoff annotation format to an annotated spaCy document : import spacy from edsnlp.connectors.brat import BratConnector # Instantiate the connector brat = BratConnector ( \"path/to/brat\" ) # Instantiate the spacy pipeline nlp = spacy . blank ( \"fr\" ) # Convert all BRAT files to a list of documents docs = brat . brat2docs ( nlp ) doc = docs [ 0 ] doc . ents # Out: [patient, pneumopathie au coronavirus, parac\u00e9tamol] doc . ents [ 0 ] . label_ # Out: Patient The connector can also go the other way around, enabling pre-annotations and an ersatz of active learning.","title":"BRAT Connector"},{"location":"utilities/connectors/labeltool/","text":"LabelTool Connector LabelTool is an in-house module enabling rapid annotation of pre-extracted entities. We provide a ready-to-use function that converts a list of annotated spaCy documents into a pandas DataFrame that is readable to LabelTool. import spacy from edsnlp.connectors.labeltool import docs2labeltool corpus = [ \"Ceci est un document m\u00e9dical.\" , \"Le patient n'est pas malade.\" , ] # Instantiate the spacy pipeline nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( medical = \"m\u00e9dical\" , malade = \"malade\" ))) nlp . add_pipe ( \"eds.negation\" ) # Convert all BRAT files to a list of documents docs = nlp . pipe ( corpus ) df = docs2labeltool ( docs , extensions = [ \"negation\" ]) The results: note_id note_text start end label lexical_variant negation 0 Ceci est un document m\u00e9dical. 21 28 medical m\u00e9dical False 1 Le patient n'est pas malade. 21 27 malade malade True","title":"LabelTool Connector"},{"location":"utilities/connectors/labeltool/#labeltool-connector","text":"LabelTool is an in-house module enabling rapid annotation of pre-extracted entities. We provide a ready-to-use function that converts a list of annotated spaCy documents into a pandas DataFrame that is readable to LabelTool. import spacy from edsnlp.connectors.labeltool import docs2labeltool corpus = [ \"Ceci est un document m\u00e9dical.\" , \"Le patient n'est pas malade.\" , ] # Instantiate the spacy pipeline nlp = spacy . blank ( \"fr\" ) nlp . add_pipe ( \"eds.sentences\" ) nlp . add_pipe ( \"eds.matcher\" , config = dict ( terms = dict ( medical = \"m\u00e9dical\" , malade = \"malade\" ))) nlp . add_pipe ( \"eds.negation\" ) # Convert all BRAT files to a list of documents docs = nlp . pipe ( corpus ) df = docs2labeltool ( docs , extensions = [ \"negation\" ]) The results: note_id note_text start end label lexical_variant negation 0 Ceci est un document m\u00e9dical. 21 28 medical m\u00e9dical False 1 Le patient n'est pas malade. 21 27 malade malade True","title":"LabelTool Connector"},{"location":"utilities/connectors/omop/","text":"OMOP Connector We provide a connector between OMOP-formatted dataframes and spaCy documents. OMOP-style dataframes Consider a corpus of just one document: Le patient est admis pour une pneumopathie au coronavirus. On lui prescrit du parac\u00e9tamol. And its OMOP-style representation, separated in two tables note and note_nlp (here with selected columns) : note : note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23 note_nlp : note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol Using the connector The following snippet expects the tables note and note_nlp to be already defined (eg through PySpark's toPandas() method). import spacy from edsnlp.connectors.omop import OmopConnector # Instantiate a spacy pipeline nlp = spacy . blank ( \"fr\" ) # Instantiate the connector connector = OmopConnector ( nlp ) # Convert OMOP tables (note and note_nlp) to a list of documents docs = connector . omop2docs ( note , note_nlp ) doc = docs [ 0 ] doc . ents # Out: [coronavirus, parac\u00e9tamol] doc . ents [ 0 ] . label_ # Out: 'disease' doc . text == note . loc [ 0 ] . note_text # Out: True The object docs now contains a list of documents that reflects the information contained in the OMOP-formatted dataframes.","title":"OMOP Connector"},{"location":"utilities/connectors/omop/#omop-connector","text":"We provide a connector between OMOP-formatted dataframes and spaCy documents.","title":"OMOP Connector"},{"location":"utilities/connectors/omop/#omop-style-dataframes","text":"Consider a corpus of just one document: Le patient est admis pour une pneumopathie au coronavirus. On lui prescrit du parac\u00e9tamol. And its OMOP-style representation, separated in two tables note and note_nlp (here with selected columns) : note : note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23 note_nlp : note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol","title":"OMOP-style dataframes"},{"location":"utilities/connectors/omop/#using-the-connector","text":"The following snippet expects the tables note and note_nlp to be already defined (eg through PySpark's toPandas() method). import spacy from edsnlp.connectors.omop import OmopConnector # Instantiate a spacy pipeline nlp = spacy . blank ( \"fr\" ) # Instantiate the connector connector = OmopConnector ( nlp ) # Convert OMOP tables (note and note_nlp) to a list of documents docs = connector . omop2docs ( note , note_nlp ) doc = docs [ 0 ] doc . ents # Out: [coronavirus, parac\u00e9tamol] doc . ents [ 0 ] . label_ # Out: 'disease' doc . text == note . loc [ 0 ] . note_text # Out: True The object docs now contains a list of documents that reflects the information contained in the OMOP-formatted dataframes.","title":"Using the connector"},{"location":"utilities/processing/","text":"Overview of processing","title":"Overview of processing"},{"location":"utilities/processing/#overview-of-processing","text":"","title":"Overview of processing"},{"location":"utilities/processing/multi/","text":"Multiprocessing","title":"Multiprocessing"},{"location":"utilities/processing/multi/#multiprocessing","text":"","title":"Multiprocessing"},{"location":"utilities/processing/single/","text":"Single processing","title":"Single processing"},{"location":"utilities/processing/single/#single-processing","text":"","title":"Single processing"},{"location":"utilities/processing/spark/","text":"Deploying on Spark We provide a simple connector to distribute a pipeline on a Spark cluster. We expose a Spark UDF (user-defined function) factory that handles the nitty gritty of distributing a pipeline over a cluster of Spark-enabled machines. Distributing a pipeline Because of the way Spark distributes Python objects, we need to re-declare custom extensions on the executors. To make this step as smooth as possible, EDS-NLP provides a BaseComponent class that implements a set_extensions method. When the pipeline is distributed, every component that extend BaseComponent rerun their set_extensions method. Since spaCy Doc objects cannot easily be serialised, the UDF we provide returns a list of detected entities along with selected qualifiers. Example See the dedicated tutorial for a step-by-step presentation. Authors and citation The Spark connector was developed by AP-HP's Data Science team.","title":"Deploying on Spark"},{"location":"utilities/processing/spark/#deploying-on-spark","text":"We provide a simple connector to distribute a pipeline on a Spark cluster. We expose a Spark UDF (user-defined function) factory that handles the nitty gritty of distributing a pipeline over a cluster of Spark-enabled machines.","title":"Deploying on Spark"},{"location":"utilities/processing/spark/#distributing-a-pipeline","text":"Because of the way Spark distributes Python objects, we need to re-declare custom extensions on the executors. To make this step as smooth as possible, EDS-NLP provides a BaseComponent class that implements a set_extensions method. When the pipeline is distributed, every component that extend BaseComponent rerun their set_extensions method. Since spaCy Doc objects cannot easily be serialised, the UDF we provide returns a list of detected entities along with selected qualifiers.","title":"Distributing a pipeline"},{"location":"utilities/processing/spark/#example","text":"See the dedicated tutorial for a step-by-step presentation.","title":"Example"},{"location":"utilities/processing/spark/#authors-and-citation","text":"The Spark connector was developed by AP-HP's Data Science team.","title":"Authors and citation"},{"location":"utilities/tests/","text":"Tests Utilities We provide a few testing utilities that simplify the process of: creating testing examples for NLP pipelines; testing documentation code blocs.","title":"Tests Utilities"},{"location":"utilities/tests/#tests-utilities","text":"We provide a few testing utilities that simplify the process of: creating testing examples for NLP pipelines; testing documentation code blocs.","title":"Tests Utilities"},{"location":"utilities/tests/blocs/","text":"Testing Code Blocs We created a utility that scans through markdown files, extracts code blocs and executes them to check that everything is indeed functional. There is more! Whenever the utility comes across an example (denoted by # Out: , see example below), an assert statement is dynamically added to the snippet to check that the output matches. For instance: a = 1 a # Out: 1 Is transformed into: a = 1 v = a assert repr ( v ) == \"1\" We can disable code checking for a specific code bloc by adding <!-- no-check --> above it: <!-- no-check --> ```python test = undeclared_function ( 42 ) ``` See the dedicated reference for more information","title":"Testing Code Blocs"},{"location":"utilities/tests/blocs/#testing-code-blocs","text":"We created a utility that scans through markdown files, extracts code blocs and executes them to check that everything is indeed functional. There is more! Whenever the utility comes across an example (denoted by # Out: , see example below), an assert statement is dynamically added to the snippet to check that the output matches. For instance: a = 1 a # Out: 1 Is transformed into: a = 1 v = a assert repr ( v ) == \"1\" We can disable code checking for a specific code bloc by adding <!-- no-check --> above it: <!-- no-check --> ```python test = undeclared_function ( 42 ) ``` See the dedicated reference for more information","title":"Testing Code Blocs"},{"location":"utilities/tests/examples/","text":"Creating Examples Testing a NER/qualifier pipeline can be a hassle. We created a utility to simplify that process. Using the parse_example method, you can define a full example in a human-readable way: from edsnlp.utils.examples import parse_example example = \"Absence d'<ent negated=true>image osseuse d'allure \u00e9volutive</ent>.\" text , entities = parse_example ( example ) text # Out: \"Absence d'image osseuse d'allure \u00e9volutive.\" entities # Out: [Entity(start_char=10, end_char=42, modifiers=[Modifier(key='negated', value=True)])] Entities are defined using the <ent> tag. You can encode complexe information by adding keys into the tag (see example above). The parse_example method strips the text of the tags, and outputs a list of Entity objects that contain: the character indices of the entity ; custom user-defined \"modifiers\". See the dedicated reference page for more information.","title":"Creating Examples"},{"location":"utilities/tests/examples/#creating-examples","text":"Testing a NER/qualifier pipeline can be a hassle. We created a utility to simplify that process. Using the parse_example method, you can define a full example in a human-readable way: from edsnlp.utils.examples import parse_example example = \"Absence d'<ent negated=true>image osseuse d'allure \u00e9volutive</ent>.\" text , entities = parse_example ( example ) text # Out: \"Absence d'image osseuse d'allure \u00e9volutive.\" entities # Out: [Entity(start_char=10, end_char=42, modifiers=[Modifier(key='negated', value=True)])] Entities are defined using the <ent> tag. You can encode complexe information by adding keys into the tag (see example above). The parse_example method strips the text of the tags, and outputs a list of Entity objects that contain: the character indices of the entity ; custom user-defined \"modifiers\". See the dedicated reference page for more information.","title":"Creating Examples"}]}